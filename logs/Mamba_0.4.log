>>> 'Pkg.add("Mamba")' log
INFO: Cloning cache of Cairo from git://github.com/JuliaGraphics/Cairo.jl.git
INFO: Cloning cache of Graphs from git://github.com/JuliaArchive/Graphs.jl.git
INFO: Cloning cache of Mamba from git://github.com/brian-j-smith/Mamba.jl.git
INFO: Installing AxisAlgorithms v0.1.6
INFO: Installing BinDeps v0.4.7
INFO: Installing Cairo v0.2.35
INFO: Installing Calculus v0.2.2
INFO: Installing ColorTypes v0.2.12
INFO: Installing Colors v0.6.9
INFO: Installing Compose v0.4.5
INFO: Installing Contour v0.2.0
INFO: Installing DataArrays v0.3.12
INFO: Installing DataFrames v0.8.5
INFO: Installing DataStructures v0.5.3
INFO: Installing DiffBase v0.0.5
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.1
INFO: Installing DualNumbers v0.3.0
INFO: Installing FileIO v0.2.2
INFO: Installing FixedPointNumbers v0.2.1
INFO: Installing FixedSizeArrays v0.2.5
INFO: Installing ForwardDiff v0.3.5
INFO: Installing GZip v0.2.20
INFO: Installing Gadfly v0.5.3
INFO: Installing Graphics v0.1.4
INFO: Installing Graphs v0.7.1
INFO: Installing Hexagons v0.0.4
INFO: Installing Hiccup v0.0.3
INFO: Installing Interpolations v0.3.8
INFO: Installing Iterators v0.3.0
INFO: Installing Juno v0.2.7
INFO: Installing KernelDensity v0.3.2
INFO: Installing Lazy v0.11.5
INFO: Installing LineSearches v0.1.5
INFO: Installing Loess v0.1.0
INFO: Installing MacroTools v0.3.6
INFO: Installing Mamba v0.9.2
INFO: Installing Measures v0.0.3
INFO: Installing Media v0.2.5
INFO: Installing NaNMath v0.2.2
INFO: Installing Optim v0.7.8
INFO: Installing PDMats v0.5.6
INFO: Installing PositiveFactorizations v0.0.4
INFO: Installing Ratios v0.0.4
INFO: Installing Reexport v0.0.3
INFO: Installing Rmath v0.1.6
INFO: Installing SHA v0.3.2
INFO: Installing Showoff v0.0.7
INFO: Installing SortingAlgorithms v0.1.1
INFO: Installing StatsBase v0.12.0
INFO: Installing StatsFuns v0.4.0
INFO: Installing URIParser v0.1.8
INFO: Installing WoodburyMatrices v0.2.2
INFO: Building Cairo
INFO: Building Rmath
INFO: Package database updated

>>> 'Pkg.test("Mamba")' log
Julia Version 0.4.7
Commit ae26b25 (2016-09-18 16:17 UTC)
Platform Info:
  System: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-113-generic #160-Ubuntu SMP Thu Mar 9 09:27:29 UTC 2017 x86_64 x86_64
Memory: 2.9392738342285156 GB (626.03125 MB free)
Uptime: 2573.0 sec
Load Avg:  1.03564453125  1.00341796875  0.7412109375
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz      59326 s         58 s       5148 s     174767 s          4 s
#2  3499 MHz      36286 s         80 s       4393 s     211862 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.4
2 required packages:
 - JSON                          0.8.3
 - Mamba                         0.9.2
50 additional packages:
 - AxisAlgorithms                0.1.6
 - BinDeps                       0.4.7
 - Cairo                         0.2.35
 - Calculus                      0.2.2
 - ColorTypes                    0.2.12
 - Colors                        0.6.9
 - Compat                        0.21.0
 - Compose                       0.4.5
 - Contour                       0.2.0
 - DataArrays                    0.3.12
 - DataFrames                    0.8.5
 - DataStructures                0.5.3
 - DiffBase                      0.0.5
 - Distances                     0.3.2
 - Distributions                 0.11.1
 - DualNumbers                   0.3.0
 - FileIO                        0.2.2
 - FixedPointNumbers             0.2.1
 - FixedSizeArrays               0.2.5
 - ForwardDiff                   0.3.5
 - GZip                          0.2.20
 - Gadfly                        0.5.3
 - Graphics                      0.1.4
 - Graphs                        0.7.1
 - Hexagons                      0.0.4
 - Hiccup                        0.0.3
 - Interpolations                0.3.8
 - Iterators                     0.3.0
 - Juno                          0.2.7
 - KernelDensity                 0.3.2
 - Lazy                          0.11.5
 - LineSearches                  0.1.5
 - Loess                         0.1.0
 - MacroTools                    0.3.6
 - Measures                      0.0.3
 - Media                         0.2.5
 - NaNMath                       0.2.2
 - Optim                         0.7.8
 - PDMats                        0.5.6
 - PositiveFactorizations        0.0.4
 - Ratios                        0.0.4
 - Reexport                      0.0.3
 - Rmath                         0.1.6
 - SHA                           0.3.2
 - Showoff                       0.0.7
 - SortingAlgorithms             0.1.1
 - StatsBase                     0.12.0
 - StatsFuns                     0.4.0
 - URIParser                     0.1.8
 - WoodburyMatrices              0.2.2
INFO: Testing Mamba
Running tests:

>>> Testing ../doc/tutorial/line.jl

WARNING: New definition 
    write(Base.IO, ForwardDiff.Partials) at /home/vagrant/.julia/v0.4/ForwardDiff/src/partials.jl:57
is ambiguous with: 
    write(Base.Base64.Base64EncodePipe, AbstractArray{UInt8, 1}) at base64.jl:89.
To fix, define 
    write(Base.Base64.Base64EncodePipe, ForwardDiff.Partials{N<:Any, UInt8})
before the new definition.
WARNING: New definition 
    any(Any, Lazy.List) at /home/vagrant/.julia/v0.4/Lazy/src/liblazy.jl:184
is ambiguous with: 
    any(Base.Predicate, Any) at reduce.jl:362.
To fix, define 
    any(Base.Predicate, Lazy.List)
before the new definition.
WARNING: New definition 
    any(Any, Lazy.List) at /home/vagrant/.julia/v0.4/Lazy/src/liblazy.jl:184
is ambiguous with: 
    any(Base.IdFun, Any) at reduce.jl:363.
To fix, define 
    any(Base.IdFun, Lazy.List)
before the new definition.
WARNING: New definition 
    any(Any, Lazy.List) at /home/vagrant/.julia/v0.4/Lazy/src/liblazy.jl:184
is ambiguous with: 
    any(AbstractArray, Any) at reducedim.jl:264.
To fix, define 
    any(AbstractArray, Lazy.List)
before the new definition.
WARNING: New definition 
    all(Any, Lazy.List) at /home/vagrant/.julia/v0.4/Lazy/src/liblazy.jl:187
is ambiguous with: 
    all(Base.Predicate, Any) at reduce.jl:369.
To fix, define 
    all(Base.Predicate, Lazy.List)
before the new definition.
WARNING: New definition 
    all(Any, Lazy.List) at /home/vagrant/.julia/v0.4/Lazy/src/liblazy.jl:187
is ambiguous with: 
    all(Base.IdFun, Any) at reduce.jl:370.
To fix, define 
    all(Base.IdFun, Lazy.List)
before the new definition.
WARNING: New definition 
    all(Any, Lazy.List) at /home/vagrant/.julia/v0.4/Lazy/src/liblazy.jl:187
is ambiguous with: 
    all(AbstractArray, Any) at reducedim.jl:264.
To fix, define 
    all(AbstractArray, Lazy.List)
before the new definition.
WARNING: New definition 
    +(AbstractArray, DataArrays.DataArray) at /home/vagrant/.julia/v0.4/DataArrays/src/operators.jl:276
is ambiguous with: 
    +(WoodburyMatrices.SymWoodbury, AbstractArray{T<:Any, 2}) at /home/vagrant/.julia/v0.4/WoodburyMatrices/src/SymWoodburyMatrices.jl:139.
To fix, define 
    +(WoodburyMatrices.SymWoodbury, DataArrays.DataArray{T<:Any, 2})
before the new definition.
WARNING: New definition 
    +(AbstractArray, DataArrays.AbstractDataArray) at /home/vagrant/.julia/v0.4/DataArrays/src/operators.jl:300
is ambiguous with: 
    +(WoodburyMatrices.SymWoodbury, AbstractArray{T<:Any, 2}) at /home/vagrant/.julia/v0.4/WoodburyMatrices/src/SymWoodburyMatrices.jl:139.
To fix, define 
    +(WoodburyMatrices.SymWoodbury, DataArrays.AbstractDataArray{T<:Any, 2})
before the new definition.
WARNING: Base.String is deprecated, use AbstractString instead.
  likely near /home/vagrant/.julia/v0.4/Graphs/src/common.jl:3
WARNING: Base.String is deprecated, use AbstractString instead.
  likely near /home/vagrant/.julia/v0.4/Graphs/src/dot.jl:80
WARNING: New definition 
    promote_rule(Type{Mamba.ScalarLogical}, Type{##268#T<:Real}) at /home/vagrant/.julia/v0.4/Mamba/src/variate.jl:20
is ambiguous with: 
    promote_rule(Type{#A<:Real}, Type{ForwardDiff.Dual{#N<:Any, #B<:Real}}) at /home/vagrant/.julia/v0.4/ForwardDiff/src/dual.jl:191.
To fix, define 
    promote_rule(Type{Mamba.ScalarLogical}, Type{ForwardDiff.Dual{#N<:Any, #B<:Real}})
before the new definition.
WARNING: New definition 
    promote_rule(Type{Mamba.ScalarStochastic}, Type{##271#T<:Real}) at /home/vagrant/.julia/v0.4/Mamba/src/variate.jl:20
is ambiguous with: 
    promote_rule(Type{#A<:Real}, Type{ForwardDiff.Dual{#N<:Any, #B<:Real}}) at /home/vagrant/.julia/v0.4/ForwardDiff/src/dual.jl:191.
To fix, define 
    promote_rule(Type{Mamba.ScalarStochastic}, Type{ForwardDiff.Dual{#N<:Any, #B<:Real}})
before the new definition.
digraph MambaModel {
	"y" [shape="ellipse", fillcolor="gray85", style="filled"];
	"mu" [shape="diamond", fillcolor="gray85", style="filled"];
		"mu" -> "y";
	"s2" [shape="ellipse"];
		"s2" -> "y";
	"beta" [shape="ellipse"];
		"beta" -> "mu";
	"xmat" [shape="box", fillcolor="gray85", style="filled"];
		"xmat" -> "mu";
}
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:46:38 of 0:46:41 remaining]
Chain 1:  10% [0:00:34 of 0:00:37 remaining]
Chain 1:  20% [0:00:19 of 0:00:23 remaining]
Chain 1:  30% [0:00:13 of 0:00:19 remaining]
Chain 1:  40% [0:00:09 of 0:00:16 remaining]
Chain 1:  50% [0:00:07 of 0:00:14 remaining]
Chain 1:  60% [0:00:05 of 0:00:13 remaining]
Chain 1:  70% [0:00:04 of 0:00:13 remaining]
Chain 1:  80% [0:00:02 of 0:00:12 remaining]
Chain 1:  90% [0:00:01 of 0:00:12 remaining]
Chain 1: 100% [0:00:00 of 0:00:12 remaining]

Chain 2:   0% [0:00:13 of 0:00:13 remaining]
Chain 2:  10% [0:00:07 of 0:00:08 remaining]
Chain 2:  20% [0:00:06 of 0:00:07 remaining]
Chain 2:  30% [0:00:05 of 0:00:07 remaining]
Chain 2:  40% [0:00:04 of 0:00:07 remaining]
Chain 2:  50% [0:00:03 of 0:00:06 remaining]
Chain 2:  60% [0:00:02 of 0:00:06 remaining]
Chain 2:  70% [0:00:02 of 0:00:06 remaining]
Chain 2:  80% [0:00:01 of 0:00:06 remaining]
Chain 2:  90% [0:00:01 of 0:00:06 remaining]
Chain 2: 100% [0:00:00 of 0:00:06 remaining]

Chain 3:   0% [0:00:05 of 0:00:05 remaining]
Chain 3:  10% [0:00:05 of 0:00:05 remaining]
Chain 3:  20% [0:00:04 of 0:00:05 remaining]
Chain 3:  30% [0:00:03 of 0:00:05 remaining]
Chain 3:  40% [0:00:03 of 0:00:05 remaining]
Chain 3:  50% [0:00:03 of 0:00:05 remaining]
Chain 3:  60% [0:00:02 of 0:00:05 remaining]
Chain 3:  70% [0:00:02 of 0:00:05 remaining]
Chain 3:  80% [0:00:01 of 0:00:05 remaining]
Chain 3:  90% [0:00:01 of 0:00:05 remaining]
Chain 3: 100% [0:00:00 of 0:00:06 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:01:16 of 0:01:16 remaining]
Chain 1:  10% [0:00:12 of 0:00:13 remaining]
Chain 1:  20% [0:00:10 of 0:00:13 remaining]
Chain 1:  30% [0:00:09 of 0:00:13 remaining]
Chain 1:  40% [0:00:07 of 0:00:12 remaining]
Chain 1:  50% [0:00:06 of 0:00:12 remaining]
Chain 1:  60% [0:00:05 of 0:00:12 remaining]
Chain 1:  70% [0:00:03 of 0:00:11 remaining]
Chain 1:  80% [0:00:02 of 0:00:11 remaining]
Chain 1:  90% [0:00:01 of 0:00:11 remaining]
Chain 1: 100% [0:00:00 of 0:00:11 remaining]

Chain 2:   0% [0:00:14 of 0:00:14 remaining]
Chain 2:  10% [0:00:11 of 0:00:12 remaining]
Chain 2:  20% [0:00:11 of 0:00:13 remaining]
Chain 2:  30% [0:00:09 of 0:00:13 remaining]
Chain 2:  40% [0:00:08 of 0:00:13 remaining]
Chain 2:  50% [0:00:06 of 0:00:12 remaining]
Chain 2:  60% [0:00:05 of 0:00:12 remaining]
Chain 2:  70% [0:00:04 of 0:00:12 remaining]
Chain 2:  80% [0:00:02 of 0:00:12 remaining]
Chain 2:  90% [0:00:01 of 0:00:12 remaining]
Chain 2: 100% [0:00:00 of 0:00:12 remaining]

Chain 3:   0% [0:00:08 of 0:00:08 remaining]
Chain 3:  10% [0:00:06 of 0:00:06 remaining]
Chain 3:  20% [0:00:05 of 0:00:07 remaining]
Chain 3:  30% [0:00:05 of 0:00:07 remaining]
Chain 3:  40% [0:00:05 of 0:00:08 remaining]
Chain 3:  50% [0:00:04 of 0:00:08 remaining]
Chain 3:  60% [0:00:03 of 0:00:07 remaining]
Chain 3:  70% [0:00:02 of 0:00:07 remaining]
Chain 3:  80% [0:00:01 of 0:00:07 remaining]
Chain 3:  90% [0:00:01 of 0:00:07 remaining]
Chain 3: 100% [0:00:00 of 0:00:07 remaining]

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:19:59 of 0:19:60 remaining]
Chain 1:  10% [0:00:11 of 0:00:13 remaining]
Chain 1:  20% [0:00:05 of 0:00:07 remaining]
Chain 1:  30% [0:00:03 of 0:00:05 remaining]
Chain 1:  40% [0:00:02 of 0:00:04 remaining]
Chain 1:  50% [0:00:02 of 0:00:03 remaining]
Chain 1:  60% [0:00:01 of 0:00:03 remaining]
Chain 1:  70% [0:00:01 of 0:00:02 remaining]
Chain 1:  80% [0:00:00 of 0:00:02 remaining]
Chain 1:  90% [0:00:00 of 0:00:02 remaining]
Chain 1: 100% [0:00:00 of 0:00:02 remaining]

Chain 2:   0% [0:00:00 of 0:00:00 remaining]
Chain 2:  10% [0:00:01 of 0:00:01 remaining]
Chain 2:  20% [0:00:01 of 0:00:01 remaining]
Chain 2:  30% [0:00:01 of 0:00:01 remaining]
Chain 2:  40% [0:00:00 of 0:00:01 remaining]
Chain 2:  50% [0:00:00 of 0:00:01 remaining]
Chain 2:  60% [0:00:00 of 0:00:01 remaining]
Chain 2:  70% [0:00:00 of 0:00:01 remaining]
Chain 2:  80% [0:00:00 of 0:00:01 remaining]
Chain 2:  90% [0:00:00 of 0:00:01 remaining]
Chain 2: 100% [0:00:00 of 0:00:01 remaining]

Chain 3:   0% [0:00:01 of 0:00:01 remaining]
Chain 3:  10% [0:00:01 of 0:00:01 remaining]
Chain 3:  20% [0:00:01 of 0:00:01 remaining]
Chain 3:  30% [0:00:00 of 0:00:01 remaining]
Chain 3:  40% [0:00:00 of 0:00:01 remaining]
Chain 3:  50% [0:00:00 of 0:00:01 remaining]
Chain 3:  60% [0:00:00 of 0:00:01 remaining]
Chain 3:  70% [0:00:00 of 0:00:01 remaining]
Chain 3:  80% [0:00:00 of 0:00:01 remaining]
Chain 3:  90% [0:00:00 of 0:00:01 remaining]
Chain 3: 100% [0:00:00 of 0:00:01 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Gelman, Rubin, and Brooks Diagnostic:
              PSRF 97.5%
     beta[1] 1.009 1.010
     beta[2] 1.009 1.010
          s2 1.008 1.016
Multivariate 1.006   NaN

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Geweke Diagnostic:
First Window Fraction = 0.1
Second Window Fraction = 0.5

        Z-score p-value
beta[1]   1.237  0.2162
beta[2]  -1.568  0.1168
     s2   1.710  0.0872

        Z-score p-value
beta[1]  -1.457  0.1452
beta[2]   1.752  0.0797
     s2  -1.428  0.1534

        Z-score p-value
beta[1]   0.550  0.5824
beta[2]  -0.440  0.6597
     s2   0.583  0.5596

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Heidelberger and Welch Diagnostic:
Target Halfwidth Ratio = 0.1
Alpha = 0.05

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.0680 0.57366275 0.053311283    1
beta[2]     738            1  0.0677 0.81285744 0.015404173    1
     s2     738            1  0.0700 1.00825202 0.094300432    1

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.1356  0.6293320 0.065092099    0
beta[2]     251            1  0.0711  0.7934633 0.019215278    1
     s2     251            1  0.4435  1.4635400 0.588158612    0

        Burn-in Stationarity p-value    Mean     Halfwidth  Test
beta[1]     251            1  0.0515  0.5883602 0.058928034    0
beta[2]    1225            1  0.1479  0.8086080 0.018478999    1
     s2     251            1  0.6664  0.9942853 0.127959523    0

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Raftery and Lewis Diagnostic:
Quantile (q) = 0.025
Accuracy (r) = 0.005
Probability (s) = 0.95

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     267      17897 3746         4.7776295
beta[2]        2     267      17897 3746         4.7776295
     s2        2     257       8689 3746         2.3195408

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        4     271 2.1759×10⁴ 3746         5.8085958
beta[2]        4     275 2.8795×10⁴ 3746         7.6868660
     s2        2     257 8.3450×10³ 3746         2.2277096

        Thinning Burn-in    Total   Nmin Dependence Factor
beta[1]        2     269 2.0647×10⁴ 3746         5.5117459
beta[2]        2     263 1.4523×10⁴ 3746         3.8769354
     s2        2     255 7.8770×10³ 3746         2.1027763

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean       SD       Naive SE       MCSE       ESS   
beta[1] 0.5971183 1.14894446 0.0095006014 0.016925598 4607.9743
beta[2] 0.8017036 0.34632566 0.0028637608 0.004793345 4875.0000
     s2 1.2203777 2.00876760 0.0166104638 0.101798287  389.3843

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.74343373 0.026573102 0.59122696 1.1878720 2.8308472
beta[2]  0.12168742 0.628297573 0.80357822 0.9719441 1.5051573
     s2  0.17091385 0.383671702 0.65371989 1.2206381 6.0313970

         95% Lower  95% Upper
beta[1] -1.75436235 2.8109571
beta[2]  0.09721501 1.4733163
     s2  0.08338409 3.8706865

           beta[1]      beta[2]        s2     
beta[1]  1.000000000 -0.905245029  0.027467317
beta[2] -0.905245029  1.000000000 -0.024489462
     s2  0.027467317 -0.024489462  1.000000000

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.24521566  -0.021411797 -0.0077424153  -0.044989417
beta[2] 0.20402485  -0.019107846  0.0033980453  -0.053869216
     s2 0.85931351   0.568056917  0.3248136852   0.024157524

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.28180489  -0.031007672    0.03930888  0.0394895028
beta[2] 0.25905976  -0.017946010    0.03613043  0.0227758214
     s2 0.92905843   0.761339226    0.58455868  0.0050215824

           Lag 2       Lag 10        Lag 20       Lag 100   
beta[1] 0.38634357 -0.0029361782  -0.032310111  0.0028806786
beta[2] 0.32822879 -0.0056670786  -0.020100663 -0.0062622517
     s2 0.68812720  0.2420402859   0.080495078 -0.0290205896

             Change Rate
     beta[1]       0.844
     beta[2]       0.844
          s2       1.000
Multivariate       1.000

MCMC Processing of 4875 Iterations x 3 Chains...

Chain 1:   0% [0:03:03 of 0:03:04 remaining]
Chain 1:  10% [0:00:03 of 0:00:04 remaining]
Chain 1:  20% [0:00:02 of 0:00:02 remaining]
Chain 1:  30% [0:00:01 of 0:00:01 remaining]
Chain 1:  40% [0:00:01 of 0:00:01 remaining]
Chain 1:  50% [0:00:00 of 0:00:01 remaining]
Chain 1:  60% [0:00:00 of 0:00:01 remaining]
Chain 1:  70% [0:00:00 of 0:00:01 remaining]
Chain 1:  80% [0:00:00 of 0:00:01 remaining]
Chain 1:  90% [0:00:00 of 0:00:01 remaining]
Chain 1: 100% [0:00:00 of 0:00:00 remaining]

Chain 2:   0% [0:04:48 of 0:04:49 remaining]
Chain 2:  10% [0:00:05 of 0:00:06 remaining]
Chain 2:  20% [0:00:02 of 0:00:03 remaining]
Chain 2:  30% [0:00:01 of 0:00:02 remaining]
Chain 2:  40% [0:00:01 of 0:00:02 remaining]
Chain 2:  50% [0:00:01 of 0:00:01 remaining]
Chain 2:  60% [0:00:00 of 0:00:01 remaining]
Chain 2:  70% [0:00:00 of 0:00:01 remaining]
Chain 2:  80% [0:00:00 of 0:00:01 remaining]
Chain 2:  90% [0:00:00 of 0:00:01 remaining]
Chain 2: 100% [0:00:00 of 0:00:01 remaining]

Chain 3:   0% [0:05:24 of 0:05:25 remaining]
Chain 3:  10% [0:00:06 of 0:00:07 remaining]
Chain 3:  20% [0:00:03 of 0:00:03 remaining]
Chain 3:  30% [0:00:02 of 0:00:02 remaining]
Chain 3:  40% [0:00:01 of 0:00:02 remaining]
Chain 3:  50% [0:00:01 of 0:00:01 remaining]
Chain 3:  60% [0:00:00 of 0:00:01 remaining]
Chain 3:  70% [0:00:00 of 0:00:01 remaining]
Chain 3:  80% [0:00:00 of 0:00:01 remaining]
Chain 3:  90% [0:00:00 of 0:00:01 remaining]
Chain 3: 100% [0:00:00 of 0:00:01 remaining]

      DIC    Effective Parameters
pD 13.828540            1.1661193
pV 22.624104            5.5639015

Iterations = 1000:5000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 2001

Empirical Posterior Estimates:
           Mean        SD      Naive SE      MCSE       ESS   
beta[1] 0.62445845 1.0285709 0.013275474 0.023818436 1864.8416
beta[2] 0.79392648 0.3096614 0.003996712 0.006516677 2001.0000

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.53050898 0.076745702 0.61120944 1.2174641 2.6906753
beta[2]  0.18846617 0.618849048 0.79323126 0.9619767 1.4502109

MCMC Simulation of 5000 Iterations x 3 Chains...

Chain 1:   0% [0:00:19 of 0:00:19 remaining]
Chain 1:  10% [0:00:03 of 0:00:04 remaining]
Chain 1:  20% [0:00:03 of 0:00:04 remaining]
Chain 1:  30% [0:00:03 of 0:00:04 remaining]
Chain 1:  40% [0:00:02 of 0:00:04 remaining]
Chain 1:  50% [0:00:02 of 0:00:04 remaining]
Chain 1:  60% [0:00:02 of 0:00:04 remaining]
Chain 1:  70% [0:00:01 of 0:00:04 remaining]
Chain 1:  80% [0:00:01 of 0:00:04 remaining]
Chain 1:  90% [0:00:00 of 0:00:04 remaining]
Chain 1: 100% [0:00:00 of 0:00:04 remaining]

Chain 2:   0% [0:00:05 of 0:00:05 remaining]
Chain 2:  10% [0:00:03 of 0:00:04 remaining]
Chain 2:  20% [0:00:03 of 0:00:03 remaining]
Chain 2:  30% [0:00:02 of 0:00:03 remaining]
Chain 2:  40% [0:00:02 of 0:00:03 remaining]
Chain 2:  50% [0:00:02 of 0:00:04 remaining]
Chain 2:  60% [0:00:01 of 0:00:03 remaining]
Chain 2:  70% [0:00:01 of 0:00:03 remaining]
Chain 2:  80% [0:00:01 of 0:00:03 remaining]
Chain 2:  90% [0:00:00 of 0:00:03 remaining]
Chain 2: 100% [0:00:00 of 0:00:03 remaining]

Chain 3:   0% [0:00:02 of 0:00:02 remaining]
Chain 3:  10% [0:00:02 of 0:00:02 remaining]
Chain 3:  20% [0:00:02 of 0:00:02 remaining]
Chain 3:  30% [0:00:02 of 0:00:03 remaining]
Chain 3:  40% [0:00:02 of 0:00:03 remaining]
Chain 3:  50% [0:00:01 of 0:00:03 remaining]
Chain 3:  60% [0:00:01 of 0:00:03 remaining]
Chain 3:  70% [0:00:01 of 0:00:03 remaining]
Chain 3:  80% [0:00:01 of 0:00:03 remaining]
Chain 3:  90% [0:00:00 of 0:00:03 remaining]
Chain 3: 100% [0:00:00 of 0:00:03 remaining]

Iterations = 252:15000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 7375

Empirical Posterior Estimates:
           Mean        SD      Naive SE       MCSE        ESS   
beta[1] 0.59655228 1.1225920 0.0075471034 0.014053505 6380.79199
beta[2] 0.80144540 0.3395731 0.0022829250 0.003954871 7372.28048
     s2 1.18366563 1.8163096 0.0122109158 0.070481708  664.08995

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.70512374 0.031582137 0.58989089 1.1783924 2.8253668
beta[2]  0.12399073 0.630638800 0.80358526 0.9703569 1.4939817
     s2  0.17075261 0.382963160 0.65372440 1.2210168 5.7449800

Object of type "Mamba.Model"
-------------------------------------------------------------------------------
y:
An unmonitored node of type "5-element Mamba.ArrayStochastic{1}"
[1.0,3.0,3.0,3.0,5.0]

Distribution:
IsoNormal(
dim: 5
μ: [3.2384475887755166,5.286627296564756,7.334807004353996,9.382986712143236,11.431166419932476]
Σ: [1.1830434911443408 0.0 0.0 0.0 0.0
 0.0 1.1830434911443408 0.0 0.0 0.0
 0.0 0.0 1.1830434911443408 0.0 0.0
 0.0 0.0 0.0 1.1830434911443408 0.0
 0.0 0.0 0.0 0.0 1.1830434911443408]
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f((Mamba.getindex)(model,:mu),(Mamba.getindex)(model,:s2))
    end)))))

Source Nodes:
[:mu,:s2]

Target Nodes:
Symbol[]
-------------------------------------------------------------------------------
s2:
A monitored node of type "Mamba.ScalarStochastic"
1.1830434911443408

Distribution:
Distributions.InverseGamma{Float64}(
invd: Distributions.Gamma{Float64}(α=0.001, θ=1000.0)
θ: 0.001
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f()
    end)))))

Source Nodes:
Symbol[]

Target Nodes:
[:y]
-------------------------------------------------------------------------------
xmat:
[1.0 1.0
 1.0 2.0
 1.0 3.0
 1.0 4.0
 1.0 5.0]
-------------------------------------------------------------------------------
beta:
A monitored node of type "2-element Mamba.ArrayStochastic{1}"
[1.1902678809862768,2.04817970778924]

Distribution:
ZeroMeanIsoNormal(
dim: 2
μ: [0.0,0.0]
Σ: [1000.0 0.0
 0.0 1000.0]
)

Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f()
    end)))))

Source Nodes:
Symbol[]

Target Nodes:
[:mu,:y]
-------------------------------------------------------------------------------
mu:
An unmonitored node of type "5-element Mamba.ArrayLogical{1}"
[3.2384475887755166,5.286627296564756,7.334807004353996,9.382986712143236,11.431166419932476]
Function:
AST(:($(Expr(:lambda, Any[:(model::Model)], Any[Any[Any[:model,:Any,18],Any[:f,:Any,18]],Any[],0,Any[]], :(begin 
        model = (top(typeassert))(model,Mamba.Model)
        f = (anonymous function)
        return f((Mamba.getindex)(model,:xmat),(Mamba.getindex)(model,:beta))
    end)))))

Source Nodes:
[:xmat,:beta]

Target Nodes:
[:y]

>>> Testing ../doc/samplers/amm.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.67372957 1.07776604 0.0152419136 0.064652461 277.89381
b1 0.77112512 0.32135034 0.0045445801 0.017982404 319.34640
s2 1.29658296 2.18979550 0.0309683849 0.139974432 244.74267

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.451022943 0.069490369 0.65978541 1.19631191 3.1685322
b1  0.050258268 0.611291896 0.79094107 0.94652458 1.4132212
s2  0.174653274 0.389575233 0.65132492 1.31853897 6.8337311


>>> Testing ../doc/samplers/amwg.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD      Naive SE       MCSE        ESS   
b0 0.5983881 1.5898262 0.0224835377 0.162560986  95.645964
b1 0.8006155 0.4358909 0.0061644284 0.042038394 107.513597
s2 2.2710644 9.1262030 0.1290640007 0.686946293 176.495925

Quantiles:
       2.5%       25.0%      50.0%      75.0%      97.5%  
b0 -2.17471713 0.03038741 0.67912808 1.30557650  3.3496859
b1  0.03755749 0.60096712 0.77599921 0.96011511  1.7513752
s2  0.16901125 0.39945048 0.70340929 1.47274298 13.3376638


>>> Testing ../doc/samplers/bhmc.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean      SD       Naive SE       MCSE         ESS   
 gamma[1] 0.5425 0.49821539 0.0049821539 0.0043794170 10000.0000
 gamma[2] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[3] 0.3125 0.46353558 0.0046353558 0.0052442850  7812.5639
 gamma[4] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[5] 0.7132 0.45228997 0.0045228997 0.0056726884  6357.0562
 gamma[6] 0.7342 0.44178035 0.0044178035 0.0055526497  6330.1242
 gamma[7] 1.0000 0.00000000 0.0000000000 0.0000000000 10000.0000
 gamma[8] 0.5202 0.49961677 0.0049961677 0.0030648875 10000.0000
 gamma[9] 0.4972 0.50001716 0.0050001716 0.0063230858  6253.3346
gamma[10] 0.7768 0.41641218 0.0041641218 0.0044537579  8741.6542

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     1     1
 gamma[4]    1     1     1     1     1
 gamma[5]    0     0     1     1     1
 gamma[6]    0     0     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     1     1
gamma[10]    0     1     1     1     1


>>> Testing ../doc/samplers/bia.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE         ESS    
 gamma[1] 0.6803 0.466383599 0.00466383599 0.0130286824  1281.40243
 gamma[2] 0.9996 0.019996999 0.00019996999 0.0004000000  2499.24992
 gamma[3] 0.0012 0.034621956 0.00034621956 0.0009458041  1339.98766
 gamma[4] 0.9973 0.051893924 0.00051893924 0.0027000000   369.40731
 gamma[5] 0.9978 0.046854877 0.00046854877 0.0022000000   453.59081
 gamma[6] 0.9997 0.017318776 0.00017318776 0.0003000000  3332.66660
 gamma[7] 0.9982 0.042390325 0.00042390325 0.0018000000   554.61102
 gamma[8] 0.6932 0.461188714 0.00461188714 0.0113109226  1662.49857
 gamma[9] 0.0000 0.000000000 0.00000000000 0.0000000000 10000.00000
gamma[10] 0.9986 0.037392243 0.00037392243 0.0014000000   713.35705

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/bmc3.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE          ESS    
 gamma[1] 0.6859 0.464179638 0.00464179638 0.0159190471   850.233153
 gamma[2] 0.9992 0.028274369 0.00028274369 0.0008000000  1249.124912
 gamma[3] 0.0125 0.111107986 0.00111107986 0.0073887370   226.125809
 gamma[4] 0.9988 0.034621956 0.00034621956 0.0012000000   832.416575
 gamma[5] 0.9570 0.202867236 0.00202867236 0.0194803709   108.449957
 gamma[6] 0.9997 0.017318776 0.00017318776 0.0002227015  6047.669940
 gamma[7] 1.0000 0.000000000 0.00000000000 0.0000000000 10000.000000
 gamma[8] 0.6883 0.463211147 0.00463211147 0.0185983897   620.307903
 gamma[9] 0.0440 0.205105355 0.00205105355 0.0196884830   108.524924
gamma[10] 0.9635 0.187540041 0.00187540041 0.0178083938   110.901780

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    0     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     1
gamma[10]    0     1     1     1     1

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE        ESS    
 gamma[1] 0.6770 0.467646094 0.00467646094 0.016071494   846.68547
 gamma[2] 0.9993 0.026449574 0.00026449574 0.000700000  1427.71420
 gamma[3] 0.0033 0.057353631 0.00057353631 0.003300000   302.06051
 gamma[4] 0.9991 0.029987996 0.00029987996 0.000900000  1110.22213
 gamma[5] 0.9982 0.042390325 0.00042390325 0.001800000   554.61102
 gamma[6] 0.9994 0.024488772 0.00024488772 0.000600000  1665.83325
 gamma[7] 0.9997 0.017318776 0.00017318776 0.000300000  3332.66660
 gamma[8] 0.6785 0.467075546 0.00467075546 0.015762361   878.07523
 gamma[9] 0.0000 0.000000000 0.00000000000 0.000000000 10000.00000
gamma[10] 0.9999 0.010000000 0.00010000000 0.000100000 10000.00000

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/bmg.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE        ESS    
 gamma[1] 0.6426 0.479257977 0.00479257977 0.019055273   632.56964
 gamma[2] 0.9974 0.050926411 0.00050926411 0.002600000   383.65375
 gamma[3] 0.0014 0.037392243 0.00037392243 0.001400000   713.35705
 gamma[4] 0.9977 0.047905527 0.00047905527 0.002300000   433.82599
 gamma[5] 0.9961 0.062331200 0.00062331200 0.003900000   255.43580
 gamma[6] 0.9996 0.019996999 0.00019996999 0.000400000  2499.24992
 gamma[7] 1.0000 0.000000000 0.00000000000 0.000000000 10000.00000
 gamma[8] 0.7135 0.452148420 0.00452148420 0.022320157   410.36240
 gamma[9] 0.0005 0.022356207 0.00022356207 0.000500000  1999.19992
gamma[10] 0.9964 0.059894897 0.00059894897 0.003600000   276.80546

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
           Mean       SD        Naive SE       MCSE        ESS    
 gamma[1] 0.6608 0.473461484 0.00473461484 0.021104684   503.28225
 gamma[2] 0.9973 0.051893924 0.00051893924 0.002700000   369.40731
 gamma[3] 0.0030 0.054692770 0.00054692770 0.003000000   332.36657
 gamma[4] 1.0000 0.000000000 0.00000000000 0.000000000 10000.00000
 gamma[5] 0.9959 0.063903039 0.00063903039 0.004100000   242.92673
 gamma[6] 0.9970 0.054692770 0.00054692770 0.003000000   332.36657
 gamma[7] 0.9996 0.019996999 0.00019996999 0.000400000  2499.24992
 gamma[8] 0.6948 0.460515111 0.00460515111 0.017472911   694.63591
 gamma[9] 0.0000 0.000000000 0.00000000000 0.000000000 10000.00000
gamma[10] 0.9990 0.031608542 0.00031608542 0.001000000   999.09991

Quantiles:
          2.5% 25.0% 50.0% 75.0% 97.5%
 gamma[1]    0     0     1     1     1
 gamma[2]    1     1     1     1     1
 gamma[3]    0     0     0     0     0
 gamma[4]    1     1     1     1     1
 gamma[5]    1     1     1     1     1
 gamma[6]    1     1     1     1     1
 gamma[7]    1     1     1     1     1
 gamma[8]    0     0     1     1     1
 gamma[9]    0     0     0     0     0
gamma[10]    1     1     1     1     1


>>> Testing ../doc/samplers/hmc.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.5772432  1.18402293 0.0167446128 0.027883682 1803.1006
b1 0.8022538  0.37671405 0.0053275412 0.008170591 2125.7719
s2 1.4971811 11.28460781 0.1595884542 0.195664541 3326.2025

Quantiles:
       2.5%        25.0%       50.0%      75.0%     97.5%  
b0 -1.714610896 0.015592733 0.55534270 1.17186976 3.0307402
b1  0.092367742 0.631187676 0.80982814 0.97620155 1.4874951
s2  0.178874019 0.383680766 0.66198223 1.22565452 7.0403625

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.59496699 1.37677793 0.0194705802 0.035517823 1502.5714
b1 0.79853916 0.41130398 0.0058167167 0.009917444 1719.9916
s2 1.80970811 8.36078976 0.1182394227 0.312469964  715.9423

Quantiles:
       2.5%        25.0%      50.0%     75.0%     97.5%  
b0 -1.89540401 -0.028843232 0.6365522 1.2257351 3.0797518
b1  0.06653081  0.605516575 0.7862590 0.9763511 1.5777984
s2  0.16937961  0.390805781 0.6795564 1.3775763 8.2227653


>>> Testing ../doc/samplers/mala.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE        ESS   
b0 0.63913008 0.96456516 0.013641011 0.109737267  77.260028
b1 0.79071667 0.29199225 0.004129394 0.030293853  92.903834
s2 1.16990867 2.21952343 0.031388801 0.159528112 193.573104

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.11636066 0.021295738 0.56942794 1.1958553 2.8185967
b1  0.13351555 0.622505238 0.81840154 0.9631912 1.3448359
s2  0.15082458 0.428174945 0.70535341 1.1853768 4.7815748

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean         SD       Naive SE      MCSE       ESS   
b0 0.61457782  1.09976885 0.0155530802 0.12576800  76.46497
b1 0.73181199  0.40025128 0.0056604079 0.04444227  81.10973
s2 2.93138741 13.82161613 0.1954671698 1.21062808 130.34554

Quantiles:
       2.5%       25.0%     50.0%     75.0%      97.5%  
b0 -1.29782609 0.00000000 0.4671962 1.0542319  3.6409243
b1 -0.09214223 0.57229743 0.7547751 0.9690808  1.4525190
s2  0.23962808 0.44855541 0.8566210 1.6689977 13.7557720


>>> Testing ../doc/samplers/nuts.jl

Iterations = 1001:5000
Thinning interval = 1
Chains = 1
Samples per chain = 4000

Empirical Posterior Estimates:
      Mean        SD       Naive SE      MCSE       ESS   
b0 0.59242566 1.12664340 0.0178137962 0.07262530 240.65621
b1 0.79955933 0.34124494 0.0053955563 0.02021753 284.88937
s2 1.23041170 2.28573689 0.0361406736 0.10397815 483.24600

Quantiles:
       2.5%        25.0%       50.0%     75.0%     97.5%  
b0 -1.60156914 -0.018689413 0.55936277 1.1737069 2.9302371
b1  0.09581153  0.620652115 0.81772866 0.9840868 1.4803526
s2  0.20677131  0.401460055 0.67740807 1.2526098 5.6494793


>>> Testing ../doc/samplers/rwm.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE       MCSE       ESS   
b0 0.50814446 0.95649687 0.0135269085 0.105741241  81.82355
b1 0.82845138 0.29152116 0.0041227318 0.031171455  87.46333
s2 1.09352799 1.51797233 0.0214673706 0.072343598 440.27852

Quantiles:
       2.5%       25.0%       50.0%     75.0%     97.5%  
b0 -1.40016641 -0.10053289 0.52105167 1.1664813 2.3420987
b1  0.27991615  0.63683768 0.81974517 1.0071005 1.4073847
s2  0.18553138  0.40129932 0.68770085 1.1990384 4.5481870


>>> Testing ../doc/samplers/slice.jl

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean        SD       Naive SE     MCSE        ESS   
b0 0.95121197 1.39800189 0.019770732 0.16058158  75.792124
b1 0.70804449 0.38972473 0.005511540 0.04083353  91.092384
s2 1.68897644 4.98568184 0.070508189 0.41400577 145.022818

Quantiles:
       2.5%       25.0%      50.0%     75.0%      97.5%  
b0 -1.18134932 0.22505842 0.75133123 1.3961206  5.2902465
b1 -0.24667682 0.57032710 0.75324890 0.9190770  1.3468726
s2  0.16329786 0.38360098 0.64859723 1.3052359 10.1810850

Iterations = 1:5000
Thinning interval = 1
Chains = 1
Samples per chain = 5000

Empirical Posterior Estimates:
      Mean       SD       Naive SE       MCSE       ESS   
b0 0.5918173 0.97247832 0.0137529202 0.088171432 121.64775
b1 0.8010611 0.29639544 0.0041916646 0.023762408 155.58300
s2 1.1918742 2.91584961 0.0412363406 0.118877515 601.63183

Quantiles:
       2.5%       25.0%      50.0%      75.0%     97.5%  
b0 -1.53289453 0.06956086 0.62086125 1.12897029 2.5922503
b1  0.19746079 0.64147194 0.79976963 0.95676223 1.4487573
s2  0.16453963 0.36687076 0.61831108 1.15173238 5.9535381


>>> Testing ../doc/samplers/slicesimplex.jl

Iterations = 1:10000
Thinning interval = 1
Chains = 1
Samples per chain = 10000

Empirical Posterior Estimates:
          Mean         SD        Naive SE        MCSE        ESS   
rho[1] 0.27754333 0.047233921 0.00047233921 0.00141592199 1112.8313
rho[2] 0.10332212 0.031263826 0.00031263826 0.00058567835 2849.4820
rho[3] 0.15258317 0.038007617 0.00038007617 0.00081859335 2155.7820
rho[4] 0.35246555 0.051090924 0.00051090924 0.00149244465 1171.9013
rho[5] 0.11408583 0.033523072 0.00033523072 0.00071863697 2176.0482

Quantiles:
           2.5%       25.0%      50.0%       75.0%      97.5%  
rho[1] 0.192874639 0.24366290 0.275752374 0.30784577 0.37691928
rho[2] 0.050788311 0.08068638 0.100682538 0.12286559 0.17351691
rho[3] 0.086277998 0.12523438 0.150148020 0.17687571 0.23401932
rho[4] 0.255446325 0.31691025 0.350797321 0.38773881 0.45478127
rho[5] 0.058278656 0.08940860 0.111306257 0.13540498 0.18591961


>>> Testing ../doc/mcmc/readcoda.jl

Iterations = 1:200
Thinning interval = 1
Chains = 1,2
Samples per chain = 200

Empirical Posterior Estimates:
         Mean       SD       Naive SE      MCSE       ESS   
alpha 3.0025394 0.53475753 0.026737877 0.018902157 200.00000
 beta 0.8013086 0.39267477 0.019633739 0.030895834 161.53482
sigma 1.0821777 0.94869150 0.047434575 0.061191837 200.00000

Quantiles:
         2.5%      25.0%   50.0%   75.0%     97.5%  
alpha  1.8322542 2.751095 3.0257 3.2709700 3.9511365
 beta -0.0125375 0.599750 0.8065 1.0079525 1.5292802
sigma  0.4329000 0.625000 0.8360 1.2378125 2.8597185


>>> Testing ../doc/mcmc/newunivardist.jl

MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:01:19 of 0:01:19 remaining]
Chain 1:  10% [0:00:19 of 0:00:21 remaining]
Chain 1:  20% [0:00:16 of 0:00:20 remaining]
Chain 1:  30% [0:00:13 of 0:00:18 remaining]
Chain 1:  40% [0:00:11 of 0:00:18 remaining]
Chain 1:  50% [0:00:09 of 0:00:17 remaining]
Chain 1:  60% [0:00:07 of 0:00:17 remaining]
Chain 1:  70% [0:00:05 of 0:00:17 remaining]
Chain 1:  80% [0:00:03 of 0:00:17 remaining]
Chain 1:  90% [0:00:02 of 0:00:17 remaining]
Chain 1: 100% [0:00:00 of 0:00:17 remaining]

Chain 2:   0% [0:00:16 of 0:00:16 remaining]
Chain 2:  10% [0:00:19 of 0:00:21 remaining]
Chain 2:  20% [0:00:18 of 0:00:22 remaining]
Chain 2:  30% [0:00:14 of 0:00:20 remaining]
Chain 2:  40% [0:00:12 of 0:00:21 remaining]
Chain 2:  50% [0:00:10 of 0:00:20 remaining]
Chain 2:  60% [0:00:08 of 0:00:20 remaining]
Chain 2:  70% [0:00:06 of 0:00:21 remaining]
Chain 2:  80% [0:00:04 of 0:00:21 remaining]
Chain 2:  90% [0:00:02 of 0:00:21 remaining]
Chain 2: 100% [0:00:00 of 0:00:21 remaining]

Chain 3:   0% [0:00:25 of 0:00:25 remaining]
Chain 3:  10% [0:00:17 of 0:00:19 remaining]
Chain 3:  20% [0:00:14 of 0:00:17 remaining]
Chain 3:  30% [0:00:12 of 0:00:17 remaining]
Chain 3:  40% [0:00:10 of 0:00:16 remaining]
Chain 3:  50% [0:00:08 of 0:00:16 remaining]
Chain 3:  60% [0:00:07 of 0:00:16 remaining]
Chain 3:  70% [0:00:05 of 0:00:17 remaining]
Chain 3:  80% [0:00:03 of 0:00:17 remaining]
Chain 3:  90% [0:00:02 of 0:00:17 remaining]
Chain 3: 100% [0:00:00 of 0:00:17 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean        SD       Naive SE       MCSE       ESS   
beta[1] 0.58240287 1.14230234 0.0094456779 0.018305636 3893.9689
beta[2] 0.80352012 0.34248004 0.0028319614 0.005181105 4369.4396
     s2 1.21355573 1.77421793 0.0146709766 0.071581268  614.3490

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.71534530 0.023088344 0.58077083 1.1524530 2.8274529
beta[2]  0.12368846 0.630518397 0.80241394 0.9732482 1.5135523
     s2  0.16868782 0.385631053 0.65901954 1.2756509 6.1752281


>>> Testing ../doc/mcmc/newmultivardist.jl

WARNING: replacing module Testing
MCMC Simulation of 10000 Iterations x 3 Chains...

Chain 1:   0% [0:00:43 of 0:00:43 remaining]
Chain 1:  10% [0:00:10 of 0:00:11 remaining]
Chain 1:  20% [0:00:08 of 0:00:10 remaining]
Chain 1:  30% [0:00:07 of 0:00:10 remaining]
Chain 1:  40% [0:00:06 of 0:00:10 remaining]
Chain 1:  50% [0:00:05 of 0:00:10 remaining]
Chain 1:  60% [0:00:04 of 0:00:09 remaining]
Chain 1:  70% [0:00:03 of 0:00:09 remaining]
Chain 1:  80% [0:00:02 of 0:00:08 remaining]
Chain 1:  90% [0:00:01 of 0:00:09 remaining]
Chain 1: 100% [0:00:00 of 0:00:08 remaining]

Chain 2:   0% [0:00:18 of 0:00:18 remaining]
Chain 2:  10% [0:00:05 of 0:00:05 remaining]
Chain 2:  20% [0:00:06 of 0:00:07 remaining]
Chain 2:  30% [0:00:05 of 0:00:07 remaining]
Chain 2:  40% [0:00:04 of 0:00:07 remaining]
Chain 2:  50% [0:00:03 of 0:00:07 remaining]
Chain 2:  60% [0:00:03 of 0:00:07 remaining]
Chain 2:  70% [0:00:02 of 0:00:07 remaining]
Chain 2:  80% [0:00:01 of 0:00:07 remaining]
Chain 2:  90% [0:00:01 of 0:00:07 remaining]
Chain 2: 100% [0:00:00 of 0:00:07 remaining]

Chain 3:   0% [0:00:04 of 0:00:04 remaining]
Chain 3:  10% [0:00:05 of 0:00:06 remaining]
Chain 3:  20% [0:00:05 of 0:00:06 remaining]
Chain 3:  30% [0:00:04 of 0:00:05 remaining]
Chain 3:  40% [0:00:03 of 0:00:05 remaining]
Chain 3:  50% [0:00:03 of 0:00:05 remaining]
Chain 3:  60% [0:00:02 of 0:00:05 remaining]
Chain 3:  70% [0:00:01 of 0:00:05 remaining]
Chain 3:  80% [0:00:01 of 0:00:05 remaining]
Chain 3:  90% [0:00:00 of 0:00:05 remaining]
Chain 3: 100% [0:00:00 of 0:00:05 remaining]

Iterations = 252:10000
Thinning interval = 2
Chains = 1,2,3
Samples per chain = 4875

Empirical Posterior Estimates:
           Mean       SD       Naive SE       MCSE         ESS   
beta[1] 0.5788931 1.11232796 0.0091978202 0.0190480893 3410.06668
beta[2] 0.8052916 0.33546852 0.0027739833 0.0055032179 3715.95238
     s2 1.1638187 1.61102871 0.0133215679 0.0632533846  648.69272

Quantiles:
            2.5%       25.0%       50.0%     75.0%     97.5%  
beta[1] -1.71161012 0.030043945 0.58723102 1.1464501 2.8420839
beta[2]  0.12334042 0.628821665 0.80501158 0.9749143 1.4853405
     s2  0.17208834 0.383853394 0.66064186 1.2465013 5.5621934

INFO: Mamba tests passed

>>> End of log
