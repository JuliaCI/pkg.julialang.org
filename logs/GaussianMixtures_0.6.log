>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.1.3
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.10.0
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.9392929077148438 GB (713.1328125 MB free)
Uptime: 23330.0 sec
Load Avg:  0.91845703125  0.96923828125  1.02392578125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3511 MHz    1468361 s         99 s     126524 s     498283 s         38 s
#2  3511 MHz     595708 s         46 s      77898 s    1569674 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.1.3
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.8
 - StatsBase                     0.10.0
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-9.357513224983115e6,[1953.28,98046.7],
[158.877 1028.07 -193.191; 117.666 -585.813 603.563],

Array{Float64,2}[
[1926.47 -73.7587 44.4363; -73.7587 2174.19 739.967; 44.4363 739.967 464.279],

[98242.5 282.091 -461.13; 282.091 98095.9 -369.075; -461.13 -369.075 99347.1]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.281706e+03
      1       1.363218e+03      -9.184880e+02 |        6
      2       1.201726e+03      -1.614922e+02 |        2
      3       1.078264e+03      -1.234620e+02 |        3
      4       9.748420e+02      -1.034216e+02 |        2
      5       9.556029e+02      -1.923905e+01 |        0
      6       9.556029e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 955.6029236710983)
INFO: K-means with 272 data points using 6 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.065395
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.776884
INFO: iteration 2, lowerbound -3.616685
INFO: iteration 3, lowerbound -3.446877
INFO: iteration 4, lowerbound -3.264202
INFO: iteration 5, lowerbound -3.088242
INFO: dropping number of Gaussions to 7
INFO: iteration 6, lowerbound -2.928546
INFO: iteration 7, lowerbound -2.784141
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.644395
INFO: iteration 9, lowerbound -2.527764
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.442326
INFO: iteration 11, lowerbound -2.387626
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.348505
INFO: iteration 13, lowerbound -2.320316
INFO: iteration 14, lowerbound -2.308268
INFO: dropping number of Gaussions to 2
INFO: iteration 15, lowerbound -2.303097
INFO: iteration 16, lowerbound -2.299264
INFO: iteration 17, lowerbound -2.299258
INFO: iteration 18, lowerbound -2.299255
INFO: iteration 19, lowerbound -2.299254
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Wed 05 Oct 2016 11:01:19 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Wed 05 Oct 2016 11:01:20 PM UTC: K-means with 272 data points using 6 iterations
11.3 data points per parameter
,Wed 05 Oct 2016 11:01:22 PM UTC: EM with 272 data points 0 iterations avll -2.065395
5.8 data points per parameter
,Wed 05 Oct 2016 11:01:23 PM UTC: GMM converted to Variational GMM
,Wed 05 Oct 2016 11:01:25 PM UTC: iteration 1, lowerbound -3.776884
,Wed 05 Oct 2016 11:01:25 PM UTC: iteration 2, lowerbound -3.616685
,Wed 05 Oct 2016 11:01:25 PM UTC: iteration 3, lowerbound -3.446877
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 4, lowerbound -3.264202
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 5, lowerbound -3.088242
,Wed 05 Oct 2016 11:01:26 PM UTC: dropping number of Gaussions to 7
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 6, lowerbound -2.928546
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 7, lowerbound -2.784141
,Wed 05 Oct 2016 11:01:26 PM UTC: dropping number of Gaussions to 5
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 8, lowerbound -2.644395
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 9, lowerbound -2.527764
,Wed 05 Oct 2016 11:01:26 PM UTC: dropping number of Gaussions to 4
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 10, lowerbound -2.442326
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 11, lowerbound -2.387626
,Wed 05 Oct 2016 11:01:26 PM UTC: dropping number of Gaussions to 3
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 12, lowerbound -2.348505
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 13, lowerbound -2.320316
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 14, lowerbound -2.308268
,Wed 05 Oct 2016 11:01:26 PM UTC: dropping number of Gaussions to 2
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 15, lowerbound -2.303097
,Wed 05 Oct 2016 11:01:26 PM UTC: iteration 16, lowerbound -2.299264
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 17, lowerbound -2.299258
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 18, lowerbound -2.299255
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 19, lowerbound -2.299254
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 20, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 21, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 22, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 23, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 24, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 25, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 26, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 27, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 28, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:27 PM UTC: iteration 29, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 30, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 31, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 32, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 33, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 34, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 35, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 36, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 37, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 38, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 39, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 40, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 41, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 42, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 43, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:28 PM UTC: iteration 44, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:29 PM UTC: iteration 45, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:29 PM UTC: iteration 46, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:29 PM UTC: iteration 47, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:29 PM UTC: iteration 48, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:29 PM UTC: iteration 49, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:29 PM UTC: iteration 50, lowerbound -2.299253
,Wed 05 Oct 2016 11:01:29 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9950227614663205
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.99502276146632
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9950227614663201
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9806111628027194
avll from llpg:  -0.9806111628027194
avll direct:     -0.9806111628027193
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0435872   -0.101019     -0.0151434     0.00412679   0.0683808    0.0582753    0.0617199   -0.0192692    -0.148804     0.156838     0.0567317    0.105587     0.0439078    0.0141185    -0.0407323    0.0732166   -0.0660008   -0.0158317    0.0659686    0.00153711   0.162906     0.0859364   -0.0292072    0.0339071    0.0347696    0.0748517  
  0.0524895   -0.00225711   -0.146104     -0.115881    -0.0529678    0.115177    -0.180608    -0.0954007     0.0287036    0.00942416  -0.0118212    0.161117    -0.108142    -0.0896254     0.106486     0.050948     0.0346686   -0.0171331    0.0563724   -0.0284152    0.0252194   -0.171528    -0.0331327    0.0174352    0.0235224   -0.0836857  
 -0.125025    -0.0639885    -0.138979      0.203373    -0.0108589    0.0461148   -0.0373977    0.152956     -0.0625276   -0.0928416    0.114174    -0.0404933    0.0533826   -0.201107     -0.0237012   -0.00203548  -0.0586288    0.00962675   0.281748    -0.0335391    0.0407004    0.131739    -0.056186     0.0205305   -0.0579321    0.0979691  
  0.0187055   -0.0846322    -0.102723      0.0731001   -0.167626     0.010968     0.00911803  -0.173212     -0.106315     0.103106    -0.230152    -0.126307     0.0491665    0.104328      0.161792     0.208659    -0.0983744    0.00594109  -0.0283       0.014197     0.16131     -0.163821    -0.0705397    0.0952245   -0.10085     -0.0652787  
  0.0133874   -0.00646308   -0.0295709    -0.064362    -0.102399    -0.00255491  -0.0374711   -0.107502      0.0267472    0.119791    -0.0963559   -0.0720729    0.01997      0.0801778    -0.0180067   -0.0948413    0.0659591    0.0832016   -0.085077    -0.0780623    0.0443687    0.0418261   -0.0300455   -0.164372     0.101367    -0.014927   
 -0.013363     0.013793      0.0369409    -0.0188734   -0.050366    -0.120695    -0.0985239    0.0993704    -0.036331    -0.114892     0.0658766   -0.0546091   -0.0383361   -0.140893      0.045293    -0.071029    -0.0877486   -0.0694627   -0.0626681   -0.00508788   0.0824698    0.0252872   -0.0272904   -0.065509    -0.0129494   -0.0304937  
  0.0840949    0.131806     -0.00870663    0.102694    -0.0501713   -0.0514076    0.0800334   -0.0267354    -0.0781211    0.0177605   -0.0729634   -0.167751    -0.00315377   0.0616292    -0.139469    -0.135041     0.0454685    0.0558982    0.0515748   -0.0222874   -0.0293026    0.0370323    0.0900459   -0.0586456   -0.0901963   -0.0227597  
 -0.0981361    0.0390584     0.000146248  -0.206244    -0.0626475    0.0370777   -0.111274    -0.00491567   -0.00730566   0.0113873    0.0630054    0.0246875   -0.130097     0.0274738    -0.136768     0.0707297    0.028087     0.0356683   -0.0117534    0.00776691  -0.0805338    0.0722702    0.00342158  -0.0409353    0.0559381    0.000965326
  0.125958    -0.0065342    -0.140271     -0.16404     -0.0321709   -0.052132    -0.0906163   -0.141317      0.0144398   -0.00172259  -0.0606072   -0.0507474   -0.00264352   0.274039     -0.0914518    0.0628947    0.0267293   -0.0197145   -0.0801237   -0.143251    -0.0881832   -0.0429285   -0.0362255    0.147239    -0.0793791    0.0777704  
  0.120146    -0.0295196     0.103847      0.0471558   -0.0838164    0.0687026    0.085344     0.0993204     0.0820485   -0.133109    -0.0638546    0.205499     0.0953919    0.0143449    -0.144139    -0.0629546    0.00161069  -0.0278991   -0.0865629   -0.0464877   -0.0917612    0.036217    -0.0456705   -0.143756     0.123553    -0.114121   
 -0.108573    -0.141494      0.0335048    -0.055959    -0.132216    -0.1065      -0.0481862    0.16813       0.0410151   -0.0377329   -0.00188782   0.126694     0.140559    -0.101406      0.202437    -0.082532     0.0893902    0.0238415    0.0767584    0.113106     0.032259     0.172409    -0.0477589   -0.0893382    0.03446      0.0927711  
 -0.0220205   -0.00776332   -0.0731495    -0.0154712   -0.0965181   -0.0593143   -0.122893     0.058454      0.112992     0.12158      0.0896769    0.0433315    0.00780336   0.119846     -0.0242112    0.00823086   0.0456142   -0.11093      0.0118613   -0.0641468   -0.0994487    0.0398559   -0.106867    -0.136759    -0.0752214   -0.102937   
  0.0468372    0.171014     -0.044373      0.107124     0.073579    -0.0477166    0.0291553    0.201374     -0.179014    -0.168069    -0.057462     0.107817    -0.0835093   -0.0338596     0.0920348    0.0287586   -0.102758    -0.0521871   -0.00389537  -0.093049     0.00738219   0.110098     0.0266528    0.0412698    0.19932      0.109352   
 -0.00231739  -0.00244264    0.0501435     0.0275074    0.06732      0.0620457   -0.0198574    0.052173      0.0712075    0.0512922   -0.0296789   -0.033623    -0.0378978   -0.0656731     0.0824395    0.0228544   -0.0403111   -0.00182399   0.141738     0.123892    -0.0717288    0.0799965   -0.0407507    0.230392    -0.0683919    0.123054   
  0.024147     0.264037      0.040318      0.0239006   -0.0121388    0.0402864    0.0875244    0.0409301    -0.0729834    0.133183    -0.00333731  -0.0880262   -0.137811    -0.000495241   0.122136    -0.0429027   -0.137537    -0.024307    -0.108024     0.0862868   -0.126302    -0.023637     0.063424     0.179365     0.114451     0.0257839  
  0.0311667   -0.000367559   0.0336814    -0.105109    -0.00263272  -0.117946     0.102043    -0.0104236     0.0185279    0.275296    -0.161166    -0.148713    -0.156629     0.00159283   -0.00328244  -0.0254268    0.0212476   -0.0439096    0.111841     0.093029     0.0695767   -0.0236891    0.0649307   -0.0701081    0.100017    -0.0823815  
  0.0244696    0.0555458     0.0163009     0.114341     0.115207    -0.0328368   -0.236141    -0.0524516     0.0934985   -0.0766959   -0.165748     0.0783901   -0.0188984    0.0720821    -0.00486887   0.0580095    0.0880161   -0.205162    -0.00767757  -0.115076     0.0558732   -0.0604693    0.0696788   -0.0973993    0.0368496   -0.0609757  
  0.0767524    0.0131899    -0.0839159     0.0386463    0.144781    -0.17769      0.00900883   0.0612647    -0.0392952   -0.0469915    0.0247375   -0.110075    -0.0241448    0.0600116    -0.148241     0.0926263    0.0180903   -0.0331909    0.0201192   -0.0416632    0.089149    -0.131572     0.170368     0.0990031    0.118845    -0.0468137  
 -0.0811589    0.0198753     0.122595     -0.0146178    0.136343     0.0937062    0.0630876   -0.017866      0.0365393    0.081601     0.0015589    0.0255692    0.0833569   -0.000657652   0.0612283   -0.129145     0.00535494   0.00585269  -0.0632106   -0.0225673   -0.0621937    0.0771891   -0.147268    -0.103059    -0.00443593  -0.0798765  
  0.0263444    0.0917589     0.0182976    -0.140497    -0.0108103    0.0203432   -0.0452866    0.0791878    -0.127827    -0.153144     0.0329934   -0.143306    -0.309363     0.12639      -0.112695     0.0501195   -0.0258749   -0.0227562   -0.117428    -0.0328248   -0.00954323   0.205699    -0.0944899   -0.00166592  -0.077493     0.000987109
 -0.0315586   -0.0835843    -0.0916108     0.0447001   -0.00227886  -0.103897     0.0647514   -0.146897      0.0109973    0.0624393   -0.0512677    0.0074748    0.0813033    0.122058     -0.0817194    0.148107     0.147121     0.0123062    0.032707    -0.09738     -0.0804338    0.00405606   0.14429      0.0668014    0.0558149   -0.0201823  
 -0.0970209   -0.202868     -0.0431149    -0.278637     0.289468    -0.0409309    0.110884     0.08746      -0.0566237   -0.00363921   0.0120825    0.00521655   0.266821     0.157807      0.0551895   -0.0282097    0.0802217    0.0523599   -0.0655402    0.0602005   -0.0792701    0.0534162   -0.0853098   -0.0941325    0.0927121   -0.0204456  
 -0.03946     -0.00148211    0.14316      -0.0321738   -0.0986894   -0.0595656   -0.0645079   -0.224024     -0.0901572    0.0212539   -0.0989157    0.213545    -0.134268     0.112038     -0.12119     -0.183931    -0.244027     0.0945979    0.0660645   -0.0148562    0.0442476   -0.100226     0.0456792    0.215652     0.0935456   -0.0736418  
  0.0953414   -0.0837416    -0.0207653     0.0979342    0.0321717   -0.175689    -0.144336     0.0690893     0.0280082    0.195269    -0.144715    -0.0357289   -0.0403424   -0.0150317    -0.089705     0.167511     0.0434517   -0.0727923    0.0700293   -0.0702223   -0.149267    -0.191364    -0.0442789   -0.102959     0.0566016   -0.0457479  
 -0.252193    -0.00871504   -0.0784955     0.0334201   -0.155242     0.0813386    0.280742    -0.0253053     0.0953877   -0.164825     0.0156794   -0.177153     0.0661139   -0.128945      0.0515389    0.0976017   -0.0989612   -0.17314      0.113449     0.0837795   -0.0601731    0.0839264    0.0349416    0.0110616   -0.0144996    0.0622061  
 -0.0233039   -0.00590554    0.062997      0.0400155   -0.113832     0.0182503   -0.0128665    0.0231566     0.0831169    0.0852968    0.0513021    0.0209312    0.047787     0.111674      0.00209393  -0.0884128   -0.187039    -0.137305     0.0996502    0.111238     0.0171352    0.0520317   -0.0419297   -0.0825363   -0.0624437    0.0493498  
 -0.0436181   -0.00110731    0.0625216     0.00890402  -0.140031     0.188157    -0.0277266    0.214654     -0.0383733    0.0827845   -0.0165541   -0.0774327   -0.0936441   -0.0802735    -0.117412     0.0563898    0.175645     0.149374    -0.249081    -0.0188825    0.11634     -0.0442127    0.0302118    0.059591     0.0838014   -0.056394   
 -0.183021     0.0119541     0.0294761     0.027963     0.0173305    0.0488864    0.0500351    0.0385604    -0.0276789    0.121917    -0.113226    -0.0502929   -0.079895    -0.0689072     0.0544971    0.103588    -0.0856053   -0.04106      0.0359516   -0.150772     0.0582701   -0.200093    -0.0522778    0.182057     0.10955      0.0253518  
  0.0691643    0.103592     -0.0566928     0.0691176   -0.0515314   -0.0168321   -0.122593     0.0267103     0.141497     0.00120588   0.09958      0.0583202   -0.181772     0.0461425     0.0301711   -0.177902    -0.130808     0.0210863    0.0338624    0.125904     0.0184592    0.118793     0.141608    -0.12694     -0.256868     0.00509558 
  0.028149    -0.00017638    0.0951289    -0.106271    -0.0540923    0.0301924    0.129631    -0.000414878  -0.00649207  -0.0518838   -0.089039    -0.00525093   0.0103208    0.159549     -0.0486612    0.0394126    0.0537164    0.0148368   -0.0358256    0.137481     0.0432432    0.126205    -0.0423837   -0.19267      0.0326622   -0.138981   
 -0.0631171    0.144738     -0.00118002    0.0800318    0.0300743    0.164738     0.0531366   -0.0065443     0.0249581    0.0724043    0.138595    -0.0576551   -0.0327362    7.46389e-5    0.0736021    0.113975    -0.0165387    0.0207141   -0.0508219    0.0711152    0.0993428    0.0515194   -0.156585    -0.0129137    0.00486735  -0.00328773 
  0.115061     0.0536077     0.101064     -0.0162433   -0.032825     0.0492146    0.123432     0.0942436     0.128147    -0.0899868   -0.0230369    0.0251354   -0.1753       0.0574992    -0.00679301  -0.049423    -0.104306     0.223236     0.0925687   -0.139115    -0.0519678    0.0259617    0.00791013  -0.173169    -0.013158     0.00951398 kind diag, method split
0: avll = -1.3934475642000121
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.393502
INFO: iteration 2, average log likelihood -1.393439
INFO: iteration 3, average log likelihood -1.392821
INFO: iteration 4, average log likelihood -1.386178
INFO: iteration 5, average log likelihood -1.369562
INFO: iteration 6, average log likelihood -1.363657
INFO: iteration 7, average log likelihood -1.362999
INFO: iteration 8, average log likelihood -1.362749
INFO: iteration 9, average log likelihood -1.362596
INFO: iteration 10, average log likelihood -1.362495
INFO: iteration 11, average log likelihood -1.362425
INFO: iteration 12, average log likelihood -1.362376
INFO: iteration 13, average log likelihood -1.362340
INFO: iteration 14, average log likelihood -1.362312
INFO: iteration 15, average log likelihood -1.362289
INFO: iteration 16, average log likelihood -1.362271
INFO: iteration 17, average log likelihood -1.362255
INFO: iteration 18, average log likelihood -1.362241
INFO: iteration 19, average log likelihood -1.362228
INFO: iteration 20, average log likelihood -1.362215
INFO: iteration 21, average log likelihood -1.362202
INFO: iteration 22, average log likelihood -1.362189
INFO: iteration 23, average log likelihood -1.362175
INFO: iteration 24, average log likelihood -1.362161
INFO: iteration 25, average log likelihood -1.362145
INFO: iteration 26, average log likelihood -1.362129
INFO: iteration 27, average log likelihood -1.362112
INFO: iteration 28, average log likelihood -1.362094
INFO: iteration 29, average log likelihood -1.362075
INFO: iteration 30, average log likelihood -1.362055
INFO: iteration 31, average log likelihood -1.362034
INFO: iteration 32, average log likelihood -1.362012
INFO: iteration 33, average log likelihood -1.361989
INFO: iteration 34, average log likelihood -1.361965
INFO: iteration 35, average log likelihood -1.361939
INFO: iteration 36, average log likelihood -1.361911
INFO: iteration 37, average log likelihood -1.361880
INFO: iteration 38, average log likelihood -1.361846
INFO: iteration 39, average log likelihood -1.361809
INFO: iteration 40, average log likelihood -1.361768
INFO: iteration 41, average log likelihood -1.361722
INFO: iteration 42, average log likelihood -1.361672
INFO: iteration 43, average log likelihood -1.361615
INFO: iteration 44, average log likelihood -1.361546
INFO: iteration 45, average log likelihood -1.361459
INFO: iteration 46, average log likelihood -1.361344
INFO: iteration 47, average log likelihood -1.361188
INFO: iteration 48, average log likelihood -1.360975
INFO: iteration 49, average log likelihood -1.360721
INFO: iteration 50, average log likelihood -1.360461
INFO: EM with 100000 data points 50 iterations avll -1.360461
952.4 data points per parameter
1: avll = [-1.3935,-1.39344,-1.39282,-1.38618,-1.36956,-1.36366,-1.363,-1.36275,-1.3626,-1.36249,-1.36243,-1.36238,-1.36234,-1.36231,-1.36229,-1.36227,-1.36225,-1.36224,-1.36223,-1.36222,-1.3622,-1.36219,-1.36218,-1.36216,-1.36215,-1.36213,-1.36211,-1.36209,-1.36207,-1.36205,-1.36203,-1.36201,-1.36199,-1.36196,-1.36194,-1.36191,-1.36188,-1.36185,-1.36181,-1.36177,-1.36172,-1.36167,-1.36161,-1.36155,-1.36146,-1.36134,-1.36119,-1.36097,-1.36072,-1.36046]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.360311
INFO: iteration 2, average log likelihood -1.359981
INFO: iteration 3, average log likelihood -1.359157
INFO: iteration 4, average log likelihood -1.353319
INFO: iteration 5, average log likelihood -1.338833
INFO: iteration 6, average log likelihood -1.327933
INFO: iteration 7, average log likelihood -1.324237
INFO: iteration 8, average log likelihood -1.322666
INFO: iteration 9, average log likelihood -1.321643
INFO: iteration 10, average log likelihood -1.320862
INFO: iteration 11, average log likelihood -1.320045
INFO: iteration 12, average log likelihood -1.318778
INFO: iteration 13, average log likelihood -1.317705
INFO: iteration 14, average log likelihood -1.317054
INFO: iteration 15, average log likelihood -1.316723
INFO: iteration 16, average log likelihood -1.316557
INFO: iteration 17, average log likelihood -1.316456
INFO: iteration 18, average log likelihood -1.316380
INFO: iteration 19, average log likelihood -1.316316
INFO: iteration 20, average log likelihood -1.316259
INFO: iteration 21, average log likelihood -1.316203
INFO: iteration 22, average log likelihood -1.316148
INFO: iteration 23, average log likelihood -1.316092
INFO: iteration 24, average log likelihood -1.316035
INFO: iteration 25, average log likelihood -1.315975
INFO: iteration 26, average log likelihood -1.315914
INFO: iteration 27, average log likelihood -1.315851
INFO: iteration 28, average log likelihood -1.315785
INFO: iteration 29, average log likelihood -1.315717
INFO: iteration 30, average log likelihood -1.315646
INFO: iteration 31, average log likelihood -1.315570
INFO: iteration 32, average log likelihood -1.315490
INFO: iteration 33, average log likelihood -1.315403
INFO: iteration 34, average log likelihood -1.315312
INFO: iteration 35, average log likelihood -1.315218
INFO: iteration 36, average log likelihood -1.315122
INFO: iteration 37, average log likelihood -1.315027
INFO: iteration 38, average log likelihood -1.314931
INFO: iteration 39, average log likelihood -1.314836
INFO: iteration 40, average log likelihood -1.314744
INFO: iteration 41, average log likelihood -1.314654
INFO: iteration 42, average log likelihood -1.314569
INFO: iteration 43, average log likelihood -1.314489
INFO: iteration 44, average log likelihood -1.314420
INFO: iteration 45, average log likelihood -1.314361
INFO: iteration 46, average log likelihood -1.314310
INFO: iteration 47, average log likelihood -1.314266
INFO: iteration 48, average log likelihood -1.314229
INFO: iteration 49, average log likelihood -1.314198
INFO: iteration 50, average log likelihood -1.314173
INFO: EM with 100000 data points 50 iterations avll -1.314173
473.9 data points per parameter
2: avll = [-1.36031,-1.35998,-1.35916,-1.35332,-1.33883,-1.32793,-1.32424,-1.32267,-1.32164,-1.32086,-1.32004,-1.31878,-1.3177,-1.31705,-1.31672,-1.31656,-1.31646,-1.31638,-1.31632,-1.31626,-1.3162,-1.31615,-1.31609,-1.31603,-1.31598,-1.31591,-1.31585,-1.31578,-1.31572,-1.31565,-1.31557,-1.31549,-1.3154,-1.31531,-1.31522,-1.31512,-1.31503,-1.31493,-1.31484,-1.31474,-1.31465,-1.31457,-1.31449,-1.31442,-1.31436,-1.31431,-1.31427,-1.31423,-1.3142,-1.31417]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.314268
INFO: iteration 2, average log likelihood -1.314118
INFO: iteration 3, average log likelihood -1.313293
INFO: iteration 4, average log likelihood -1.304682
INFO: iteration 5, average log likelihood -1.282559
INFO: iteration 6, average log likelihood -1.270055
INFO: iteration 7, average log likelihood -1.265591
INFO: iteration 8, average log likelihood -1.262998
INFO: iteration 9, average log likelihood -1.261005
INFO: iteration 10, average log likelihood -1.259423
INFO: iteration 11, average log likelihood -1.258199
INFO: iteration 12, average log likelihood -1.257398
INFO: iteration 13, average log likelihood -1.256880
INFO: iteration 14, average log likelihood -1.256518
INFO: iteration 15, average log likelihood -1.256257
INFO: iteration 16, average log likelihood -1.256068
INFO: iteration 17, average log likelihood -1.255919
INFO: iteration 18, average log likelihood -1.255797
INFO: iteration 19, average log likelihood -1.255695
INFO: iteration 20, average log likelihood -1.255604
INFO: iteration 21, average log likelihood -1.255521
INFO: iteration 22, average log likelihood -1.255445
INFO: iteration 23, average log likelihood -1.255375
INFO: iteration 24, average log likelihood -1.255311
INFO: iteration 25, average log likelihood -1.255254
INFO: iteration 26, average log likelihood -1.255204
INFO: iteration 27, average log likelihood -1.255163
INFO: iteration 28, average log likelihood -1.255129
INFO: iteration 29, average log likelihood -1.255101
INFO: iteration 30, average log likelihood -1.255078
INFO: iteration 31, average log likelihood -1.255060
INFO: iteration 32, average log likelihood -1.255045
INFO: iteration 33, average log likelihood -1.255034
INFO: iteration 34, average log likelihood -1.255025
INFO: iteration 35, average log likelihood -1.255019
INFO: iteration 36, average log likelihood -1.255014
INFO: iteration 37, average log likelihood -1.255010
INFO: iteration 38, average log likelihood -1.255007
INFO: iteration 39, average log likelihood -1.255005
INFO: iteration 40, average log likelihood -1.255003
INFO: iteration 41, average log likelihood -1.255002
INFO: iteration 42, average log likelihood -1.255001
INFO: iteration 43, average log likelihood -1.255000
INFO: iteration 44, average log likelihood -1.254999
INFO: iteration 45, average log likelihood -1.254999
INFO: iteration 46, average log likelihood -1.254999
INFO: iteration 47, average log likelihood -1.254998
INFO: iteration 48, average log likelihood -1.254998
INFO: iteration 49, average log likelihood -1.254998
INFO: iteration 50, average log likelihood -1.254998
INFO: EM with 100000 data points 50 iterations avll -1.254998
236.4 data points per parameter
3: avll = [-1.31427,-1.31412,-1.31329,-1.30468,-1.28256,-1.27005,-1.26559,-1.263,-1.261,-1.25942,-1.2582,-1.2574,-1.25688,-1.25652,-1.25626,-1.25607,-1.25592,-1.2558,-1.25569,-1.2556,-1.25552,-1.25544,-1.25538,-1.25531,-1.25525,-1.2552,-1.25516,-1.25513,-1.2551,-1.25508,-1.25506,-1.25505,-1.25503,-1.25503,-1.25502,-1.25501,-1.25501,-1.25501,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.255178
INFO: iteration 2, average log likelihood -1.254970
WARNING: Variances had to be floored 11
INFO: iteration 3, average log likelihood -1.253784
WARNING: Variances had to be floored 11
INFO: iteration 4, average log likelihood -1.240938
WARNING: Variances had to be floored 11
INFO: iteration 5, average log likelihood -1.201493
WARNING: Variances had to be floored 11 12
INFO: iteration 6, average log likelihood -1.175154
WARNING: Variances had to be floored 8 11
INFO: iteration 7, average log likelihood -1.171725
WARNING: Variances had to be floored 10 11
INFO: iteration 8, average log likelihood -1.167164
WARNING: Variances had to be floored 11 12
INFO: iteration 9, average log likelihood -1.165997
WARNING: Variances had to be floored 8 11
INFO: iteration 10, average log likelihood -1.165207
WARNING: Variances had to be floored 11
INFO: iteration 11, average log likelihood -1.163343
WARNING: Variances had to be floored 11 12
INFO: iteration 12, average log likelihood -1.151735
WARNING: Variances had to be floored 8 10 11
INFO: iteration 13, average log likelihood -1.157685
WARNING: Variances had to be floored 11
INFO: iteration 14, average log likelihood -1.169381
WARNING: Variances had to be floored 11 12
INFO: iteration 15, average log likelihood -1.155747
WARNING: Variances had to be floored 8 11
INFO: iteration 16, average log likelihood -1.161599
WARNING: Variances had to be floored 11
INFO: iteration 17, average log likelihood -1.163548
WARNING: Variances had to be floored 11 12
INFO: iteration 18, average log likelihood -1.154813
WARNING: Variances had to be floored 8 11
INFO: iteration 19, average log likelihood -1.161178
WARNING: Variances had to be floored 11
INFO: iteration 20, average log likelihood -1.162664
WARNING: Variances had to be floored 11 12
INFO: iteration 21, average log likelihood -1.152784
WARNING: Variances had to be floored 8 11
INFO: iteration 22, average log likelihood -1.157188
WARNING: Variances had to be floored 10 11
INFO: iteration 23, average log likelihood -1.157469
WARNING: Variances had to be floored 11 12
INFO: iteration 24, average log likelihood -1.156923
WARNING: Variances had to be floored 8 11
INFO: iteration 25, average log likelihood -1.157605
WARNING: Variances had to be floored 11
INFO: iteration 26, average log likelihood -1.158818
WARNING: Variances had to be floored 11 12
INFO: iteration 27, average log likelihood -1.150083
WARNING: Variances had to be floored 11
INFO: iteration 28, average log likelihood -1.156596
WARNING: Variances had to be floored 8 11
INFO: iteration 29, average log likelihood -1.152285
WARNING: Variances had to be floored 11 12
INFO: iteration 30, average log likelihood -1.153831
WARNING: Variances had to be floored 11
INFO: iteration 31, average log likelihood -1.156622
WARNING: Variances had to be floored 8 11
INFO: iteration 32, average log likelihood -1.151369
WARNING: Variances had to be floored 11 12
INFO: iteration 33, average log likelihood -1.152467
WARNING: Variances had to be floored 11
INFO: iteration 34, average log likelihood -1.155871
WARNING: Variances had to be floored 8 11
INFO: iteration 35, average log likelihood -1.151154
WARNING: Variances had to be floored 11 12
INFO: iteration 36, average log likelihood -1.152436
WARNING: Variances had to be floored 11
INFO: iteration 37, average log likelihood -1.155863
WARNING: Variances had to be floored 8 11
INFO: iteration 38, average log likelihood -1.151150
WARNING: Variances had to be floored 11 12
INFO: iteration 39, average log likelihood -1.152434
WARNING: Variances had to be floored 11
INFO: iteration 40, average log likelihood -1.155861
WARNING: Variances had to be floored 8 11
INFO: iteration 41, average log likelihood -1.151145
WARNING: Variances had to be floored 11 12
INFO: iteration 42, average log likelihood -1.152432
WARNING: Variances had to be floored 11
INFO: iteration 43, average log likelihood -1.155859
WARNING: Variances had to be floored 8 11
INFO: iteration 44, average log likelihood -1.151141
WARNING: Variances had to be floored 11 12
INFO: iteration 45, average log likelihood -1.152430
WARNING: Variances had to be floored 11
INFO: iteration 46, average log likelihood -1.155858
WARNING: Variances had to be floored 8 11
INFO: iteration 47, average log likelihood -1.151136
WARNING: Variances had to be floored 11 12
INFO: iteration 48, average log likelihood -1.152427
WARNING: Variances had to be floored 11
INFO: iteration 49, average log likelihood -1.155856
WARNING: Variances had to be floored 8 11
INFO: iteration 50, average log likelihood -1.151131
INFO: EM with 100000 data points 50 iterations avll -1.151131
118.1 data points per parameter
4: avll = [-1.25518,-1.25497,-1.25378,-1.24094,-1.20149,-1.17515,-1.17172,-1.16716,-1.166,-1.16521,-1.16334,-1.15174,-1.15768,-1.16938,-1.15575,-1.1616,-1.16355,-1.15481,-1.16118,-1.16266,-1.15278,-1.15719,-1.15747,-1.15692,-1.1576,-1.15882,-1.15008,-1.1566,-1.15229,-1.15383,-1.15662,-1.15137,-1.15247,-1.15587,-1.15115,-1.15244,-1.15586,-1.15115,-1.15243,-1.15586,-1.15115,-1.15243,-1.15586,-1.15114,-1.15243,-1.15586,-1.15114,-1.15243,-1.15586,-1.15113]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 21 22 23 24
INFO: iteration 1, average log likelihood -1.152556
WARNING: Variances had to be floored 21 22 23 24
INFO: iteration 2, average log likelihood -1.147836
WARNING: Variances had to be floored 15 16 21 22 23 24
INFO: iteration 3, average log likelihood -1.143598
WARNING: Variances had to be floored 18 19 21 22 23 24 31 32
INFO: iteration 4, average log likelihood -1.116159
WARNING: Variances had to be floored 11 18 19 21 22 23 24
INFO: iteration 5, average log likelihood -1.096677
WARNING: Variances had to be floored 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 6, average log likelihood -1.070744
WARNING: Variances had to be floored 7 11 18 21 22 23 24
INFO: iteration 7, average log likelihood -1.086465
WARNING: Variances had to be floored 18 19 21 22 23 24 31 32
INFO: iteration 8, average log likelihood -1.063599
WARNING: Variances had to be floored 11 15 16 18 19 20 21 22 23 24
INFO: iteration 9, average log likelihood -1.066196
WARNING: Variances had to be floored 7 18 21 22 23 24 31 32
INFO: iteration 10, average log likelihood -1.065570
WARNING: Variances had to be floored 11 18 19 21 22 23 24
INFO: iteration 11, average log likelihood -1.064537
WARNING: Variances had to be floored 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 12, average log likelihood -1.051637
WARNING: Variances had to be floored 11 18 21 22 23 24
INFO: iteration 13, average log likelihood -1.073655
WARNING: Variances had to be floored 7 15 16 18 19 21 22 23 24 31 32
INFO: iteration 14, average log likelihood -1.049268
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 15, average log likelihood -1.070097
WARNING: Variances had to be floored 15 16 18 21 22 23 24 31 32
INFO: iteration 16, average log likelihood -1.060495
WARNING: Variances had to be floored 11 18 19 21 22 23 24
INFO: iteration 17, average log likelihood -1.065693
WARNING: Variances had to be floored 7 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 18, average log likelihood -1.050686
WARNING: Variances had to be floored 11 18 21 22 23 24
INFO: iteration 19, average log likelihood -1.077537
WARNING: Variances had to be floored 15 16 18 19 21 22 23 24 31 32
INFO: iteration 20, average log likelihood -1.052254
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 21, average log likelihood -1.066934
WARNING: Variances had to be floored 15 16 18 21 22 23 24 31 32
INFO: iteration 22, average log likelihood -1.059287
WARNING: Variances had to be floored 7 11 18 19 21 22 23 24
INFO: iteration 23, average log likelihood -1.064140
WARNING: Variances had to be floored 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 24, average log likelihood -1.055364
WARNING: Variances had to be floored 11 18 21 22 23 24
INFO: iteration 25, average log likelihood -1.075110
WARNING: Variances had to be floored 15 16 18 19 21 22 23 24 31 32
INFO: iteration 26, average log likelihood -1.051766
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 27, average log likelihood -1.066620
WARNING: Variances had to be floored 7 15 16 18 21 22 23 24 31 32
INFO: iteration 28, average log likelihood -1.058778
WARNING: Variances had to be floored 11 18 19 21 22 23 24
INFO: iteration 29, average log likelihood -1.068902
WARNING: Variances had to be floored 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 30, average log likelihood -1.052876
WARNING: Variances had to be floored 11 18 21 22 23 24
INFO: iteration 31, average log likelihood -1.074420
WARNING: Variances had to be floored 15 16 18 19 21 22 23 24 31 32
INFO: iteration 32, average log likelihood -1.050810
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 33, average log likelihood -1.063924
WARNING: Variances had to be floored 15 16 18 21 22 23 24 31 32
INFO: iteration 34, average log likelihood -1.054710
WARNING: Variances had to be floored 11 18 19 21 22 23 24
INFO: iteration 35, average log likelihood -1.060362
WARNING: Variances had to be floored 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 36, average log likelihood -1.046656
WARNING: Variances had to be floored 11 18 21 22 23 24
INFO: iteration 37, average log likelihood -1.068703
WARNING: Variances had to be floored 15 16 18 19 21 22 23 24 31 32
INFO: iteration 38, average log likelihood -1.045939
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 39, average log likelihood -1.061113
WARNING: Variances had to be floored 15 16 18 21 22 23 24 31 32
INFO: iteration 40, average log likelihood -1.054205
WARNING: Variances had to be floored 11 18 19 21 22 23 24
INFO: iteration 41, average log likelihood -1.060314
WARNING: Variances had to be floored 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 42, average log likelihood -1.046611
WARNING: Variances had to be floored 11 18 21 22 23 24
INFO: iteration 43, average log likelihood -1.068625
WARNING: Variances had to be floored 15 16 18 19 21 22 23 24 31 32
INFO: iteration 44, average log likelihood -1.045842
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 45, average log likelihood -1.060947
WARNING: Variances had to be floored 15 16 18 21 22 23 24 31 32
INFO: iteration 46, average log likelihood -1.053939
WARNING: Variances had to be floored 11 18 19 21 22 23 24
INFO: iteration 47, average log likelihood -1.059937
WARNING: Variances had to be floored 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 48, average log likelihood -1.046031
WARNING: Variances had to be floored 11 18 21 22 23 24
INFO: iteration 49, average log likelihood -1.067809
WARNING: Variances had to be floored 15 16 18 19 21 22 23 24 31 32
INFO: iteration 50, average log likelihood -1.045002
INFO: EM with 100000 data points 50 iterations avll -1.045002
59.0 data points per parameter
5: avll = [-1.15256,-1.14784,-1.1436,-1.11616,-1.09668,-1.07074,-1.08647,-1.0636,-1.0662,-1.06557,-1.06454,-1.05164,-1.07365,-1.04927,-1.0701,-1.0605,-1.06569,-1.05069,-1.07754,-1.05225,-1.06693,-1.05929,-1.06414,-1.05536,-1.07511,-1.05177,-1.06662,-1.05878,-1.0689,-1.05288,-1.07442,-1.05081,-1.06392,-1.05471,-1.06036,-1.04666,-1.0687,-1.04594,-1.06111,-1.05421,-1.06031,-1.04661,-1.06863,-1.04584,-1.06095,-1.05394,-1.05994,-1.04603,-1.06781,-1.045]
[-1.39345,-1.3935,-1.39344,-1.39282,-1.38618,-1.36956,-1.36366,-1.363,-1.36275,-1.3626,-1.36249,-1.36243,-1.36238,-1.36234,-1.36231,-1.36229,-1.36227,-1.36225,-1.36224,-1.36223,-1.36222,-1.3622,-1.36219,-1.36218,-1.36216,-1.36215,-1.36213,-1.36211,-1.36209,-1.36207,-1.36205,-1.36203,-1.36201,-1.36199,-1.36196,-1.36194,-1.36191,-1.36188,-1.36185,-1.36181,-1.36177,-1.36172,-1.36167,-1.36161,-1.36155,-1.36146,-1.36134,-1.36119,-1.36097,-1.36072,-1.36046,-1.36031,-1.35998,-1.35916,-1.35332,-1.33883,-1.32793,-1.32424,-1.32267,-1.32164,-1.32086,-1.32004,-1.31878,-1.3177,-1.31705,-1.31672,-1.31656,-1.31646,-1.31638,-1.31632,-1.31626,-1.3162,-1.31615,-1.31609,-1.31603,-1.31598,-1.31591,-1.31585,-1.31578,-1.31572,-1.31565,-1.31557,-1.31549,-1.3154,-1.31531,-1.31522,-1.31512,-1.31503,-1.31493,-1.31484,-1.31474,-1.31465,-1.31457,-1.31449,-1.31442,-1.31436,-1.31431,-1.31427,-1.31423,-1.3142,-1.31417,-1.31427,-1.31412,-1.31329,-1.30468,-1.28256,-1.27005,-1.26559,-1.263,-1.261,-1.25942,-1.2582,-1.2574,-1.25688,-1.25652,-1.25626,-1.25607,-1.25592,-1.2558,-1.25569,-1.2556,-1.25552,-1.25544,-1.25538,-1.25531,-1.25525,-1.2552,-1.25516,-1.25513,-1.2551,-1.25508,-1.25506,-1.25505,-1.25503,-1.25503,-1.25502,-1.25501,-1.25501,-1.25501,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.255,-1.25518,-1.25497,-1.25378,-1.24094,-1.20149,-1.17515,-1.17172,-1.16716,-1.166,-1.16521,-1.16334,-1.15174,-1.15768,-1.16938,-1.15575,-1.1616,-1.16355,-1.15481,-1.16118,-1.16266,-1.15278,-1.15719,-1.15747,-1.15692,-1.1576,-1.15882,-1.15008,-1.1566,-1.15229,-1.15383,-1.15662,-1.15137,-1.15247,-1.15587,-1.15115,-1.15244,-1.15586,-1.15115,-1.15243,-1.15586,-1.15115,-1.15243,-1.15586,-1.15114,-1.15243,-1.15586,-1.15114,-1.15243,-1.15586,-1.15113,-1.15256,-1.14784,-1.1436,-1.11616,-1.09668,-1.07074,-1.08647,-1.0636,-1.0662,-1.06557,-1.06454,-1.05164,-1.07365,-1.04927,-1.0701,-1.0605,-1.06569,-1.05069,-1.07754,-1.05225,-1.06693,-1.05929,-1.06414,-1.05536,-1.07511,-1.05177,-1.06662,-1.05878,-1.0689,-1.05288,-1.07442,-1.05081,-1.06392,-1.05471,-1.06036,-1.04666,-1.0687,-1.04594,-1.06111,-1.05421,-1.06031,-1.04661,-1.06863,-1.04584,-1.06095,-1.05394,-1.05994,-1.04603,-1.06781,-1.045]
32×26 Array{Float64,2}:
  0.0169965   -0.015308      0.0117216    -0.0509227   -0.101782    -0.0671314    -0.0347637    -0.10265       0.0245808    0.128148    -0.100199     -0.0737826    0.0321752    0.117768     -0.0131746   -0.0928942    0.0914017    0.0922151   -0.0770515   -0.0583001    0.0426192   0.0724011   -0.0510665  -0.163669     0.108557      0.00124599 
 -0.0404835   -0.00507251    0.141423     -0.0129842   -0.104117    -0.0627159    -0.0317455    -0.228162     -0.089623     0.0529436   -0.142009      0.247822    -0.116529     0.115294     -0.0927131   -0.175608    -0.224826     0.0906438    0.0528535   -0.0155658    0.0322728  -0.103293     0.027267    0.217313     0.107044     -0.070629   
  0.0320232    0.00315349    0.020693     -0.00277737  -0.0354678   -0.00936532   -0.000832382   0.0368725    -5.14103e-5  -0.0340591   -0.0428692    -0.0408048   -0.0624517   -0.0264556     0.0779       0.00224155  -0.0792552    0.0549658    0.0374998    0.00349722   0.0178648   0.0206507   -0.0188533  -0.0186214   -0.032961     -0.000999975
  0.0453506    0.0755009    -0.000890669   0.0718179    0.0735168   -0.00882443    0.0273114     0.197675     -0.171023    -0.172662    -0.0403979     0.093494    -0.0940143   -0.0346023     0.0831928    0.0309992   -0.0998828   -0.0435864    0.00341803  -0.0736031    0.029124    0.109906     0.0388092   0.0915872    0.193625      0.119733   
  0.00953845   0.117138      0.0142133    -0.180309    -0.0109157    0.0312313    -0.0154571     0.0765494    -0.12708     -0.151613     0.0293077    -0.143625    -0.318457     0.104257     -0.11859      0.0803484   -0.0348627   -0.0380565   -0.110989    -0.0321371   -0.025392    0.20471     -0.122762    0.00960325  -0.0777883     0.00171556 
  0.0446945   -0.104367     -0.0176715    -0.0203491    0.0638381    0.0536885     0.066813     -0.0193111    -0.148098     0.179708     0.0551395     0.0908233    0.0514434    0.0609423    -0.0309147    0.0862403   -0.0661736   -0.0815446    0.0493629    0.00365563   0.167595    0.0282504   -0.0435514   0.034536     0.0307637     0.0699974  
  0.0135868   -0.0338036    -0.149221      0.0187883   -0.0215723   -0.00994004   -0.0927475     0.0125477    -0.0177995   -0.0499897    0.0594154    -0.0333289    0.00752482   0.0440115    -0.040929     0.0215458   -0.00844632  -0.0177974    0.110231    -0.0855724   -0.0371705   0.0432547   -0.0565405   0.0797175   -0.0851681     0.0828353  
 -0.0793934    0.0344287     0.118424      0.00130086   0.137774     0.0936857     0.062308     -0.0457586     0.0567446    0.0785243   -0.00520416    0.0139373    0.0916036    0.0244413     0.0619848   -0.121793     0.0492991    0.0106779   -0.056554    -0.0178164   -0.0181203   0.0715115   -0.145201   -0.100438    -0.00571572   -0.0872194  
  0.101491     0.21931       0.0104848     0.0613208   -0.013129    -0.023835      0.0513438     0.0240226    -0.205605     0.172309     0.0931267    -0.125892    -0.0947945    0.00263439    0.116679    -0.721378    -0.115823    -0.021258    -0.0912025    0.0120813   -0.0185219   0.0203923    0.222503    0.101201     0.0070055     0.152473   
 -0.0080199    0.299717      0.0480332    -0.105728    -0.013294     0.000100729   0.0974361     0.0451135    -0.0683876    0.0853788   -0.0895789    -0.0305286   -0.151768     0.00291703    0.119086     0.541754    -0.22124     -0.0271776   -0.128668     0.148558    -0.179538   -0.0840483   -0.0593114   0.2854       0.261524     -0.0298081  
  0.0751606    0.105952     -0.0643824     0.069393    -0.0521512   -0.000157403  -0.129334      0.000344491   0.119261    -0.00205193   0.103041      0.0489423   -0.185821     0.0413591     0.0293859   -0.132374    -0.137653     0.0200678    0.0255686    0.117977     0.0165251   0.109475     0.14046    -0.103454    -0.257549      0.00695406 
 -0.0267339    0.000519665   0.0697082     0.00673746  -0.144028     0.193997     -0.038098      0.212901     -0.0526966    0.121746    -0.0200685    -0.0466843   -0.0922258   -0.0891311    -0.116153     0.0291749    0.183723     0.153828    -0.252139    -0.014127     0.0879402  -0.0464203    0.0194416   0.0333406    0.0853741    -0.0547612  
  0.0403558    0.0564139    -0.0339493     0.146349     0.228643    -0.0422742    -0.319233     -0.049161      0.0691702   -0.132769    -0.15835       0.0902106   -0.0371322   -0.0331396    -0.0242326   -2.27278e-5   0.0686063   -0.203398    -0.0760915   -0.131635    -0.0375283  -0.049352     0.048916   -0.0501133    0.0671096    -0.439206   
  0.0382763    0.0509291     0.044935      0.0744832    0.00923261  -0.0775876    -0.220961     -0.050377      0.119761    -0.0395896   -0.177601      0.207334    -0.00179976   0.186206      0.0270221    0.0844898    0.0299167   -0.188872     0.0801813   -0.100609     0.0845802  -0.11185      0.0978422  -0.166752    -0.0175073     0.392858   
  0.301348     0.0561192    -0.155191     -0.112245    -0.027906     0.0969754    -0.210116     -0.0956946     0.0171626    0.00225296  -0.027884      0.156457    -0.105784    -0.0424552     0.104581     0.0644353    0.0125018    0.121309    -1.15667     -0.045497     0.127481   -0.265875     0.201829   -0.072667     0.0240467    -0.149079   
 -0.236143    -0.0587682    -0.134241     -0.043161    -0.0538261    0.0688755    -0.124165     -0.0933731     0.0109569    0.00855196   0.00120561    0.157255    -0.072486    -0.116618      0.103677     0.053273     0.0363544   -0.107322     1.67653     -0.00356833  -0.0174461   0.0679154   -0.319193    0.129451     0.0231879    -0.0869675  
 -0.0131685   -0.00894107   -0.0866016    -0.013411    -0.0927296   -0.0484395    -0.104129      0.0600334     0.109736     0.131128     0.0887932     0.0405297   -0.00549574   0.119684     -0.0313643    0.00833026   0.062177    -0.115645     0.0152924   -0.0536192   -0.0980493   0.0316754   -0.0998599  -0.136165    -0.0726234    -0.106922   
 -0.109209    -0.141441      0.0336247    -0.0549199   -0.137104    -0.0992267    -0.0484739     0.172724      0.03647     -0.0353027   -0.000987621   0.085163     0.143722    -0.104756      0.201453    -0.0840653    0.0621719    0.0265989    0.0714842    0.11979      0.030564    0.156336    -0.008394   -0.0895595    0.0341906     0.0760704  
  0.092979    -0.0965599    -0.00113934    0.0977714    0.0452803   -0.194911     -0.120251      0.0670176     0.0284772    0.212952    -0.144649     -0.0472842   -0.0439909   -0.0143461    -0.0774492    0.161092     0.0416255   -0.101063     0.0705958   -0.0743597   -0.158577   -0.13195     -0.100755   -0.0935095    0.0394248    -0.0679282  
  0.0341149   -0.00217971    0.0201558    -0.104736     0.0420923   -0.118794      0.0643307    -0.02998       0.0452639    0.257795    -0.160187     -0.153356    -0.161136    -0.0347967    -0.00854399  -0.0282436    0.0224992   -0.032265     0.113412     0.102746     0.0714307  -0.0214268    0.0522452  -0.0764431    0.090157     -0.0669623  
  0.0954066   -0.0344366     0.0737478     0.0934675   -0.0761282    0.0577893     0.134294      0.0919437     0.0487808   -0.756774    -0.0669022     0.202561     0.0959672    0.177536     -0.181886    -0.124695     0.724539    -0.0354615   -0.0151073    0.160198    -0.0906368   0.0385133   -0.0962447  -0.148815     0.140865     -0.158879   
  0.134196    -0.0367979     0.0920278    -0.0273973   -0.0896513    0.0728217     0.0636055     0.097246      0.0982883    0.417631    -0.05554       0.202692     0.096093    -0.196112     -0.11417     -0.0409609   -0.664391    -0.0513307   -0.138533    -0.329967    -0.0908623  -0.00858108   0.09723    -0.146254     0.109422     -0.128124   
 -0.0081365   -0.0174896    -0.157421      0.0375786    0.0308065   -0.0681391    -0.0549902    -0.159477     -0.244029     0.10947     -0.0606713     0.0078832    0.0811842    0.0877022     0.0826948    0.147972     0.147005     0.176696    -0.00580864  -0.108596    -0.0636085   0.0715218    0.119382    0.0359637   -0.0247887    -0.0703362  
 -0.0416631   -0.188225      0.0166405     0.057061    -0.0549784   -0.121453      0.110015     -0.143788      0.273388     0.0626082   -0.048917      0.00781834   0.0779447    0.157839     -0.26347      0.147889     0.146639    -0.136482     0.0870844   -0.0876399   -0.0984144  -0.0589898    0.177399    0.0948629    0.124139      0.115299   
 -0.097759    -0.229246     -0.041742     -0.278981     0.297827    -0.0431809     0.10476       0.0964598    -0.0365196   -0.0220524    0.00582102    0.00648132   0.271292     0.15796       0.0608992   -0.0357881    0.0809798    0.0549602   -0.0833006    0.0752666   -0.0770626   0.0499182   -0.0834556  -0.0945272    0.0815523    -0.0164671  
 -0.255798    -0.013497     -0.0925127     0.0322458   -0.165552     0.0790224     0.282359     -0.02873       0.0941592   -0.191995     0.035265     -0.174983     0.0772631   -0.12723       0.0533561    0.107569    -0.098419    -0.171653     0.124431     0.0790808   -0.0616755   0.128152     0.0371906   0.00845185  -0.027558      0.0385352  
  0.0334696   -0.109084      0.10489       0.0656367   -0.276384    -0.0964125    -0.0481748    -0.342644      0.193077    -0.0125985    0.0563443     0.0220907    0.10521      0.0370458     0.0199056   -0.0966998   -0.186727    -0.140203     0.110217     0.0570948   -0.0674088   0.0310559   -0.0409402  -0.0895215   -0.0541336     0.0443646  
 -0.215036     0.0191774     0.0791072    -0.00441753   0.0487349    0.0902089     0.00287432    0.34389       0.0136104    0.111507     0.0410621     0.0130619   -0.0300792    0.12868       0.00192027  -0.0238583   -0.191001    -0.134503     0.0797169    0.17174      0.051296   -0.0249007   -0.012108   -0.0558684   -0.00594515    0.0345621  
 -0.0372826    0.00660079   -0.0261231     0.0311082    0.0672628   -0.0611319     0.0461543     0.0282876    -0.031489     0.0404082   -0.0741551    -0.0773326   -0.0502385    0.00325474   -0.0426943    0.104781    -0.0328773   -0.0274414    0.0270312   -0.106183     0.0736774  -0.164848     0.0582534   0.149226     0.102635     -0.0109142  
 -0.0302385    0.0819395     0.00337645   -0.0760146   -0.0851715    0.00638563    0.00716042   -0.0286738    -0.0344965    0.0334224   -0.0156979    -0.0761432   -0.0689825    0.0799261    -0.118772    -0.0111805    0.031806     0.0493475    0.021833     0.00997388  -0.0463353   0.0584869    0.0390017  -0.0383289    0.00801357   -0.0197351  
  0.0276398    0.0156752     0.084194     -0.0759509   -0.0606113    0.0298998     0.178782     -0.00483558   -0.0103533   -0.0624928   -0.0900312    -0.00891813   0.0201043    0.159162     -0.030843     0.067789     0.044934     0.00593122  -0.0436075    0.130081     0.0553359   0.111147    -0.043168   -0.182813     0.0318755    -0.127308   
 -0.0767809    0.155626      0.00155879    0.0787025    0.0306995    0.165387      0.0631106    -0.00762125   -0.0170274    0.052558     0.136876     -0.0604425   -0.0375891    0.000298213   0.0715736    0.114298    -0.0155613    0.0194816   -0.04596      0.0658564    0.098497    0.0514117   -0.154899   -0.0282261   -0.000368107   0.00919464 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 1, average log likelihood -1.060143
WARNING: Variances had to be floored 11 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 2, average log likelihood -1.040589
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 3, average log likelihood -1.060121
WARNING: Variances had to be floored 11 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 4, average log likelihood -1.040579
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 5, average log likelihood -1.060116
WARNING: Variances had to be floored 11 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 6, average log likelihood -1.040577
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 7, average log likelihood -1.060113
WARNING: Variances had to be floored 11 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 8, average log likelihood -1.040576
WARNING: Variances had to be floored 11 18 19 20 21 22 23 24
INFO: iteration 9, average log likelihood -1.060110
WARNING: Variances had to be floored 11 15 16 18 19 20 21 22 23 24 31 32
INFO: iteration 10, average log likelihood -1.040576
INFO: EM with 100000 data points 10 iterations avll -1.040576
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.324169e+05
      1       6.601493e+05      -1.722676e+05 |       32
      2       6.256696e+05      -3.447964e+04 |       32
      3       6.090555e+05      -1.661416e+04 |       32
      4       6.011544e+05      -7.901059e+03 |       32
      5       5.969947e+05      -4.159727e+03 |       32
      6       5.945680e+05      -2.426730e+03 |       32
      7       5.928205e+05      -1.747452e+03 |       32
      8       5.910933e+05      -1.727227e+03 |       32
      9       5.894753e+05      -1.617941e+03 |       32
     10       5.881198e+05      -1.355571e+03 |       32
     11       5.872408e+05      -8.789202e+02 |       32
     12       5.867297e+05      -5.110986e+02 |       32
     13       5.862243e+05      -5.054372e+02 |       32
     14       5.857561e+05      -4.682035e+02 |       32
     15       5.854249e+05      -3.311725e+02 |       32
     16       5.852165e+05      -2.084035e+02 |       31
     17       5.851202e+05      -9.630993e+01 |       32
     18       5.850800e+05      -4.018751e+01 |       31
     19       5.850616e+05      -1.842220e+01 |       31
     20       5.850498e+05      -1.184734e+01 |       27
     21       5.850428e+05      -6.907320e+00 |       28
     22       5.850368e+05      -6.010070e+00 |       29
     23       5.850316e+05      -5.232435e+00 |       24
     24       5.850270e+05      -4.602390e+00 |       25
     25       5.850235e+05      -3.497009e+00 |       29
     26       5.850187e+05      -4.786639e+00 |       26
     27       5.850134e+05      -5.327847e+00 |       27
     28       5.850081e+05      -5.261642e+00 |       28
     29       5.850019e+05      -6.252357e+00 |       29
     30       5.849941e+05      -7.773264e+00 |       28
     31       5.849844e+05      -9.750634e+00 |       30
     32       5.849743e+05      -1.010015e+01 |       27
     33       5.849636e+05      -1.061397e+01 |       28
     34       5.849556e+05      -8.075393e+00 |       28
     35       5.849490e+05      -6.615130e+00 |       22
     36       5.849428e+05      -6.162740e+00 |       30
     37       5.849371e+05      -5.677624e+00 |       26
     38       5.849322e+05      -4.877652e+00 |       25
     39       5.849292e+05      -3.043414e+00 |       25
     40       5.849274e+05      -1.825439e+00 |       19
     41       5.849256e+05      -1.731486e+00 |       19
     42       5.849246e+05      -1.052667e+00 |       14
     43       5.849240e+05      -5.828976e-01 |       14
     44       5.849235e+05      -5.153472e-01 |       12
     45       5.849226e+05      -8.438825e-01 |       12
     46       5.849214e+05      -1.189577e+00 |       15
     47       5.849205e+05      -9.496209e-01 |       18
     48       5.849194e+05      -1.062710e+00 |       16
     49       5.849184e+05      -1.037002e+00 |       13
     50       5.849176e+05      -8.104036e-01 |       16
K-means terminated without convergence after 50 iterations (objv = 584917.5885153343)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.312888
INFO: iteration 2, average log likelihood -1.282068
INFO: iteration 3, average log likelihood -1.247670
INFO: iteration 4, average log likelihood -1.213393
WARNING: Variances had to be floored 15
INFO: iteration 5, average log likelihood -1.178202
WARNING: Variances had to be floored 29
INFO: iteration 6, average log likelihood -1.143958
WARNING: Variances had to be floored 2 6
INFO: iteration 7, average log likelihood -1.101069
WARNING: Variances had to be floored 10 19 24
INFO: iteration 8, average log likelihood -1.074319
WARNING: Variances had to be floored 15 17 21 29 31
INFO: iteration 9, average log likelihood -1.056751
WARNING: Variances had to be floored 16
INFO: iteration 10, average log likelihood -1.097416
WARNING: Variances had to be floored 6 23 28
INFO: iteration 11, average log likelihood -1.063790
WARNING: Variances had to be floored 2 15 29
INFO: iteration 12, average log likelihood -1.066176
WARNING: Variances had to be floored 13 19 21 31
INFO: iteration 13, average log likelihood -1.070977
WARNING: Variances had to be floored 10 24
INFO: iteration 14, average log likelihood -1.065274
WARNING: Variances had to be floored 6 15 17 23 28 29
INFO: iteration 15, average log likelihood -1.044205
WARNING: Variances had to be floored 2 16
INFO: iteration 16, average log likelihood -1.098200
WARNING: Variances had to be floored 3
INFO: iteration 17, average log likelihood -1.074554
WARNING: Variances had to be floored 13 15 21 31
INFO: iteration 18, average log likelihood -1.044509
WARNING: Variances had to be floored 2 6 9 19 23 28 29
INFO: iteration 19, average log likelihood -1.044977
WARNING: Variances had to be floored 10
INFO: iteration 20, average log likelihood -1.090068
WARNING: Variances had to be floored 3 15 16 17
INFO: iteration 21, average log likelihood -1.055205
WARNING: Variances had to be floored 21
INFO: iteration 22, average log likelihood -1.071457
WARNING: Variances had to be floored 2 6 13 19 29 31
INFO: iteration 23, average log likelihood -1.022322
WARNING: Variances had to be floored 3 10 15 23 24 28
INFO: iteration 24, average log likelihood -1.065732
WARNING: Variances had to be floored 16
INFO: iteration 25, average log likelihood -1.112622
WARNING: Variances had to be floored 9 21
INFO: iteration 26, average log likelihood -1.076052
WARNING: Variances had to be floored 6 15 29
INFO: iteration 27, average log likelihood -1.063309
WARNING: Variances had to be floored 2 13 17 19 31
INFO: iteration 28, average log likelihood -1.053347
WARNING: Variances had to be floored 21 23 28
INFO: iteration 29, average log likelihood -1.079128
WARNING: Variances had to be floored 10 15 29
INFO: iteration 30, average log likelihood -1.080925
WARNING: Variances had to be floored 6
INFO: iteration 31, average log likelihood -1.084497
WARNING: Variances had to be floored 2 16 19 31
INFO: iteration 32, average log likelihood -1.037537
WARNING: Variances had to be floored 13 15 21 23 28 29
INFO: iteration 33, average log likelihood -1.057076
WARNING: Variances had to be floored 3 9
INFO: iteration 34, average log likelihood -1.096667
WARNING: Variances had to be floored 6 17
INFO: iteration 35, average log likelihood -1.063183
WARNING: Variances had to be floored 2 10 15 19 24 31
INFO: iteration 36, average log likelihood -1.032013
WARNING: Variances had to be floored 16 28 29
INFO: iteration 37, average log likelihood -1.084874
WARNING: Variances had to be floored 21 23
INFO: iteration 38, average log likelihood -1.083273
WARNING: Variances had to be floored 13 15
INFO: iteration 39, average log likelihood -1.051076
WARNING: Variances had to be floored 2 3 6 9 17 19 28 29 31
INFO: iteration 40, average log likelihood -1.033151
INFO: iteration 41, average log likelihood -1.123937
WARNING: Variances had to be floored 10 23
INFO: iteration 42, average log likelihood -1.073624
WARNING: Variances had to be floored 15 16
INFO: iteration 43, average log likelihood -1.063550
WARNING: Variances had to be floored 13 21 24 29
INFO: iteration 44, average log likelihood -1.047993
WARNING: Variances had to be floored 2 6 31
INFO: iteration 45, average log likelihood -1.064897
WARNING: Variances had to be floored 15 17 19 28
INFO: iteration 46, average log likelihood -1.049729
WARNING: Variances had to be floored 3 10
INFO: iteration 47, average log likelihood -1.065665
WARNING: Variances had to be floored 9 13 16 21 24 29
INFO: iteration 48, average log likelihood -1.047292
WARNING: Variances had to be floored 6 15
INFO: iteration 49, average log likelihood -1.101416
INFO: iteration 50, average log likelihood -1.087053
INFO: EM with 100000 data points 50 iterations avll -1.087053
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0200533    0.0549765    0.00832456   0.0396438   -0.0937537   0.103074     -0.0819607   0.110067     0.0323593    0.0587962     0.0510563    0.00225903  -0.137767    -0.0246176   -0.0408662   -0.0569601   0.0184007    0.0856664   -0.115791     0.0553283   0.0584844    0.0351146    0.0799486   -0.0446706   -0.0868519   -0.021196  
 -0.0585974   -0.0558128   -0.0999878    0.0732444   -0.128018    0.0307526     0.0223652  -0.151396    -0.0851823    0.100589     -0.190419    -0.122414     0.0191841    0.101952     0.190706     0.185915   -0.0855087    0.0132164   -0.0266013    0.0781042   0.185493    -0.136458    -0.0813347    0.0688022   -0.092564    -0.0613791 
  0.0945076    0.0538165   -0.0804546   -0.0130079    0.164645   -0.195716      0.0839801   0.0498669   -0.0364638    0.0102013    -0.0174957   -0.11584     -0.0476715    0.0451994   -0.131695     0.0624909   0.0221093   -0.028194     0.0363979   -0.0388558   0.0781474   -0.108942     0.174548     0.0533456    0.167156    -0.0501823 
  0.0580644    0.126341    -0.0125083    0.0507505   -0.0935198  -0.0321153     0.0933676  -0.0361021   -0.060725     0.0244852    -0.0635888   -0.138963    -0.00795571   0.107185    -0.160883    -0.124719    0.0624257    0.0559421    0.0479923    0.029004   -0.039052     0.0541913    0.0852235   -0.0543548   -0.0856203   -0.0359835 
 -0.0162942    0.0306608    0.0376383   -0.0425438   -0.0573485  -0.115279     -0.089898    0.0983638   -0.0647936   -0.115454      0.0613788   -0.0464048   -0.0201745   -0.141977     0.0249302   -0.0673502  -0.0833374   -0.0574648   -0.0518343   -0.0348827   0.0612705    0.0451439   -0.0138145   -0.0702826    0.0176498   -0.0476696 
  0.196514     0.0846542    0.0795718   -0.0159421   -0.012152    0.0770168     0.130586    0.103603     0.116082    -0.0852864    -0.0259964    0.026814    -0.201595     0.0656125   -0.0237312   -0.0401704  -0.103609     0.21776      0.112991    -0.129943   -0.0780073    0.137819     0.0397619   -0.170151    -0.0327281    0.0128299 
 -0.0793444    0.0329764    0.120219     0.0053417    0.13572     0.0943824     0.0627629  -0.0445391    0.0590592    0.0786748    -0.00396809   0.0156678    0.0913877    0.0237271    0.0636329   -0.120624    0.0477217    0.010815    -0.0536998   -0.0159459  -0.0172722    0.0699532   -0.142817    -0.100819    -0.00624931  -0.0868888 
 -0.0953075   -0.042392     0.0939165    0.027975    -0.116315   -0.000260222  -0.024112    0.00138723   0.103597     0.0461885     0.0517444    0.0191677    0.0380705    0.082611     0.00648434  -0.0605539  -0.188793    -0.137199     0.0933254    0.116314   -0.0092099    0.00170275  -0.0264429   -0.0714558   -0.0296065    0.0411469 
  0.0952772    0.0252491   -0.0343175    0.156473    -0.0812815   0.0664478    -0.0560138   0.013896     0.13178      0.00520107    0.0256091    0.224919    -0.0489305    0.412707    -0.218184     0.0703434   0.0643334   -0.00143427  -0.0625366   -0.0222788  -0.0850329    0.0341501    0.103832    -0.17821      0.208899    -0.0534185 
  0.016201    -0.0879956    0.00327238  -0.0959946   -0.0297655  -0.106702      0.103527   -0.0691741    0.104545     0.215921     -0.134325    -0.135474    -0.171679    -0.0216153   -0.00709375  -0.0926515   0.0421495   -0.0168138    0.100756     0.113687    0.0508523    0.0123626    0.0640026   -0.0861512    0.113634    -0.0997153 
  0.0486856    0.252457     0.0291736   -0.0188219   -0.0133752  -0.0148543     0.072052    0.0325077   -0.138737     0.131177      0.00776509  -0.0804988   -0.120861     0.00424559   0.118372    -0.12038    -0.160105    -0.0237      -0.106229     0.0771094  -0.0937124   -0.028353     0.0877535    0.182627     0.123781     0.0651094 
 -0.255548    -0.0138745   -0.0935987    0.0319174   -0.165363    0.0798659     0.282298   -0.0278368    0.0944132   -0.190569      0.0354516   -0.17352      0.0784995   -0.126795     0.0533711    0.106412   -0.0983986   -0.171561     0.124344     0.0786925  -0.0614289    0.128076     0.0374493    0.00816979  -0.0273245    0.038858  
 -0.178183     0.038947     0.0240689   -0.317432    -0.0627655   0.0475473    -0.123216   -0.0131665   -0.00686406   0.0309972     0.0660103   -0.0369786   -0.143819     0.104738    -0.156922     0.151877    0.0179515    0.0467189    0.0161845   -0.0226691  -0.0812174    0.0768279   -0.0144643   -0.00942853   0.139062     0.00247118
  0.036495    -0.0605368   -0.0316316    0.0355364   -0.0314263  -0.124353     -0.0991647   0.0710852    0.0647076    0.161141     -0.0408722   -0.0116347   -0.0164865    0.0370916   -0.0367285    0.0680103   0.054404    -0.0934378    0.0482526   -0.0516338  -0.111227    -0.0414114   -0.0986721   -0.112623    -0.00611911  -0.083829  
  0.0704895    0.0163287   -0.138348    -0.084956    -0.0318575   0.0722461    -0.162837   -0.084999     0.0191428   -0.000114594  -0.0103884    0.146655    -0.0904162   -0.0855892    0.105376     0.0519133   0.0509472    0.0305081    0.0651161   -0.0275855   0.0653116   -0.136427    -0.0119676    0.00239849   0.0254904   -0.126447  
 -0.0306376   -0.135316    -0.0585526    0.0654394    0.0237423  -0.113936     -0.0439997  -0.143281     0.0571383    0.156741     -0.0355583    0.0225332    0.0601405    0.143455    -0.133603     0.136359    0.129739     0.0172291    0.00579033  -0.0905954  -0.0788718    0.0270419    0.195664     0.0746513    0.0639704    0.0444856 
 -0.103687    -0.067763    -0.181147     0.181252    -0.0244901   0.0373094    -0.0586065   0.144635    -0.0406179   -0.0842053     0.142375    -0.0355359    0.0600581   -0.169772    -0.0354675    0.0021619  -0.0285993   -0.00079123   0.26741     -0.0359894   0.0130927    0.127377    -0.0583336   -0.0142594   -0.0444778    0.103689  
 -0.0964687   -0.231617    -0.0388731   -0.278722     0.29459    -0.0420886     0.104762    0.0958289   -0.0380226   -0.0187645     0.00500487   0.00640794   0.269978     0.15771      0.060207    -0.0362895   0.0804913    0.0548503   -0.0822907    0.0782223  -0.0758275    0.0512032   -0.0806073   -0.095074     0.0822319   -0.0168371 
 -0.0531709    0.0695892   -0.0273582    0.0733956    0.0137026   0.0806161     0.0676414  -0.0601446   -0.0269807    0.0224055     0.0917661   -0.0190009    0.0149309    0.040752     0.0130198    0.121417    0.0537528    0.0161775    0.0219218   -0.01458    -0.00259793   0.0340243   -0.0563489    0.0154312   -0.00554457   0.0392294 
  0.0245793   -0.0127349    0.00698661  -0.0546226   -0.1063     -0.0680751    -0.0365829  -0.109312     0.0271134    0.123439     -0.0988435   -0.0795619    0.0254168    0.129404    -0.0121031   -0.0936488   0.0942223    0.0883101   -0.0894236   -0.0598727   0.0478897    0.0727465   -0.0547096   -0.171837     0.111129    -0.00163174
  0.120599    -0.0280656    0.0842848    0.0180018   -0.0689386   0.0645111     0.0922402   0.0883702    0.0821154   -0.149096     -0.049305     0.196871     0.0818113   -0.0311071   -0.160346    -0.0825528   0.00483764  -0.0344717   -0.0782632   -0.0927287  -0.0908472    0.0158023    0.0120949   -0.146621     0.12946     -0.134402  
  0.0135921    0.115802     0.0150398   -0.18185     -0.0122577   0.0259032    -0.0169644   0.0775516   -0.127147    -0.151949      0.0310235   -0.143459    -0.324964     0.106358    -0.118442     0.0787146  -0.0329591   -0.0374844   -0.111983    -0.0322925  -0.0278903    0.204752    -0.119992     0.00728262  -0.0778403    0.00152241
  0.0367324   -0.0141729    0.0112812   -0.100629     0.0106152  -0.107196     -0.0469579  -0.0598472    0.00978226   0.26459      -0.156969    -0.135563    -0.134469    -0.019435    -0.0153882    0.0553157   0.023744    -0.0167304    0.110134     0.0570351   0.0649129   -0.00585353   0.032339    -0.0765133    0.0037278   -0.0454337 
  0.0216865    0.0448087    0.092943    -0.0697302   -0.0560857   0.0384762     0.187753   -0.00591354  -0.00969104  -0.0572269    -0.0694465   -0.00915717   0.016749     0.146335    -0.0329248    0.0693949   0.0422223    0.00980671  -0.0463969    0.132834    0.0631716    0.111209    -0.050618    -0.188746     0.0386699   -0.12492   
 -0.00495202  -0.012806     0.0442527    0.0208097    0.0689352   0.0486994    -0.0180145   0.0494187    0.0660081    0.0367469    -0.0479244   -0.0513112   -0.0399217   -0.0687479    0.103944     0.0266673  -0.0528426    0.0545845    0.141618     0.106      -0.0879874    0.0795198   -0.0348923    0.2308      -0.0672027    0.117461  
  0.0476906    0.0806169   -0.00823674   0.0757345    0.0734492  -0.0135743     0.0317667   0.204974    -0.188179    -0.174176     -0.0426444    0.103982    -0.0889248   -0.0336207    0.0868115    0.0278461  -0.0997594   -0.0426084   -0.012096    -0.080273    0.0433865    0.111919     0.0423588    0.0644949    0.211627     0.115371  
 -0.0426301   -0.00564375   0.142393    -0.00970858  -0.104048   -0.0630807    -0.0305945  -0.228295    -0.0890494    0.0546515    -0.14139      0.247679    -0.119935     0.11523     -0.0912626   -0.175667   -0.230184     0.0918602    0.0501957   -0.0156068   0.0320861   -0.102748     0.0303701    0.216177     0.105333    -0.0711698 
 -0.156266    -0.0078415    0.0426927    0.0379043    0.0386906   0.0196772     0.0458819   0.0384634   -0.02695      0.0946861    -0.119316    -0.0484106   -0.0925472   -0.0445393    0.041189     0.103965   -0.0758369   -0.0339214    0.0501813   -0.156291    0.0524209   -0.187111    -0.0346279    0.189605     0.106057     0.0244044 
 -0.119601    -0.113714     0.0450795   -0.080996    -0.130322   -0.065439     -0.0521963   0.161121     0.0242022   -0.0309035     0.0117051    0.0962092    0.110691    -0.136958     0.183759    -0.0735335   0.0664607    0.0275084    0.057705     0.113193    0.0116368    0.144146    -0.00502025  -0.0763657    0.0410307    0.0862834 
  0.0470092   -0.102575    -0.014069    -0.0204997    0.0623721   0.0465683     0.0673667  -0.0190969   -0.148915     0.178476      0.0555686    0.0966083    0.0501071    0.0614653   -0.0358907    0.0883248  -0.0657249   -0.0844201    0.0516612    0.0040739   0.1652       0.0281889   -0.0539734    0.0299663    0.032836     0.0698174 
  0.124937    -0.00275125  -0.142111    -0.15102     -0.0376482  -0.0523858    -0.128434   -0.133549     0.00144275   0.0011757    -0.0216272   -0.0134619   -0.043597     0.27199     -0.0508181    0.0458373   0.0257234   -0.0422055   -0.0736912   -0.142252   -0.100427    -0.0411208   -0.0484964    0.132291    -0.0982518    0.0489368 
  0.0376522    0.0513525    0.00108805   0.109592     0.115179   -0.0595333    -0.274824   -0.0500722    0.0948238   -0.0863274    -0.162051     0.143226    -0.0209363    0.0691559    0.00281367   0.0435221   0.0457035   -0.191802     0.0050062   -0.114836    0.0194126   -0.0823628    0.0693145   -0.106356     0.0258733   -0.029041  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 17 21 23 31
INFO: iteration 1, average log likelihood -1.023442
WARNING: Variances had to be floored 2 6 13 15 16 17 19 21 24 28 29 31
INFO: iteration 2, average log likelihood -0.982836
WARNING: Variances had to be floored 2 3 17 21 23 24 28 31
INFO: iteration 3, average log likelihood -1.014792
WARNING: Variances had to be floored 2 6 10 13 15 16 17 19 21 24 29 31
INFO: iteration 4, average log likelihood -0.990274
WARNING: Variances had to be floored 2 17 21 23 24 28 31
INFO: iteration 5, average log likelihood -1.014134
WARNING: Variances had to be floored 2 3 6 13 15 16 17 19 21 24 28 29 31
INFO: iteration 6, average log likelihood -0.986227
WARNING: Variances had to be floored 2 17 21 23 24 31
INFO: iteration 7, average log likelihood -1.019623
WARNING: Variances had to be floored 2 6 10 13 15 16 17 19 21 24 28 29 31
INFO: iteration 8, average log likelihood -0.984974
WARNING: Variances had to be floored 2 3 17 21 23 24 28 31
INFO: iteration 9, average log likelihood -1.014888
WARNING: Variances had to be floored 2 6 13 15 16 17 19 21 24 29 31
INFO: iteration 10, average log likelihood -0.991097
INFO: EM with 100000 data points 10 iterations avll -0.991097
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0992043   -0.0597264  -0.0885673    0.0823477    0.0553835     0.0395477  -0.104027    -0.0388295   -0.045858    -0.0101731    -0.0894036    0.0165907   -0.0898556   -0.0413752    0.0237716  -0.113268      0.00114933  -0.151339    -0.221528    -0.159015    -0.0887152   -0.101425     0.0140535    -0.0939709    0.100253    -0.0840171  
 -0.107874    -0.0634606   0.0215952   -0.00409162  -0.05687       0.188337    0.0111417    0.166623     0.124349     0.141083      0.187314    -0.109075    -0.0626419    0.0475199   -0.0726259  -0.109545      0.0656263   -0.217651     0.00517156  -0.0264572    0.054723     0.131735    -0.000373086  -0.0437694   -0.0214434    0.00390859 
  0.00328494  -0.115284   -0.105878    -0.166206     0.0902731    -0.0122489  -0.108175     0.0042486   -0.047956    -0.0245207     0.112458     0.0988681   -0.107156     0.102019     0.116331    0.0630657    -0.0448314    0.136474     0.00721175   0.0158676   -0.040542    -0.0015681    0.160443     -0.0735191    0.0687203    0.0012729  
 -0.0586535   -0.0494035  -0.0713156   -0.0856942    0.0714937     0.0255893   0.0310619   -0.00303167   0.00131336   0.0683867     0.118016    -0.180274     0.190717     0.0666483   -0.072513   -0.154064      0.0151822    0.238236     0.0937649   -0.0753634   -0.0167579   -0.112859    -0.108654     -0.0138659   -0.19713      0.108486   
 -0.118875    -0.123371    0.105773    -0.0815975    0.013454      0.0117715   0.168468     0.0835025   -0.0681984   -0.0988196     0.0243429   -0.0504235    0.0921956   -0.192612     0.0778737  -0.0747731    -0.0210046   -0.152023    -0.0416545    0.00631882   0.0862789    0.0740069   -0.0090865    -0.229222     0.0912199   -0.0808399  
  0.0328954   -0.0521742  -0.0698172    0.0695819    0.0559627    -0.226138   -0.00289801  -0.0530647   -0.0152823    0.231125     -0.0671003   -0.0320393    0.0515155    0.115265    -0.114152   -0.147684      0.154599     0.118405     0.100738     0.231585    -0.123948     0.0853162    0.0218934     0.0104159    0.0328663    0.102071   
 -0.142445     0.0613152  -0.15574      0.0716382   -0.00525715   -0.0091309  -0.00347311  -0.184602     0.0413132   -0.072858      0.0598191   -0.00635333  -0.0570106    0.0384292   -0.0137694   0.146056      0.0524306    0.0269995    0.0574833   -0.184823     0.133023     0.00290103  -0.118346      0.1602       0.0953743    0.00667641 
 -0.0961054   -0.042747    0.0660331    0.0164912   -0.0193107     0.0471067   0.118847    -0.0552859   -0.18161      0.0436477    -0.0363461   -0.0626904   -0.230759    -0.105308    -0.130937   -0.141361     -0.0268721   -0.109428    -0.137281     0.018142     0.00270418   0.0116986    0.00852521    0.0171424    0.0215454    0.0530464  
 -0.035379    -0.0105693   0.0146184   -0.31352      0.122029      0.0308052   0.129895     0.0350493    0.103495    -0.0042002    -0.0613516   -0.0390871    0.0788888   -0.0603405   -0.0614871   0.0461978     0.0988156    0.0414255   -0.0987494    0.00273909  -0.0449925   -0.0887113   -0.0519051     0.0984307   -0.0586005    0.0792507  
 -0.0471899    0.0435595   0.0543643    0.0094835   -0.113714      0.0940503  -0.0263768    0.0312973    0.0576211   -0.0728095     0.0490379   -0.0654439    0.0144291    0.0465314    0.0549124   0.107756     -0.0977987    0.0577308   -0.0878868   -0.059836     0.00500169  -0.0321657   -0.0807516     0.0759014   -0.0268858   -0.060381   
 -0.254822    -0.0859322  -0.069111     0.091845     0.103345      0.042098    0.106411     0.0501265    0.0345088   -0.0725886    -0.139892     0.1542      -0.115689     0.0105802   -0.0889183  -0.0680388     0.00929957  -0.0643718   -0.0548153   -0.177733     0.124591    -0.055624     0.0878713     0.0320669    0.0976552    0.0466115  
 -0.00687443   0.0249588  -0.0922691    0.00950353  -0.0299609    -0.0638374  -0.101445     0.134492    -0.156393     0.0938292     0.00877717  -0.108628     0.171996    -0.228746     0.0613591  -0.073288     -0.0762736   -0.0785116   -0.177725     0.164134    -0.0257272   -0.0268075   -0.0425767    -0.0683217   -0.0864163   -0.0389164  
  0.110881    -0.105165   -0.00559908   0.140083     0.110087     -0.0258775  -0.0210875    0.00262218  -0.0796969   -0.00445893    0.131879    -0.0117331   -0.0203414   -0.0774533    0.1147     -0.0474192     0.113444     0.141347     0.0518571    0.0233021   -0.23786      0.07795     -0.0403313     0.0713978   -0.178149     0.0449812  
 -0.059321     0.0374121   0.034223     0.00280209   0.139463      0.0345189  -0.0420263    0.0477053   -0.0677452    0.128988      0.206583     0.0498708    0.122174     0.118807    -0.105044   -0.065328     -0.240226     0.00323576   0.0253668    0.0814993    0.00808928  -0.139363     0.0240435    -0.115451    -0.0874581   -0.0173192  
  0.148365    -0.0771203  -0.0607909    0.0597012    0.108735     -0.0632131   0.19234      0.162046    -0.00955141  -0.137175     -0.00527397   0.00605977   0.166147     0.0818989   -0.107877   -0.0950294     0.101105     0.125569    -0.0767949    0.0138647   -0.0187646    0.146846     0.116931      0.0424236   -0.00317813   0.109035   
  0.113816     0.144867    0.0623477   -0.149245    -0.0225808    -0.13459     0.00817486   0.148987    -0.0232063    0.0111669    -0.0826963    0.056011    -0.0868869    0.0505014   -0.0923854   0.102139     -0.161205     0.0273096    0.106358    -0.09392      0.012757     0.009505     0.105287     -0.0901702    0.0730206    0.144645   
 -0.0957269    0.06262     0.00258288   0.00481549   0.124997     -0.105369   -0.0658696    0.195496     0.0637989    0.17857      -0.162677    -0.0286232    0.0298579   -0.0579074    0.0727574   0.0138545     0.046333    -0.112076    -0.0299329    0.0447562   -0.0444177    0.022593    -0.127573      0.0586409   -0.0532815   -0.0659061  
  0.106455     0.162541    0.032987     0.0601078   -0.211012     -0.125294    0.114746    -0.0491046    0.0297623    0.0353167     0.0236279    0.0234289   -0.148457     0.0695535   -0.0219699  -0.004831      0.0189095    0.130939    -0.0376472    0.0484268    0.137478    -0.0726259    0.0763534    -0.136073    -0.0664237    0.000795792
  0.0412888    0.129594   -0.151827    -0.0772273   -0.067779      0.0850799  -0.0473841    0.0140131   -0.0771551   -0.206054      0.0253626   -0.135313    -0.092987     0.00991544   0.138428   -0.266972      0.0387138   -0.0975377   -0.168983    -0.135068    -0.209241     0.101773    -0.000722172  -0.0175803    0.226384     0.0558996  
 -0.263033    -0.0707007  -0.0137742   -0.108309     0.0502694     0.0287275  -0.0344819   -0.206401    -0.00714928  -0.111003     -0.0794918   -0.0775722    0.124696    -0.0309865    0.142846    0.0205719    -0.139534    -0.0238756    0.073886     0.054317    -0.0327043    0.0934546    0.0422504    -0.0953166   -0.0295152   -0.0292445  
 -0.00356087   0.0370331   0.0633081   -0.0129181   -0.0460446     0.0576669   0.0134554    0.10776     -0.00629886   0.110905     -0.070054    -0.0223576   -0.015348     0.00433921   0.0474601   0.0345138    -0.0158979   -0.0380321    0.0856354   -0.141673     0.155357     0.0119562   -0.124623     -0.00507813   0.147857     0.0347238  
 -0.0291084   -0.124819    0.128855    -0.0289631   -0.106523     -0.134898    0.0404435    0.0378566    0.0742981   -0.123323      0.101526    -0.113919     0.0657738   -0.0318931    0.150813    0.0548428    -0.227031    -0.173827    -0.0780356    0.0894673    0.225653    -0.0390363    0.0282397     0.103635    -0.0102451   -0.0287127  
  0.0139161   -0.0153445  -0.00254592  -0.0684835   -0.185597      0.129973    0.0288157    0.038996    -0.0190982    0.0217565    -0.00226059   0.0358008   -0.0366694    0.0983303   -0.120965   -0.0115806     0.0691429   -0.0269454   -0.117287     0.080564     0.0517717    0.0589093   -0.0294928     0.0334941   -0.0115431    0.0461898  
  0.0935942    0.0615382  -0.123306     0.058801     0.0395011    -0.159712   -0.0248461   -0.0140083    0.0167583   -0.260672     -0.100379    -0.0171284    0.0561441   -0.0485092    0.107662   -0.118212     -0.0478524   -0.153742     0.0141667    0.104948    -0.0269625    0.0230089    0.0454409     0.111575     0.222848    -0.0958988  
  0.108367    -0.226116   -0.177455     0.0513896    0.121326     -0.0769619  -0.229616    -0.124083    -0.0549256    0.259868      0.108906     0.0320988    0.0900109   -0.0898178   -0.11054     0.223713      0.289152     0.110104     0.0418418   -0.0282011   -0.0721272    0.0171305   -0.0600953     0.0975777    0.0283439   -0.076399   
 -0.0425377    0.108837    0.0786087   -0.0419699   -0.0799222     0.103979   -0.0584637    0.099395     0.0444653    0.0765756    -0.0815847    0.154493    -0.0944886    0.144286     0.126879   -0.0591917    -0.0668967    0.0535947    0.00558845  -0.0657177   -0.0267731   -0.0813238    0.224471     -0.0840671   -0.0683259    0.00655422 
 -0.0583949    0.0882031   0.0603346    0.127567    -0.0913877    -0.0984603   0.146911     0.00349234   0.0531047   -0.0242772     0.184194    -0.0165434    0.130975    -0.0766479   -0.148658   -0.106799     -0.0377693    0.134575     0.0372744    0.0724256    0.00920987  -0.0990484    0.105972      0.155244    -0.00571727   0.139337   
  0.0983774    0.0109953  -0.0743941   -0.0977141    0.140036     -0.107942    0.198067    -0.0359854    0.0463173    0.0325819    -0.0560143    0.0579054    0.071046     0.056842    -0.0286105   0.0938496     0.0928271   -0.0624644    0.130259    -0.0638527    0.0860176   -0.141253    -0.0995485     0.159222    -0.0186257   -0.0381154  
  0.0660125    0.0312526   0.126613    -0.0230627   -0.000161051   0.0487957   0.203757    -0.0824294    0.0268348   -0.0404353     0.155843     0.0619681   -0.0609093    0.0847441    0.0402566  -0.0399124    -0.123795     0.033063    -0.0528937    0.184854     0.102468    -0.0301981    0.142008     -0.0611779    0.0600992    0.019601   
 -0.157486     0.0738337  -0.0259328   -0.0990085   -0.0229504    -0.0328884   0.130819    -0.0169369    0.0835436   -0.0999395     0.0792985    0.0200449    0.0632904   -0.0618986   -0.0179199   0.0395072    -0.126158    -0.0414725    0.0436583   -0.0485358    0.0263863    0.0585052    0.135223     -0.0292715   -0.0342381    0.0145608  
 -0.0352876   -0.0563271  -0.0781685   -0.0580665   -0.037513      0.142795    0.0704967   -0.0313817    0.0278427    0.000737068  -0.0904998    0.0419078   -0.0384603   -0.100212    -0.0766548   0.0920959    -0.0236416   -0.0464014   -0.0684465    0.179288     0.109573     0.0793326    0.0780478    -0.0686453   -0.0378153    0.145813   
  0.0542911    0.0315187  -0.0330025   -0.0296951    0.0445242    -0.161185   -0.241974     0.0934713   -0.0229885    0.0848138    -0.0386291    0.20339      5.01243e-5   0.00400502  -0.0330468   0.000880021  -0.005876     0.00450589   0.0947974   -0.045422    -0.0667559   -0.0281229   -0.0319234    -0.0622234    0.0512718   -0.152152   kind full, method split
0: avll = -1.4310496398663124
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.431068
INFO: iteration 2, average log likelihood -1.431010
INFO: iteration 3, average log likelihood -1.430963
INFO: iteration 4, average log likelihood -1.430901
INFO: iteration 5, average log likelihood -1.430817
INFO: iteration 6, average log likelihood -1.430695
INFO: iteration 7, average log likelihood -1.430508
INFO: iteration 8, average log likelihood -1.430198
INFO: iteration 9, average log likelihood -1.429670
INFO: iteration 10, average log likelihood -1.428847
INFO: iteration 11, average log likelihood -1.427815
INFO: iteration 12, average log likelihood -1.426868
INFO: iteration 13, average log likelihood -1.426243
INFO: iteration 14, average log likelihood -1.425917
INFO: iteration 15, average log likelihood -1.425767
INFO: iteration 16, average log likelihood -1.425699
INFO: iteration 17, average log likelihood -1.425669
INFO: iteration 18, average log likelihood -1.425655
INFO: iteration 19, average log likelihood -1.425648
INFO: iteration 20, average log likelihood -1.425645
INFO: iteration 21, average log likelihood -1.425643
INFO: iteration 22, average log likelihood -1.425642
INFO: iteration 23, average log likelihood -1.425642
INFO: iteration 24, average log likelihood -1.425641
INFO: iteration 25, average log likelihood -1.425641
INFO: iteration 26, average log likelihood -1.425641
INFO: iteration 27, average log likelihood -1.425641
INFO: iteration 28, average log likelihood -1.425640
INFO: iteration 29, average log likelihood -1.425640
INFO: iteration 30, average log likelihood -1.425640
INFO: iteration 31, average log likelihood -1.425640
INFO: iteration 32, average log likelihood -1.425640
INFO: iteration 33, average log likelihood -1.425640
INFO: iteration 34, average log likelihood -1.425639
INFO: iteration 35, average log likelihood -1.425639
INFO: iteration 36, average log likelihood -1.425639
INFO: iteration 37, average log likelihood -1.425639
INFO: iteration 38, average log likelihood -1.425639
INFO: iteration 39, average log likelihood -1.425639
INFO: iteration 40, average log likelihood -1.425639
INFO: iteration 41, average log likelihood -1.425639
INFO: iteration 42, average log likelihood -1.425639
INFO: iteration 43, average log likelihood -1.425639
INFO: iteration 44, average log likelihood -1.425639
INFO: iteration 45, average log likelihood -1.425639
INFO: iteration 46, average log likelihood -1.425639
INFO: iteration 47, average log likelihood -1.425639
INFO: iteration 48, average log likelihood -1.425639
INFO: iteration 49, average log likelihood -1.425639
INFO: iteration 50, average log likelihood -1.425639
INFO: EM with 100000 data points 50 iterations avll -1.425639
952.4 data points per parameter
1: avll = [-1.43107,-1.43101,-1.43096,-1.4309,-1.43082,-1.4307,-1.43051,-1.4302,-1.42967,-1.42885,-1.42782,-1.42687,-1.42624,-1.42592,-1.42577,-1.4257,-1.42567,-1.42565,-1.42565,-1.42565,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.425653
INFO: iteration 2, average log likelihood -1.425588
INFO: iteration 3, average log likelihood -1.425527
INFO: iteration 4, average log likelihood -1.425448
INFO: iteration 5, average log likelihood -1.425343
INFO: iteration 6, average log likelihood -1.425207
INFO: iteration 7, average log likelihood -1.425044
INFO: iteration 8, average log likelihood -1.424866
INFO: iteration 9, average log likelihood -1.424693
INFO: iteration 10, average log likelihood -1.424540
INFO: iteration 11, average log likelihood -1.424416
INFO: iteration 12, average log likelihood -1.424319
INFO: iteration 13, average log likelihood -1.424246
INFO: iteration 14, average log likelihood -1.424192
INFO: iteration 15, average log likelihood -1.424152
INFO: iteration 16, average log likelihood -1.424123
INFO: iteration 17, average log likelihood -1.424101
INFO: iteration 18, average log likelihood -1.424084
INFO: iteration 19, average log likelihood -1.424072
INFO: iteration 20, average log likelihood -1.424062
INFO: iteration 21, average log likelihood -1.424054
INFO: iteration 22, average log likelihood -1.424048
INFO: iteration 23, average log likelihood -1.424043
INFO: iteration 24, average log likelihood -1.424038
INFO: iteration 25, average log likelihood -1.424034
INFO: iteration 26, average log likelihood -1.424030
INFO: iteration 27, average log likelihood -1.424027
INFO: iteration 28, average log likelihood -1.424024
INFO: iteration 29, average log likelihood -1.424022
INFO: iteration 30, average log likelihood -1.424019
INFO: iteration 31, average log likelihood -1.424017
INFO: iteration 32, average log likelihood -1.424015
INFO: iteration 33, average log likelihood -1.424012
INFO: iteration 34, average log likelihood -1.424011
INFO: iteration 35, average log likelihood -1.424009
INFO: iteration 36, average log likelihood -1.424007
INFO: iteration 37, average log likelihood -1.424005
INFO: iteration 38, average log likelihood -1.424004
INFO: iteration 39, average log likelihood -1.424002
INFO: iteration 40, average log likelihood -1.424001
INFO: iteration 41, average log likelihood -1.423999
INFO: iteration 42, average log likelihood -1.423998
INFO: iteration 43, average log likelihood -1.423997
INFO: iteration 44, average log likelihood -1.423995
INFO: iteration 45, average log likelihood -1.423994
INFO: iteration 46, average log likelihood -1.423993
INFO: iteration 47, average log likelihood -1.423992
INFO: iteration 48, average log likelihood -1.423991
INFO: iteration 49, average log likelihood -1.423990
INFO: iteration 50, average log likelihood -1.423989
INFO: EM with 100000 data points 50 iterations avll -1.423989
473.9 data points per parameter
2: avll = [-1.42565,-1.42559,-1.42553,-1.42545,-1.42534,-1.42521,-1.42504,-1.42487,-1.42469,-1.42454,-1.42442,-1.42432,-1.42425,-1.42419,-1.42415,-1.42412,-1.4241,-1.42408,-1.42407,-1.42406,-1.42405,-1.42405,-1.42404,-1.42404,-1.42403,-1.42403,-1.42403,-1.42402,-1.42402,-1.42402,-1.42402,-1.42401,-1.42401,-1.42401,-1.42401,-1.42401,-1.42401,-1.424,-1.424,-1.424,-1.424,-1.424,-1.424,-1.424,-1.42399,-1.42399,-1.42399,-1.42399,-1.42399,-1.42399]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.424000
INFO: iteration 2, average log likelihood -1.423945
INFO: iteration 3, average log likelihood -1.423897
INFO: iteration 4, average log likelihood -1.423841
INFO: iteration 5, average log likelihood -1.423771
INFO: iteration 6, average log likelihood -1.423686
INFO: iteration 7, average log likelihood -1.423587
INFO: iteration 8, average log likelihood -1.423479
INFO: iteration 9, average log likelihood -1.423368
INFO: iteration 10, average log likelihood -1.423262
INFO: iteration 11, average log likelihood -1.423166
INFO: iteration 12, average log likelihood -1.423082
INFO: iteration 13, average log likelihood -1.423010
INFO: iteration 14, average log likelihood -1.422948
INFO: iteration 15, average log likelihood -1.422893
INFO: iteration 16, average log likelihood -1.422844
INFO: iteration 17, average log likelihood -1.422800
INFO: iteration 18, average log likelihood -1.422759
INFO: iteration 19, average log likelihood -1.422722
INFO: iteration 20, average log likelihood -1.422687
INFO: iteration 21, average log likelihood -1.422655
INFO: iteration 22, average log likelihood -1.422625
INFO: iteration 23, average log likelihood -1.422598
INFO: iteration 24, average log likelihood -1.422573
INFO: iteration 25, average log likelihood -1.422550
INFO: iteration 26, average log likelihood -1.422528
INFO: iteration 27, average log likelihood -1.422508
INFO: iteration 28, average log likelihood -1.422490
INFO: iteration 29, average log likelihood -1.422473
INFO: iteration 30, average log likelihood -1.422456
INFO: iteration 31, average log likelihood -1.422441
INFO: iteration 32, average log likelihood -1.422426
INFO: iteration 33, average log likelihood -1.422412
INFO: iteration 34, average log likelihood -1.422398
INFO: iteration 35, average log likelihood -1.422385
INFO: iteration 36, average log likelihood -1.422372
INFO: iteration 37, average log likelihood -1.422360
INFO: iteration 38, average log likelihood -1.422348
INFO: iteration 39, average log likelihood -1.422336
INFO: iteration 40, average log likelihood -1.422325
INFO: iteration 41, average log likelihood -1.422313
INFO: iteration 42, average log likelihood -1.422302
INFO: iteration 43, average log likelihood -1.422292
INFO: iteration 44, average log likelihood -1.422281
INFO: iteration 45, average log likelihood -1.422271
INFO: iteration 46, average log likelihood -1.422261
INFO: iteration 47, average log likelihood -1.422252
INFO: iteration 48, average log likelihood -1.422243
INFO: iteration 49, average log likelihood -1.422234
INFO: iteration 50, average log likelihood -1.422225
INFO: EM with 100000 data points 50 iterations avll -1.422225
236.4 data points per parameter
3: avll = [-1.424,-1.42395,-1.4239,-1.42384,-1.42377,-1.42369,-1.42359,-1.42348,-1.42337,-1.42326,-1.42317,-1.42308,-1.42301,-1.42295,-1.42289,-1.42284,-1.4228,-1.42276,-1.42272,-1.42269,-1.42265,-1.42263,-1.4226,-1.42257,-1.42255,-1.42253,-1.42251,-1.42249,-1.42247,-1.42246,-1.42244,-1.42243,-1.42241,-1.4224,-1.42239,-1.42237,-1.42236,-1.42235,-1.42234,-1.42232,-1.42231,-1.4223,-1.42229,-1.42228,-1.42227,-1.42226,-1.42225,-1.42224,-1.42223,-1.42222]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422226
INFO: iteration 2, average log likelihood -1.422161
INFO: iteration 3, average log likelihood -1.422101
INFO: iteration 4, average log likelihood -1.422033
INFO: iteration 5, average log likelihood -1.421951
INFO: iteration 6, average log likelihood -1.421853
INFO: iteration 7, average log likelihood -1.421740
INFO: iteration 8, average log likelihood -1.421619
INFO: iteration 9, average log likelihood -1.421495
INFO: iteration 10, average log likelihood -1.421376
INFO: iteration 11, average log likelihood -1.421264
INFO: iteration 12, average log likelihood -1.421162
INFO: iteration 13, average log likelihood -1.421069
INFO: iteration 14, average log likelihood -1.420986
INFO: iteration 15, average log likelihood -1.420911
INFO: iteration 16, average log likelihood -1.420844
INFO: iteration 17, average log likelihood -1.420784
INFO: iteration 18, average log likelihood -1.420731
INFO: iteration 19, average log likelihood -1.420683
INFO: iteration 20, average log likelihood -1.420640
INFO: iteration 21, average log likelihood -1.420601
INFO: iteration 22, average log likelihood -1.420566
INFO: iteration 23, average log likelihood -1.420534
INFO: iteration 24, average log likelihood -1.420504
INFO: iteration 25, average log likelihood -1.420477
INFO: iteration 26, average log likelihood -1.420452
INFO: iteration 27, average log likelihood -1.420429
INFO: iteration 28, average log likelihood -1.420408
INFO: iteration 29, average log likelihood -1.420388
INFO: iteration 30, average log likelihood -1.420369
INFO: iteration 31, average log likelihood -1.420352
INFO: iteration 32, average log likelihood -1.420335
INFO: iteration 33, average log likelihood -1.420320
INFO: iteration 34, average log likelihood -1.420305
INFO: iteration 35, average log likelihood -1.420292
INFO: iteration 36, average log likelihood -1.420279
INFO: iteration 37, average log likelihood -1.420266
INFO: iteration 38, average log likelihood -1.420254
INFO: iteration 39, average log likelihood -1.420243
INFO: iteration 40, average log likelihood -1.420232
INFO: iteration 41, average log likelihood -1.420222
INFO: iteration 42, average log likelihood -1.420212
INFO: iteration 43, average log likelihood -1.420202
INFO: iteration 44, average log likelihood -1.420193
INFO: iteration 45, average log likelihood -1.420184
INFO: iteration 46, average log likelihood -1.420175
INFO: iteration 47, average log likelihood -1.420167
INFO: iteration 48, average log likelihood -1.420159
INFO: iteration 49, average log likelihood -1.420151
INFO: iteration 50, average log likelihood -1.420143
INFO: EM with 100000 data points 50 iterations avll -1.420143
118.1 data points per parameter
4: avll = [-1.42223,-1.42216,-1.4221,-1.42203,-1.42195,-1.42185,-1.42174,-1.42162,-1.4215,-1.42138,-1.42126,-1.42116,-1.42107,-1.42099,-1.42091,-1.42084,-1.42078,-1.42073,-1.42068,-1.42064,-1.4206,-1.42057,-1.42053,-1.4205,-1.42048,-1.42045,-1.42043,-1.42041,-1.42039,-1.42037,-1.42035,-1.42034,-1.42032,-1.42031,-1.42029,-1.42028,-1.42027,-1.42025,-1.42024,-1.42023,-1.42022,-1.42021,-1.4202,-1.42019,-1.42018,-1.42018,-1.42017,-1.42016,-1.42015,-1.42014]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420145
INFO: iteration 2, average log likelihood -1.420074
INFO: iteration 3, average log likelihood -1.420004
INFO: iteration 4, average log likelihood -1.419919
INFO: iteration 5, average log likelihood -1.419810
INFO: iteration 6, average log likelihood -1.419672
INFO: iteration 7, average log likelihood -1.419502
INFO: iteration 8, average log likelihood -1.419306
INFO: iteration 9, average log likelihood -1.419096
INFO: iteration 10, average log likelihood -1.418885
INFO: iteration 11, average log likelihood -1.418686
INFO: iteration 12, average log likelihood -1.418505
INFO: iteration 13, average log likelihood -1.418345
INFO: iteration 14, average log likelihood -1.418205
INFO: iteration 15, average log likelihood -1.418083
INFO: iteration 16, average log likelihood -1.417977
INFO: iteration 17, average log likelihood -1.417885
INFO: iteration 18, average log likelihood -1.417805
INFO: iteration 19, average log likelihood -1.417735
INFO: iteration 20, average log likelihood -1.417673
INFO: iteration 21, average log likelihood -1.417619
INFO: iteration 22, average log likelihood -1.417571
INFO: iteration 23, average log likelihood -1.417528
INFO: iteration 24, average log likelihood -1.417489
INFO: iteration 25, average log likelihood -1.417454
INFO: iteration 26, average log likelihood -1.417423
INFO: iteration 27, average log likelihood -1.417394
INFO: iteration 28, average log likelihood -1.417367
INFO: iteration 29, average log likelihood -1.417342
INFO: iteration 30, average log likelihood -1.417318
INFO: iteration 31, average log likelihood -1.417296
INFO: iteration 32, average log likelihood -1.417276
INFO: iteration 33, average log likelihood -1.417256
INFO: iteration 34, average log likelihood -1.417237
INFO: iteration 35, average log likelihood -1.417220
INFO: iteration 36, average log likelihood -1.417203
INFO: iteration 37, average log likelihood -1.417186
INFO: iteration 38, average log likelihood -1.417171
INFO: iteration 39, average log likelihood -1.417155
INFO: iteration 40, average log likelihood -1.417141
INFO: iteration 41, average log likelihood -1.417127
INFO: iteration 42, average log likelihood -1.417113
INFO: iteration 43, average log likelihood -1.417100
INFO: iteration 44, average log likelihood -1.417087
INFO: iteration 45, average log likelihood -1.417075
INFO: iteration 46, average log likelihood -1.417063
INFO: iteration 47, average log likelihood -1.417051
INFO: iteration 48, average log likelihood -1.417040
INFO: iteration 49, average log likelihood -1.417029
INFO: iteration 50, average log likelihood -1.417019
INFO: EM with 100000 data points 50 iterations avll -1.417019
59.0 data points per parameter
5: avll = [-1.42015,-1.42007,-1.42,-1.41992,-1.41981,-1.41967,-1.4195,-1.41931,-1.4191,-1.41889,-1.41869,-1.41851,-1.41834,-1.4182,-1.41808,-1.41798,-1.41789,-1.4178,-1.41773,-1.41767,-1.41762,-1.41757,-1.41753,-1.41749,-1.41745,-1.41742,-1.41739,-1.41737,-1.41734,-1.41732,-1.4173,-1.41728,-1.41726,-1.41724,-1.41722,-1.4172,-1.41719,-1.41717,-1.41716,-1.41714,-1.41713,-1.41711,-1.4171,-1.41709,-1.41707,-1.41706,-1.41705,-1.41704,-1.41703,-1.41702]
[-1.43105,-1.43107,-1.43101,-1.43096,-1.4309,-1.43082,-1.4307,-1.43051,-1.4302,-1.42967,-1.42885,-1.42782,-1.42687,-1.42624,-1.42592,-1.42577,-1.4257,-1.42567,-1.42565,-1.42565,-1.42565,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42564,-1.42565,-1.42559,-1.42553,-1.42545,-1.42534,-1.42521,-1.42504,-1.42487,-1.42469,-1.42454,-1.42442,-1.42432,-1.42425,-1.42419,-1.42415,-1.42412,-1.4241,-1.42408,-1.42407,-1.42406,-1.42405,-1.42405,-1.42404,-1.42404,-1.42403,-1.42403,-1.42403,-1.42402,-1.42402,-1.42402,-1.42402,-1.42401,-1.42401,-1.42401,-1.42401,-1.42401,-1.42401,-1.424,-1.424,-1.424,-1.424,-1.424,-1.424,-1.424,-1.42399,-1.42399,-1.42399,-1.42399,-1.42399,-1.42399,-1.424,-1.42395,-1.4239,-1.42384,-1.42377,-1.42369,-1.42359,-1.42348,-1.42337,-1.42326,-1.42317,-1.42308,-1.42301,-1.42295,-1.42289,-1.42284,-1.4228,-1.42276,-1.42272,-1.42269,-1.42265,-1.42263,-1.4226,-1.42257,-1.42255,-1.42253,-1.42251,-1.42249,-1.42247,-1.42246,-1.42244,-1.42243,-1.42241,-1.4224,-1.42239,-1.42237,-1.42236,-1.42235,-1.42234,-1.42232,-1.42231,-1.4223,-1.42229,-1.42228,-1.42227,-1.42226,-1.42225,-1.42224,-1.42223,-1.42222,-1.42223,-1.42216,-1.4221,-1.42203,-1.42195,-1.42185,-1.42174,-1.42162,-1.4215,-1.42138,-1.42126,-1.42116,-1.42107,-1.42099,-1.42091,-1.42084,-1.42078,-1.42073,-1.42068,-1.42064,-1.4206,-1.42057,-1.42053,-1.4205,-1.42048,-1.42045,-1.42043,-1.42041,-1.42039,-1.42037,-1.42035,-1.42034,-1.42032,-1.42031,-1.42029,-1.42028,-1.42027,-1.42025,-1.42024,-1.42023,-1.42022,-1.42021,-1.4202,-1.42019,-1.42018,-1.42018,-1.42017,-1.42016,-1.42015,-1.42014,-1.42015,-1.42007,-1.42,-1.41992,-1.41981,-1.41967,-1.4195,-1.41931,-1.4191,-1.41889,-1.41869,-1.41851,-1.41834,-1.4182,-1.41808,-1.41798,-1.41789,-1.4178,-1.41773,-1.41767,-1.41762,-1.41757,-1.41753,-1.41749,-1.41745,-1.41742,-1.41739,-1.41737,-1.41734,-1.41732,-1.4173,-1.41728,-1.41726,-1.41724,-1.41722,-1.4172,-1.41719,-1.41717,-1.41716,-1.41714,-1.41713,-1.41711,-1.4171,-1.41709,-1.41707,-1.41706,-1.41705,-1.41704,-1.41703,-1.41702]
32×26 Array{Float64,2}:
 -0.867963    0.119291     0.363911    -0.244325     0.276971    -0.250737    -0.0571456   0.550277   -0.0347574    0.201108     0.681302    0.276761   -0.645923      0.169677    -0.250889    -0.102585   -0.303442    -0.285098   -0.369838     0.145618     0.187738     0.0736391   0.0306271   0.0310738    0.326406    0.0493945  
 -0.619412   -0.00923676   0.587116    -0.147404     0.279375    -0.4108      -0.119733    0.18749    -0.272663    -0.20866      0.839663    0.341588   -0.424723     -0.176491    -0.458958    -0.574675    0.672503    -0.109249    0.0173311    0.482651     0.216959    -0.273566   -0.215546   -0.582876     0.384225    0.194793   
 -0.0479381   0.434892    -0.122213     0.238683     0.445351    -0.295198     0.335018    0.326722    0.237154     0.00949369   0.422       0.57017    -0.163938     -0.363052    -0.557278     0.148334    0.484659     0.463712    0.447838     0.600701     0.205839     0.0342016   0.52754     0.37497     -0.553585   -0.284996   
  0.312967    0.118149    -0.213653     0.453011     0.264003    -0.653298     0.211221    0.224946   -0.0982014   -0.180206     0.485862   -0.60247    -0.81004      -0.148492    -0.46683      0.357273    0.208309    -0.116275   -0.619325     1.15969     -0.580499    -0.186227    0.089329    0.00853316  -0.455791    0.431209   
 -0.019249    0.0361427   -0.244624    -0.0203255    0.217058    -0.126933     0.212955    0.0487453  -0.110506    -0.103645     0.0316788  -0.049645   -0.0289102    -0.206252    -0.168045     0.0722329  -0.115502    -0.155803    0.00913051   0.0492214   -0.214657     0.0132588   0.0782838   0.0471492   -0.219906    0.135732   
  0.157137   -0.0811932    0.166167     0.0881423   -0.163063    -0.269452    -0.130878    0.252678   -0.216065    -0.0260147   -0.144124    0.242295    0.127771      0.115549     0.216345    -0.0159597   0.246837     0.0727665  -0.0784309   -0.0842149    0.412945     0.0125175   0.0982123  -0.0632767    0.303715    0.120426   
 -0.125768   -0.0334557    0.130757    -0.0200949   -0.0188191    0.22524     -0.122782   -0.278091    0.235078     0.299606     0.0794629  -0.0116949   0.178079      0.0613219    0.136084    -0.322163   -0.156405     0.215277    0.228783    -0.170261     0.100659    -0.119842   -0.184685   -0.534166     0.125452   -0.211806   
 -0.205315    0.307428     0.142915    -0.200345    -0.407881     0.148045    -0.0364087  -0.141389    0.341903    -0.0968644    0.170352   -0.0419289  -0.352012      0.204447     0.0785203   -0.0413585   0.00854426   0.127179   -0.215627     0.208007    -0.0182186    0.234869    0.160446    0.494527    -0.0286855  -0.165393   
 -0.0180718  -0.699358    -0.304761    -0.269053    -0.425216    -0.0718052    0.0769898   0.380403   -0.51645      0.22176     -0.0912068  -0.384864   -0.385151      0.171382    -0.48815     -0.28032    -0.23194      0.363787    0.376133    -0.13885     -0.272674    -0.0761974   0.0324938   0.510278    -0.131725   -0.0547488  
 -0.0330097   0.0149833   -0.312476     0.0387877    0.262522     0.249162     0.521227   -0.305768   -0.729469    -0.214969     0.0294881  -0.198652   -0.293304      0.233982    -0.164656     0.889113   -0.114671    -0.224803    0.835034     0.468604    -1.02343      0.0758094  -0.340836    0.235476     0.0910223   0.0735915  
  0.0754134   0.408683     0.331586    -0.364385     0.240509     0.201479     0.301093    0.0699913   0.139422    -0.425115    -0.253583   -0.23307    -0.477008      0.271716     0.15555      0.908499   -0.181784    -0.28173    -0.332919     0.279876     0.476841    -0.0794638  -0.299192    0.258766     0.121557    0.0361221  
  0.381343   -0.0946125    0.396814     0.689952     0.039892     0.19256      0.392034    0.163152   -0.419545     0.0169023   -0.636123    0.205381   -0.0456115     0.281723     0.235486     0.585097    0.423212    -0.448861   -0.31449     -0.0733858    0.338935    -0.597971    0.496083    0.117633     0.205141    0.491815   
 -0.200353   -0.196379     0.0300555    0.136662     0.37196     -0.163688     0.318613   -0.219887   -0.0837505   -0.0577957   -0.322092   -0.114226    0.665961     -0.303219    -0.52372      0.297613    0.275964    -0.394772   -0.0274947   -0.216103    -0.148127     0.335873    0.0412284  -0.476473    -0.123204    0.0681474  
 -0.540654   -0.0545973   -0.681673     0.54433      0.557874    -0.0103699   -0.569747    0.222748   -0.00618365  -0.243845     0.065508    0.139245    0.453959     -0.560514    -0.21383     -0.117986    0.015788    -0.264207   -0.0426727   -0.294339    -0.0760014    0.0181411  -0.18173     0.614594     0.172274   -0.245862   
  0.254978    0.178847    -0.406567     0.194965    -0.157498    -0.0995949   -0.212783   -0.395047   -0.112026    -0.973749    -0.583144   -0.0312164   0.64634      -0.143542     0.269499     0.236638   -0.100132     0.169936    0.0490442    0.126457    -0.217412     0.322043   -0.141869    0.364241    -0.0204893   0.470214   
  0.763421   -0.120406    -0.569165     0.10648     -0.170251     0.25186     -0.0304926  -0.10464     0.136244     0.345838    -0.726949   -0.107335    0.516175     -0.0588855    0.439631     0.377122   -0.423792    -0.0475752   0.0277246   -0.267003    -0.263689     0.181082    0.203287    0.179671    -0.463737   -0.240021   
 -0.377748   -0.303852    -0.025976    -0.162866     0.169504     0.116855     1.03731     0.3054     -0.263708     0.246725     0.0424577  -0.225641   -0.0224574     0.238866    -0.0831835   -0.235245    0.554804    -0.595512   -0.143703    -0.3958       0.54447     -0.679752   -0.468951   -0.368937    -0.195183   -0.288614   
 -0.536408   -0.553203     0.00821049  -0.321828    -0.480348     0.527161     0.302312   -0.4375      0.225024     0.307458    -0.220665   -0.472637    0.207494      0.835875    -0.152356    -0.358118    0.233246     0.285221   -0.285469     0.503709     0.130096    -0.667086   -0.240422    0.00660122  -0.12865    -0.410874   
  0.0928431   0.0403763    0.167308    -0.0231541   -0.0932348   -0.450173    -0.0879156   0.0399019   0.641262     0.741857     0.370629    0.268051    0.0761817    -0.302279    -0.240025    -0.926082    0.148795     0.226375   -0.478768    -0.229149     0.391763    -0.410261    1.00607    -0.25759      0.0427618  -0.202967   
 -0.0448503   0.219363     0.729923    -0.00687193   0.0691482    0.367216    -0.67042     0.907087    0.37898      0.380697    -0.0686754  -0.130626   -0.415676     -0.0303712   -0.252727    -0.577873   -0.0749649    0.758006    0.195835    -0.83902      0.377327     0.0862005  -0.234611   -0.263248     0.454746    0.246792   
  0.491709    0.146623    -0.168389     0.18475      0.0630732   -0.00505979  -0.602179    0.128356   -0.114067    -0.0099175   -0.052629    0.251906   -0.461699      0.384905     0.64214     -0.41238    -0.248935     0.247165    0.0265313    0.628088    -0.00342559  -0.876811   -0.0759622   0.63282      0.341796    0.16415    
  0.584477    0.0213354    0.621845    -0.299628    -0.807648    -0.0230501   -0.0106681   0.0284445  -0.12364      0.194527     0.260864    0.374083   -0.0285848     0.74221      0.838353    -0.0306848   0.199766     0.397161   -0.0201531    0.161303     0.358396     0.131361    0.341226   -0.459202     0.126071    0.000827523
 -0.139207   -0.129532    -0.115713     0.106019    -0.363238     0.40112     -0.300453   -0.332014    0.272501     0.119507    -0.443268    0.0318776   1.00226       0.15521      0.576135    -0.303507    0.0987369    0.555195    0.528509    -0.901929     0.40665      0.0311984  -0.265245   -0.0487447    0.270856   -0.169285   
 -0.0643852  -0.389644    -0.0409223   -0.659757    -0.390478     0.267179    -0.139024   -0.421157   -0.142747    -0.0596543   -0.287148   -0.532564    0.388683      0.14178      0.6178       0.0421913  -0.845177    -0.46947    -0.626422    -0.507915    -0.253763     0.0316502  -0.514706   -0.480024     0.556005    0.510581   
 -0.455543    0.19291     -0.0162261   -0.206315     0.302873     0.253982     0.0700923  -0.604347    0.845342    -0.0247827    0.358006   -0.381864   -0.261604     -0.709368    -0.615931    -0.60924    -0.664624    -0.0553317  -0.150462    -0.227177    -0.797832     0.131525   -0.210364   -0.252335    -0.109494   -0.192665   
 -0.431705    0.545765    -0.21269     -0.867233    -0.306228    -0.0477435   -0.143366   -0.543659    0.575012     0.238473     0.255003   -0.15928    -0.626515      0.324801    -0.175814    -0.281546   -0.300293     0.10311     0.624043    -0.113919     0.0421683    0.377293   -0.8914      0.21385     -0.50788    -0.292218   
 -0.582376   -0.0318177   -0.18082     -0.173354    -0.367327    -0.167743     0.464717    0.0888956   0.273304    -0.296562     0.758267    0.124136    0.366347     -0.526075    -0.0855464    0.0988894   0.179928     0.064976   -0.0826822   -0.244077    -0.170064     1.21895     0.195222   -0.131996    -0.295943   -0.750973   
  0.522733    0.278563    -0.0225004    0.0174195   -0.00125917  -0.057263    -0.74424     0.21959    -0.0234078   -0.109515     0.239729    0.39763    -0.274692     -0.630181     0.00956041   0.20613    -0.47598     -0.0807353  -0.0482437   -0.00542547  -0.40885      0.977236    0.737422    0.113283     0.206807    0.362215   
 -0.107986    0.0430385    0.466489    -0.254992    -0.510527    -0.0426259   -0.0777017   0.13739     0.0308606    0.0252249    0.146777    0.0929868  -0.678634      0.459652     0.098549     0.0141042  -0.0551946    0.0481357  -0.4464       0.529219     0.131055    -0.0226583   0.207853    0.294772     0.208976   -0.0765773  
 -0.169818   -0.069692    -0.201941     0.375145     0.604101    -0.0500337    0.211574   -0.0170016  -0.151842    -0.24445     -0.23816    -0.197348    0.354183     -0.442588    -0.462088     0.258895    0.108841    -0.337773    0.104482    -0.135325    -0.236666     0.0619134  -0.0394014   0.00786429   0.0289802   0.201672   
  0.809052   -0.008327    -0.173183    -0.0516909    0.816444     0.097178     0.451414    0.196068   -0.264432     0.0479593    0.240457    0.052613   -0.000154124  -0.475945     0.693249     0.260871   -0.273385    -0.13009     0.206844    -0.46824      0.241958    -0.350922   -0.338462   -0.862712    -0.272873    0.261949   
  0.384635   -0.149735    -0.930089     0.186027     0.610655    -0.37207     -0.216546    0.0449556  -0.447081     0.512745     0.198332    0.18439     0.678379      0.00867474  -0.127709    -1.16119    -0.0847859   -0.100227    0.298269     0.31722     -0.465506    -0.493939   -0.0246981  -0.112574    -0.541916    0.274363   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417009
INFO: iteration 2, average log likelihood -1.416999
INFO: iteration 3, average log likelihood -1.416990
INFO: iteration 4, average log likelihood -1.416981
INFO: iteration 5, average log likelihood -1.416972
INFO: iteration 6, average log likelihood -1.416964
INFO: iteration 7, average log likelihood -1.416956
INFO: iteration 8, average log likelihood -1.416948
INFO: iteration 9, average log likelihood -1.416940
INFO: iteration 10, average log likelihood -1.416933
INFO: EM with 100000 data points 10 iterations avll -1.416933
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.894447e+05
      1       7.233361e+05      -1.661086e+05 |       32
      2       7.046156e+05      -1.872059e+04 |       32
      3       6.978102e+05      -6.805342e+03 |       32
      4       6.946284e+05      -3.181859e+03 |       32
      5       6.927986e+05      -1.829745e+03 |       32
      6       6.915014e+05      -1.297209e+03 |       32
      7       6.905148e+05      -9.866018e+02 |       32
      8       6.897088e+05      -8.060430e+02 |       32
      9       6.890036e+05      -7.051658e+02 |       32
     10       6.883821e+05      -6.214851e+02 |       32
     11       6.878241e+05      -5.580242e+02 |       32
     12       6.873197e+05      -5.043811e+02 |       32
     13       6.868970e+05      -4.226796e+02 |       32
     14       6.865547e+05      -3.423477e+02 |       32
     15       6.862464e+05      -3.083101e+02 |       32
     16       6.860013e+05      -2.450392e+02 |       32
     17       6.858040e+05      -1.972864e+02 |       32
     18       6.856257e+05      -1.782902e+02 |       32
     19       6.854726e+05      -1.531688e+02 |       32
     20       6.853379e+05      -1.346453e+02 |       32
     21       6.852151e+05      -1.228041e+02 |       32
     22       6.850878e+05      -1.272894e+02 |       32
     23       6.849511e+05      -1.367827e+02 |       32
     24       6.848260e+05      -1.250972e+02 |       32
     25       6.847162e+05      -1.097422e+02 |       32
     26       6.846170e+05      -9.917642e+01 |       32
     27       6.845283e+05      -8.876582e+01 |       32
     28       6.844533e+05      -7.493979e+01 |       32
     29       6.843766e+05      -7.669965e+01 |       32
     30       6.842963e+05      -8.033388e+01 |       32
     31       6.842146e+05      -8.168495e+01 |       32
     32       6.841430e+05      -7.160132e+01 |       32
     33       6.840740e+05      -6.904595e+01 |       32
     34       6.839976e+05      -7.632937e+01 |       32
     35       6.839299e+05      -6.777284e+01 |       32
     36       6.838601e+05      -6.975171e+01 |       32
     37       6.837892e+05      -7.093023e+01 |       32
     38       6.837256e+05      -6.357759e+01 |       32
     39       6.836696e+05      -5.598883e+01 |       32
     40       6.836204e+05      -4.923739e+01 |       32
     41       6.835765e+05      -4.383794e+01 |       32
     42       6.835348e+05      -4.176314e+01 |       32
     43       6.834933e+05      -4.149093e+01 |       32
     44       6.834529e+05      -4.041138e+01 |       32
     45       6.834153e+05      -3.756003e+01 |       32
     46       6.833785e+05      -3.677220e+01 |       32
     47       6.833427e+05      -3.586467e+01 |       32
     48       6.833116e+05      -3.106152e+01 |       32
     49       6.832829e+05      -2.869242e+01 |       32
     50       6.832541e+05      -2.882521e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 683254.0990785917)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.428744
INFO: iteration 2, average log likelihood -1.423914
INFO: iteration 3, average log likelihood -1.422767
INFO: iteration 4, average log likelihood -1.422036
INFO: iteration 5, average log likelihood -1.421246
INFO: iteration 6, average log likelihood -1.420330
INFO: iteration 7, average log likelihood -1.419474
INFO: iteration 8, average log likelihood -1.418875
INFO: iteration 9, average log likelihood -1.418522
INFO: iteration 10, average log likelihood -1.418310
INFO: iteration 11, average log likelihood -1.418164
INFO: iteration 12, average log likelihood -1.418052
INFO: iteration 13, average log likelihood -1.417960
INFO: iteration 14, average log likelihood -1.417882
INFO: iteration 15, average log likelihood -1.417815
INFO: iteration 16, average log likelihood -1.417756
INFO: iteration 17, average log likelihood -1.417703
INFO: iteration 18, average log likelihood -1.417656
INFO: iteration 19, average log likelihood -1.417613
INFO: iteration 20, average log likelihood -1.417574
INFO: iteration 21, average log likelihood -1.417538
INFO: iteration 22, average log likelihood -1.417504
INFO: iteration 23, average log likelihood -1.417474
INFO: iteration 24, average log likelihood -1.417445
INFO: iteration 25, average log likelihood -1.417418
INFO: iteration 26, average log likelihood -1.417393
INFO: iteration 27, average log likelihood -1.417369
INFO: iteration 28, average log likelihood -1.417346
INFO: iteration 29, average log likelihood -1.417325
INFO: iteration 30, average log likelihood -1.417304
INFO: iteration 31, average log likelihood -1.417285
INFO: iteration 32, average log likelihood -1.417266
INFO: iteration 33, average log likelihood -1.417247
INFO: iteration 34, average log likelihood -1.417230
INFO: iteration 35, average log likelihood -1.417212
INFO: iteration 36, average log likelihood -1.417195
INFO: iteration 37, average log likelihood -1.417178
INFO: iteration 38, average log likelihood -1.417162
INFO: iteration 39, average log likelihood -1.417146
INFO: iteration 40, average log likelihood -1.417130
INFO: iteration 41, average log likelihood -1.417114
INFO: iteration 42, average log likelihood -1.417098
INFO: iteration 43, average log likelihood -1.417083
INFO: iteration 44, average log likelihood -1.417068
INFO: iteration 45, average log likelihood -1.417053
INFO: iteration 46, average log likelihood -1.417038
INFO: iteration 47, average log likelihood -1.417024
INFO: iteration 48, average log likelihood -1.417010
INFO: iteration 49, average log likelihood -1.416996
INFO: iteration 50, average log likelihood -1.416982
INFO: EM with 100000 data points 50 iterations avll -1.416982
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.613088    -0.0842483    0.551233    -0.379494    -0.738771     0.0348792   -0.0363735   -0.0721549  -0.164877     0.427097     0.175153    0.275295     0.0562267   0.586694     0.682319   -0.248365    0.00996711   0.497674     0.155123     0.00162599   0.248052    0.051262     0.173103   -0.520935     0.154854      0.020331 
 -0.840717     0.00146338   0.386482    -0.0815978    0.394623    -0.362157    -0.00819802   0.35439    -0.233182    -0.0458771    0.856705    0.353792    -0.456997   -0.165514    -0.529633   -0.433722    0.332368    -0.268067    -0.0955369    0.327649     0.157686   -0.153276    -0.100063   -0.344475     0.404736      0.177821 
  0.170965     0.410181     0.487173    -0.064846    -0.184616    -0.560316    -0.459075     0.512379    0.182882     0.24116     -0.30079     0.655597     0.1479     -0.249924    -0.119591   -0.595465    0.10041      0.0728986   -0.262686    -0.582534     0.840439    0.26387      0.355798   -0.384016     0.56243       0.400439 
 -0.237913    -0.654351     0.109753     0.432154    -0.600822    -0.0597892    0.366652     0.455881    0.152898    -0.499017    -0.0712267  -0.386037     0.269372   -0.605074     0.0240978   0.561047   -0.16092      0.302954    -0.376894    -0.376907     0.191166    0.11275     -0.25207    -0.0485818    0.620948      0.234207 
  0.0722227    0.192116     0.303951    -0.0955626    0.203163     0.408211    -0.748679     0.473857    0.296016     0.00679616   0.116942   -0.313442    -0.221006   -0.0569448   -0.172681   -0.587637   -0.368265     0.770145     0.427502    -0.801224    -0.141646   -0.0467009   -0.523377    0.00553522   0.525265      0.0412733
 -0.410012     0.310571    -0.175292     0.283792    -0.266852     0.487531    -0.0548052   -0.51551     0.870377     0.163951     0.0912126  -0.15931      0.583771    0.033741     0.568263   -0.168907    0.0724681    0.587283     0.258654    -0.516995     0.274222   -0.0262233   -0.512502    0.110209    -0.260363     -0.608077 
  0.521352     0.172931     0.518045    -0.0837088   -0.138201    -0.0783542    0.0999886    0.291904   -0.158059    -0.551436    -0.302382   -0.0608237   -0.819435    0.610461     0.600703    0.587128   -0.00837575  -0.0364623   -0.269927     0.475087     0.487641   -0.287352     0.0129146   0.425178     0.146917      0.0915963
 -0.242699    -0.252664     0.263933    -0.692049    -0.553643     0.201498    -0.159885    -0.259172    0.205826     0.241091    -0.334886   -0.57882      0.130289    0.505582     0.267653   -0.24013    -0.402581    -0.282273    -1.20724     -0.0286476    0.105479   -0.255127    -0.0361637  -0.109042     0.207313      0.21858  
  0.785442    -0.0183725   -0.541283     0.164002    -0.103461     0.026893    -0.0160666   -0.206808    0.0712449   -0.0625292   -0.955248   -0.277246     0.81291    -0.0755299    0.346769    0.439074   -0.188468     0.0698069    0.142583    -0.333441    -0.114507    0.114299    -0.0746252   0.135417    -0.323263      0.0723863
  0.596159     0.0742704    0.264883     0.510257     0.891185     0.398579     0.0805685    0.029214   -0.153705     0.32755     -0.0469764   0.203681     0.278865   -0.163063     0.425148    0.0463698   0.191833    -0.0610499    0.209746    -0.362731     0.451873   -0.612988     0.10342    -0.65296      0.0844521     0.353419 
 -0.0137269    0.270969     0.283648     0.3958      -0.188049    -0.344814     0.133073    -0.400112    0.0603394   -0.746008     0.211048    0.460836     0.554078    0.0204033    0.442112    0.458713    0.731672    -0.325077    -0.332824     0.201096     0.207743    0.526191     0.381952   -0.108048    -0.104344      0.096347 
  0.293853    -0.752315    -0.459659     0.0736779   -0.116728     0.0953588   -0.129745     0.251609   -0.576346     0.32091     -0.194252   -0.0107627   -0.299451    0.666278     0.0246524  -0.610134   -0.302547     0.271679     0.339577     0.340009    -0.0140796  -0.918204    -0.0472594   0.406316     0.0803306     0.190494 
  0.234877     0.0267297   -0.255811     0.0662213   -0.037591    -0.685454     0.262586     0.249828    0.625779     0.408494     0.575635   -0.0668968   -0.148914   -0.440841    -0.364731   -0.710538    0.118695     0.341133    -0.473736    -0.0549764    0.0424944  -0.417347     0.977169    0.0172781   -0.630451     -0.171166 
  0.00814179   0.529528    -0.145302     0.14267      0.451319    -0.248391     0.362665     0.351408    0.193974    -0.0200794    0.460511    0.656615    -0.129776   -0.466799    -0.677425    0.204741    0.429095     0.527734     0.593272     0.553696     0.134501    0.233831     0.430657    0.266794    -0.58174      -0.39747  
 -0.181326     0.44733      0.508534    -0.0827525    0.317787     0.361896     0.431857    -0.0529465   0.00754267  -0.0816251   -0.190807   -0.00189193  -0.38534     0.124076    -0.228157    0.698971   -0.0350416   -0.526494    -0.224112     0.0111983    0.360888   -0.256355    -0.221994    0.0837924    0.0530205     0.234462 
  0.751237     0.311958    -0.0192277    0.580287     0.199551    -0.766969     0.0802488    0.198716   -0.189764    -0.0305665    0.744505   -0.805316    -1.18085     0.00399137  -0.499476    0.499055    0.321284    -0.12184     -0.579728     1.42317     -0.779707   -0.307773    -0.23621     0.0315138   -0.283803      0.436117 
  0.129138    -0.198481    -0.0849114    0.0716137    0.220265    -0.255143     0.101068     0.503236   -0.82606     -0.124286    -0.0315928   0.296828    -0.270627   -0.417191    -0.327721    0.541001   -0.080685    -0.53784     -0.00727602   0.154342    -0.608509    0.451291     0.907504    0.14746      0.158294      0.506289 
 -0.0831125    0.179076    -1.04451      0.460254     0.412333    -0.241592    -0.769267     0.0387617  -0.150433     0.179692     0.17806     0.496146     0.40091    -0.354563     0.219933   -0.699467   -0.333545    -0.243527     0.0123586    0.159277    -0.358414   -0.0564881    0.0104205   0.431273     0.000284747  -0.241431 
 -0.0249146   -0.178493    -0.21028      0.0241088    0.129754    -0.338292     0.0817904    0.248207   -0.354328    -0.124415     0.0516596   0.0293136    0.0144005  -0.180797    -0.296965    0.0280972   0.0347936   -0.0262988    0.128192     0.00715511  -0.208987    0.131623     0.0826368   0.10377     -0.115305      0.0854421
 -0.248263    -0.040489    -0.0121504   -0.217089    -0.0737981    0.0651956    0.102097     0.119646    0.130376     0.105491     0.126956   -0.139802    -0.0666189   0.40441      0.226883   -0.303995    0.362816     0.0220363   -0.143053     0.0277038    0.583017   -0.306639    -0.200404    0.0337743    0.0168058    -0.24196  
 -0.0331378    0.0204816   -0.00856739   0.00375231  -0.161396     9.144e-5     0.106843     0.0114216  -0.179836    -0.257579     0.0312135  -0.0155515    0.0343578   0.122369     0.129206    0.0585955   0.0181264    0.0388889    0.0234553    0.0543524   -0.0584495   0.0974611   -0.134025    0.110171     0.0609271     0.064977 
  0.314501    -0.0937339    0.0692045    0.278719    -0.0935103    0.101067    -0.125651    -0.0156482  -0.0544388    0.111998    -0.356224    0.136968     0.196604    0.0130109    0.247887    0.144689    0.0128165    0.00244676  -0.0803359   -0.0795455    0.129409   -0.0520648    0.17373    -0.0647459    0.150758      0.0517128
  0.718402     0.0769763   -0.026223    -0.210961    -0.288129     0.273688    -0.771927    -0.171237    0.245658     0.0923087    0.193832    0.223882    -0.179287   -0.581529     0.057396    0.236901   -0.892807     0.237077     0.0535081   -0.142483    -0.587135    1.22189      0.481582   -0.0315398   -0.0535064    -0.0161203
 -0.463264     0.0741962    0.0837242   -0.051643     0.00334192   0.145535     0.0904591   -0.313224    0.393595     0.215553     0.271075   -0.0533917    0.07209    -0.257026    -0.242953   -0.303495   -0.212103    -0.00898078   0.0452414   -0.136329    -0.206191    0.134001    -0.024869   -0.364169    -0.0451077    -0.181286 
  0.417892    -0.0149831   -0.459413    -0.422881     0.433102     0.0621107    0.538722    -0.178921   -0.313944    -0.186215     0.052146   -0.222064     0.185103   -0.166812     0.571612    0.319569   -0.509802    -0.471495     0.00839663  -0.246194    -0.183484   -0.140597    -0.49829    -0.659593    -0.146022      0.196184 
 -0.0561867    0.590453    -0.240695    -0.110395    -0.0285219   -0.00479312  -0.25547     -0.111099    0.270168    -0.498445     0.112699   -0.0350193   -0.23935    -0.103449     0.214921    0.135119   -0.262747     0.00340129  -0.181337     0.317099    -0.168362    0.316807     0.0860899   0.57061     -0.0653612     0.214486 
 -0.686138    -0.732014    -0.190082    -0.175917    -0.242634     0.372123    -0.130847    -0.418617   -0.0720431   -0.0247821   -0.853146    0.382939     1.1382      0.14229      0.226403   -0.554386    0.108645     0.317163     0.664668    -0.741149     0.213299    0.432074     0.146165   -0.0268902    0.359402     -0.118488 
 -0.460425     0.198516    -0.0371239   -1.0604      -0.446626    -0.225086     0.055997    -0.319495    0.342387     0.201405     0.501939   -0.206971    -0.691931    0.322653    -0.286855   -0.376366   -0.219342     0.0512888    0.420266     0.0998998   -0.0212265   0.376528    -0.608441    0.123902    -0.503113     -0.306033 
 -0.40973      0.129658     0.69724     -0.0984777   -0.465805     0.240658    -0.502155     0.227003    0.42117      0.553599     0.394364    0.430201    -0.557094    0.499716    -0.038212   -0.167975    0.279439     0.398617    -0.263257     0.461594     0.198409    0.00472014   0.633001    0.255187     0.683723     -0.345762 
 -0.546263    -0.0121389   -0.362618     0.317173     0.895249    -0.165711    -0.0293297   -0.099294    0.070974    -0.250871    -0.296223   -0.434458     0.349551   -0.731294    -0.855182    0.159582    0.0311728   -0.524851    -0.0254583   -0.147996    -0.327808    0.225483    -0.28662     0.0488254   -0.24251       0.171815 
 -0.350442    -0.558537     0.0536633   -0.00427693   0.095543     0.0465684    1.06909      0.177427   -0.321518     0.350403    -0.0804802  -0.111292     0.214168    0.237543    -0.322448   -0.217056    0.614008    -0.475296    -0.105702    -0.280463     0.454151   -0.46221     -0.244932   -0.5569      -0.170855     -0.39561  
 -0.244248    -0.333644    -0.316871    -0.116784    -0.0870168    0.358754     0.46653     -0.343856   -0.270425    -0.0819382   -0.203274   -0.514532    -0.0357887   0.300154    -0.403435    0.444667   -0.0393663    0.0619137    0.279107     0.362375    -0.860351   -0.0715059   -0.166518    0.379496    -0.16593      -0.190752 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416969
INFO: iteration 2, average log likelihood -1.416955
INFO: iteration 3, average log likelihood -1.416942
INFO: iteration 4, average log likelihood -1.416930
INFO: iteration 5, average log likelihood -1.416917
INFO: iteration 6, average log likelihood -1.416905
INFO: iteration 7, average log likelihood -1.416892
INFO: iteration 8, average log likelihood -1.416880
INFO: iteration 9, average log likelihood -1.416869
INFO: iteration 10, average log likelihood -1.416857
INFO: EM with 100000 data points 10 iterations avll -1.416857
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
