>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.1.2
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.6
INFO: Installing StatsBase v0.10.0
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.9392929077148438 GB (673.0234375 MB free)
Uptime: 23864.0 sec
Load Avg:  1.05908203125  1.04248046875  1.0478515625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3503 MHz    1428729 s       7042 s     159428 s     538782 s         50 s
#2  3503 MHz     688004 s         91 s      89636 s    1516865 s          2 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.7.1
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.1.2
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.6
 - StatsBase                     0.10.0
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-9.160966691338731e6,[90651.9,9348.06],
[-6021.8 12003.9 -6011.64; 6111.09 -11643.9 5418.42],

Array{Float64,2}[
[88654.9 5681.69 -2173.97; 5681.69 83262.8 3377.99; -2173.97 3377.99 88296.6],

[11772.2 -6213.57 2672.62; -6213.57 16664.9 -3778.47; 2672.62 -3778.47 11250.9]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.410156e+03
      1       9.572376e+02      -4.529181e+02 |        6
      2       9.052391e+02      -5.199848e+01 |        2
      3       9.014339e+02      -3.805206e+00 |        2
      4       8.965260e+02      -4.907853e+00 |        2
      5       8.872708e+02      -9.255221e+00 |        2
      6       8.863451e+02      -9.257196e-01 |        0
      7       8.863451e+02       0.000000e+00 |        0
K-means converged with 7 iterations (objv = 886.3450672714462)
INFO: K-means with 272 data points using 7 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.086357
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.868155
INFO: iteration 2, lowerbound -3.761779
INFO: iteration 3, lowerbound -3.645670
INFO: iteration 4, lowerbound -3.494919
INFO: iteration 5, lowerbound -3.312696
INFO: iteration 6, lowerbound -3.117445
INFO: iteration 7, lowerbound -2.942218
INFO: dropping number of Gaussions to 7
INFO: iteration 8, lowerbound -2.806543
INFO: dropping number of Gaussions to 6
INFO: iteration 9, lowerbound -2.717670
INFO: dropping number of Gaussions to 5
INFO: iteration 10, lowerbound -2.665135
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.619139
INFO: iteration 12, lowerbound -2.572658
INFO: iteration 13, lowerbound -2.531585
INFO: iteration 14, lowerbound -2.490767
INFO: iteration 15, lowerbound -2.452026
INFO: iteration 16, lowerbound -2.416155
INFO: iteration 17, lowerbound -2.382945
INFO: iteration 18, lowerbound -2.352358
INFO: iteration 19, lowerbound -2.326544
INFO: iteration 20, lowerbound -2.310504
INFO: iteration 21, lowerbound -2.308115
INFO: dropping number of Gaussions to 2
INFO: iteration 22, lowerbound -2.302915
INFO: iteration 23, lowerbound -2.299259
INFO: iteration 24, lowerbound -2.299256
INFO: iteration 25, lowerbound -2.299254
INFO: iteration 26, lowerbound -2.299254
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Mon 03 Oct 2016 11:07:25 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Mon 03 Oct 2016 11:07:27 AM UTC: K-means with 272 data points using 7 iterations
11.3 data points per parameter
,Mon 03 Oct 2016 11:07:29 AM UTC: EM with 272 data points 0 iterations avll -2.086357
5.8 data points per parameter
,Mon 03 Oct 2016 11:07:29 AM UTC: GMM converted to Variational GMM
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 1, lowerbound -3.868155
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 2, lowerbound -3.761779
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 3, lowerbound -3.645670
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 4, lowerbound -3.494919
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 5, lowerbound -3.312696
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 6, lowerbound -3.117445
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 7, lowerbound -2.942218
,Mon 03 Oct 2016 11:07:32 AM UTC: dropping number of Gaussions to 7
,Mon 03 Oct 2016 11:07:32 AM UTC: iteration 8, lowerbound -2.806543
,Mon 03 Oct 2016 11:07:33 AM UTC: dropping number of Gaussions to 6
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 9, lowerbound -2.717670
,Mon 03 Oct 2016 11:07:33 AM UTC: dropping number of Gaussions to 5
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 10, lowerbound -2.665135
,Mon 03 Oct 2016 11:07:33 AM UTC: dropping number of Gaussions to 3
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 11, lowerbound -2.619139
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 12, lowerbound -2.572658
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 13, lowerbound -2.531585
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 14, lowerbound -2.490767
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 15, lowerbound -2.452026
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 16, lowerbound -2.416155
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 17, lowerbound -2.382945
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 18, lowerbound -2.352358
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 19, lowerbound -2.326544
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 20, lowerbound -2.310504
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 21, lowerbound -2.308115
,Mon 03 Oct 2016 11:07:33 AM UTC: dropping number of Gaussions to 2
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 22, lowerbound -2.302915
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 23, lowerbound -2.299259
,Mon 03 Oct 2016 11:07:33 AM UTC: iteration 24, lowerbound -2.299256
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 25, lowerbound -2.299254
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 26, lowerbound -2.299254
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 27, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 28, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 29, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 30, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 31, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 32, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 33, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 34, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 35, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 36, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:34 AM UTC: iteration 37, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 38, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 39, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 40, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 41, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 42, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 43, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 44, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 45, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 46, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 47, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 48, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 49, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: iteration 50, lowerbound -2.299253
,Mon 03 Oct 2016 11:07:35 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000004
avll from stats: -1.0143151249151698
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -1.0143151249151698
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -1.0143151249151698
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -1.0025729754384485
avll from llpg:  -1.0025729754384483
avll direct:     -1.0025729754384483
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0114589    0.0894328   -0.078174     0.0882019   -0.0142279    0.0454698    0.0721029   -0.0904075   -0.327936    -0.181048     0.0355406   -0.0730291   -0.125417    -0.144165    -0.0124785    -0.13765     -0.00471642   0.118597     0.122598      0.203934     0.0338121   -0.211016    -0.106584   -0.0292797    0.0778543    -0.0139886 
  0.10249      0.132921     0.0521422    0.139137     0.0325358   -0.0488374   -0.00887105   0.149932    -0.184111    -0.069948    -0.0368485    0.0171906    0.0244989   -0.00453766   0.153262     -0.0437491    0.00104309   0.0178634   -0.0768926    -0.0418073   -0.116968    -0.0608964   -0.136969    0.0152478    0.0784108     0.0318577 
 -0.0906702   -0.0185622    0.00198342   0.174304    -0.019505    -0.0588828    0.131981     0.0951579    0.0563594    0.0692849    0.0202161   -0.0703467    0.0179795   -0.0360783    0.0509313    -0.00969328  -0.213379    -0.190461    -0.0762252    -0.0586758    0.0914497    0.0161765    0.125377   -0.113201     0.000660086  -0.0261792 
 -0.0406964   -0.0580707    0.158135    -0.00812734   0.039315    -0.173249     0.0844329    0.0526968   -0.0411416   -0.124067     0.0511927   -0.0972469    0.0317907   -0.0120568   -0.0598001    -0.090714     0.183549     0.0433866   -0.153325     -0.0290213    0.0849765   -0.130281    -0.0508947   0.059175     0.0507771     0.0629442 
 -0.177225     0.0468882    0.00141196  -0.095652     0.0227259    0.0865777    0.0220509    0.219634    -0.0109049   -0.135156    -0.105354    -0.0221945   -0.109655     0.0628376    0.0917979    -0.17373     -0.0575441   -0.00459409  -0.0496326     0.087097     0.0286375   -0.0472787   -0.0261514  -0.0681899   -0.00422695   -0.150261  
 -0.115632    -0.0229346   -0.024529    -0.139158     0.148176     0.00885188  -0.128093     0.107872    -0.0312528   -0.059894    -0.0581895   -0.00920781   0.0382417   -0.0817737    0.0607502    -0.135013     0.265369    -0.137142     0.04083      -0.0980337   -0.13977     -0.021584     0.0284141   0.133283    -0.0305788    -0.00879696
 -0.16545      0.0978608    0.150588    -0.0252376    0.130652     0.0797445   -0.0198533    0.112036    -0.0953246   -0.0382882   -0.0175496    0.033607     0.111933    -0.0255436    0.15261       0.0131784   -0.0113224    0.0739745    0.0103277     0.0236999   -0.0466989    0.133219    -0.0264424   0.0456787    0.220556      0.0106599 
 -0.0425433    0.020767    -0.10749     -0.125072    -0.0564288    0.0340255   -0.0920535    0.0151414    0.0364165   -0.0376985   -0.137395    -0.052107     0.0357851    0.0851224    0.0550821    -0.0992119   -0.0375173   -0.0459727    0.00639505   -0.0892096   -0.0934746    0.113206     0.01069     0.153701    -0.0962671    -0.0848152 
 -0.01484      0.117557    -0.0511983   -0.00288624   0.0612652   -0.136068     0.0187598    0.04559     -0.0701174    0.0270576   -0.00840991  -0.0972215    0.186436    -0.0150401   -0.17999       0.150014    -0.119726     0.0238618    0.0842871     0.0479565   -0.0192454   -0.0677751   -0.0273623  -0.172126     0.0377089     0.0360847 
  0.0742229    0.013836    -0.117352     0.173751    -0.00770117   0.039253    -0.0715394    0.017506    -0.0176665    0.109432    -0.063304    -0.135925    -0.0543383   -0.157807     0.122055      0.158004     0.138742    -0.0601953   -0.0175511     0.170068     0.0995083   -0.00682304   0.0545554   0.298622     0.0775447     0.112331  
 -0.0646207    0.0677816   -0.167157     0.130884     0.271768     0.12446     -0.120599     0.169637     0.00126035  -0.0960341    0.0951407    0.116752     0.0448831    0.0402651   -0.0274113     0.0858585   -0.0549135    0.040778     0.000289826  -0.0591123   -0.0968318    0.101123     0.198653   -0.00667071  -0.0911392     0.0063768 
  0.0248136   -0.051488     0.0389588   -0.00610515   0.0631664    0.0385986   -0.025989     0.0717566   -0.14865      0.145444     0.055218    -0.176943    -0.0453894   -0.157903    -0.056451      0.0607451    0.0252756   -0.045876     0.205394     -0.0284796   -0.142628     0.163887    -0.0264347   0.0685802   -0.195301      0.156118  
  0.0350804   -0.019641    -0.0251822    0.0212045   -0.0260955    0.235005     0.00733569  -0.0890813   -0.00558509   0.00335368  -0.0785667   -0.0353318   -0.0154869   -0.00973471   0.16076       0.0939508   -0.0721598   -0.0300877    0.0512994    -0.150448     0.116353     0.139876     0.0988112   0.0233704   -0.107664     -0.149971  
 -0.0202912    0.194211    -0.00474536   0.00670006   0.0915345   -0.0908736   -0.07128     -0.101395    -0.119717     0.0851618    0.0136869    0.0203151    0.0682715   -0.019194    -0.0740199     0.0517743   -0.0389631   -0.148817     0.0632217    -0.099237    -0.140328    -0.0840237    0.133701   -0.121294     0.0128774     0.0439312 
  0.00976598  -0.00692749  -0.107268     0.109457    -0.153807    -0.111941     0.0513493   -0.00563534   0.0613145   -0.0472282    0.171345     0.0973466   -0.0286787   -0.117591     0.208835     -0.138026     0.0438476   -0.172037    -0.00524557    0.0437485   -0.0390146   -0.0741498    0.0566008  -0.0818278    0.0881969    -0.164878  
  0.121841     0.0128695    0.0618869   -0.222498     0.0479984   -0.0224126   -0.0139273   -0.0666282    0.0951817   -0.273548    -0.12763      0.0226169   -0.284848    -0.0343616   -0.063596     -0.106051    -0.0626025    0.0826282    0.0391864    -0.0298063    0.0251756   -0.0111973   -0.115799   -0.176103    -0.117669     -0.0608435 
  0.0803213    0.0608681   -0.0339485   -0.0177249   -0.109394     0.169117     0.200699    -0.162649    -0.00927258  -0.10312      0.0872486   -0.186179    -0.074426    -0.118023    -0.0811781     0.0985708   -0.0640191    0.0547621   -0.0399138    -0.123946    -0.00645475  -5.02498e-5  -0.0147064  -0.179645     0.0556233    -0.131371  
 -0.00585234   0.109282    -0.101273    -0.153736     0.0781087   -0.0872004   -0.0169989    0.0782043    0.294171    -0.150467     0.0699494    0.00275197   0.0854857   -0.0363076    0.000661184   0.0368399    0.0348929    0.00362421   0.13518       0.0478014   -0.0755222   -0.0407599    0.0398725  -0.0604134    0.037202      0.0471572 
  0.119569     0.0109062    0.137579     0.0908368    0.0363386    0.0110492   -0.0132145   -0.0975866    0.0575217   -0.0878779    0.0582193   -0.0738057   -0.172795    -0.0504073   -0.0725727    -0.115574    -0.0438123   -0.017504     0.0178546     0.0288333    0.015283     0.0847174   -0.153388   -0.123568     0.125299      0.149012  
 -0.109365     0.0211938    0.0385873    0.0670606    0.0279079   -0.0739361   -0.0805424   -0.0333518    0.165188    -0.0120698   -0.0537412   -0.0029871    0.17016      0.00809541   0.107171      0.0556513    0.099198    -0.173349    -0.051607     -0.128211    -0.0762148    0.0445472   -0.0790142  -0.159749     0.103287      0.0298388 
 -0.0884337    0.216269     0.0976353   -0.0186971    0.122202     0.0376688   -0.0907431   -0.0338199    0.0950722   -0.0314415   -0.0907741   -0.0380959   -0.0230234    0.101998     0.0687831    -0.0341478    0.0497034   -0.0164075   -0.0275523     0.0781833   -0.00860241  -0.149792    -0.0166901  -0.0765107    0.0534042    -0.082544  
 -0.112585    -0.0675239    4.76791e-5   0.0139133   -0.0132238   -0.0425798   -0.0758024    0.0838327   -0.0426753   -0.0303347   -0.0474429    0.00617233  -0.180679     0.273772    -0.0997859    -0.0090353   -0.0192113   -0.102405     0.0692888    -0.0128746    0.056065     0.0380496    0.1075      0.0684991    0.0203135    -0.0175475 
 -0.150903    -0.00240904  -0.0517196    0.138492    -0.00544955   0.00452832  -0.0189137   -0.151647    -0.141863     0.00315864   0.0203847    0.25394     -0.0575814   -0.0697981    0.0956159     0.0257721    0.0976229    0.143805     0.0176815     0.0177454    0.080437     0.0464506   -0.101691    0.236568    -0.0870697     0.0765785 
 -0.0698876   -0.0291059    0.0134674    0.146782     0.0502539    0.00320846  -0.0362975   -0.103718    -0.137046    -0.0676698   -0.153068     0.00854373  -0.0379954   -0.037479     0.0581045    -0.00281472  -0.137986    -0.0197006    0.097159      0.00330557  -0.116336     0.179286     0.13864     0.0621978    0.0177079    -0.117192  
 -0.160106    -0.0307207   -0.0440543    0.047346    -0.123359     0.0800607    0.128687    -0.0845517    0.187443    -0.125612     0.00474173   0.249242     0.0680281   -0.123928    -0.059663      0.12587      0.109802    -0.207524    -0.0770804     0.120318     0.04966     -0.00609694  -0.0451882   0.0155714    0.0847732     0.0182812 
  0.0679683   -0.0701401    0.0835145    0.00478937  -0.0806254    0.0982224    0.0767617   -0.00209641   0.0154714   -0.0122338   -0.0728598    0.046179    -0.00168129  -0.0600914    0.067914     -0.0623684    0.0238283    0.0478095   -0.121123     -0.00817748  -0.0236079   -0.0520463    0.0168966  -0.0812527   -0.155017      0.107775  
  0.0418689    0.165075    -0.201207    -0.0704353    0.0970305    0.0221275    0.0894335   -0.185671    -0.00531775  -0.114695     0.0324415    0.0770623   -0.0287146   -0.0536367    0.117074      0.159201     0.108089     0.0331114    0.0736552    -0.167806     0.0207483   -0.0503979    0.118649    0.110663    -0.0432913    -0.0339079 
 -0.0042279   -0.0354525   -0.103754    -0.0687009   -0.10716      0.0310471    0.0430299   -0.0555941   -0.0608074   -0.00968564   0.085404    -0.0612976    0.128841    -0.14853      0.105599      0.251041     0.0225016   -0.19545     -0.00655044    0.0717956    0.141582     0.0593392   -0.0426606  -0.0144021    0.0446778     0.03034   
  0.0579308   -0.0569622    0.136468    -0.0170707    0.101158     0.00322066   0.118747    -0.0479962   -0.0266736   -0.00928255   0.0476301    0.233323     0.0491121   -0.246455     0.0778224     0.0337086   -0.0692134   -0.00689152   0.0394848     0.157665    -0.0112479    0.065827     0.108463    0.133504     0.0871894    -0.112187  
  0.0333865    0.0907918    0.0663257    0.224154    -0.0419012   -0.0783951    0.0528397    0.0157478    0.0407662   -0.0268162    0.105744     0.0651954    0.0573668    0.0374962   -0.156828      0.179279     0.03023     -0.133116    -0.0207629    -0.0351494    0.0525935   -0.0117729    0.0715112  -0.0113805    0.0972006     0.0447441 
  0.033396    -0.0473278   -0.131149     0.0218662    0.128432     0.0387866    0.0436139   -0.00615541   0.0605315    0.158199    -0.0843113    0.0443209    0.0981856   -0.026063     0.0258434    -0.227698     0.0708189    0.0972586   -0.0367983     0.322303    -0.0122887    0.0163187   -0.0503918   0.108266     0.0958915    -4.5786e-5 
 -0.0852846   -0.0251352    0.00240305  -0.00940793   0.0810073    0.0596958   -0.169248    -0.0120631   -0.129839    -0.252186    -0.0851466   -0.0851147    0.113417    -0.0905484   -0.026046     -0.0361202   -0.145509    -0.0764067   -0.000686929   0.111944    -0.017051    -0.15316     -0.251431    0.154626     0.142579     -0.19704   kind diag, method split
0: avll = -1.464282843587964
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.464412
INFO: iteration 2, average log likelihood -1.464314
INFO: iteration 3, average log likelihood -1.463933
INFO: iteration 4, average log likelihood -1.459419
INFO: iteration 5, average log likelihood -1.446230
INFO: iteration 6, average log likelihood -1.439027
INFO: iteration 7, average log likelihood -1.437323
INFO: iteration 8, average log likelihood -1.436501
INFO: iteration 9, average log likelihood -1.435966
INFO: iteration 10, average log likelihood -1.435628
INFO: iteration 11, average log likelihood -1.435402
INFO: iteration 12, average log likelihood -1.435229
INFO: iteration 13, average log likelihood -1.435081
INFO: iteration 14, average log likelihood -1.434936
INFO: iteration 15, average log likelihood -1.434770
INFO: iteration 16, average log likelihood -1.434518
INFO: iteration 17, average log likelihood -1.434093
INFO: iteration 18, average log likelihood -1.433723
INFO: iteration 19, average log likelihood -1.433507
INFO: iteration 20, average log likelihood -1.433364
INFO: iteration 21, average log likelihood -1.433253
INFO: iteration 22, average log likelihood -1.433163
INFO: iteration 23, average log likelihood -1.433090
INFO: iteration 24, average log likelihood -1.433034
INFO: iteration 25, average log likelihood -1.432993
INFO: iteration 26, average log likelihood -1.432964
INFO: iteration 27, average log likelihood -1.432944
INFO: iteration 28, average log likelihood -1.432929
INFO: iteration 29, average log likelihood -1.432919
INFO: iteration 30, average log likelihood -1.432911
INFO: iteration 31, average log likelihood -1.432906
INFO: iteration 32, average log likelihood -1.432901
INFO: iteration 33, average log likelihood -1.432898
INFO: iteration 34, average log likelihood -1.432896
INFO: iteration 35, average log likelihood -1.432894
INFO: iteration 36, average log likelihood -1.432892
INFO: iteration 37, average log likelihood -1.432891
INFO: iteration 38, average log likelihood -1.432890
INFO: iteration 39, average log likelihood -1.432889
INFO: iteration 40, average log likelihood -1.432888
INFO: iteration 41, average log likelihood -1.432888
INFO: iteration 42, average log likelihood -1.432887
INFO: iteration 43, average log likelihood -1.432887
INFO: iteration 44, average log likelihood -1.432887
INFO: iteration 45, average log likelihood -1.432887
INFO: iteration 46, average log likelihood -1.432886
INFO: iteration 47, average log likelihood -1.432886
INFO: iteration 48, average log likelihood -1.432886
INFO: iteration 49, average log likelihood -1.432886
INFO: iteration 50, average log likelihood -1.432886
INFO: EM with 100000 data points 50 iterations avll -1.432886
952.4 data points per parameter
1: avll = [-1.46441,-1.46431,-1.46393,-1.45942,-1.44623,-1.43903,-1.43732,-1.4365,-1.43597,-1.43563,-1.4354,-1.43523,-1.43508,-1.43494,-1.43477,-1.43452,-1.43409,-1.43372,-1.43351,-1.43336,-1.43325,-1.43316,-1.43309,-1.43303,-1.43299,-1.43296,-1.43294,-1.43293,-1.43292,-1.43291,-1.43291,-1.4329,-1.4329,-1.4329,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.433048
INFO: iteration 2, average log likelihood -1.432914
INFO: iteration 3, average log likelihood -1.432559
INFO: iteration 4, average log likelihood -1.429720
INFO: iteration 5, average log likelihood -1.420653
INFO: iteration 6, average log likelihood -1.410659
INFO: iteration 7, average log likelihood -1.405649
INFO: iteration 8, average log likelihood -1.402520
INFO: iteration 9, average log likelihood -1.399924
INFO: iteration 10, average log likelihood -1.397805
INFO: iteration 11, average log likelihood -1.396073
INFO: iteration 12, average log likelihood -1.394729
INFO: iteration 13, average log likelihood -1.393718
INFO: iteration 14, average log likelihood -1.392985
INFO: iteration 15, average log likelihood -1.392472
INFO: iteration 16, average log likelihood -1.392086
INFO: iteration 17, average log likelihood -1.391715
INFO: iteration 18, average log likelihood -1.391263
INFO: iteration 19, average log likelihood -1.390704
INFO: iteration 20, average log likelihood -1.390122
INFO: iteration 21, average log likelihood -1.389677
INFO: iteration 22, average log likelihood -1.389371
INFO: iteration 23, average log likelihood -1.389162
INFO: iteration 24, average log likelihood -1.389015
INFO: iteration 25, average log likelihood -1.388908
INFO: iteration 26, average log likelihood -1.388828
INFO: iteration 27, average log likelihood -1.388767
INFO: iteration 28, average log likelihood -1.388718
INFO: iteration 29, average log likelihood -1.388680
INFO: iteration 30, average log likelihood -1.388649
INFO: iteration 31, average log likelihood -1.388622
INFO: iteration 32, average log likelihood -1.388599
INFO: iteration 33, average log likelihood -1.388580
INFO: iteration 34, average log likelihood -1.388563
INFO: iteration 35, average log likelihood -1.388548
INFO: iteration 36, average log likelihood -1.388535
INFO: iteration 37, average log likelihood -1.388523
INFO: iteration 38, average log likelihood -1.388512
INFO: iteration 39, average log likelihood -1.388503
INFO: iteration 40, average log likelihood -1.388495
INFO: iteration 41, average log likelihood -1.388488
INFO: iteration 42, average log likelihood -1.388482
INFO: iteration 43, average log likelihood -1.388476
INFO: iteration 44, average log likelihood -1.388472
INFO: iteration 45, average log likelihood -1.388467
INFO: iteration 46, average log likelihood -1.388464
INFO: iteration 47, average log likelihood -1.388460
INFO: iteration 48, average log likelihood -1.388456
INFO: iteration 49, average log likelihood -1.388452
INFO: iteration 50, average log likelihood -1.388449
INFO: EM with 100000 data points 50 iterations avll -1.388449
473.9 data points per parameter
2: avll = [-1.43305,-1.43291,-1.43256,-1.42972,-1.42065,-1.41066,-1.40565,-1.40252,-1.39992,-1.3978,-1.39607,-1.39473,-1.39372,-1.39298,-1.39247,-1.39209,-1.39171,-1.39126,-1.3907,-1.39012,-1.38968,-1.38937,-1.38916,-1.38901,-1.38891,-1.38883,-1.38877,-1.38872,-1.38868,-1.38865,-1.38862,-1.3886,-1.38858,-1.38856,-1.38855,-1.38853,-1.38852,-1.38851,-1.3885,-1.38849,-1.38849,-1.38848,-1.38848,-1.38847,-1.38847,-1.38846,-1.38846,-1.38846,-1.38845,-1.38845]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.388647
INFO: iteration 2, average log likelihood -1.388458
INFO: iteration 3, average log likelihood -1.388038
INFO: iteration 4, average log likelihood -1.384343
INFO: iteration 5, average log likelihood -1.370557
INFO: iteration 6, average log likelihood -1.355630
INFO: iteration 7, average log likelihood -1.347771
INFO: iteration 8, average log likelihood -1.343445
INFO: iteration 9, average log likelihood -1.340487
INFO: iteration 10, average log likelihood -1.338074
INFO: iteration 11, average log likelihood -1.335909
INFO: iteration 12, average log likelihood -1.333890
INFO: iteration 13, average log likelihood -1.332404
INFO: iteration 14, average log likelihood -1.331704
INFO: iteration 15, average log likelihood -1.331329
INFO: iteration 16, average log likelihood -1.331042
INFO: iteration 17, average log likelihood -1.330780
INFO: iteration 18, average log likelihood -1.330521
INFO: iteration 19, average log likelihood -1.330256
INFO: iteration 20, average log likelihood -1.329972
INFO: iteration 21, average log likelihood -1.329675
INFO: iteration 22, average log likelihood -1.329380
INFO: iteration 23, average log likelihood -1.329102
INFO: iteration 24, average log likelihood -1.328855
INFO: iteration 25, average log likelihood -1.328659
INFO: iteration 26, average log likelihood -1.328507
INFO: iteration 27, average log likelihood -1.328385
INFO: iteration 28, average log likelihood -1.328282
INFO: iteration 29, average log likelihood -1.328187
INFO: iteration 30, average log likelihood -1.328097
INFO: iteration 31, average log likelihood -1.328002
INFO: iteration 32, average log likelihood -1.327890
INFO: iteration 33, average log likelihood -1.327751
INFO: iteration 34, average log likelihood -1.327593
INFO: iteration 35, average log likelihood -1.327435
INFO: iteration 36, average log likelihood -1.327280
INFO: iteration 37, average log likelihood -1.327146
INFO: iteration 38, average log likelihood -1.327047
INFO: iteration 39, average log likelihood -1.326977
INFO: iteration 40, average log likelihood -1.326930
INFO: iteration 41, average log likelihood -1.326898
INFO: iteration 42, average log likelihood -1.326876
INFO: iteration 43, average log likelihood -1.326859
INFO: iteration 44, average log likelihood -1.326845
INFO: iteration 45, average log likelihood -1.326834
INFO: iteration 46, average log likelihood -1.326826
INFO: iteration 47, average log likelihood -1.326820
INFO: iteration 48, average log likelihood -1.326815
INFO: iteration 49, average log likelihood -1.326811
INFO: iteration 50, average log likelihood -1.326808
INFO: EM with 100000 data points 50 iterations avll -1.326808
236.4 data points per parameter
3: avll = [-1.38865,-1.38846,-1.38804,-1.38434,-1.37056,-1.35563,-1.34777,-1.34344,-1.34049,-1.33807,-1.33591,-1.33389,-1.3324,-1.3317,-1.33133,-1.33104,-1.33078,-1.33052,-1.33026,-1.32997,-1.32967,-1.32938,-1.3291,-1.32886,-1.32866,-1.32851,-1.32838,-1.32828,-1.32819,-1.3281,-1.328,-1.32789,-1.32775,-1.32759,-1.32743,-1.32728,-1.32715,-1.32705,-1.32698,-1.32693,-1.3269,-1.32688,-1.32686,-1.32684,-1.32683,-1.32683,-1.32682,-1.32681,-1.32681,-1.32681]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.327065
INFO: iteration 2, average log likelihood -1.326767
INFO: iteration 3, average log likelihood -1.325138
INFO: iteration 4, average log likelihood -1.307653
WARNING: Variances had to be floored 1
INFO: iteration 5, average log likelihood -1.269670
WARNING: Variances had to be floored 2 9
INFO: iteration 6, average log likelihood -1.249163
WARNING: Variances had to be floored 1 5 10
INFO: iteration 7, average log likelihood -1.240715
WARNING: Variances had to be floored 6 11
INFO: iteration 8, average log likelihood -1.252284
WARNING: Variances had to be floored 1 2 9
INFO: iteration 9, average log likelihood -1.240146
INFO: iteration 10, average log likelihood -1.248596
WARNING: Variances had to be floored 5 10 11
INFO: iteration 11, average log likelihood -1.229304
WARNING: Variances had to be floored 1 9
INFO: iteration 12, average log likelihood -1.244901
WARNING: Variances had to be floored 2 6
INFO: iteration 13, average log likelihood -1.244891
WARNING: Variances had to be floored 1 11
INFO: iteration 14, average log likelihood -1.236987
WARNING: Variances had to be floored 5 9 10
INFO: iteration 15, average log likelihood -1.233117
WARNING: Variances had to be floored 1 2
INFO: iteration 16, average log likelihood -1.248318
WARNING: Variances had to be floored 11
INFO: iteration 17, average log likelihood -1.250255
WARNING: Variances had to be floored 6
INFO: iteration 18, average log likelihood -1.239512
WARNING: Variances had to be floored 1 5 9
INFO: iteration 19, average log likelihood -1.222399
WARNING: Variances had to be floored 2 10 11
INFO: iteration 20, average log likelihood -1.248216
WARNING: Variances had to be floored 1
INFO: iteration 21, average log likelihood -1.247741
WARNING: Variances had to be floored 9
INFO: iteration 22, average log likelihood -1.232768
WARNING: Variances had to be floored 1 2 5 6 11
INFO: iteration 23, average log likelihood -1.223946
WARNING: Variances had to be floored 10
INFO: iteration 24, average log likelihood -1.258358
WARNING: Variances had to be floored 9
INFO: iteration 25, average log likelihood -1.250570
WARNING: Variances had to be floored 1 11
INFO: iteration 26, average log likelihood -1.233345
WARNING: Variances had to be floored 2 5
INFO: iteration 27, average log likelihood -1.232538
WARNING: Variances had to be floored 1 9 10
INFO: iteration 28, average log likelihood -1.238752
WARNING: Variances had to be floored 6 11
INFO: iteration 29, average log likelihood -1.246190
WARNING: Variances had to be floored 1 2
INFO: iteration 30, average log likelihood -1.240358
WARNING: Variances had to be floored 5
INFO: iteration 31, average log likelihood -1.234794
WARNING: Variances had to be floored 9 11
INFO: iteration 32, average log likelihood -1.240335
WARNING: Variances had to be floored 1 10
INFO: iteration 33, average log likelihood -1.236548
WARNING: Variances had to be floored 2
INFO: iteration 34, average log likelihood -1.235195
WARNING: Variances had to be floored 1 5 6 9 11
INFO: iteration 35, average log likelihood -1.223187
INFO: iteration 36, average log likelihood -1.259331
WARNING: Variances had to be floored 1 2 10
INFO: iteration 37, average log likelihood -1.234083
WARNING: Variances had to be floored 9 11
INFO: iteration 38, average log likelihood -1.243070
WARNING: Variances had to be floored 5
INFO: iteration 39, average log likelihood -1.243243
WARNING: Variances had to be floored 1
INFO: iteration 40, average log likelihood -1.233837
WARNING: Variances had to be floored 2 6 9 10 11
INFO: iteration 41, average log likelihood -1.228836
WARNING: Variances had to be floored 1
INFO: iteration 42, average log likelihood -1.251669
WARNING: Variances had to be floored 5
INFO: iteration 43, average log likelihood -1.237579
WARNING: Variances had to be floored 1 2 11
INFO: iteration 44, average log likelihood -1.232882
WARNING: Variances had to be floored 9
INFO: iteration 45, average log likelihood -1.243489
WARNING: Variances had to be floored 10
INFO: iteration 46, average log likelihood -1.237741
WARNING: Variances had to be floored 1 5 6 11
INFO: iteration 47, average log likelihood -1.221656
WARNING: Variances had to be floored 2 9
INFO: iteration 48, average log likelihood -1.249294
WARNING: Variances had to be floored 1
INFO: iteration 49, average log likelihood -1.249870
WARNING: Variances had to be floored 10 11
INFO: iteration 50, average log likelihood -1.233543
INFO: EM with 100000 data points 50 iterations avll -1.233543
118.1 data points per parameter
4: avll = [-1.32706,-1.32677,-1.32514,-1.30765,-1.26967,-1.24916,-1.24071,-1.25228,-1.24015,-1.2486,-1.2293,-1.2449,-1.24489,-1.23699,-1.23312,-1.24832,-1.25025,-1.23951,-1.2224,-1.24822,-1.24774,-1.23277,-1.22395,-1.25836,-1.25057,-1.23334,-1.23254,-1.23875,-1.24619,-1.24036,-1.23479,-1.24034,-1.23655,-1.2352,-1.22319,-1.25933,-1.23408,-1.24307,-1.24324,-1.23384,-1.22884,-1.25167,-1.23758,-1.23288,-1.24349,-1.23774,-1.22166,-1.24929,-1.24987,-1.23354]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4 9 10 17 18
INFO: iteration 1, average log likelihood -1.228457
WARNING: Variances had to be floored 1 2 3 4 9 10 17 18
INFO: iteration 2, average log likelihood -1.219271
WARNING: Variances had to be floored 1 2 3 4 9 10 11 17 18 21 22
INFO: iteration 3, average log likelihood -1.215201
WARNING: Variances had to be floored 1 2 3 4 9 10 12 17 18 20
INFO: iteration 4, average log likelihood -1.211092
WARNING: Variances had to be floored 1 2 3 4 7 9 10 17 18 19
INFO: iteration 5, average log likelihood -1.175355
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 21 22
INFO: iteration 6, average log likelihood -1.163455
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 7, average log likelihood -1.160519
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 19 22
INFO: iteration 8, average log likelihood -1.145040
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 21
INFO: iteration 9, average log likelihood -1.142641
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18
INFO: iteration 10, average log likelihood -1.147247
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18 19 21
INFO: iteration 11, average log likelihood -1.134165
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18
INFO: iteration 12, average log likelihood -1.140260
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18
INFO: iteration 13, average log likelihood -1.128862
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 19 21 22
INFO: iteration 14, average log likelihood -1.133229
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 15, average log likelihood -1.142754
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 21
INFO: iteration 16, average log likelihood -1.129552
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 19 22
INFO: iteration 17, average log likelihood -1.125751
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18
INFO: iteration 18, average log likelihood -1.145961
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18 22
INFO: iteration 19, average log likelihood -1.132834
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 19 21
INFO: iteration 20, average log likelihood -1.130865
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 22
INFO: iteration 21, average log likelihood -1.135382
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18
INFO: iteration 22, average log likelihood -1.140431
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18 19 21
INFO: iteration 23, average log likelihood -1.130467
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 24, average log likelihood -1.134737
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18
INFO: iteration 25, average log likelihood -1.132912
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 19 22
INFO: iteration 26, average log likelihood -1.134148
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 27, average log likelihood -1.141304
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 21
INFO: iteration 28, average log likelihood -1.128795
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 19 22
INFO: iteration 29, average log likelihood -1.127546
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18
INFO: iteration 30, average log likelihood -1.145749
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18 21
INFO: iteration 31, average log likelihood -1.132724
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 19 22
INFO: iteration 32, average log likelihood -1.126837
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18
INFO: iteration 33, average log likelihood -1.137851
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 34, average log likelihood -1.135994
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18 19
INFO: iteration 35, average log likelihood -1.135608
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 36, average log likelihood -1.131988
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 21
INFO: iteration 37, average log likelihood -1.124260
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 19 22
INFO: iteration 38, average log likelihood -1.135765
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 39, average log likelihood -1.141719
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 40, average log likelihood -1.122948
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 19 21
INFO: iteration 41, average log likelihood -1.119992
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 42, average log likelihood -1.142489
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 43, average log likelihood -1.138008
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 44, average log likelihood -1.122975
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 19 21
INFO: iteration 45, average log likelihood -1.119904
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 46, average log likelihood -1.142485
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 47, average log likelihood -1.137998
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 48, average log likelihood -1.122968
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 12 17 18 19 21
INFO: iteration 49, average log likelihood -1.119896
WARNING: Variances had to be floored 1 2 3 4 9 10 11 16 17 18 22
INFO: iteration 50, average log likelihood -1.142491
INFO: EM with 100000 data points 50 iterations avll -1.142491
59.0 data points per parameter
5: avll = [-1.22846,-1.21927,-1.2152,-1.21109,-1.17536,-1.16346,-1.16052,-1.14504,-1.14264,-1.14725,-1.13417,-1.14026,-1.12886,-1.13323,-1.14275,-1.12955,-1.12575,-1.14596,-1.13283,-1.13087,-1.13538,-1.14043,-1.13047,-1.13474,-1.13291,-1.13415,-1.1413,-1.1288,-1.12755,-1.14575,-1.13272,-1.12684,-1.13785,-1.13599,-1.13561,-1.13199,-1.12426,-1.13577,-1.14172,-1.12295,-1.11999,-1.14249,-1.13801,-1.12297,-1.1199,-1.14248,-1.138,-1.12297,-1.1199,-1.14249]
[-1.46428,-1.46441,-1.46431,-1.46393,-1.45942,-1.44623,-1.43903,-1.43732,-1.4365,-1.43597,-1.43563,-1.4354,-1.43523,-1.43508,-1.43494,-1.43477,-1.43452,-1.43409,-1.43372,-1.43351,-1.43336,-1.43325,-1.43316,-1.43309,-1.43303,-1.43299,-1.43296,-1.43294,-1.43293,-1.43292,-1.43291,-1.43291,-1.4329,-1.4329,-1.4329,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43289,-1.43305,-1.43291,-1.43256,-1.42972,-1.42065,-1.41066,-1.40565,-1.40252,-1.39992,-1.3978,-1.39607,-1.39473,-1.39372,-1.39298,-1.39247,-1.39209,-1.39171,-1.39126,-1.3907,-1.39012,-1.38968,-1.38937,-1.38916,-1.38901,-1.38891,-1.38883,-1.38877,-1.38872,-1.38868,-1.38865,-1.38862,-1.3886,-1.38858,-1.38856,-1.38855,-1.38853,-1.38852,-1.38851,-1.3885,-1.38849,-1.38849,-1.38848,-1.38848,-1.38847,-1.38847,-1.38846,-1.38846,-1.38846,-1.38845,-1.38845,-1.38865,-1.38846,-1.38804,-1.38434,-1.37056,-1.35563,-1.34777,-1.34344,-1.34049,-1.33807,-1.33591,-1.33389,-1.3324,-1.3317,-1.33133,-1.33104,-1.33078,-1.33052,-1.33026,-1.32997,-1.32967,-1.32938,-1.3291,-1.32886,-1.32866,-1.32851,-1.32838,-1.32828,-1.32819,-1.3281,-1.328,-1.32789,-1.32775,-1.32759,-1.32743,-1.32728,-1.32715,-1.32705,-1.32698,-1.32693,-1.3269,-1.32688,-1.32686,-1.32684,-1.32683,-1.32683,-1.32682,-1.32681,-1.32681,-1.32681,-1.32706,-1.32677,-1.32514,-1.30765,-1.26967,-1.24916,-1.24071,-1.25228,-1.24015,-1.2486,-1.2293,-1.2449,-1.24489,-1.23699,-1.23312,-1.24832,-1.25025,-1.23951,-1.2224,-1.24822,-1.24774,-1.23277,-1.22395,-1.25836,-1.25057,-1.23334,-1.23254,-1.23875,-1.24619,-1.24036,-1.23479,-1.24034,-1.23655,-1.2352,-1.22319,-1.25933,-1.23408,-1.24307,-1.24324,-1.23384,-1.22884,-1.25167,-1.23758,-1.23288,-1.24349,-1.23774,-1.22166,-1.24929,-1.24987,-1.23354,-1.22846,-1.21927,-1.2152,-1.21109,-1.17536,-1.16346,-1.16052,-1.14504,-1.14264,-1.14725,-1.13417,-1.14026,-1.12886,-1.13323,-1.14275,-1.12955,-1.12575,-1.14596,-1.13283,-1.13087,-1.13538,-1.14043,-1.13047,-1.13474,-1.13291,-1.13415,-1.1413,-1.1288,-1.12755,-1.14575,-1.13272,-1.12684,-1.13785,-1.13599,-1.13561,-1.13199,-1.12426,-1.13577,-1.14172,-1.12295,-1.11999,-1.14249,-1.13801,-1.12297,-1.1199,-1.14248,-1.138,-1.12297,-1.1199,-1.14249]
32×26 Array{Float64,2}:
 -0.00440763    0.13191      0.296104    -0.00729037   0.00677985   0.144985    -0.206922     0.0744926   -0.187092    -0.0817626    0.0792777   -0.173758    -0.0341621   -0.128899   -0.0911147    0.0591076   -0.0284585   -0.148554     0.204968    -0.0292378   -0.132668     0.180285     -0.0199382   -0.206206    -0.205072     0.159419  
  0.105765     -0.232465    -0.223934    -0.00684315   0.0263345   -0.0654368    0.118973     0.0755477   -0.116777     0.267385     0.0389201   -0.182149    -0.0519502   -0.163943   -0.0672373    0.0563661    0.101893     0.107109     0.205045    -0.0190097   -0.157467     0.136416     -0.0191172    0.368284    -0.20131      0.154137  
 -0.189861     -0.0243131   -0.110039    -0.116443     0.366807     0.0115896   -0.0190481    0.0741711    0.0977924    0.166257    -0.0577067   -0.41817      0.0293238   -0.0702954   0.0643691   -0.15671      0.246333    -0.137694     0.0642857   -0.132301    -0.165022    -0.169888      0.032834     0.129666     0.0155098    0.198475  
  0.0184425    -0.0240962    0.0666338   -0.183355    -0.0657136   -0.0180816   -0.242338     0.144672    -0.109941    -0.163354    -0.0578211    0.411027     0.0431965   -0.089231    0.0493469   -0.125274     0.281405    -0.164444     0.0144091   -0.0739415   -0.148771     0.0936244    -0.0363687    0.136853    -0.0668224   -0.174437  
 -0.101985      0.0036183    0.0378482    0.145599    -0.00701224  -0.050727     0.118253     0.116312     0.0489023    0.0702925    0.0168109   -0.0509069    0.00828514  -0.0077063   0.0553647   -0.0186889   -0.197841    -0.180796    -0.0725948   -0.0533265    0.0411097    0.016148      0.127523    -0.108068    -0.0130176   -0.0356396 
 -0.171909      0.0393862    0.0178539   -0.104127    -0.0738032    0.0954735    0.0181564    0.239865    -0.0147588   -0.111418    -0.104266    -0.0339337   -0.106091     0.0579425   0.106626    -0.166078    -0.0213683    0.0504376   -0.0331904    0.0962685    0.0399094   -0.0976534    -0.0291984   -0.0721144   -0.00673463  -0.135658  
 -0.0822609    -0.0225771    0.010247    -0.00921852   0.0729954    0.0510741   -0.154844    -0.00629971  -0.126892    -0.250285    -0.0868424   -0.110799     0.109003    -0.0827555  -0.0156447   -0.0345075   -0.157925    -0.0832038   -0.00192482   0.129493    -0.019978    -0.139492     -0.212085     0.160959     0.147569    -0.196059  
 -0.175459      0.0982962    0.139015    -0.0307855    0.137        0.0390122   -0.0404685    0.117444    -0.0953039   -0.0387654   -0.0146143    0.0334077    0.116438    -0.0266043   0.137011     0.0115676    0.0112134    0.0787269    0.0250113    0.0281985   -0.0563964    0.12514      -0.012793     0.0461303    0.221434     0.0214658 
  0.0722746     0.0909595    0.0656077    0.23689     -0.018247     0.0310162    0.0511322    0.0157019    0.040543     0.0900071    0.0298707    0.051644     0.132364     0.0380424  -0.157871     0.209482     0.054083    -0.133351    -0.0339747    0.0171277    0.110065    -0.00881945    0.0543565   -0.79735      0.111853     0.182085  
 -0.00164966    0.0902689    0.06509      0.223157     0.0306191   -0.202424     0.0540764    0.013632     0.0409303   -0.158597     0.179225     0.0342284   -0.00166448   0.0374833  -0.159389     0.173191     0.00465947  -0.133267    -0.00962054  -0.10448      0.00461074  -0.0125854     0.11999      0.669896     0.0899778   -0.0616767 
 -0.0386281    -0.0674961    0.134449    -0.027769    -0.034314    -0.126979     0.155698     0.049223    -0.0469884   -0.0116291   -0.0138482   -0.0657634   -0.694276    -0.0211467  -0.152926    -0.195861     0.185443     0.358331    -0.0887502   -0.0396851    0.101616    -0.080167     -0.193538     0.244726     0.0362095    0.422402  
 -0.0376106    -0.0541772    0.13736     -0.0072376    0.102872    -0.165369     0.0572888    0.0461889   -0.0398019   -0.240464     0.109769    -0.0794157    0.358952     0.0169839  -0.0128461   -0.0435671    0.179012    -0.231731    -0.132198    -0.0312894    0.0890312   -0.160074      0.00299263  -0.0469119    0.0524495   -0.114738  
 -0.00238828    0.0562708    0.00785029   0.146113     0.0265548   -0.0135379   -0.0353281    0.0121577   -0.129375    -0.0636103   -0.103572    -0.0144299   -0.0182582   -0.0137096   0.0876293   -0.0279692   -0.0662076   -0.00989089   0.0109843   -0.0196017   -0.105515     0.0556874     6.80116e-5   0.0461205    0.0398237   -0.0473056 
 -0.0125561     0.0407996   -0.0220345    0.0189356   -0.00844931  -0.0129626    0.010676    -0.0215412   -0.0657536    0.00400232  -0.0362727    0.064343     0.0611017   -0.0436219  -0.0292506    0.0111918   -0.0343013    0.0524656    0.00705465   0.00853149  -0.0266407    0.000109285  -0.031255    -0.00613209  -0.0764569    0.0513532 
  0.0543981    -0.0579236    0.104662    -0.00932677   0.0957661   -0.001522     0.11729     -0.0457942   -0.0285631   -0.00945342   0.0531098    0.234568     0.0604699   -0.252365    0.077945     0.0386868    0.00819247  -0.0398946    0.0399583    0.156579    -0.0295983    0.0605353     0.102638     0.13825      0.101017    -0.0705646 
  0.0126196     0.0900793   -0.0687083    0.0909317   -0.0111916    0.0340307    0.0878225   -0.0964529   -0.309014    -0.171071     0.0346664   -0.0600503   -0.146061    -0.14344    -0.0159439   -0.157877     0.00337069   0.184972     0.123298     0.204379     0.0220575   -0.204961     -0.121006     0.059811     0.0680377   -0.00731579
 -0.119623     -0.0680441    0.117257    -0.227107     0.10125      0.258452    -0.0757666    0.0742487    0.0150232   -0.0917216    0.114887    -0.0268913   -0.179236     0.316011   -0.0917171   -0.00847791   0.0236651   -0.123456     0.0852828    0.285869     0.059335     0.0404922     0.286903     0.0607197    0.119406     0.279953  
 -0.0979342    -0.0679105   -0.0644451    0.233595    -0.0737106   -0.344487    -0.0760313    0.0720201   -0.0933136    0.0198229   -0.157773     0.0414013   -0.182101     0.218658   -0.105487     0.0111852   -0.106213    -0.0678728    0.0481563   -0.371204     0.0528966    0.0410737    -0.129281     0.0735464   -0.0634133   -0.285302  
 -0.0175727     0.00227518  -0.0902289    0.0735526   -0.154581    -0.0865857    0.0270417    0.0058565    0.0973539   -0.0542843    0.156538     0.0772053   -0.029889    -0.0857779   0.194779    -0.14218      0.0190633   -0.171635     0.0381653    0.0409295   -0.0338866   -0.0308822     0.0387334   -0.0527989    0.0680863   -0.159017  
  0.068422     -0.0136177   -0.117202     0.165752     0.01482      0.0249984   -0.0417918    0.0144424   -0.0338396    0.103024    -0.0323297   -0.126376    -0.0466934   -0.162476    0.108475     0.156923     0.117895    -0.068188     0.00338172   0.160536     0.117509    -0.000358842   0.0541058    0.290796     0.0960831    0.0993119 
 -0.0505034     0.0704327   -0.12273     -0.166884    -0.0947697    0.0298929   -0.0737199   -0.00938731   0.0977626   -0.0520057   -0.137586    -0.0247785    0.0375715    0.0304691   0.0932951   -0.0963058   -0.0338626   -0.0144932    0.0481396   -0.0747527   -0.109932     0.0968426     0.00477301   0.142168    -0.0788775   -0.0793228 
  0.123722      0.0403377    0.061832    -0.230714     0.0532072   -0.0204242   -0.0135208   -0.0661408    0.0536411   -0.25378     -0.12759      0.0250999   -0.284828    -0.0244982  -0.0637104   -0.0966953   -0.0766354    0.101262     0.0475926   -0.0322352    0.0225143   -0.00540923   -0.132498    -0.17774     -0.126705    -0.12495   
 -0.147392      0.216199     0.102129     0.0893651    0.123029     0.0590298   -0.193852    -0.00895011  -0.76552     -0.0278107   -0.10421     -0.0457024   -0.024385     0.222005    0.0105787   -0.0458237    0.025883    -0.0527735   -0.0329954    0.0810031   -0.0221115   -0.164145     -0.00974863  -0.0742426    0.043828    -0.0726688 
 -0.093617      0.21804      0.092196    -0.141046     0.122731     0.0536776   -0.0356308    0.0188311    1.00139     -0.0414241   -0.0939864   -0.0330485   -0.0290717   -0.0147338   0.110983    -0.0267855    0.129945     0.0320461   -0.00245912   0.0594634    0.024354    -0.166414     -0.0178547   -0.0822618    0.0744851   -0.0925984 
 -0.104865     -0.0365507   -0.0785153    0.0764102    0.00679357   0.0441635    0.0642348   -0.0385485    0.0815738    0.00894753  -0.0351567    0.153872     0.0763387   -0.0674402  -0.0205818   -0.0448246    0.0801757   -0.0554116   -0.071958     0.213282     0.0350299    0.01536      -0.0511238    0.0709804    0.0888999    0.0110241 
 -0.0805013    -0.00156424  -0.0349494   -0.00527455  -0.0419193   -0.00500506  -0.0245422   -0.0370182    0.0487362    0.00139712   0.0182971   -0.0400621    0.15814     -0.0780884   0.12626      0.151974     0.074354    -0.182127    -0.0343659   -0.0258002    0.0831892    0.0953643    -0.0531268   -0.0798329    0.0657944    0.033503  
  0.0241836     0.0177237   -0.0303006    0.0075466   -0.0640847    0.201745     0.085106    -0.114147    -0.0110791   -0.0456574    0.00148483  -0.103001    -0.0385791   -0.0812239   0.00429064   0.0866173   -0.0633189    0.0125746    0.0252482   -0.162036     0.0505346    0.0628569     0.0439056   -0.0642058   -0.0451286   -0.139521  
  0.060454      0.164429    -0.203048    -0.0607557    0.112419     0.045419     0.0735012   -0.16847     -0.0144578   -0.118085     0.0327255    0.0729806   -0.0263387   -0.0520965   0.113389     0.193286     0.107594     0.0319474    0.0763516   -0.171256     0.0936272   -0.0496017     0.0938444    0.0655917   -0.0202095   -0.0310298 
 -0.0200373     0.0636865   -0.261407     0.0939379    0.248475     0.117749    -0.102606     0.178181     0.00502336  -0.099517     0.116117     0.118512     0.0457671    0.0564022  -0.0345169    0.0499112   -0.0945018    0.0587495   -0.0165259   -0.0571564   -0.0255832    0.125895      0.175337     0.0149595   -0.0991396   -0.0626309 
 -0.000758457   0.104771    -0.101487    -0.173858     0.073177    -0.0898256   -0.00844843   0.0903918    0.277145    -0.140406     0.0521592    0.00161488   0.0639727   -0.0241386   0.00954992   0.0432283    0.0388841    0.0161035    0.127078     0.0296309   -0.0940092   -0.0328146     0.0354286   -0.0624087    0.0312996    0.0753133 
 -0.0139347     0.193479    -0.0462498    0.00423314   0.100553    -0.0844798   -0.0662293   -0.092131    -0.117686     0.104082     0.0190126    0.0165017    0.0693115   -0.0203759  -0.0619931    0.0748195   -0.0257633   -0.163871     0.0141194   -0.101681    -0.130892    -0.0655756     0.102709    -0.121587     0.0103621    0.0174862 
  0.105167      0.0105434    0.142497     0.0871703    0.0367717    0.00895671  -0.0244663   -0.0769119    0.049507    -0.0786063    0.0570217   -0.0733792   -0.172446    -0.0618727  -0.035212    -0.0954023   -0.0439883   -0.0358399    0.016616     0.0243646    0.00955052   0.116267     -0.145193    -0.123467     0.110032     0.189854  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 1, average log likelihood -1.137990
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 16 17 18 22
INFO: iteration 2, average log likelihood -1.116822
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 11 12 17 18 19 21
INFO: iteration 3, average log likelihood -1.119823
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 16 17 18 22
INFO: iteration 4, average log likelihood -1.126048
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18
INFO: iteration 5, average log likelihood -1.127715
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 11 12 16 17 18 19 22
INFO: iteration 6, average log likelihood -1.108054
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 17 18 21
INFO: iteration 7, average log likelihood -1.127086
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 16 17 18 19 22
INFO: iteration 8, average log likelihood -1.121033
WARNING: Variances had to be floored 1 2 3 4 5 7 9 10 11 12 17 18
INFO: iteration 9, average log likelihood -1.126467
WARNING: Variances had to be floored 1 2 3 4 7 9 10 12 16 17 18 19 21 22
INFO: iteration 10, average log likelihood -1.113945
INFO: EM with 100000 data points 10 iterations avll -1.113945
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.694325e+05
      1       7.639670e+05      -2.054654e+05 |       32
      2       7.335961e+05      -3.037095e+04 |       32
      3       7.137602e+05      -1.983592e+04 |       32
      4       7.017092e+05      -1.205095e+04 |       32
      5       6.944747e+05      -7.234510e+03 |       32
      6       6.902639e+05      -4.210778e+03 |       32
      7       6.877455e+05      -2.518456e+03 |       32
      8       6.859774e+05      -1.768028e+03 |       32
      9       6.843858e+05      -1.591678e+03 |       32
     10       6.830486e+05      -1.337114e+03 |       32
     11       6.817303e+05      -1.318355e+03 |       32
     12       6.806128e+05      -1.117480e+03 |       32
     13       6.797530e+05      -8.598059e+02 |       32
     14       6.790459e+05      -7.071081e+02 |       32
     15       6.785090e+05      -5.368565e+02 |       32
     16       6.781992e+05      -3.098747e+02 |       32
     17       6.780468e+05      -1.523769e+02 |       32
     18       6.779434e+05      -1.034284e+02 |       32
     19       6.778532e+05      -9.016166e+01 |       32
     20       6.777649e+05      -8.824681e+01 |       32
     21       6.776647e+05      -1.002609e+02 |       32
     22       6.775679e+05      -9.676522e+01 |       32
     23       6.774898e+05      -7.815357e+01 |       32
     24       6.774120e+05      -7.776813e+01 |       32
     25       6.773346e+05      -7.740629e+01 |       31
     26       6.772528e+05      -8.176998e+01 |       32
     27       6.771557e+05      -9.712588e+01 |       32
     28       6.770213e+05      -1.344054e+02 |       32
     29       6.767761e+05      -2.452222e+02 |       32
     30       6.763863e+05      -3.897605e+02 |       32
     31       6.758554e+05      -5.309425e+02 |       32
     32       6.752707e+05      -5.846211e+02 |       32
     33       6.747853e+05      -4.854755e+02 |       32
     34       6.744585e+05      -3.267636e+02 |       32
     35       6.742576e+05      -2.009393e+02 |       32
     36       6.741380e+05      -1.195757e+02 |       32
     37       6.740502e+05      -8.775116e+01 |       32
     38       6.739664e+05      -8.381275e+01 |       32
     39       6.738912e+05      -7.520090e+01 |       32
     40       6.737967e+05      -9.451119e+01 |       31
     41       6.736687e+05      -1.280191e+02 |       32
     42       6.734765e+05      -1.921683e+02 |       32
     43       6.732648e+05      -2.117028e+02 |       32
     44       6.731014e+05      -1.634131e+02 |       32
     45       6.730035e+05      -9.792983e+01 |       32
     46       6.729460e+05      -5.744144e+01 |       30
     47       6.729041e+05      -4.198836e+01 |       30
     48       6.728769e+05      -2.711187e+01 |       31
     49       6.728619e+05      -1.506074e+01 |       30
     50       6.728541e+05      -7.747029e+00 |       23
K-means terminated without convergence after 50 iterations (objv = 672854.1342229827)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.387885
INFO: iteration 2, average log likelihood -1.359799
INFO: iteration 3, average log likelihood -1.329787
INFO: iteration 4, average log likelihood -1.299485
INFO: iteration 5, average log likelihood -1.268676
INFO: iteration 6, average log likelihood -1.230637
WARNING: Variances had to be floored 3 16 24 31
INFO: iteration 7, average log likelihood -1.166984
WARNING: Variances had to be floored 2 6 12
INFO: iteration 8, average log likelihood -1.162277
WARNING: Variances had to be floored 13 15 18
INFO: iteration 9, average log likelihood -1.167754
WARNING: Variances had to be floored 14 27
INFO: iteration 10, average log likelihood -1.146167
WARNING: Variances had to be floored 3 16 24 28 29
INFO: iteration 11, average log likelihood -1.134892
WARNING: Variances had to be floored 12
INFO: iteration 12, average log likelihood -1.181557
WARNING: Variances had to be floored 2
INFO: iteration 13, average log likelihood -1.152533
WARNING: Variances had to be floored 6 13 15 16 18 31
INFO: iteration 14, average log likelihood -1.112859
WARNING: Variances had to be floored 3 14 24 27
INFO: iteration 15, average log likelihood -1.160864
WARNING: Variances had to be floored 29
INFO: iteration 16, average log likelihood -1.188160
WARNING: Variances had to be floored 2 12
INFO: iteration 17, average log likelihood -1.156007
WARNING: Variances had to be floored 16 31
INFO: iteration 18, average log likelihood -1.135559
WARNING: Variances had to be floored 3 6 13 15 17 18 24
INFO: iteration 19, average log likelihood -1.102611
WARNING: Variances had to be floored 14 27 29
INFO: iteration 20, average log likelihood -1.169568
WARNING: Variances had to be floored 2 9 12
INFO: iteration 21, average log likelihood -1.151030
WARNING: Variances had to be floored 16 31
INFO: iteration 22, average log likelihood -1.164376
WARNING: Variances had to be floored 3 17 24
INFO: iteration 23, average log likelihood -1.138034
WARNING: Variances had to be floored 2 13 14 15 18 29
INFO: iteration 24, average log likelihood -1.126181
WARNING: Variances had to be floored 6 12 27 31
INFO: iteration 25, average log likelihood -1.156051
WARNING: Variances had to be floored 16
INFO: iteration 26, average log likelihood -1.174280
WARNING: Variances had to be floored 3 17 24
INFO: iteration 27, average log likelihood -1.142067
WARNING: Variances had to be floored 2 9
INFO: iteration 28, average log likelihood -1.137718
WARNING: Variances had to be floored 6 13 14 15 18 29 31
INFO: iteration 29, average log likelihood -1.111556
WARNING: Variances had to be floored 16 17 24 27
INFO: iteration 30, average log likelihood -1.163951
WARNING: Variances had to be floored 3
INFO: iteration 31, average log likelihood -1.183284
WARNING: Variances had to be floored 2 12
INFO: iteration 32, average log likelihood -1.150565
WARNING: Variances had to be floored 6 17 24 31
INFO: iteration 33, average log likelihood -1.119461
WARNING: Variances had to be floored 3 13 14 15 16 18 29
INFO: iteration 34, average log likelihood -1.127030
WARNING: Variances had to be floored 2 9 27
INFO: iteration 35, average log likelihood -1.181778
INFO: iteration 36, average log likelihood -1.183873
WARNING: Variances had to be floored 3 17 24 31
INFO: iteration 37, average log likelihood -1.124811
WARNING: Variances had to be floored 2 6 16
INFO: iteration 38, average log likelihood -1.132069
WARNING: Variances had to be floored 9 13 14 15 18 29
INFO: iteration 39, average log likelihood -1.136074
WARNING: Variances had to be floored 12 17 24 27 31
INFO: iteration 40, average log likelihood -1.159911
WARNING: Variances had to be floored 3
INFO: iteration 41, average log likelihood -1.183005
WARNING: Variances had to be floored 2 16
INFO: iteration 42, average log likelihood -1.145968
WARNING: Variances had to be floored 9 17 24 31
INFO: iteration 43, average log likelihood -1.122812
WARNING: Variances had to be floored 3 6 13 14 15 18 29
INFO: iteration 44, average log likelihood -1.129935
WARNING: Variances had to be floored 27
INFO: iteration 45, average log likelihood -1.180195
WARNING: Variances had to be floored 2 16
INFO: iteration 46, average log likelihood -1.145323
WARNING: Variances had to be floored 12 17 24
INFO: iteration 47, average log likelihood -1.147373
WARNING: Variances had to be floored 3 9
INFO: iteration 48, average log likelihood -1.138929
WARNING: Variances had to be floored 2 13 14 15 18 29
INFO: iteration 49, average log likelihood -1.117200
WARNING: Variances had to be floored 16 17 24 27
INFO: iteration 50, average log likelihood -1.164759
INFO: EM with 100000 data points 50 iterations avll -1.164759
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0132527    0.192833    -0.0478426    0.00267275    0.0989692   -0.0850877   -0.0664668   -0.0922065   -0.117695      0.10204       0.0200619    0.0167537     0.068746    -0.0208453    -0.0639915    0.0746849    -0.0250841   -0.164813     0.0146154   -0.101673    -0.129866    -0.065487      0.102849    -0.121299     0.0106936    0.0191193 
 -0.0262962    0.121935    -0.0580287    0.117406     -0.0186584    0.0333283    0.0554179   -0.133395    -0.439516     -0.159801      0.00270236  -0.0407583    -0.169023    -0.144274      0.00375288  -0.147166      0.0219493    0.196318     0.0846307    0.171583     0.0419504   -0.125019     -0.131378     0.128028     0.0691605    0.00468864
 -0.0797525   -0.0239479   -0.00655149  -0.00804355    0.0631596    0.0538807   -0.157678    -0.00639717  -0.12521      -0.24627      -0.0809631   -0.115397      0.107235    -0.083412      0.00252203  -0.0361694    -0.155478    -0.0827138   -0.00160089   0.126337    -0.0187553   -0.140764     -0.207398     0.159171     0.145673    -0.191936  
 -0.0206599    0.0523689   -0.241221     0.0713332     0.237514     0.113132    -0.0976689    0.158978     0.0043328    -0.113574      0.10145      0.117444      0.0405016    0.0429207    -0.0477739    0.0585078    -0.0943596    0.0545483   -0.0100974   -0.0530036   -0.0335676    0.116525      0.152469     0.00331597  -0.094936    -0.0822673 
  0.0583904    0.156415    -0.200836    -0.0596848     0.110645     0.0400306    0.0745851   -0.164596    -0.0186926    -0.120433      0.0321678    0.0727212    -0.0257419   -0.0576056     0.111549     0.183526      0.106553     0.0318553    0.0758931   -0.169365     0.0861937   -0.0491815     0.0938993    0.0591123   -0.0212219   -0.0314934 
 -0.0223361    0.0428793   -0.0368477   -0.0823009    -0.0607522   -0.0859718    0.0432254    0.0424001   -0.0219897    -0.0137392     0.0355438   -0.0192192     0.355909    -0.0821643    -0.135334     0.197674      0.106212    -0.186998    -0.00908403   0.016799     0.19628     -0.00245839    0.00264417   0.168317     0.111198     0.14983   
  0.0164297   -0.0425506   -0.127354     0.04447       0.128794     0.0358864    0.0292835   -0.00762381   0.0239312     0.135018     -0.081819     0.0508572     0.0885624   -0.0298863     0.0354394   -0.21893       0.0741506    0.0956301   -0.0685913    0.316073    -0.0119092    0.0368061    -0.0531794    0.119447     0.0949468    0.00143521
  0.10282      0.010749     0.141363     0.0858618     0.0361446    0.00950831  -0.0253199   -0.0783726    0.0497223    -0.0778377     0.0571474   -0.0722874    -0.172723    -0.0599967    -0.0359141   -0.0934673    -0.0439763   -0.0359069    0.0168482    0.0242167    0.00702501   0.115326     -0.143627    -0.125263     0.109124     0.18915   
 -0.0391117   -0.0554688    0.123397     0.000762619   0.0533505   -0.135369     0.0965382    0.0495998   -0.0385345    -0.182718      0.0483345   -0.0729654    -0.121809     0.000425352  -0.0912593   -0.186243      0.171167     0.0126988   -0.121462    -0.0296886    0.101265    -0.116255      0.0199708    0.0699722    0.0513245    0.0609311 
  0.0695541    0.00842721  -0.110531     0.161568      0.0131537    0.0186928   -0.0504841    0.0220141   -0.0322103     0.0968291    -0.0366282   -0.120041     -0.0567545   -0.158991      0.119461     0.156081      0.118685    -0.0560731    0.00932355   0.15398      0.108629    -0.000849107   0.0498506    0.274841     0.0838234    0.101853  
 -0.12352      0.00244451  -0.00512669  -0.127972      0.0531185    0.0354959   -0.0640676    0.166358    -0.00899283   -0.0434556    -0.081856    -0.0259107    -0.0261511   -0.0223371     0.0789044   -0.151965      0.125822    -0.0615335    0.00634492  -0.0218754   -0.0648586   -0.0648474    -0.0182752    0.0405363   -0.0185933   -0.0621872 
 -0.0371948   -0.0575704    0.130165    -0.0572766     0.0370303   -0.155061     0.100307     0.0456864   -0.0467837    -0.141559      0.0625628   -0.0803907     0.00261221  -0.00164344   -0.043428    -0.0259198     0.180231     0.00855669  -0.0950787   -0.0276687    0.0760771   -0.1312       -0.199757     0.0234154    0.0377751    0.114099  
 -0.011068     0.0630508   -0.0792211   -0.16835      -0.0652393    0.049414    -0.0663745    0.0165435    0.0455534    -0.0409025    -0.106068     0.00542349    0.0253916    0.0987792     0.0431117   -0.100786     -0.0113489   -0.0559169   -0.00173971  -0.0790403   -0.133087     0.100633      0.0837607    0.1045      -0.121207    -0.0691825 
 -0.0943028   -0.0716333   -0.0624296    0.183108     -0.0251084    0.0138564   -0.0274341   -0.134065    -0.132269     -0.000649088  -0.0215421    0.274437     -0.0584049   -0.0605224     0.110123     0.0173503     0.0201256    0.133268     0.00548209  -0.02068      0.114961     0.0587264    -0.102312     0.221155    -0.102412     0.0513956 
  0.0301399   -0.110457     0.0686241    0.00990543   -0.0782162    0.154749     0.079425     0.00622205   0.000729133  -0.00631507   -0.0293573    0.0468583    -0.00814117  -0.0985453     0.0620156   -0.0591806     0.025085     0.00872866  -0.111833     0.00851582  -0.175552    -0.0133357     0.0545602   -0.104259    -0.199731     0.132471  
  0.0529865   -0.0457176    0.0392728   -0.00727236    0.0198851    0.0471108   -0.0419095    0.0748393   -0.155227      0.0923952     0.0592824   -0.179022     -0.044197    -0.144313     -0.0806662    0.0584186     0.0329504   -0.0197119    0.205294    -0.0239521   -0.143064     0.157183     -0.0207347    0.0774905   -0.20525      0.157184  
  0.00887152   0.00178691  -0.0752673    0.0987155    -0.166597    -0.101527     0.03801      0.00440852   0.112836     -0.057618      0.171844     0.0820625    -0.0475369   -0.108327      0.1822      -0.138315      0.0268373   -0.177875     0.0380594    0.0633496   -0.0339155   -0.0468524     0.0569215   -0.0874039    0.0773685   -0.164941  
  0.00588865  -0.028638    -0.0653028   -0.115947     -0.0727615    0.0207714    0.00444222  -0.0407722   -0.0323537    -0.0263953     0.00312995  -0.0605193     0.0363711   -0.112023      0.186653     0.17834       0.0281804   -0.188287     0.00619815   0.0344885    0.231005     0.0965316    -0.050266    -0.0795056    0.0380913    0.026992  
  0.0538475   -0.0575994    0.0998743   -0.00683965    0.096299    -0.00113713   0.115284    -0.0467136   -0.028946     -0.0107684     0.0529495    0.233376      0.056992    -0.251074      0.0774906    0.0383462     0.0153733   -0.0409812    0.0405064    0.15423     -0.0304174    0.0576616     0.101183     0.135988     0.0999595   -0.068506  
  0.00519307   0.104434    -0.0827826   -0.185155      0.0831724   -0.0875637   -0.00891877   0.0952872    0.290056     -0.168791      0.0434121    0.0082009     0.0444773   -0.0281285     0.00085123   0.0391058     0.0478132    0.0156602    0.128129     0.0307028   -0.0825709   -0.0365015     0.0289902   -0.0614815    0.0239785    0.0811831 
 -0.00562747   0.123742    -0.0541913    0.017066      0.034071    -0.0905258    0.0403967    0.0155677   -0.0816347    -0.00152528   -0.00873055  -0.0520002     0.113932    -0.0256573    -0.126263     0.0296206    -0.0973946    0.0512592    0.0879161    0.0970469   -0.0154803   -0.0674855    -0.0350952   -0.10264      0.00393175   0.0137592 
 -0.171879     0.0972453    0.139717    -0.0300324     0.136278     0.0407256   -0.0368434    0.113148    -0.095594     -0.0454846    -0.0184614    0.0324651     0.114709    -0.0281332     0.130838     0.0108876     0.00969373   0.0736362    0.0251748    0.0244593   -0.0550863    0.12261      -0.0189626    0.0475175    0.220587     0.012834  
 -0.115761     0.0200619    0.0458501    0.0609607     0.0260675   -0.0399702   -0.082775    -0.0404993    0.166501     -0.0358762    -0.0404496   -0.00303764    0.19164      0.00182119    0.0949313    0.0567871     0.0859317   -0.179581    -0.0584275   -0.121377    -0.075095     0.0970852    -0.0692795   -0.144413     0.096384     0.0356366 
 -0.10539     -0.0676528    0.0270116   -0.00133725    0.0181514   -0.0370856   -0.0758726    0.0714981   -0.038607     -0.0393532    -0.0192966    0.00610517   -0.18164      0.264902     -0.0965789    0.000966744  -0.0421883   -0.0965496    0.0650224   -0.0381428    0.0559538    0.0395507     0.0798235    0.0655231    0.0275164   -0.00143874
 -0.0753057   -0.0503107   -0.0299592    0.148053      0.0395922    0.0193258   -0.0655966   -0.10512     -0.106337     -0.0616545    -0.152104    -0.0480484    -0.0516037   -0.0473624     0.0409798   -0.00562985   -0.13194     -0.0506784    0.103999     0.0107764   -0.110718     0.165898      0.140961     0.0696902    0.0242157   -0.12368   
  0.0161717    0.0594927   -0.033021    -0.00139785   -0.103403     0.191523     0.16817     -0.152143    -0.0247417    -0.0941399     0.0881483   -0.167122     -0.0675626   -0.147049     -0.0871342    0.0754314    -0.0556341    0.0681079   -0.0423347   -0.131254    -0.00520611   0.0020043    -0.0138944   -0.164188     0.0377741   -0.126212  
 -0.0891309    0.0176465    0.0230266    0.180156     -0.00662806  -0.0582593    0.130952     0.114745     0.05571       0.0868956     0.0350322   -0.0326408     0.0128031   -0.0242733     0.0535981   -0.00856863   -0.197133    -0.169544    -0.0699909   -0.0595639    0.0394345    0.0154471     0.10355     -0.112089    -0.0113752   -0.0218987 
 -0.118889     0.217282     0.0970896   -0.0266542     0.123035     0.0543921   -0.119083     0.00579159   0.0783006    -0.036486     -0.10013     -0.0391425    -0.0292009    0.107724      0.0601868   -0.0386996     0.0733374   -0.0114108   -0.0183462    0.0677607    5.81603e-5  -0.16334      -0.0144236   -0.0780323    0.0559642   -0.0772372 
  0.046438     0.0396289    0.00100353  -0.346734      0.0285859   -0.00387031  -0.0352305   -0.0865186    0.158211     -0.191931     -0.107632     0.0245509    -0.208415     0.0207938    -0.0772659   -0.102562     -0.083055     0.13927      0.0254859   -0.0667658   -0.00274697   0.0198534    -0.135561    -0.0781782   -0.149957    -0.157882  
  0.0816675    0.169789     0.0769501    0.156043      0.0309743   -0.0497321   -0.00263053   0.141996    -0.177025     -0.0739434    -0.0561699    0.0351769     0.0070241    0.00216303    0.152915    -0.0421456    -0.00236533   0.0388628   -0.07066     -0.046985    -0.112175    -0.0627507    -0.150673     0.00880049   0.0668699    0.0279921 
  0.044193     0.0340287    0.020542     0.125611     -0.00896538   0.0761431    0.0290401   -0.0362853    0.0203421    -0.0117887     0.0104174   -0.000321201   0.0236162    6.03032e-5   -0.0305497    0.150451     -0.0235231   -0.0839529    0.0329237   -0.12205      0.0916945    0.0567041     0.093359    -0.0336073   -0.0231078   -0.0566972 
 -0.201806    -0.0275363   -0.0356725    0.108385     -0.115833     0.0594449    0.0970152   -0.0533373    0.160586     -0.113018      0.00902926   0.252298      0.0695201   -0.103368     -0.0896161    0.129293      0.0901148   -0.214872    -0.0663085    0.126881     0.0787587   -0.019584     -0.0437112    0.0137055    0.0875238    0.0173573 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3
INFO: iteration 1, average log likelihood -1.184138
WARNING: Variances had to be floored 3 12
INFO: iteration 2, average log likelihood -1.140678
WARNING: Variances had to be floored 2 3 6 9 13 17 24
INFO: iteration 3, average log likelihood -1.095745
WARNING: Variances had to be floored 3 12 14 15 16 18 29
INFO: iteration 4, average log likelihood -1.127436
WARNING: Variances had to be floored 3 27
INFO: iteration 5, average log likelihood -1.141645
WARNING: Variances had to be floored 2 3 9 12 13 17 24
INFO: iteration 6, average log likelihood -1.111718
WARNING: Variances had to be floored 3 6 16 29
INFO: iteration 7, average log likelihood -1.136084
WARNING: Variances had to be floored 3 12 14 15 18
INFO: iteration 8, average log likelihood -1.123312
WARNING: Variances had to be floored 2 3 9 13 17 24 27
INFO: iteration 9, average log likelihood -1.109410
WARNING: Variances had to be floored 3 12 16 29
INFO: iteration 10, average log likelihood -1.146331
INFO: EM with 100000 data points 10 iterations avll -1.146331
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0299371    0.0540467     0.0622771    0.122478     -0.0555199   -0.00914474  -0.0801453    -0.209206    -0.0953941    0.185788    -0.0267882    0.0881314    0.112004    -0.21077     -0.137132    -0.0111177   -0.0912969    -0.0661196    0.0199558   -0.139135    -0.167571    -0.0303525   -0.0307688    0.00661728   -0.298099     0.0631568 
 -0.0655218   -0.0162685    -0.11427     -0.0158417    -0.209621    -0.139138     0.127748      0.113389    -0.15442      0.0679578    0.166575     0.0330826   -0.0521919    0.0169329    0.023767    -0.0017911    0.184206      0.10878     -0.117936     0.00385547  -0.0458807   -0.0858809    0.00460944   0.00414646   -0.0128476   -0.0862627 
 -0.0716571   -0.00424901   -0.0676935   -0.0914447    -0.167997    -0.0878971   -0.000124545  -0.217187    -0.00747314   0.116483    -0.0408885    0.0345142    0.122109    -0.00420789  -0.123547    -0.0751317   -0.0949104     0.037009    -0.0622835   -0.011633     0.111958     0.0145813   -0.037572     0.18818       0.0627137   -0.0726046 
  0.0684516    0.127932      0.0111722   -0.1462       -0.0521196    0.067855     0.121375     -0.12181      0.0201414   -0.0579748   -0.102759     0.19505      0.0635002    0.0459152    0.0191731   -0.119228     0.0948699    -0.0801332   -0.132974    -0.0748461    0.0214281   -0.0123388   -0.120235    -0.00978636   -0.0529082   -0.238187  
  0.0220778    0.0586918     0.0846487    0.0414064    -0.0394975   -0.0653202   -0.129265     -0.00580635   0.100926    -0.11037      0.113308     0.0410468   -0.0284732    0.0892707    0.109862    -0.164625     0.0445836     0.0692397    0.0599487    0.00329028   0.0674861   -0.0593763    0.0426314   -0.0185606     0.0411462   -0.0351341 
  0.0342007   -0.0227512    -0.0732483    0.000168155  -0.195526    -0.139821    -0.0838022     0.0360184    0.0305085   -0.0430348   -0.0238536    0.109344     0.0692379    0.0413124    0.049751    -0.054274    -0.0272874     0.187459    -0.0448201    0.105494     0.094857    -0.0818106   -0.0240102   -0.124371     -0.121565     0.0808923 
  0.142769    -0.0035748     0.193779    -0.0630083    -0.0495414    0.0111953    0.0596292     0.0737084    0.0487375   -0.0251214   -0.20032      0.0325105   -0.206317     0.0851388   -0.0117033    0.103966     0.0862839     0.0321417    0.136701     0.0228065   -0.224326    -0.0503791    0.0115882   -0.0259367     0.0636156   -0.164849  
  0.0225766    0.0490474    -0.00385264   0.101        -0.0565495   -0.0384319   -0.0950975     0.0862615    0.0581957    0.00857033   0.00601808  -0.0838646    0.164748     0.175041     0.0035428   -0.118655    -0.0665583     0.0349212   -0.0881173    0.0492907   -0.0396986    0.0683237   -0.240945     0.158163     -0.0153914    0.0600197 
  0.0204071   -0.0102477     0.0253983    0.0889945     0.00946024  -0.150103     0.0994702     0.0418635   -0.0675037   -0.114508    -0.0736243   -0.0627112   -0.0392529    0.0736735    0.0453545   -0.0177206   -0.0548749    -0.0311864    0.128994     0.134262     0.0924991    0.0874816   -0.119464     0.0695629     0.0866781   -0.218261  
 -0.136235    -0.0313269    -0.0417936    0.217385     -0.0117537    0.10926     -0.180008     -0.0589714   -0.104897     0.161341     0.0462076    0.264705    -0.142091     0.0623291    0.00328542   0.105202    -0.0337654    -0.139755    -0.021559     0.0106607    0.00716734   0.0461834   -0.00332424   0.028104     -0.0846262   -0.0143068 
  0.0564093   -0.0174892     0.169227     0.15651      -0.0295952   -0.0426571   -0.0930325    -0.0949739    0.119903     0.0179584    0.0697352    0.133349     0.00814149  -0.154642     0.210768     0.107033     0.0620565    -0.167191    -0.140294    -0.0436598   -0.143347    -0.0347496    0.0771066   -0.0569114    -0.197684     0.0765614 
  0.00744136  -0.128441      0.00628931  -0.0319543    -0.0585041   -0.00773215   0.0374342    -0.0571784    0.11655     -0.112874     0.0822458   -0.0480532   -0.0372622    0.0336648   -0.0562606    0.129104     0.203211      0.0543493    0.00576006   0.00240751   0.126261     0.0637368   -0.200232    -0.0498824     0.0514934    0.00164487
 -0.194524    -0.0241026     0.0311766   -0.0164489     0.0207815    0.187665     0.0248369    -0.0535487    0.0893277   -0.0855883   -0.0686981    0.175226    -0.0960056    0.00526509   0.0493756    0.0858461   -0.000997221   0.0440437    0.024294    -0.0152371    0.0192265    0.0929246   -0.14281     -0.115039     -0.0313733    0.0961876 
 -0.0505648   -0.23013       0.0754685    0.0303779     0.0490535   -0.0753983   -0.0770106     0.0302179   -0.0598127    0.0799593   -0.0482532   -0.216247    -0.0104717   -0.0498775   -0.12911      0.0499816   -0.0211455     0.0372464    0.0275361    0.0917229   -0.042343    -0.136059     0.14846      0.109143      0.0529133    0.17506   
 -0.084626     0.0375927     0.00888038   0.00128844   -0.0059008   -0.0646509    0.0511413     0.0565879   -0.0622972    0.0443112   -0.132161    -0.118722    -0.0707249    0.155966    -0.0244302    0.025767    -0.0260142     0.0494977   -0.142501     0.0873832   -0.00363044   0.0383852    0.0814393    0.0375924    -0.0688247    0.0170216 
 -0.0562189   -0.0582506    -0.00527243  -0.153667     -0.137469    -0.171479     0.142794      0.0671482   -0.0750927    0.105187     0.0602024    0.0924657    0.117778     0.110309     0.0573793    0.0238623    0.0611448     0.00800791   0.0249006    0.014783    -0.119979     0.154017    -0.0779239   -0.0649312     0.112321     0.122259  
  0.107735    -0.000867663   0.12351      0.0918005    -0.155627     0.0256367   -0.0693308     0.122506    -0.135689     0.117738    -0.0562978   -0.018794     0.0816672    0.0451614   -0.151477     0.0589385   -0.0572797    -0.114552     0.0529179   -0.127817    -0.208457     0.0681505   -0.0145286   -0.107125      0.22432     -0.0188325 
 -0.0650784    0.06879      -0.0776395   -0.0399349     0.0119656    0.100741     0.0567338    -0.0140734    0.170751    -0.00776304  -0.046856    -0.101176     0.0454896   -0.0735558   -0.0910814   -0.0644687   -0.0638522     0.194691    -0.0852202    0.0890659   -0.0333366   -0.00917988  -0.110846    -0.146631     -0.0711507    0.0405095 
  0.260178    -0.158044      0.0292513    0.117499      0.04474      0.0979336    0.00737802   -0.0105433    0.192476     0.0787112   -0.0687318    0.102375    -0.16807     -0.0839929   -0.121205    -0.107062     0.0295714     0.0511969   -0.16237     -0.00866529  -0.20821      0.0944738    0.180946    -0.00427762   -0.133461    -0.0916507 
  0.0776606   -0.0193006    -0.0851831   -0.127802      0.00250406  -0.0385889    0.0377436     0.017073    -0.06634     -0.0363158    0.00710253   0.128382     0.123138    -0.194384     0.00323684  -0.164986     0.0576072    -0.0961149    0.0708156   -0.0410526   -0.0550749   -0.00927354   0.0790567   -0.0338213     0.114034     0.115063  
 -0.113338     0.0839159     0.119357    -0.142544      0.142007    -0.114297    -0.190612      0.0528068   -0.0413297    0.0191788   -0.151678    -0.126299     0.0291465    0.125075     0.0610656   -0.0722464    0.150957     -0.0572002    0.0469738    0.0749786   -0.100759     0.0577479    0.207493    -0.0936918     0.153136     0.102635  
  0.0498144    0.0960848    -0.0269744    0.0793319     0.0439858   -0.142915     0.00436678   -0.210304     0.10131     -0.191052     0.110605     0.0216267    0.00943422   0.206347    -0.00773229   0.00845339   0.00300917   -0.0876614   -0.00946905   0.0692993   -0.162306     0.080815    -0.15854     -0.071735     -0.332611    -0.150454  
 -0.0426073   -0.0701979    -0.0368164   -0.273883      0.0733778    0.0977799    0.0523923     0.139323    -0.109561     0.0347141   -0.0154555   -0.12682      0.0583626    0.0210731   -0.0749999    0.0157782    0.199344      0.1485       0.101282     0.124755     0.179897     0.00953095  -0.163021     0.0662039     0.0859484    0.0383749 
  0.0349153    0.165103     -0.0992644   -0.18348       0.112247     0.0166785    0.155319      0.0404509    0.1471       0.00321484  -0.117763    -0.160326     0.0874044   -0.0510772   -0.10652     -0.00258509   0.141412     -0.0586947    0.0821447   -0.0137692    0.0171776    0.0277775   -0.00553289   0.0414367     0.139771     0.177055  
  0.0351223    0.0171559     0.14168      0.0563599     0.109187     0.155341    -0.191858      0.1294       0.0818119   -0.0854512    0.0449243   -0.00102372  -0.00442308  -0.0313394    0.0826404   -0.16618     -0.0252347    -0.0228452    0.049115    -0.202104     0.0339051   -0.0743253    0.00585632   0.114882     -0.0334933   -0.19393   
 -0.0204978    0.0487952    -0.238571    -0.0779414    -0.0427388   -0.0237697   -0.0577345     0.0979159   -0.0196457    0.00840139   0.101914     0.0232551   -0.046533    -0.0845865    0.0462293   -0.167014     0.144475      0.170721     0.0913428   -0.109178     0.0706759   -0.142006     0.0207406   -0.0788054     0.0224044    0.0378111 
  0.183358    -0.15222       0.22941     -0.0316925    -0.025194    -0.0380551    0.136391     -0.0117481   -0.197489    -0.108734     0.170491    -0.100434    -0.174074     0.0715553    0.0230222   -0.115864    -0.046166     -0.229018    -0.0178938    0.00502735   0.0722137    0.0575586    0.0192602   -0.000267846  -0.00146449   0.129437  
 -0.100337     0.145541      0.00136281  -0.038626      0.0259597   -0.234015    -0.111532     -0.0634108   -0.0546485    0.0578619    0.0501918   -0.108272    -0.0978403    0.0496268   -0.065745    -0.0559441   -0.0379495     0.0679163   -0.219718    -0.121636     0.109947    -0.00900127   0.0120199   -0.0249283    -0.143606     0.122042  
  0.0299803    0.0843593    -0.0158657   -0.0411432     0.0209946    0.103458     0.0116998     0.0106781   -0.00956078   0.0620085    0.0410055   -0.0312808    0.141838    -0.00926728  -0.0407875    0.123678     0.0960449     0.00318401   0.281412     0.00322548  -0.0159562    0.0609236   -0.0422063    0.084929      0.0127711   -0.0772437 
 -0.0149742    0.0144523     0.102209    -0.103707      0.0723914    0.0335742   -0.0520291    -0.166363    -0.0227114   -0.0614148   -0.0235698    0.0633954   -0.0484915    0.163191     0.170054     0.0606612   -0.030769     -0.0419687    0.063353     0.105797    -0.0401115   -0.162606     0.116367    -0.189277     -0.0851062    0.100756  
  0.149181    -0.0185808    -0.0473101   -0.136059     -0.129592    -0.0357648   -0.0339289    -0.0750787   -0.102071    -0.00801755   0.150446    -0.133267     0.0487227    0.0412689    0.0962612   -0.00445934   0.0886642     0.0302861    0.123304     0.0149086   -0.0463589   -0.169669    -0.258141     0.139575      0.20079      0.0509223 
  0.117514    -0.0475878    -0.160439     0.0420795     0.00984926   0.0195429   -0.0236957     0.0154386    0.0800509   -0.0258502   -0.0962138    0.124854     0.212024    -0.0276045   -0.0633593    0.0807574   -0.0919558     0.230629    -0.0904397    0.0875361   -0.0974111   -0.00655383  -0.0181602   -0.129084     -0.0568561    0.0128137 kind full, method split
0: avll = -1.4187396426997028
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418758
INFO: iteration 2, average log likelihood -1.418696
INFO: iteration 3, average log likelihood -1.418646
INFO: iteration 4, average log likelihood -1.418582
INFO: iteration 5, average log likelihood -1.418497
INFO: iteration 6, average log likelihood -1.418377
INFO: iteration 7, average log likelihood -1.418197
INFO: iteration 8, average log likelihood -1.417901
INFO: iteration 9, average log likelihood -1.417382
INFO: iteration 10, average log likelihood -1.416531
INFO: iteration 11, average log likelihood -1.415412
INFO: iteration 12, average log likelihood -1.414371
INFO: iteration 13, average log likelihood -1.413696
INFO: iteration 14, average log likelihood -1.413362
INFO: iteration 15, average log likelihood -1.413217
INFO: iteration 16, average log likelihood -1.413158
INFO: iteration 17, average log likelihood -1.413133
INFO: iteration 18, average log likelihood -1.413123
INFO: iteration 19, average log likelihood -1.413118
INFO: iteration 20, average log likelihood -1.413116
INFO: iteration 21, average log likelihood -1.413115
INFO: iteration 22, average log likelihood -1.413115
INFO: iteration 23, average log likelihood -1.413114
INFO: iteration 24, average log likelihood -1.413114
INFO: iteration 25, average log likelihood -1.413114
INFO: iteration 26, average log likelihood -1.413113
INFO: iteration 27, average log likelihood -1.413113
INFO: iteration 28, average log likelihood -1.413113
INFO: iteration 29, average log likelihood -1.413113
INFO: iteration 30, average log likelihood -1.413113
INFO: iteration 31, average log likelihood -1.413113
INFO: iteration 32, average log likelihood -1.413113
INFO: iteration 33, average log likelihood -1.413113
INFO: iteration 34, average log likelihood -1.413113
INFO: iteration 35, average log likelihood -1.413112
INFO: iteration 36, average log likelihood -1.413112
INFO: iteration 37, average log likelihood -1.413112
INFO: iteration 38, average log likelihood -1.413112
INFO: iteration 39, average log likelihood -1.413112
INFO: iteration 40, average log likelihood -1.413112
INFO: iteration 41, average log likelihood -1.413112
INFO: iteration 42, average log likelihood -1.413112
INFO: iteration 43, average log likelihood -1.413112
INFO: iteration 44, average log likelihood -1.413112
INFO: iteration 45, average log likelihood -1.413112
INFO: iteration 46, average log likelihood -1.413112
INFO: iteration 47, average log likelihood -1.413112
INFO: iteration 48, average log likelihood -1.413112
INFO: iteration 49, average log likelihood -1.413112
INFO: iteration 50, average log likelihood -1.413112
INFO: EM with 100000 data points 50 iterations avll -1.413112
952.4 data points per parameter
1: avll = [-1.41876,-1.4187,-1.41865,-1.41858,-1.4185,-1.41838,-1.4182,-1.4179,-1.41738,-1.41653,-1.41541,-1.41437,-1.4137,-1.41336,-1.41322,-1.41316,-1.41313,-1.41312,-1.41312,-1.41312,-1.41312,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413127
INFO: iteration 2, average log likelihood -1.413071
INFO: iteration 3, average log likelihood -1.413023
INFO: iteration 4, average log likelihood -1.412962
INFO: iteration 5, average log likelihood -1.412882
INFO: iteration 6, average log likelihood -1.412782
INFO: iteration 7, average log likelihood -1.412669
INFO: iteration 8, average log likelihood -1.412556
INFO: iteration 9, average log likelihood -1.412454
INFO: iteration 10, average log likelihood -1.412367
INFO: iteration 11, average log likelihood -1.412294
INFO: iteration 12, average log likelihood -1.412230
INFO: iteration 13, average log likelihood -1.412174
INFO: iteration 14, average log likelihood -1.412123
INFO: iteration 15, average log likelihood -1.412078
INFO: iteration 16, average log likelihood -1.412037
INFO: iteration 17, average log likelihood -1.412000
INFO: iteration 18, average log likelihood -1.411967
INFO: iteration 19, average log likelihood -1.411936
INFO: iteration 20, average log likelihood -1.411907
INFO: iteration 21, average log likelihood -1.411881
INFO: iteration 22, average log likelihood -1.411857
INFO: iteration 23, average log likelihood -1.411834
INFO: iteration 24, average log likelihood -1.411814
INFO: iteration 25, average log likelihood -1.411795
INFO: iteration 26, average log likelihood -1.411779
INFO: iteration 27, average log likelihood -1.411765
INFO: iteration 28, average log likelihood -1.411753
INFO: iteration 29, average log likelihood -1.411742
INFO: iteration 30, average log likelihood -1.411733
INFO: iteration 31, average log likelihood -1.411725
INFO: iteration 32, average log likelihood -1.411718
INFO: iteration 33, average log likelihood -1.411712
INFO: iteration 34, average log likelihood -1.411707
INFO: iteration 35, average log likelihood -1.411702
INFO: iteration 36, average log likelihood -1.411698
INFO: iteration 37, average log likelihood -1.411694
INFO: iteration 38, average log likelihood -1.411690
INFO: iteration 39, average log likelihood -1.411687
INFO: iteration 40, average log likelihood -1.411684
INFO: iteration 41, average log likelihood -1.411681
INFO: iteration 42, average log likelihood -1.411679
INFO: iteration 43, average log likelihood -1.411676
INFO: iteration 44, average log likelihood -1.411674
INFO: iteration 45, average log likelihood -1.411672
INFO: iteration 46, average log likelihood -1.411670
INFO: iteration 47, average log likelihood -1.411668
INFO: iteration 48, average log likelihood -1.411666
INFO: iteration 49, average log likelihood -1.411665
INFO: iteration 50, average log likelihood -1.411663
INFO: EM with 100000 data points 50 iterations avll -1.411663
473.9 data points per parameter
2: avll = [-1.41313,-1.41307,-1.41302,-1.41296,-1.41288,-1.41278,-1.41267,-1.41256,-1.41245,-1.41237,-1.41229,-1.41223,-1.41217,-1.41212,-1.41208,-1.41204,-1.412,-1.41197,-1.41194,-1.41191,-1.41188,-1.41186,-1.41183,-1.41181,-1.4118,-1.41178,-1.41177,-1.41175,-1.41174,-1.41173,-1.41172,-1.41172,-1.41171,-1.41171,-1.4117,-1.4117,-1.41169,-1.41169,-1.41169,-1.41168,-1.41168,-1.41168,-1.41168,-1.41167,-1.41167,-1.41167,-1.41167,-1.41167,-1.41166,-1.41166]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411672
INFO: iteration 2, average log likelihood -1.411624
INFO: iteration 3, average log likelihood -1.411581
INFO: iteration 4, average log likelihood -1.411531
INFO: iteration 5, average log likelihood -1.411469
INFO: iteration 6, average log likelihood -1.411396
INFO: iteration 7, average log likelihood -1.411311
INFO: iteration 8, average log likelihood -1.411220
INFO: iteration 9, average log likelihood -1.411129
INFO: iteration 10, average log likelihood -1.411041
INFO: iteration 11, average log likelihood -1.410960
INFO: iteration 12, average log likelihood -1.410887
INFO: iteration 13, average log likelihood -1.410823
INFO: iteration 14, average log likelihood -1.410766
INFO: iteration 15, average log likelihood -1.410716
INFO: iteration 16, average log likelihood -1.410672
INFO: iteration 17, average log likelihood -1.410634
INFO: iteration 18, average log likelihood -1.410601
INFO: iteration 19, average log likelihood -1.410572
INFO: iteration 20, average log likelihood -1.410547
INFO: iteration 21, average log likelihood -1.410525
INFO: iteration 22, average log likelihood -1.410506
INFO: iteration 23, average log likelihood -1.410488
INFO: iteration 24, average log likelihood -1.410473
INFO: iteration 25, average log likelihood -1.410459
INFO: iteration 26, average log likelihood -1.410446
INFO: iteration 27, average log likelihood -1.410434
INFO: iteration 28, average log likelihood -1.410422
INFO: iteration 29, average log likelihood -1.410412
INFO: iteration 30, average log likelihood -1.410402
INFO: iteration 31, average log likelihood -1.410392
INFO: iteration 32, average log likelihood -1.410383
INFO: iteration 33, average log likelihood -1.410375
INFO: iteration 34, average log likelihood -1.410367
INFO: iteration 35, average log likelihood -1.410359
INFO: iteration 36, average log likelihood -1.410351
INFO: iteration 37, average log likelihood -1.410343
INFO: iteration 38, average log likelihood -1.410336
INFO: iteration 39, average log likelihood -1.410329
INFO: iteration 40, average log likelihood -1.410322
INFO: iteration 41, average log likelihood -1.410315
INFO: iteration 42, average log likelihood -1.410308
INFO: iteration 43, average log likelihood -1.410302
INFO: iteration 44, average log likelihood -1.410295
INFO: iteration 45, average log likelihood -1.410289
INFO: iteration 46, average log likelihood -1.410282
INFO: iteration 47, average log likelihood -1.410276
INFO: iteration 48, average log likelihood -1.410269
INFO: iteration 49, average log likelihood -1.410263
INFO: iteration 50, average log likelihood -1.410257
INFO: EM with 100000 data points 50 iterations avll -1.410257
236.4 data points per parameter
3: avll = [-1.41167,-1.41162,-1.41158,-1.41153,-1.41147,-1.4114,-1.41131,-1.41122,-1.41113,-1.41104,-1.41096,-1.41089,-1.41082,-1.41077,-1.41072,-1.41067,-1.41063,-1.4106,-1.41057,-1.41055,-1.41053,-1.41051,-1.41049,-1.41047,-1.41046,-1.41045,-1.41043,-1.41042,-1.41041,-1.4104,-1.41039,-1.41038,-1.41037,-1.41037,-1.41036,-1.41035,-1.41034,-1.41034,-1.41033,-1.41032,-1.41032,-1.41031,-1.4103,-1.4103,-1.41029,-1.41028,-1.41028,-1.41027,-1.41026,-1.41026]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410261
INFO: iteration 2, average log likelihood -1.410206
INFO: iteration 3, average log likelihood -1.410160
INFO: iteration 4, average log likelihood -1.410108
INFO: iteration 5, average log likelihood -1.410046
INFO: iteration 6, average log likelihood -1.409971
INFO: iteration 7, average log likelihood -1.409884
INFO: iteration 8, average log likelihood -1.409789
INFO: iteration 9, average log likelihood -1.409690
INFO: iteration 10, average log likelihood -1.409593
INFO: iteration 11, average log likelihood -1.409501
INFO: iteration 12, average log likelihood -1.409417
INFO: iteration 13, average log likelihood -1.409340
INFO: iteration 14, average log likelihood -1.409270
INFO: iteration 15, average log likelihood -1.409205
INFO: iteration 16, average log likelihood -1.409145
INFO: iteration 17, average log likelihood -1.409089
INFO: iteration 18, average log likelihood -1.409037
INFO: iteration 19, average log likelihood -1.408989
INFO: iteration 20, average log likelihood -1.408944
INFO: iteration 21, average log likelihood -1.408902
INFO: iteration 22, average log likelihood -1.408863
INFO: iteration 23, average log likelihood -1.408827
INFO: iteration 24, average log likelihood -1.408793
INFO: iteration 25, average log likelihood -1.408761
INFO: iteration 26, average log likelihood -1.408730
INFO: iteration 27, average log likelihood -1.408700
INFO: iteration 28, average log likelihood -1.408672
INFO: iteration 29, average log likelihood -1.408646
INFO: iteration 30, average log likelihood -1.408620
INFO: iteration 31, average log likelihood -1.408595
INFO: iteration 32, average log likelihood -1.408571
INFO: iteration 33, average log likelihood -1.408548
INFO: iteration 34, average log likelihood -1.408527
INFO: iteration 35, average log likelihood -1.408506
INFO: iteration 36, average log likelihood -1.408486
INFO: iteration 37, average log likelihood -1.408467
INFO: iteration 38, average log likelihood -1.408449
INFO: iteration 39, average log likelihood -1.408432
INFO: iteration 40, average log likelihood -1.408416
INFO: iteration 41, average log likelihood -1.408400
INFO: iteration 42, average log likelihood -1.408386
INFO: iteration 43, average log likelihood -1.408372
INFO: iteration 44, average log likelihood -1.408359
INFO: iteration 45, average log likelihood -1.408346
INFO: iteration 46, average log likelihood -1.408334
INFO: iteration 47, average log likelihood -1.408323
INFO: iteration 48, average log likelihood -1.408312
INFO: iteration 49, average log likelihood -1.408302
INFO: iteration 50, average log likelihood -1.408292
INFO: EM with 100000 data points 50 iterations avll -1.408292
118.1 data points per parameter
4: avll = [-1.41026,-1.41021,-1.41016,-1.41011,-1.41005,-1.40997,-1.40988,-1.40979,-1.40969,-1.40959,-1.4095,-1.40942,-1.40934,-1.40927,-1.40921,-1.40915,-1.40909,-1.40904,-1.40899,-1.40894,-1.4089,-1.40886,-1.40883,-1.40879,-1.40876,-1.40873,-1.4087,-1.40867,-1.40865,-1.40862,-1.40859,-1.40857,-1.40855,-1.40853,-1.40851,-1.40849,-1.40847,-1.40845,-1.40843,-1.40842,-1.4084,-1.40839,-1.40837,-1.40836,-1.40835,-1.40833,-1.40832,-1.40831,-1.4083,-1.40829]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408291
INFO: iteration 2, average log likelihood -1.408227
INFO: iteration 3, average log likelihood -1.408166
INFO: iteration 4, average log likelihood -1.408094
INFO: iteration 5, average log likelihood -1.408005
INFO: iteration 6, average log likelihood -1.407893
INFO: iteration 7, average log likelihood -1.407758
INFO: iteration 8, average log likelihood -1.407605
INFO: iteration 9, average log likelihood -1.407440
INFO: iteration 10, average log likelihood -1.407274
INFO: iteration 11, average log likelihood -1.407112
INFO: iteration 12, average log likelihood -1.406959
INFO: iteration 13, average log likelihood -1.406819
INFO: iteration 14, average log likelihood -1.406693
INFO: iteration 15, average log likelihood -1.406580
INFO: iteration 16, average log likelihood -1.406480
INFO: iteration 17, average log likelihood -1.406392
INFO: iteration 18, average log likelihood -1.406315
INFO: iteration 19, average log likelihood -1.406247
INFO: iteration 20, average log likelihood -1.406187
INFO: iteration 21, average log likelihood -1.406134
INFO: iteration 22, average log likelihood -1.406086
INFO: iteration 23, average log likelihood -1.406042
INFO: iteration 24, average log likelihood -1.406002
INFO: iteration 25, average log likelihood -1.405966
INFO: iteration 26, average log likelihood -1.405932
INFO: iteration 27, average log likelihood -1.405900
INFO: iteration 28, average log likelihood -1.405870
INFO: iteration 29, average log likelihood -1.405841
INFO: iteration 30, average log likelihood -1.405814
INFO: iteration 31, average log likelihood -1.405788
INFO: iteration 32, average log likelihood -1.405763
INFO: iteration 33, average log likelihood -1.405739
INFO: iteration 34, average log likelihood -1.405716
INFO: iteration 35, average log likelihood -1.405694
INFO: iteration 36, average log likelihood -1.405672
INFO: iteration 37, average log likelihood -1.405651
INFO: iteration 38, average log likelihood -1.405631
INFO: iteration 39, average log likelihood -1.405612
INFO: iteration 40, average log likelihood -1.405593
INFO: iteration 41, average log likelihood -1.405574
INFO: iteration 42, average log likelihood -1.405557
INFO: iteration 43, average log likelihood -1.405540
INFO: iteration 44, average log likelihood -1.405523
INFO: iteration 45, average log likelihood -1.405507
INFO: iteration 46, average log likelihood -1.405491
INFO: iteration 47, average log likelihood -1.405476
INFO: iteration 48, average log likelihood -1.405461
INFO: iteration 49, average log likelihood -1.405447
INFO: iteration 50, average log likelihood -1.405433
INFO: EM with 100000 data points 50 iterations avll -1.405433
59.0 data points per parameter
5: avll = [-1.40829,-1.40823,-1.40817,-1.40809,-1.408,-1.40789,-1.40776,-1.4076,-1.40744,-1.40727,-1.40711,-1.40696,-1.40682,-1.40669,-1.40658,-1.40648,-1.40639,-1.40631,-1.40625,-1.40619,-1.40613,-1.40609,-1.40604,-1.406,-1.40597,-1.40593,-1.4059,-1.40587,-1.40584,-1.40581,-1.40579,-1.40576,-1.40574,-1.40572,-1.40569,-1.40567,-1.40565,-1.40563,-1.40561,-1.40559,-1.40557,-1.40556,-1.40554,-1.40552,-1.40551,-1.40549,-1.40548,-1.40546,-1.40545,-1.40543]
[-1.41874,-1.41876,-1.4187,-1.41865,-1.41858,-1.4185,-1.41838,-1.4182,-1.4179,-1.41738,-1.41653,-1.41541,-1.41437,-1.4137,-1.41336,-1.41322,-1.41316,-1.41313,-1.41312,-1.41312,-1.41312,-1.41312,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41311,-1.41313,-1.41307,-1.41302,-1.41296,-1.41288,-1.41278,-1.41267,-1.41256,-1.41245,-1.41237,-1.41229,-1.41223,-1.41217,-1.41212,-1.41208,-1.41204,-1.412,-1.41197,-1.41194,-1.41191,-1.41188,-1.41186,-1.41183,-1.41181,-1.4118,-1.41178,-1.41177,-1.41175,-1.41174,-1.41173,-1.41172,-1.41172,-1.41171,-1.41171,-1.4117,-1.4117,-1.41169,-1.41169,-1.41169,-1.41168,-1.41168,-1.41168,-1.41168,-1.41167,-1.41167,-1.41167,-1.41167,-1.41167,-1.41166,-1.41166,-1.41167,-1.41162,-1.41158,-1.41153,-1.41147,-1.4114,-1.41131,-1.41122,-1.41113,-1.41104,-1.41096,-1.41089,-1.41082,-1.41077,-1.41072,-1.41067,-1.41063,-1.4106,-1.41057,-1.41055,-1.41053,-1.41051,-1.41049,-1.41047,-1.41046,-1.41045,-1.41043,-1.41042,-1.41041,-1.4104,-1.41039,-1.41038,-1.41037,-1.41037,-1.41036,-1.41035,-1.41034,-1.41034,-1.41033,-1.41032,-1.41032,-1.41031,-1.4103,-1.4103,-1.41029,-1.41028,-1.41028,-1.41027,-1.41026,-1.41026,-1.41026,-1.41021,-1.41016,-1.41011,-1.41005,-1.40997,-1.40988,-1.40979,-1.40969,-1.40959,-1.4095,-1.40942,-1.40934,-1.40927,-1.40921,-1.40915,-1.40909,-1.40904,-1.40899,-1.40894,-1.4089,-1.40886,-1.40883,-1.40879,-1.40876,-1.40873,-1.4087,-1.40867,-1.40865,-1.40862,-1.40859,-1.40857,-1.40855,-1.40853,-1.40851,-1.40849,-1.40847,-1.40845,-1.40843,-1.40842,-1.4084,-1.40839,-1.40837,-1.40836,-1.40835,-1.40833,-1.40832,-1.40831,-1.4083,-1.40829,-1.40829,-1.40823,-1.40817,-1.40809,-1.408,-1.40789,-1.40776,-1.4076,-1.40744,-1.40727,-1.40711,-1.40696,-1.40682,-1.40669,-1.40658,-1.40648,-1.40639,-1.40631,-1.40625,-1.40619,-1.40613,-1.40609,-1.40604,-1.406,-1.40597,-1.40593,-1.4059,-1.40587,-1.40584,-1.40581,-1.40579,-1.40576,-1.40574,-1.40572,-1.40569,-1.40567,-1.40565,-1.40563,-1.40561,-1.40559,-1.40557,-1.40556,-1.40554,-1.40552,-1.40551,-1.40549,-1.40548,-1.40546,-1.40545,-1.40543]
32×26 Array{Float64,2}:
 -0.645706    -0.742643   -0.204388     0.59384     0.0282306   -0.33666     -0.236282    -0.255037     0.190977     0.0246145   0.0173379   0.296876    -0.590199     0.0216325   -0.0401359   0.133762    -0.0212001   -0.116296     0.402703     -0.146898    -0.0886734    0.651577   -0.591738    -0.213661   -0.0412065   0.143611   
  0.0751784    0.108322   -0.397709     0.0662266   0.30587     -0.0822637   -0.0368805   -0.0821161    0.0975861   -0.055421    0.332219    0.0925007   -0.712542    -0.124126    -0.171668   -0.00933711   0.490561     0.0363121    0.486384      0.112364    -0.0128411   -0.200036   -0.123166     0.308466   -0.157191    0.265999   
  0.0792274    0.0567927   0.237191     0.131026   -0.149384    -0.236408    -0.0988553    0.00168117   0.0116314    0.0773915   0.0903431   0.0572098    0.13937      0.0978564    0.11115     0.0790001    0.0222162    0.258997    -0.130679      0.0966596   -0.0458384    0.0522428   0.00594614  -0.182872   -0.120034   -0.276971   
 -0.165743    -0.0803753  -0.148664    -0.120092    0.0542441    0.139092     0.0294309   -0.0959774   -0.0467939   -0.10938    -0.0556857  -0.046358    -0.0238931   -0.0350204   -0.063368   -0.115851     0.0993351   -0.185007     0.0488124    -0.00537961   0.0199649   -0.122114   -0.157795     0.0373023  -0.0623334   0.116882   
 -0.441365     0.153895   -0.325457    -0.248238   -0.153653    -0.0945485    0.284499    -0.12152      0.458896    -0.375584    0.0118715   0.522393     0.155693    -0.0303933   -0.389298   -0.115881     0.0635828   -0.0218638   -0.34494      -0.515513    -0.697572    -0.380896   -0.239494     0.31094    -0.492017    0.161105   
 -0.403477     0.188682   -0.0547428   -0.219248    0.0437717    0.254016     0.128719     0.302988     0.0961715   -0.389151   -0.143259   -0.127665     0.423753     0.340128     0.342595   -0.0656831   -0.330235    -0.280063    -0.328308     -0.0285123   -0.338148    -0.642616   -0.569441    -0.254602    0.300597   -0.0185178  
  0.320417    -0.226648    0.0267544    0.476307    0.858159     0.448096     0.0925254   -0.299576     0.00797807   0.44283     0.187724   -0.345745     0.121442    -0.428053     0.386623    0.359419    -0.169615     0.0281011   -0.327733     -0.169585    -0.25003     -0.391636   -0.803521     0.11515    -0.335289    0.280245   
 -0.123662     0.122308   -0.00891156   0.101792    0.369811     0.0253638    0.494422    -0.350626     0.441742     0.180681   -0.189456    0.493266     0.123474    -0.130822    -0.283924    0.167824    -0.412132     0.0147306    0.0440908     0.00953348   0.478037     0.125309   -0.702687     0.383042    0.0968558   0.480774   
  0.288435    -0.192756   -0.0182341    0.07565    -0.666621    -0.0104555   -0.560393     0.0663436   -0.0347553   -0.0209923  -0.30703     0.109353    -0.140354    -0.501814    -0.072399    0.201173     0.576085    -0.111041    -0.207664     -0.0502841   -0.0725781   -0.465403    0.223271     0.0109412  -0.103806   -0.762705   
 -0.163897     0.043332    0.409846     0.0274618  -0.59126     -0.147772     0.0170903    0.0160489   -0.442112    -0.421991    0.17953    -0.0194187   -0.177052     0.799657     0.0310229  -0.55867      0.604868    -0.0899173   -0.129489      0.114324    -0.679418    -0.182373    0.0690098   -0.21233     0.180573   -0.885066   
  0.472613    -0.110842   -0.220415     0.0606401   0.197398     0.1047       0.0931835    0.0733866   -0.144222     0.0827154  -0.0926327   0.0108269    0.00711078  -0.0962093    0.211117   -0.534711     0.083134    -0.0849956   -0.0820065    -0.234772    -0.733025     0.120591    0.730915    -0.280834   -0.3312     -0.0759962  
  0.600494     0.341915   -0.130543    -0.111088   -0.15147      0.402869     0.112398     0.240181    -0.25185     -0.15902    -0.0096677  -0.00278479   0.167355     0.0424062    0.585126    0.0425908   -0.487334    -0.16317     -0.134706     -0.194345     0.0789277   -0.230559    0.31762      0.134441    0.327623   -0.0254884  
 -0.0256913    0.197893    0.0121381   -0.0964059  -0.100046    -0.140979    -0.328578    -0.146971     0.00281002  -0.349027   -0.152731    0.320354     0.134171    -0.133166    -0.0990614   0.0904807   -0.289608     0.2197       0.148461      0.167361     0.246983     0.651177    0.652723     0.135528   -0.361566    0.107875   
  0.393178    -0.170952    0.1874       0.257283   -0.0953611   -0.19579     -0.0838701   -0.32486     -0.15859      0.994317   -0.105354   -0.318243     0.131294    -0.25574     -0.0823335  -0.0600834    0.0496339    0.303066     0.178582      0.0338346    0.636327     1.06949     0.439027    -0.130784    0.0758738  -0.000317492
  0.0767477    0.0302815   0.181832    -0.323513   -0.25055     -0.079275     0.00325129   0.266004     0.116529     0.309889    0.0614223  -0.440876     0.26006     -0.00294382  -0.485438    0.0253715   -0.00775596   0.021907     0.0817708    -0.126875     0.0441995    0.366463    0.0472308   -0.09214     0.685796    0.467347   
 -0.0401993   -0.232031    0.448465    -0.192672    0.658171    -0.216402     0.248621     0.731848    -0.171796     0.355199   -0.391859   -0.321797    -0.338621     0.327477    -0.114739   -0.0218531   -0.0917843    0.00326159   0.0434092     0.782743     0.179935     0.0405894   0.055845    -0.0910379   0.292012    0.488957   
  0.677321    -0.142141   -0.39084      0.0120848   0.272669    -0.311162    -0.447645    -0.227513    -0.0149501   -0.596262   -0.495908    0.0719934   -0.231536    -0.018383    -0.0425932  -0.0902175   -0.714964     0.610514     0.107893      0.705037     0.0763675    0.089715    0.121859     0.398273   -0.265081   -0.491654   
  0.182076     0.0648702   0.653787    -0.113578   -0.196153     0.0500762    0.133852    -0.0358787   -0.226652    -0.779907   -0.351637   -0.379018    -0.528149     0.151537    -0.231114    0.868405     0.0989507    0.751857     0.1024        0.660661    -0.175644    -0.115686   -0.406865     0.130516   -0.0163473  -0.429356   
 -0.0979553   -0.267227   -0.58726     -0.0577522  -0.648007     0.0300656   -0.4854      -1.14881     -0.175476    -0.63305     0.217091   -0.0626211    0.0212051   -0.0359205    0.497856    0.253568    -0.151768    -0.263539     0.131136     -0.582054     0.1635      -0.665933   -0.357322    -0.0163076  -0.38225    -0.538785   
 -0.187752     0.443463   -0.175959     0.102872   -0.00521012  -0.21414     -0.027452     0.243015     0.321234    -0.165377    0.434868    0.501713    -0.89301     -0.0326479    0.361611    0.0301204    0.385093     0.705311    -0.314948     -0.32476     -0.619516    -0.181016   -0.338815     0.0643837  -0.571334   -0.733308   
 -0.490774    -0.141479    0.366173    -0.0306035   0.0133581   -0.0480242   -0.0912985   -0.125645     0.14321      0.176681   -0.445636   -0.128528     0.722109     0.0378369   -0.344811   -0.0233187    0.124919     0.436889    -0.434234      0.625154     0.064966     0.252566   -0.613757    -0.67143    -0.553458   -0.250292   
 -0.00490884  -0.108834    0.285681     0.658601   -0.422178    -0.036066     0.304198    -0.0209964   -0.294766     0.127559    0.708121   -0.0625454    0.243862    -0.171019     0.203028    0.405457     0.166143     0.466487     0.000553059  -0.028207     0.208318    -0.245551   -0.153714    -0.481558    0.130683   -0.820364   
  0.220629     0.496351    0.58702     -0.442371   -0.761694     0.00300457   0.121556     0.175176    -0.436018     0.153082   -0.109653   -0.183979     0.867074    -0.12679     -0.338369   -0.473698     0.0262515    0.248206    -0.393208     -0.389467     0.126426    -0.0977981   0.654271     0.0683523   0.0464393  -0.517105   
 -0.621297     0.463965   -0.13967     -0.360159   -0.854767    -0.0938068    0.0972758    0.179888    -0.14224     -0.499967    0.060344    0.934474    -0.249256     0.557401    -0.105316   -0.387896    -0.0512233   -0.13427      0.744057      0.419826     0.22355      0.547116    1.02746     -0.490252    0.301548   -0.35366    
  0.0173354   -0.468608    0.39452      0.218741   -0.337978     0.0708134    0.0367057   -0.224893    -0.654789     0.226049   -0.528654   -0.764954     0.76349     -0.0445814   -0.774881    0.241382    -0.25771     -0.86334      0.259606      0.477588     0.802637    -0.0463845   0.353571     0.2153      0.610179    0.691052   
  0.169554     0.136082   -0.327104    -0.223893    0.430568     0.166706     0.241755     0.172635    -0.655348     0.0883815  -0.459527   -0.312051    -0.244835     0.0672886    0.466464   -0.081395    -0.305904    -0.864436    -0.0672355    -0.230029    -0.145017    -0.107682    0.369913    -0.04856     0.488009    0.486025   
 -0.590094    -0.40095     0.278018    -0.595944   -0.0127279    0.482998     0.247076     0.157967    -0.048868     0.405142    0.0590569  -0.589703    -0.142973     0.104508     0.0601197   0.0514876    1.13969     -0.459519    -0.197542     -0.2848       0.0875691   -0.708727   -0.906989     0.0397156   0.289371    0.264484   
 -0.217888    -0.0239089  -0.320369    -0.211164   -0.153941    -0.0572353   -0.437192     0.0259719    0.0537054    0.706823    0.420619    0.149784     0.202211     0.0716509    0.262909   -0.397665     0.348255    -0.745609     0.261172     -0.520701     0.280935    -0.0424762   0.264588    -0.116392    0.291194    0.629279   
  0.171514    -0.406947   -0.266339     0.546615    0.524349     0.0944985    0.00448054  -0.117833     0.406119    -0.120137    0.325042   -0.0341861   -0.34284     -0.137934     0.17122     0.239995     0.120792     0.0677535    0.448727     -0.0389375   -0.195245    -0.23356    -0.304412    -0.0389278   0.0428249   0.295088   
  0.0159268    0.250579    0.122458    -0.299821    0.18575     -0.185634    -0.0247877    0.553112     0.586902     0.233763   -0.209216    0.255888    -0.0523474   -0.131737    -0.646912    0.0100052    0.181398     0.0792278    0.0475002     0.169365    -0.00388501   0.234306   -0.110202     0.370808    0.239379    0.73924    
  0.45547     -0.261647   -0.0996803   -0.656802   -0.3772       0.194843    -0.693602    -0.458644     0.0438574   -0.0454378  -0.181341    0.10607     -0.0091196    0.347655     0.308118   -0.623055     0.0781677    0.00777438  -0.0424342     0.10795     -0.0137587    0.531957    0.656771     0.210811   -0.617368    0.060706   
 -0.278508     0.775504    0.389659    -0.0803821   0.466737    -0.365049     0.427822    -0.134101     0.0780653   -0.0286532   0.621461   -0.0477116    0.336138     0.924557     0.200953   -0.501204    -0.52793      0.164175    -0.142669     -0.0593233    0.0578549    0.373491    0.182324    -0.247351   -0.205506    0.647348   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405419
INFO: iteration 2, average log likelihood -1.405406
INFO: iteration 3, average log likelihood -1.405393
INFO: iteration 4, average log likelihood -1.405380
INFO: iteration 5, average log likelihood -1.405367
INFO: iteration 6, average log likelihood -1.405354
INFO: iteration 7, average log likelihood -1.405342
INFO: iteration 8, average log likelihood -1.405330
INFO: iteration 9, average log likelihood -1.405318
INFO: iteration 10, average log likelihood -1.405306
INFO: EM with 100000 data points 10 iterations avll -1.405306
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.465356e+05
      1       6.979467e+05      -2.485889e+05 |       32
      2       6.861130e+05      -1.183375e+04 |       32
      3       6.813943e+05      -4.718633e+03 |       32
      4       6.789272e+05      -2.467119e+03 |       32
      5       6.773303e+05      -1.596932e+03 |       32
      6       6.761115e+05      -1.218754e+03 |       32
      7       6.750710e+05      -1.040532e+03 |       32
      8       6.741603e+05      -9.106915e+02 |       32
      9       6.734285e+05      -7.317783e+02 |       32
     10       6.728455e+05      -5.830056e+02 |       32
     11       6.723308e+05      -5.147374e+02 |       32
     12       6.718788e+05      -4.519506e+02 |       32
     13       6.714900e+05      -3.888579e+02 |       32
     14       6.711293e+05      -3.607133e+02 |       32
     15       6.708132e+05      -3.160203e+02 |       32
     16       6.705341e+05      -2.790957e+02 |       32
     17       6.702660e+05      -2.681399e+02 |       32
     18       6.700192e+05      -2.467456e+02 |       32
     19       6.697878e+05      -2.314495e+02 |       32
     20       6.695782e+05      -2.095654e+02 |       32
     21       6.693990e+05      -1.792137e+02 |       32
     22       6.692375e+05      -1.615572e+02 |       32
     23       6.691090e+05      -1.284926e+02 |       32
     24       6.689899e+05      -1.190824e+02 |       32
     25       6.688714e+05      -1.185303e+02 |       32
     26       6.687624e+05      -1.089674e+02 |       32
     27       6.686630e+05      -9.939743e+01 |       32
     28       6.685756e+05      -8.744059e+01 |       32
     29       6.684808e+05      -9.470961e+01 |       32
     30       6.683883e+05      -9.254054e+01 |       32
     31       6.683061e+05      -8.218919e+01 |       32
     32       6.682318e+05      -7.433254e+01 |       32
     33       6.681643e+05      -6.750367e+01 |       32
     34       6.680933e+05      -7.094304e+01 |       32
     35       6.680294e+05      -6.396344e+01 |       32
     36       6.679698e+05      -5.955914e+01 |       32
     37       6.679139e+05      -5.588715e+01 |       32
     38       6.678575e+05      -5.639123e+01 |       32
     39       6.678003e+05      -5.726996e+01 |       32
     40       6.677423e+05      -5.791608e+01 |       32
     41       6.676864e+05      -5.597116e+01 |       32
     42       6.676375e+05      -4.883933e+01 |       32
     43       6.675997e+05      -3.782178e+01 |       32
     44       6.675677e+05      -3.202795e+01 |       32
     45       6.675373e+05      -3.043239e+01 |       32
     46       6.675090e+05      -2.830344e+01 |       32
     47       6.674855e+05      -2.346829e+01 |       32
     48       6.674628e+05      -2.264262e+01 |       32
     49       6.674410e+05      -2.180690e+01 |       32
     50       6.674176e+05      -2.348159e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 667417.5511096332)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417604
INFO: iteration 2, average log likelihood -1.412482
INFO: iteration 3, average log likelihood -1.410992
INFO: iteration 4, average log likelihood -1.409818
INFO: iteration 5, average log likelihood -1.408659
INFO: iteration 6, average log likelihood -1.407730
INFO: iteration 7, average log likelihood -1.407158
INFO: iteration 8, average log likelihood -1.406839
INFO: iteration 9, average log likelihood -1.406643
INFO: iteration 10, average log likelihood -1.406502
INFO: iteration 11, average log likelihood -1.406388
INFO: iteration 12, average log likelihood -1.406290
INFO: iteration 13, average log likelihood -1.406202
INFO: iteration 14, average log likelihood -1.406124
INFO: iteration 15, average log likelihood -1.406052
INFO: iteration 16, average log likelihood -1.405986
INFO: iteration 17, average log likelihood -1.405926
INFO: iteration 18, average log likelihood -1.405871
INFO: iteration 19, average log likelihood -1.405821
INFO: iteration 20, average log likelihood -1.405776
INFO: iteration 21, average log likelihood -1.405734
INFO: iteration 22, average log likelihood -1.405696
INFO: iteration 23, average log likelihood -1.405661
INFO: iteration 24, average log likelihood -1.405629
INFO: iteration 25, average log likelihood -1.405599
INFO: iteration 26, average log likelihood -1.405571
INFO: iteration 27, average log likelihood -1.405545
INFO: iteration 28, average log likelihood -1.405521
INFO: iteration 29, average log likelihood -1.405498
INFO: iteration 30, average log likelihood -1.405477
INFO: iteration 31, average log likelihood -1.405457
INFO: iteration 32, average log likelihood -1.405438
INFO: iteration 33, average log likelihood -1.405420
INFO: iteration 34, average log likelihood -1.405402
INFO: iteration 35, average log likelihood -1.405386
INFO: iteration 36, average log likelihood -1.405370
INFO: iteration 37, average log likelihood -1.405355
INFO: iteration 38, average log likelihood -1.405340
INFO: iteration 39, average log likelihood -1.405326
INFO: iteration 40, average log likelihood -1.405312
INFO: iteration 41, average log likelihood -1.405299
INFO: iteration 42, average log likelihood -1.405287
INFO: iteration 43, average log likelihood -1.405274
INFO: iteration 44, average log likelihood -1.405262
INFO: iteration 45, average log likelihood -1.405251
INFO: iteration 46, average log likelihood -1.405239
INFO: iteration 47, average log likelihood -1.405228
INFO: iteration 48, average log likelihood -1.405218
INFO: iteration 49, average log likelihood -1.405207
INFO: iteration 50, average log likelihood -1.405197
INFO: EM with 100000 data points 50 iterations avll -1.405197
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.452299    0.160309     0.111139     0.178232   -0.147369    -0.412888   -0.320121    -0.0247553    5.37743e-5  -0.0580214  -0.043872     0.208245    0.286452   -0.227453   -0.313336    -0.137594   -0.480677    0.552358    0.220885      0.231055     0.256629     0.616558      0.893874    0.199445   -0.20822    -0.0875025  
 -0.808762    0.557163    -0.168174    -0.401939   -0.741573     0.162932    0.184937     0.0986706   -0.101343    -0.406576    0.0633707    1.06579    -0.270886    0.881395   -0.349325    -0.597298   -0.3995     -0.0274093   0.842641      0.926098     0.298112     1.03761       0.948753   -1.00364     0.199055   -0.294246   
 -0.395776    0.21729      0.309367    -0.0861526   0.661831    -0.210987    0.596695    -0.213207    -0.210513     0.0414731   0.0718772   -0.422064    0.483953    0.515299    0.1407      -0.133028   -0.850547   -0.196425   -0.0594208     0.0177518    0.244014     0.32005      -0.0113355  -0.209627    0.124011    0.844376   
  0.903687   -0.337327     0.0684091    0.0254385  -0.633444     0.182027   -0.608169     0.0939669    0.834597     0.295378   -0.174453     0.51985    -0.59126    -0.424778    0.0578768   -0.082094    0.993864    0.254599   -0.0747115     0.216196    -0.546357     0.0621281     0.306779    0.0410942  -0.129731   -0.508035   
 -0.418844   -0.171504     0.204211    -0.804722    0.0963488    0.633988    0.348651     0.551888    -0.251743     0.236268   -0.134092    -0.764492   -0.273295    0.146312    0.040713    -0.0232759   0.848658   -0.256812   -0.0484406     0.270557     0.12582     -0.702916     -0.731196    0.167257    0.303022    0.315496   
 -0.727136    0.133837    -0.348149    -0.474801   -0.0890132   -0.0659085   0.188635     0.0974303    0.562358    -0.0495405  -0.421251     0.323516    0.605761   -0.0763501  -0.57236     -0.322395    0.0422657  -0.0605053  -0.278065     -0.15938     -0.706452    -0.312604     -0.341399   -0.371998   -0.315858    0.190645   
  0.272242    0.457055     0.180207    -0.221809   -0.0839869   -0.23004     0.487494     0.165927    -0.0200562   -0.231357    0.0962441    0.332545   -0.305264   -0.0847759  -0.52704     -0.0455361  -0.391772    0.542079   -0.895646     -0.395616    -0.366093    -0.0342575    -0.250471    0.965541   -0.455241   -0.821483   
 -0.841697   -1.02566      0.238859     0.762935   -0.146883    -0.232508    0.0755386   -0.482751     0.0947812   -0.0833586   0.0785665    0.0874041  -0.304882    0.0948403  -0.295405    -0.150143    0.222088   -0.0200656   0.132637     -0.136147    -0.220699     0.673401     -0.613135    0.0091736  -0.361999   -0.424371   
  0.604534   -0.361611     0.265082     0.613584    0.897861     0.521899   -0.0236645   -0.425007     0.0616623    0.459729    0.0145528   -0.503044    0.121696   -0.691232    0.462526     0.566533    0.0301033   0.0258953  -0.395169     -0.294007    -0.349733    -0.599535     -0.962435    0.450068   -0.436271    0.157369   
 -0.389322    0.06223      0.436432     0.0397588   0.256155     0.0914293  -0.195877    -0.0522642    0.173547     0.25319    -0.0862166    0.176409    0.421162    0.22762    -0.229897     0.0437732   0.166348    0.677235   -0.149278      0.884311     0.263582     0.207903     -0.550326   -0.402933   -0.663426   -0.0949399  
  0.41901    -0.428118    -0.953954    -0.256419    0.0968058   -0.118968   -0.441532    -1.00852      0.230312    -0.477642   -0.582768    -0.350401    0.0566324  -0.0896904  -0.0937761    0.0195941  -0.679048    0.172132    0.104462      0.520199    -0.0745578   -0.315569     -0.548629    0.269985   -0.179484   -0.474569   
  0.137925   -0.00283543  -0.133887    -0.245306    0.321001     0.0839709   0.00224536   0.229895     0.0369638    0.230783   -0.185243    -0.0717696  -0.0641027   0.10101    -0.00940759  -0.192173    0.0762327  -0.272125    0.057512     -0.087048    -0.103575     0.0549667     0.181538    0.0530187   0.202941    0.600518   
 -0.246269   -0.106124    -0.335731    -0.212018   -0.142088    -0.0692085  -0.403371    -0.0324705   -0.0101482    0.62023     0.3507       0.028966    0.0420168   0.055306    0.23722     -0.366241    0.379297   -0.746286    0.35459      -0.495835     0.194889     0.000795004   0.267117   -0.130799    0.313586    0.622912   
  0.336105   -0.179819     0.389767    -0.25822     0.214625    -0.162776   -0.00432523   0.679106    -0.33548      0.458538   -0.407175    -0.665993   -0.240139    0.202426   -0.0901958   -0.0631941  -0.337702   -0.0783193   0.0811064     0.817089     0.475927     0.295266      0.137067    0.0342495   0.63616     0.320351   
  0.155706    0.0132296    0.763685    -0.124933   -0.0966102   -0.0571153   0.169601     0.00162053  -0.234129    -0.986369   -0.478881    -0.476284   -0.510541    0.374575   -0.298713     0.801543    0.158397    0.651701    0.0358064     0.819602    -0.403421    -0.170179     -0.385669   -0.120973    0.0765255  -0.377577   
  0.306648    0.32646     -0.197489    -0.978067   -0.698465     0.54576    -0.341603    -0.6369      -0.216605    -0.492019   -0.257481     0.345753   -0.0785867   0.0147913   0.478559    -0.225738   -0.342821   -0.414687   -0.0170201    -0.230301     0.0296651    0.314032      0.467189    0.477886   -0.527284   -0.000902802
 -0.272406   -0.0992167   -0.15597      0.114849   -0.180716     0.131646    0.278317     0.623881     0.0177986   -0.713994   -0.0521726    0.0832377   0.129872    0.216131    0.294326     0.0359298  -0.38826    -0.491939   -0.627607     -0.417921    -0.40748     -0.384646     -0.0876676   0.254603    0.332335    0.0196612  
  0.216147    0.297461     0.297769    -0.21415    -0.242232    -0.0658742  -0.0901577    0.178731    -0.0271783   -0.0886307  -0.0960936   -0.0315505   0.324173    0.18461     0.216172     0.0907555  -0.206799    0.148488   -0.182554      0.0896411   -0.0710627    0.0529254     0.256538   -0.151262    0.0600526  -0.0988366  
 -0.156315    0.0428086    0.0690541   -0.149987    0.399019    -0.136581    0.202325     0.0411655    0.676981     0.104597   -0.202915     0.263541   -0.15075    -0.200948   -0.535647     0.329234   -0.0555294   0.0216775   0.182524      0.0832571    0.283157     0.140526     -0.617257    0.389185    0.198939    0.917279   
 -0.242329    0.310335     0.59106      0.615491   -0.00125139  -0.283866    0.581301     0.525064    -0.126161     0.377043    0.63641      0.0115888   0.0393735   0.156198   -0.223723     0.0611125   0.561293    0.0374624   0.144906     -0.164099    -0.323276    -0.155404     -0.273546   -0.526865    0.477727   -0.126947   
 -0.172294    0.270498     0.427764    -0.676161   -0.813895    -0.0182     -0.0638186   -0.0109268   -0.164331     0.309572   -0.090815    -0.311561    0.711962    0.167795   -0.479832    -0.534996    0.322913   -0.0463683  -0.380353     -0.415654    -0.0307199    0.154495      0.358072   -0.0490579   0.388572   -0.208696   
  0.0301423  -0.192085     0.0669756    0.29089    -0.200947    -0.199435   -0.12526     -0.131319    -0.0149732    0.254176    0.00539822  -0.0414027   0.129125   -0.207255   -0.151201     0.243603   -0.0390877   0.154142    0.148845     -0.0410618    0.304793     0.369743     -0.0390112  -0.11986     0.197283   -0.0426096  
  0.246256   -0.0698317    0.00598199  -0.137117    0.279032    -0.173322    0.0133419   -0.169357     0.0377494    0.152576   -0.0628935    0.0568203  -0.224495    0.169836    0.0197046   -0.407443    0.134771   -0.0179129  -0.0163352     0.119913    -0.113629     0.612728      0.669746    0.0964825  -0.475806    0.333514   
 -0.424405    0.268177    -0.43894     -0.0451911  -0.1115      -0.0439551  -0.0611325   -0.477715     0.213168    -0.40797     0.601275     0.277366   -0.225232   -0.157983    0.131841     0.11066     0.420882    0.0873421  -0.0210305    -0.585757    -0.237414    -0.699502     -0.502586    0.132639   -0.553431   -0.0318993  
 -0.159984    0.0421798    0.0464001    0.50113    -0.437044    -0.241586    0.0312575   -0.258645     0.0177182   -0.322256    0.339278     0.227656   -0.117399    0.015345    0.367493     0.234976    0.154123    0.323432    0.000705862  -0.0937918   -0.00916032  -0.39402      -0.244587   -0.177798   -0.223204   -0.845869   
 -0.139464   -0.0439906   -0.113037    -0.0215386   0.108454     0.0402467   0.0639266   -0.0833798    0.0163668   -0.0886323   0.0272223    0.025341   -0.134499   -0.0155019  -0.0609606   -0.10349     0.109158   -0.0864105   0.040179      0.00140111  -0.0908977   -0.14031      -0.215623    0.0759136  -0.10299     0.0605468  
 -0.26651     0.0483791   -0.243309    -0.0934722   0.286487     0.205838   -0.28772     -0.358269     0.0837136    0.365653   -0.104563     0.381413   -0.187447    0.657096    1.06198      0.284844   -0.186937   -0.0394531  -0.672495     -0.0933503   -0.0392516   -0.123991     -0.650702   -0.19155     0.17501    -0.347159   
  0.130451    0.0580598    0.0249244   -0.0130273  -0.502159    -0.216043   -0.270403     0.0315818   -0.393353    -0.456937   -0.0747582    0.135129   -0.187665    0.160875    0.220151    -0.289139    0.390502    0.0621959  -0.0600123     0.15025     -0.412368    -0.366312      0.511045   -0.201906   -0.197593   -0.96534    
  0.395367   -0.145668     0.0316688    0.196951   -0.363964     0.461095    0.135087    -0.135436    -0.692127     0.235415    0.351903    -0.476446    0.588107   -0.226122    0.358168     0.0490757  -0.0273772   0.241508   -0.264916     -0.159243     0.278585    -0.0857735     0.108883   -0.681111   -0.0251389  -0.72182    
  0.218595   -0.134705    -0.555654     0.386963    0.430773    -0.123636   -0.156669     0.0959995    0.101993    -0.207338    0.299002     0.179039   -0.912734   -0.107557    0.148019     0.217645   -0.0050579   0.233933    0.672522      0.0509123   -0.0721236    0.0496655    -0.043999    0.0901851  -0.0173069   0.242384   
  0.0443799  -0.610688     0.210385     0.373225   -0.631807    -0.0363755  -0.5766      -0.18581     -0.591717    -0.027425   -0.460641    -0.441464    0.324094   -0.618821   -0.55093      0.500668    0.477801   -0.482083   -0.00297441    0.017271     0.678036    -0.593477      0.0186782   0.200674    0.126138   -0.172767   
  0.691417    0.336587    -0.361318    -0.0523237   0.317457     0.684384    0.21323      0.166491     0.0412916    0.22744    -0.133634     0.0990636   0.459692   -0.138793    0.252802    -0.316859   -0.27697    -0.276659    0.155574     -0.186988    -0.113054    -0.455186      0.2121     -0.0037627   0.253232    0.39082    INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405187
INFO: iteration 2, average log likelihood -1.405177
INFO: iteration 3, average log likelihood -1.405168
INFO: iteration 4, average log likelihood -1.405159
INFO: iteration 5, average log likelihood -1.405150
INFO: iteration 6, average log likelihood -1.405141
INFO: iteration 7, average log likelihood -1.405133
INFO: iteration 8, average log likelihood -1.405125
INFO: iteration 9, average log likelihood -1.405117
INFO: iteration 10, average log likelihood -1.405109
INFO: EM with 100000 data points 10 iterations avll -1.405109
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
