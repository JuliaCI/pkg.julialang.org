>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.1.3
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.10.0
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.9392929077148438 GB (695.21484375 MB free)
Uptime: 24135.0 sec
Load Avg:  1.0029296875  1.0146484375  1.04541015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3498 MHz    1485864 s       4394 s     173510 s     496944 s         49 s
#2  3498 MHz     646850 s         37 s      93918 s    1571612 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.1.3
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.8
 - StatsBase                     0.10.0
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-996004.9773471715,[57964.7,42035.3],
[-10430.3 -3413.83 29867.0; 10136.1 3631.2 -29608.5],

Array{Float64,2}[
[55205.2 -4824.57 -1713.3; -4824.57 50034.8 -10000.4; -1713.3 -10000.4 39706.7],

[44819.6 5145.55 1789.58; 5145.55 50476.0 10051.3; 1789.58 10051.3 59746.7]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.249481e+03
      1       9.279072e+02      -3.215737e+02 |        5
      2       8.989137e+02      -2.899346e+01 |        2
      3       8.963701e+02      -2.543578e+00 |        2
      4       8.948530e+02      -1.517132e+00 |        0
      5       8.948530e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 894.8530093757026)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.073953
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.744347
INFO: iteration 2, lowerbound -3.578028
INFO: iteration 3, lowerbound -3.409213
INFO: iteration 4, lowerbound -3.229913
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -3.051764
INFO: iteration 6, lowerbound -2.897976
INFO: iteration 7, lowerbound -2.797237
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.745242
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.691883
INFO: dropping number of Gaussions to 3
INFO: iteration 10, lowerbound -2.626655
INFO: iteration 11, lowerbound -2.557667
INFO: iteration 12, lowerbound -2.489139
INFO: iteration 13, lowerbound -2.428327
INFO: iteration 14, lowerbound -2.380671
INFO: iteration 15, lowerbound -2.345217
INFO: iteration 16, lowerbound -2.320474
INFO: iteration 17, lowerbound -2.308306
INFO: dropping number of Gaussions to 2
INFO: iteration 18, lowerbound -2.303102
INFO: iteration 19, lowerbound -2.299264
INFO: iteration 20, lowerbound -2.299258
INFO: iteration 21, lowerbound -2.299255
INFO: iteration 22, lowerbound -2.299254
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Tue 04 Oct 2016 11:12:01 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Tue 04 Oct 2016 11:12:04 AM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Tue 04 Oct 2016 11:12:06 AM UTC: EM with 272 data points 0 iterations avll -2.073953
5.8 data points per parameter
,Tue 04 Oct 2016 11:12:06 AM UTC: GMM converted to Variational GMM
,Tue 04 Oct 2016 11:12:09 AM UTC: iteration 1, lowerbound -3.744347
,Tue 04 Oct 2016 11:12:09 AM UTC: iteration 2, lowerbound -3.578028
,Tue 04 Oct 2016 11:12:09 AM UTC: iteration 3, lowerbound -3.409213
,Tue 04 Oct 2016 11:12:09 AM UTC: iteration 4, lowerbound -3.229913
,Tue 04 Oct 2016 11:12:09 AM UTC: dropping number of Gaussions to 7
,Tue 04 Oct 2016 11:12:09 AM UTC: iteration 5, lowerbound -3.051764
,Tue 04 Oct 2016 11:12:09 AM UTC: iteration 6, lowerbound -2.897976
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 7, lowerbound -2.797237
,Tue 04 Oct 2016 11:12:10 AM UTC: dropping number of Gaussions to 6
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 8, lowerbound -2.745242
,Tue 04 Oct 2016 11:12:10 AM UTC: dropping number of Gaussions to 4
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 9, lowerbound -2.691883
,Tue 04 Oct 2016 11:12:10 AM UTC: dropping number of Gaussions to 3
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 10, lowerbound -2.626655
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 11, lowerbound -2.557667
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 12, lowerbound -2.489139
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 13, lowerbound -2.428327
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 14, lowerbound -2.380671
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 15, lowerbound -2.345217
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 16, lowerbound -2.320474
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 17, lowerbound -2.308306
,Tue 04 Oct 2016 11:12:10 AM UTC: dropping number of Gaussions to 2
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 18, lowerbound -2.303102
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 19, lowerbound -2.299264
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 20, lowerbound -2.299258
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 21, lowerbound -2.299255
,Tue 04 Oct 2016 11:12:10 AM UTC: iteration 22, lowerbound -2.299254
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 23, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 24, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 25, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 26, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 27, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 28, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 29, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 30, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 31, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 32, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 33, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 34, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 35, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 36, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:11 AM UTC: iteration 37, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 38, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 39, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 40, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 41, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 42, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 43, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 44, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 45, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 46, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 47, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 48, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:12 AM UTC: iteration 49, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:13 AM UTC: iteration 50, lowerbound -2.299253
,Tue 04 Oct 2016 11:12:13 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0059000384686347
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -1.0059000384686347
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -1.0059000384686347
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0034449936184906
avll from llpg:  -1.0034449936184906
avll direct:     -1.0034449936184904
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.00537396  -0.0283624    0.0470308     0.0181329    0.0974911   -0.181712    -0.0967645    0.0379331    0.0247983    0.04333      0.0562314    0.0624385   -0.0259166   -0.0202729    -0.0730271    0.0813378   -0.00750677  -0.0818058   -0.0952393     0.0398185    0.122303    0.180537    -0.034137    -0.109128    -0.00842292  -0.0395054  
  0.0100844    0.163019    -0.0973451    -0.188915    -0.0697504    0.123776     0.0728516    0.200128    -0.0246202    0.0919283   -0.0132202    0.135146    -0.139413     0.0311344    -0.0336624   -0.074924    -0.108548     0.0170178    0.0931652     0.121222     0.11935     0.115714    -0.0380444    0.0354639   -0.051426     0.0913829  
 -0.0553865    0.107972     0.0782047     0.0672264    0.0542628   -0.0567834    0.056725    -0.0723813    0.0735309   -0.0513512    0.0946406   -0.121861     0.0376277   -0.0278281     0.0203862    0.074925     0.0889221    0.00857317  -0.029908      0.0390861    0.128108   -0.0914599   -0.207844     0.0443036   -0.0222803    0.0448202  
 -0.130301     0.108619     0.0146373    -0.148417     0.0679943    0.0683329   -0.0857151    0.0332565   -0.205976    -0.0964545    0.0526763   -0.0721133   -0.173942    -0.0133489     0.105344    -0.00137388  -0.0520951   -0.0725045    0.0764113     0.0886934    0.0212025  -0.0181103    0.11818     -0.00455221   0.00935666  -0.0147759  
 -0.122058    -0.0620277    0.0514709    -0.13894      0.00987035   0.0185291    0.0207009   -0.0700199   -0.111557    -0.0112321    0.221978     0.101151    -0.158088     0.07823       0.102574     0.0646814    0.124976    -0.131762     0.00739913   -0.0579515   -0.0751574  -0.173468     0.290653     0.156291     0.042967    -0.0345733  
 -0.0985182   -0.00521616  -0.0251634    -0.0382195   -0.150601    -0.008455    -0.00942001   0.00748568  -0.0868557    0.00449988   0.14643      0.0393252    0.125727     0.0536429    -0.00992925  -0.0522313   -0.0307051   -0.0117076    0.107737      0.125337     0.11734     0.0608902    0.0093165   -0.0985889   -0.0426084   -0.220637   
 -0.0206059    0.00938415  -0.0426219    -0.0202045   -0.0219298   -0.0383815   -0.174008     0.0550963    0.0389837   -0.141241    -0.17754      0.115037    -0.0109709   -0.00653217    0.011357    -0.160115    -0.112822    -0.0239643   -0.057736     -0.219938     0.0575059   0.0382377   -0.0234658   -0.025045     0.0448159    0.0421126  
  0.166236    -0.092364    -0.215519      0.0150852   -0.00391084   0.018251    -0.00110659   0.0485499    0.042266     0.126036    -0.198741     0.074052    -0.123426     0.094313     -0.129099    -0.0272514   -0.0437327    0.0207328   -0.0897544     0.200926     0.135818   -0.0473766   -0.0300952   -0.0679255   -0.154642    -0.0302468  
  0.08889      0.00671981  -0.000245299  -0.102933     0.126939    -0.105405    -0.0261104   -0.0300369   -0.00302134   0.0347741   -0.0686431    0.0733813    0.0651951   -0.00554236   -0.0450065    0.22767     -0.126955    -0.0329956   -0.0296561    -0.0807773   -0.0847493   0.143195    -0.0801932   -0.0763725    0.0263426   -0.106701   
  0.00854734  -0.138622     0.103069      0.0548577    0.0535447    0.0964654   -0.0523017   -0.22507     -0.188931    -0.0504301    0.0139601    0.103695    -0.0925222   -0.0347004     0.0304702   -0.00480232  -0.0215461   -0.072578     0.086304      0.226475    -0.0493312  -0.0764109   -0.100352     0.0524367    0.0428958    0.0815447  
  0.0956764    0.216668     0.0514182     0.0841075   -0.238486     0.0432361    0.0702937    0.026819    -0.110231     0.0470836    0.0376924   -0.0547948   -0.0212847   -0.0057964     0.0143595    0.0308069   -0.00695699   0.0853257   -0.0075894    -0.0880705   -0.0945449   0.028812    -0.00328246   0.0464738   -0.0304859   -0.141349   
  0.00156297  -0.0332229   -0.143784     -0.112805    -0.129823     0.0145403    0.243251     0.021551    -0.0608573   -0.0694871    0.00592111  -0.0249468    0.0162756   -0.142788     -0.109081    -0.0622849    0.0386      -0.172691    -0.0923617    -0.150274     0.0755961   0.0947951   -0.106755     0.0826152   -0.107925     0.149514   
 -0.0669019    0.163983    -0.128086     -0.201668    -0.0546265    0.0645881   -0.134007     0.146714    -0.0622064    0.0868868    0.0210005   -0.111301     0.0751439    0.0196678     0.0174919   -0.0207516   -0.0884603   -0.102064    -0.0103622     0.0900293   -0.144207   -0.0827568    0.0252104    0.0851146    0.152593    -0.0407787  
  0.097582    -0.0492418   -0.226408      0.0332144   -0.312387    -0.0995525    0.205819    -0.0644372   -0.00759831   0.0419664    0.0055976   -0.136673    -0.0411401    0.0940065    -0.0706771   -0.0264996   -0.0990868   -0.149351     0.0330714     0.0630394    0.198611   -0.0130452   -0.0149615    0.0907473    0.0549068   -0.105034   
 -0.080892     0.0108276   -0.104603     -0.00299228   0.0902143    0.0363767    0.122696    -0.139508    -0.11731      0.0801229    0.062818    -0.042914    -0.0750301    0.10852      -0.148736    -0.0566122   -0.00638067   0.0901291    0.162075      0.0499086   -0.113536    0.0851081    0.172403     0.0312766   -0.137386     0.124563   
  0.0155503    0.0743958    0.0807751     0.0145915    0.0314549    0.00962703   0.00631542   0.102394     0.00343498   0.0901935   -0.237156    -0.112354     0.076132    -0.0265457    -0.0699011   -0.0576357   -0.00182393   0.028159     0.176435      0.154782    -0.0315759   0.0805995   -0.0437962    0.0162879    0.0290227    0.115472   
 -0.0126282   -0.10531     -0.0220829    -0.114054    -0.141502    -0.0597772   -0.162189    -0.0108951   -0.065095     0.175053     0.110831     0.146654     0.0120228   -0.0845672    -0.0396735    0.0421372    0.100328     0.0286152   -0.0522277     0.139412    -0.0691634  -0.0333124    0.151103    -0.0498425   -0.057221    -0.226741   
  0.253707     0.228818    -0.000819672  -0.0034945    0.0191502    0.0179513   -0.0778345   -0.0445625    0.031285    -0.0332005    0.0740717    0.0770149   -0.139437     0.0328991    -0.0217997   -0.196343    -0.0457546    0.0687132    0.00364305    0.00756647  -0.118413    0.0319816    0.0311763    0.0128685    0.0491603    0.0895142  
 -0.125871     0.089031     0.0875238    -0.109352    -0.0242383    0.0144365   -0.0888999    0.189455    -0.0738167   -0.0719671    0.112087    -0.0857802    0.109706    -0.0439472    -0.0827199    0.0629088   -0.129564    -0.118627     0.128068      0.0225695    0.0427645   0.0691535   -0.0946138    0.0291699    0.0397368    0.0405298  
  0.0212195    0.0392793    0.0408199     0.0285401   -0.0577972    0.045427    -0.130479     0.164495    -0.0227359   -0.00618245   0.260842     0.106122    -0.00526188   0.0482678     0.0618209    0.105128    -0.0777937   -0.0346905   -0.0568285     0.104732     0.014912   -0.159371     0.0972792    0.0361675   -0.116992    -0.165915   
 -0.0916207    0.103601    -0.00622888   -0.0650423   -0.0208115   -0.0824311    0.0648437   -0.0644577   -0.0875833    0.0409719    0.259615    -0.0191319    0.0447798   -0.184019     -0.0288027    0.0259065   -0.0342666    0.15596     -0.163578     -0.0575133    0.115708   -0.162725    -0.154264    -0.0924331   -0.125553    -0.248469   
  0.146704    -0.0560074    0.0180149    -0.0406723    0.10786     -0.0396437    0.0734448    0.0311311    0.0330782   -0.149751     0.180737    -0.0168182    0.0507203    0.0427543     0.0356387    0.104166     0.0274635   -0.0172191    0.0752675    -0.0346112   -0.124804   -0.0663468    0.1363      -0.0663467    0.103598    -0.0147549  
 -0.0639983   -0.0487498   -0.00380047   -0.0100275   -0.16072      0.150843    -0.0612008    0.0169748   -0.124792    -0.0888532   -0.0793386    0.108393     0.141916    -0.320782     -0.0707858   -0.0491248    0.123343     0.280571     0.114282     -0.103457     0.0891139  -0.0966037   -0.0148084   -0.115278    -0.0580568    0.0518602  
  0.00836211  -0.25496     -0.0264163    -0.0358533    0.0510833    0.0430209   -0.00723109   0.159097    -0.0610388    0.17131      0.0220244   -0.0483542   -0.00756938   0.0814964    -0.0621981   -0.0321076   -0.0129943    0.0319503   -0.000644848  -0.00685618  -0.0689022  -0.00355728   0.18893     -0.0962817   -0.136435     0.128824   
  0.0855639   -0.0686016    0.0546452    -0.164958    -0.0130741   -0.0497271   -0.0624425   -0.0911896   -0.109739    -0.0222864    0.0645105   -0.192966     0.0574517    0.0915778    -0.00790186   0.00407159  -0.201031    -0.0132481   -0.0843164     0.291906    -0.0890574  -0.0369828   -0.0305223    0.138008     0.0876992    0.0589591  
 -0.060066    -0.132784    -0.0489996     0.0268238   -0.0155523    0.00637849   0.0574154   -0.0774874   -0.136697     0.0209238    0.172665     0.00453052  -0.0161632   -0.00614233   -0.0272579    0.0849803    0.145952     0.0522045   -0.0828567    -0.0887498   -0.0297918   0.058959    -0.0255239    0.0566653   -0.027486    -0.000373014
 -0.142172    -0.0535537    0.0173999     0.0675479    0.0671861   -0.141828    -0.214185     0.195259     0.0311374    0.0283121   -0.0706421    0.187973    -0.0627994   -0.0139101    -0.00193026  -0.00613299  -0.0305391   -0.100971    -0.0282693     0.0568798   -0.0898843  -0.15481      0.12322      0.00695054  -0.062626    -0.149641   
 -0.0282825   -0.0150125    0.0245963     0.010059    -0.072177     0.222657    -0.150007     0.0407404    0.125681    -0.0659166    0.0821658    0.0241001   -0.0383229   -0.136833     -0.0747737   -0.071631    -0.060824    -0.088845     0.0306954    -0.264723     0.090102    0.0347325   -0.0775601    0.154732     0.14942     -0.0879291  
  0.0370929    0.141384    -0.0476266    -0.13655     -0.154128    -0.00208553  -0.040081     0.096411     0.104996     0.0771205    0.0504986    0.0170499    0.00662445   0.000748557  -0.0300355   -0.0924809    0.0909324    0.0649763    0.13137      -0.22978      0.0148366   0.0338468   -0.0269973    0.0512145   -0.101482     0.0729486  
  0.00840556  -0.0484053    0.145962     -0.037737     0.0936643    0.0873857   -0.0327867    0.0145876   -0.0259331   -0.0389513    0.00686135   0.175558     0.0629071   -0.0600348     0.0229232   -0.0722933   -0.0602838   -0.0677624   -0.141804     -0.0067306   -0.0307513  -0.103887     0.0749621   -0.0850857   -0.03164     -0.0200495  
  0.0293015   -0.134125    -0.044805      0.0505242   -0.113695     0.111934     0.185729    -0.00138944   0.166661    -0.0279163    0.00627792  -0.119177     0.00965361   0.022401     -0.0867729   -0.0435242   -0.0902877    0.0160748    0.181266     -0.115093    -0.0523829  -0.0576164   -0.090609     0.0413894    0.0531467    0.0132108  
  0.134204     0.119529    -0.116966      0.0348437    0.115049    -0.116386     0.177984    -0.00483877  -0.0968432   -0.1369       0.0816801    0.00726031   0.00092374  -0.0521846     0.199207     0.145099    -0.0415973   -0.164029     0.135894      0.0544591    0.0079295  -0.0370076    0.0656328   -0.0154174   -0.0365109    0.00514369 kind diag, method split
0: avll = -1.4201495890227986
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.420230
INFO: iteration 2, average log likelihood -1.420164
INFO: iteration 3, average log likelihood -1.419914
INFO: iteration 4, average log likelihood -1.416945
INFO: iteration 5, average log likelihood -1.404272
INFO: iteration 6, average log likelihood -1.394402
INFO: iteration 7, average log likelihood -1.392601
INFO: iteration 8, average log likelihood -1.392031
INFO: iteration 9, average log likelihood -1.391730
INFO: iteration 10, average log likelihood -1.391558
INFO: iteration 11, average log likelihood -1.391442
INFO: iteration 12, average log likelihood -1.391341
INFO: iteration 13, average log likelihood -1.391229
INFO: iteration 14, average log likelihood -1.391101
INFO: iteration 15, average log likelihood -1.390974
INFO: iteration 16, average log likelihood -1.390867
INFO: iteration 17, average log likelihood -1.390783
INFO: iteration 18, average log likelihood -1.390714
INFO: iteration 19, average log likelihood -1.390653
INFO: iteration 20, average log likelihood -1.390598
INFO: iteration 21, average log likelihood -1.390547
INFO: iteration 22, average log likelihood -1.390497
INFO: iteration 23, average log likelihood -1.390445
INFO: iteration 24, average log likelihood -1.390383
INFO: iteration 25, average log likelihood -1.390290
INFO: iteration 26, average log likelihood -1.390114
INFO: iteration 27, average log likelihood -1.389790
INFO: iteration 28, average log likelihood -1.389455
INFO: iteration 29, average log likelihood -1.389236
INFO: iteration 30, average log likelihood -1.389111
INFO: iteration 31, average log likelihood -1.389041
INFO: iteration 32, average log likelihood -1.388999
INFO: iteration 33, average log likelihood -1.388972
INFO: iteration 34, average log likelihood -1.388954
INFO: iteration 35, average log likelihood -1.388942
INFO: iteration 36, average log likelihood -1.388933
INFO: iteration 37, average log likelihood -1.388926
INFO: iteration 38, average log likelihood -1.388921
INFO: iteration 39, average log likelihood -1.388917
INFO: iteration 40, average log likelihood -1.388914
INFO: iteration 41, average log likelihood -1.388911
INFO: iteration 42, average log likelihood -1.388909
INFO: iteration 43, average log likelihood -1.388907
INFO: iteration 44, average log likelihood -1.388906
INFO: iteration 45, average log likelihood -1.388905
INFO: iteration 46, average log likelihood -1.388903
INFO: iteration 47, average log likelihood -1.388903
INFO: iteration 48, average log likelihood -1.388902
INFO: iteration 49, average log likelihood -1.388901
INFO: iteration 50, average log likelihood -1.388901
INFO: EM with 100000 data points 50 iterations avll -1.388901
952.4 data points per parameter
1: avll = [-1.42023,-1.42016,-1.41991,-1.41694,-1.40427,-1.3944,-1.3926,-1.39203,-1.39173,-1.39156,-1.39144,-1.39134,-1.39123,-1.3911,-1.39097,-1.39087,-1.39078,-1.39071,-1.39065,-1.3906,-1.39055,-1.3905,-1.39045,-1.39038,-1.39029,-1.39011,-1.38979,-1.38946,-1.38924,-1.38911,-1.38904,-1.389,-1.38897,-1.38895,-1.38894,-1.38893,-1.38893,-1.38892,-1.38892,-1.38891,-1.38891,-1.38891,-1.38891,-1.38891,-1.3889,-1.3889,-1.3889,-1.3889,-1.3889,-1.3889]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.389003
INFO: iteration 2, average log likelihood -1.388903
INFO: iteration 3, average log likelihood -1.388601
INFO: iteration 4, average log likelihood -1.385730
INFO: iteration 5, average log likelihood -1.374122
INFO: iteration 6, average log likelihood -1.362078
INFO: iteration 7, average log likelihood -1.357099
INFO: iteration 8, average log likelihood -1.354439
INFO: iteration 9, average log likelihood -1.352569
INFO: iteration 10, average log likelihood -1.351329
INFO: iteration 11, average log likelihood -1.350521
INFO: iteration 12, average log likelihood -1.349979
INFO: iteration 13, average log likelihood -1.349610
INFO: iteration 14, average log likelihood -1.349354
INFO: iteration 15, average log likelihood -1.349175
INFO: iteration 16, average log likelihood -1.349055
INFO: iteration 17, average log likelihood -1.348974
INFO: iteration 18, average log likelihood -1.348921
INFO: iteration 19, average log likelihood -1.348886
INFO: iteration 20, average log likelihood -1.348863
INFO: iteration 21, average log likelihood -1.348846
INFO: iteration 22, average log likelihood -1.348834
INFO: iteration 23, average log likelihood -1.348825
INFO: iteration 24, average log likelihood -1.348817
INFO: iteration 25, average log likelihood -1.348810
INFO: iteration 26, average log likelihood -1.348805
INFO: iteration 27, average log likelihood -1.348800
INFO: iteration 28, average log likelihood -1.348796
INFO: iteration 29, average log likelihood -1.348792
INFO: iteration 30, average log likelihood -1.348789
INFO: iteration 31, average log likelihood -1.348786
INFO: iteration 32, average log likelihood -1.348783
INFO: iteration 33, average log likelihood -1.348780
INFO: iteration 34, average log likelihood -1.348777
INFO: iteration 35, average log likelihood -1.348775
INFO: iteration 36, average log likelihood -1.348772
INFO: iteration 37, average log likelihood -1.348770
INFO: iteration 38, average log likelihood -1.348768
INFO: iteration 39, average log likelihood -1.348766
INFO: iteration 40, average log likelihood -1.348764
INFO: iteration 41, average log likelihood -1.348762
INFO: iteration 42, average log likelihood -1.348760
INFO: iteration 43, average log likelihood -1.348758
INFO: iteration 44, average log likelihood -1.348756
INFO: iteration 45, average log likelihood -1.348754
INFO: iteration 46, average log likelihood -1.348753
INFO: iteration 47, average log likelihood -1.348751
INFO: iteration 48, average log likelihood -1.348749
INFO: iteration 49, average log likelihood -1.348748
INFO: iteration 50, average log likelihood -1.348747
INFO: EM with 100000 data points 50 iterations avll -1.348747
473.9 data points per parameter
2: avll = [-1.389,-1.3889,-1.3886,-1.38573,-1.37412,-1.36208,-1.3571,-1.35444,-1.35257,-1.35133,-1.35052,-1.34998,-1.34961,-1.34935,-1.34917,-1.34905,-1.34897,-1.34892,-1.34889,-1.34886,-1.34885,-1.34883,-1.34882,-1.34882,-1.34881,-1.3488,-1.3488,-1.3488,-1.34879,-1.34879,-1.34879,-1.34878,-1.34878,-1.34878,-1.34877,-1.34877,-1.34877,-1.34877,-1.34877,-1.34876,-1.34876,-1.34876,-1.34876,-1.34876,-1.34875,-1.34875,-1.34875,-1.34875,-1.34875,-1.34875]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.348890
INFO: iteration 2, average log likelihood -1.348770
INFO: iteration 3, average log likelihood -1.348558
INFO: iteration 4, average log likelihood -1.346504
INFO: iteration 5, average log likelihood -1.335284
INFO: iteration 6, average log likelihood -1.318616
INFO: iteration 7, average log likelihood -1.309838
INFO: iteration 8, average log likelihood -1.305056
INFO: iteration 9, average log likelihood -1.301768
INFO: iteration 10, average log likelihood -1.299461
INFO: iteration 11, average log likelihood -1.298321
INFO: iteration 12, average log likelihood -1.297533
INFO: iteration 13, average log likelihood -1.296667
INFO: iteration 14, average log likelihood -1.295642
INFO: iteration 15, average log likelihood -1.294437
INFO: iteration 16, average log likelihood -1.293120
INFO: iteration 17, average log likelihood -1.291908
INFO: iteration 18, average log likelihood -1.290816
INFO: iteration 19, average log likelihood -1.289751
INFO: iteration 20, average log likelihood -1.288738
INFO: iteration 21, average log likelihood -1.287942
INFO: iteration 22, average log likelihood -1.287393
INFO: iteration 23, average log likelihood -1.287004
INFO: iteration 24, average log likelihood -1.286723
INFO: iteration 25, average log likelihood -1.286493
INFO: iteration 26, average log likelihood -1.286265
INFO: iteration 27, average log likelihood -1.286042
INFO: iteration 28, average log likelihood -1.285869
INFO: iteration 29, average log likelihood -1.285741
INFO: iteration 30, average log likelihood -1.285652
INFO: iteration 31, average log likelihood -1.285594
INFO: iteration 32, average log likelihood -1.285556
INFO: iteration 33, average log likelihood -1.285530
INFO: iteration 34, average log likelihood -1.285511
INFO: iteration 35, average log likelihood -1.285497
INFO: iteration 36, average log likelihood -1.285485
INFO: iteration 37, average log likelihood -1.285475
INFO: iteration 38, average log likelihood -1.285467
INFO: iteration 39, average log likelihood -1.285458
INFO: iteration 40, average log likelihood -1.285451
INFO: iteration 41, average log likelihood -1.285443
INFO: iteration 42, average log likelihood -1.285435
INFO: iteration 43, average log likelihood -1.285427
INFO: iteration 44, average log likelihood -1.285419
INFO: iteration 45, average log likelihood -1.285410
INFO: iteration 46, average log likelihood -1.285400
INFO: iteration 47, average log likelihood -1.285390
INFO: iteration 48, average log likelihood -1.285379
INFO: iteration 49, average log likelihood -1.285367
INFO: iteration 50, average log likelihood -1.285355
INFO: EM with 100000 data points 50 iterations avll -1.285355
236.4 data points per parameter
3: avll = [-1.34889,-1.34877,-1.34856,-1.3465,-1.33528,-1.31862,-1.30984,-1.30506,-1.30177,-1.29946,-1.29832,-1.29753,-1.29667,-1.29564,-1.29444,-1.29312,-1.29191,-1.29082,-1.28975,-1.28874,-1.28794,-1.28739,-1.287,-1.28672,-1.28649,-1.28627,-1.28604,-1.28587,-1.28574,-1.28565,-1.28559,-1.28556,-1.28553,-1.28551,-1.2855,-1.28549,-1.28548,-1.28547,-1.28546,-1.28545,-1.28544,-1.28543,-1.28543,-1.28542,-1.28541,-1.2854,-1.28539,-1.28538,-1.28537,-1.28536]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.285537
INFO: iteration 2, average log likelihood -1.285309
INFO: iteration 3, average log likelihood -1.284559
INFO: iteration 4, average log likelihood -1.277872
INFO: iteration 5, average log likelihood -1.257305
INFO: iteration 6, average log likelihood -1.234684
INFO: iteration 7, average log likelihood -1.222687
INFO: iteration 8, average log likelihood -1.216196
INFO: iteration 9, average log likelihood -1.211823
INFO: iteration 10, average log likelihood -1.208530
WARNING: Variances had to be floored 14
INFO: iteration 11, average log likelihood -1.205841
INFO: iteration 12, average log likelihood -1.215250
INFO: iteration 13, average log likelihood -1.208412
WARNING: Variances had to be floored 5
INFO: iteration 14, average log likelihood -1.204707
INFO: iteration 15, average log likelihood -1.214371
INFO: iteration 16, average log likelihood -1.208448
WARNING: Variances had to be floored 14
INFO: iteration 17, average log likelihood -1.205539
INFO: iteration 18, average log likelihood -1.214837
INFO: iteration 19, average log likelihood -1.208036
WARNING: Variances had to be floored 5
INFO: iteration 20, average log likelihood -1.204502
INFO: iteration 21, average log likelihood -1.214254
INFO: iteration 22, average log likelihood -1.208555
WARNING: Variances had to be floored 14
INFO: iteration 23, average log likelihood -1.205764
INFO: iteration 24, average log likelihood -1.214980
INFO: iteration 25, average log likelihood -1.208161
WARNING: Variances had to be floored 5
INFO: iteration 26, average log likelihood -1.204637
INFO: iteration 27, average log likelihood -1.214196
INFO: iteration 28, average log likelihood -1.208611
WARNING: Variances had to be floored 14
INFO: iteration 29, average log likelihood -1.205908
INFO: iteration 30, average log likelihood -1.215178
INFO: iteration 31, average log likelihood -1.208393
WARNING: Variances had to be floored 5
INFO: iteration 32, average log likelihood -1.204860
INFO: iteration 33, average log likelihood -1.214127
INFO: iteration 34, average log likelihood -1.208645
WARNING: Variances had to be floored 14
INFO: iteration 35, average log likelihood -1.206051
INFO: iteration 36, average log likelihood -1.215416
INFO: iteration 37, average log likelihood -1.208719
INFO: iteration 38, average log likelihood -1.205199
WARNING: Variances had to be floored 5
INFO: iteration 39, average log likelihood -1.202821
INFO: iteration 40, average log likelihood -1.213437
WARNING: Variances had to be floored 14
INFO: iteration 41, average log likelihood -1.207779
INFO: iteration 42, average log likelihood -1.216624
INFO: iteration 43, average log likelihood -1.209938
INFO: iteration 44, average log likelihood -1.206574
INFO: iteration 45, average log likelihood -1.204192
WARNING: Variances had to be floored 5
INFO: iteration 46, average log likelihood -1.202258
WARNING: Variances had to be floored 14
INFO: iteration 47, average log likelihood -1.212526
INFO: iteration 48, average log likelihood -1.218316
INFO: iteration 49, average log likelihood -1.211138
INFO: iteration 50, average log likelihood -1.207852
INFO: EM with 100000 data points 50 iterations avll -1.207852
118.1 data points per parameter
4: avll = [-1.28554,-1.28531,-1.28456,-1.27787,-1.25731,-1.23468,-1.22269,-1.2162,-1.21182,-1.20853,-1.20584,-1.21525,-1.20841,-1.20471,-1.21437,-1.20845,-1.20554,-1.21484,-1.20804,-1.2045,-1.21425,-1.20856,-1.20576,-1.21498,-1.20816,-1.20464,-1.2142,-1.20861,-1.20591,-1.21518,-1.20839,-1.20486,-1.21413,-1.20864,-1.20605,-1.21542,-1.20872,-1.2052,-1.20282,-1.21344,-1.20778,-1.21662,-1.20994,-1.20657,-1.20419,-1.20226,-1.21253,-1.21832,-1.21114,-1.20785]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.205939
INFO: iteration 2, average log likelihood -1.203638
WARNING: Variances had to be floored 9 10 27 28
INFO: iteration 3, average log likelihood -1.199337
WARNING: Variances had to be floored 24
INFO: iteration 4, average log likelihood -1.182306
WARNING: Variances had to be floored 4 24
INFO: iteration 5, average log likelihood -1.140995
WARNING: Variances had to be floored 7 9 10 14 24 27 28
INFO: iteration 6, average log likelihood -1.123590
WARNING: Variances had to be floored 4 24
INFO: iteration 7, average log likelihood -1.128313
WARNING: Variances had to be floored 7
INFO: iteration 8, average log likelihood -1.114481
WARNING: Variances had to be floored 4 9 10 14 24 25 27 28
INFO: iteration 9, average log likelihood -1.101663
WARNING: Variances had to be floored 7 24
INFO: iteration 10, average log likelihood -1.127122
WARNING: Variances had to be floored 4 9 10 24 27
INFO: iteration 11, average log likelihood -1.112440
WARNING: Variances had to be floored 7 14 24 28
INFO: iteration 12, average log likelihood -1.108993
WARNING: Variances had to be floored 4 9 10 24 25
INFO: iteration 13, average log likelihood -1.115175
WARNING: Variances had to be floored 7 24
INFO: iteration 14, average log likelihood -1.118345
WARNING: Variances had to be floored 4 9 10 14 24 28
INFO: iteration 15, average log likelihood -1.108060
WARNING: Variances had to be floored 7 24 27
INFO: iteration 16, average log likelihood -1.116631
WARNING: Variances had to be floored 4 9 10 24
INFO: iteration 17, average log likelihood -1.109447
WARNING: Variances had to be floored 7 14 24 25 28
INFO: iteration 18, average log likelihood -1.107462
WARNING: Variances had to be floored 4 9 10 24
INFO: iteration 19, average log likelihood -1.122797
WARNING: Variances had to be floored 7 24
INFO: iteration 20, average log likelihood -1.112813
WARNING: Variances had to be floored 4 9 10 14 24 28
INFO: iteration 21, average log likelihood -1.105283
WARNING: Variances had to be floored 7 24 25
INFO: iteration 22, average log likelihood -1.116575
WARNING: Variances had to be floored 4 9 10 24 28
INFO: iteration 23, average log likelihood -1.115233
WARNING: Variances had to be floored 7 14 24
INFO: iteration 24, average log likelihood -1.107630
WARNING: Variances had to be floored 4 9 10 24 25 27 28
INFO: iteration 25, average log likelihood -1.101662
WARNING: Variances had to be floored 7 24
INFO: iteration 26, average log likelihood -1.118986
WARNING: Variances had to be floored 4 9 10 14 24 28
INFO: iteration 27, average log likelihood -1.107913
WARNING: Variances had to be floored 7 24 25
INFO: iteration 28, average log likelihood -1.112708
WARNING: Variances had to be floored 4 9 10 24 27 28
INFO: iteration 29, average log likelihood -1.102437
WARNING: Variances had to be floored 7 14 24
INFO: iteration 30, average log likelihood -1.111616
WARNING: Variances had to be floored 4 9 10 24 25 28
INFO: iteration 31, average log likelihood -1.113628
WARNING: Variances had to be floored 7 24
INFO: iteration 32, average log likelihood -1.113656
WARNING: Variances had to be floored 4 9 10 14 24 25 27 28
INFO: iteration 33, average log likelihood -1.095097
WARNING: Variances had to be floored 7 24
INFO: iteration 34, average log likelihood -1.125085
WARNING: Variances had to be floored 4 9 10 24 28
INFO: iteration 35, average log likelihood -1.110748
WARNING: Variances had to be floored 7 14 24 25
INFO: iteration 36, average log likelihood -1.104929
WARNING: Variances had to be floored 4 9 10 24 27 28
INFO: iteration 37, average log likelihood -1.108774
WARNING: Variances had to be floored 7 24
INFO: iteration 38, average log likelihood -1.113883
WARNING: Variances had to be floored 4 9 10 14 24 25 28
INFO: iteration 39, average log likelihood -1.105223
WARNING: Variances had to be floored 7 24
INFO: iteration 40, average log likelihood -1.119257
WARNING: Variances had to be floored 4 9 10 24 25 27 28
INFO: iteration 41, average log likelihood -1.096941
WARNING: Variances had to be floored 7 14 24
INFO: iteration 42, average log likelihood -1.116578
WARNING: Variances had to be floored 4 9 10 24 28
INFO: iteration 43, average log likelihood -1.116216
WARNING: Variances had to be floored 7 24 25
INFO: iteration 44, average log likelihood -1.106938
WARNING: Variances had to be floored 4 9 10 14 24 27 28
INFO: iteration 45, average log likelihood -1.100314
WARNING: Variances had to be floored 7 24
INFO: iteration 46, average log likelihood -1.119927
WARNING: Variances had to be floored 4 9 10 24 25 28
INFO: iteration 47, average log likelihood -1.108279
WARNING: Variances had to be floored 7 14 24
INFO: iteration 48, average log likelihood -1.111945
WARNING: Variances had to be floored 4 9 10 24 25 27 28
INFO: iteration 49, average log likelihood -1.103422
WARNING: Variances had to be floored 7 24
INFO: iteration 50, average log likelihood -1.119029
INFO: EM with 100000 data points 50 iterations avll -1.119029
59.0 data points per parameter
5: avll = [-1.20594,-1.20364,-1.19934,-1.18231,-1.14099,-1.12359,-1.12831,-1.11448,-1.10166,-1.12712,-1.11244,-1.10899,-1.11518,-1.11834,-1.10806,-1.11663,-1.10945,-1.10746,-1.1228,-1.11281,-1.10528,-1.11658,-1.11523,-1.10763,-1.10166,-1.11899,-1.10791,-1.11271,-1.10244,-1.11162,-1.11363,-1.11366,-1.0951,-1.12508,-1.11075,-1.10493,-1.10877,-1.11388,-1.10522,-1.11926,-1.09694,-1.11658,-1.11622,-1.10694,-1.10031,-1.11993,-1.10828,-1.11195,-1.10342,-1.11903]
[-1.42015,-1.42023,-1.42016,-1.41991,-1.41694,-1.40427,-1.3944,-1.3926,-1.39203,-1.39173,-1.39156,-1.39144,-1.39134,-1.39123,-1.3911,-1.39097,-1.39087,-1.39078,-1.39071,-1.39065,-1.3906,-1.39055,-1.3905,-1.39045,-1.39038,-1.39029,-1.39011,-1.38979,-1.38946,-1.38924,-1.38911,-1.38904,-1.389,-1.38897,-1.38895,-1.38894,-1.38893,-1.38893,-1.38892,-1.38892,-1.38891,-1.38891,-1.38891,-1.38891,-1.38891,-1.3889,-1.3889,-1.3889,-1.3889,-1.3889,-1.3889,-1.389,-1.3889,-1.3886,-1.38573,-1.37412,-1.36208,-1.3571,-1.35444,-1.35257,-1.35133,-1.35052,-1.34998,-1.34961,-1.34935,-1.34917,-1.34905,-1.34897,-1.34892,-1.34889,-1.34886,-1.34885,-1.34883,-1.34882,-1.34882,-1.34881,-1.3488,-1.3488,-1.3488,-1.34879,-1.34879,-1.34879,-1.34878,-1.34878,-1.34878,-1.34877,-1.34877,-1.34877,-1.34877,-1.34877,-1.34876,-1.34876,-1.34876,-1.34876,-1.34876,-1.34875,-1.34875,-1.34875,-1.34875,-1.34875,-1.34875,-1.34889,-1.34877,-1.34856,-1.3465,-1.33528,-1.31862,-1.30984,-1.30506,-1.30177,-1.29946,-1.29832,-1.29753,-1.29667,-1.29564,-1.29444,-1.29312,-1.29191,-1.29082,-1.28975,-1.28874,-1.28794,-1.28739,-1.287,-1.28672,-1.28649,-1.28627,-1.28604,-1.28587,-1.28574,-1.28565,-1.28559,-1.28556,-1.28553,-1.28551,-1.2855,-1.28549,-1.28548,-1.28547,-1.28546,-1.28545,-1.28544,-1.28543,-1.28543,-1.28542,-1.28541,-1.2854,-1.28539,-1.28538,-1.28537,-1.28536,-1.28554,-1.28531,-1.28456,-1.27787,-1.25731,-1.23468,-1.22269,-1.2162,-1.21182,-1.20853,-1.20584,-1.21525,-1.20841,-1.20471,-1.21437,-1.20845,-1.20554,-1.21484,-1.20804,-1.2045,-1.21425,-1.20856,-1.20576,-1.21498,-1.20816,-1.20464,-1.2142,-1.20861,-1.20591,-1.21518,-1.20839,-1.20486,-1.21413,-1.20864,-1.20605,-1.21542,-1.20872,-1.2052,-1.20282,-1.21344,-1.20778,-1.21662,-1.20994,-1.20657,-1.20419,-1.20226,-1.21253,-1.21832,-1.21114,-1.20785,-1.20594,-1.20364,-1.19934,-1.18231,-1.14099,-1.12359,-1.12831,-1.11448,-1.10166,-1.12712,-1.11244,-1.10899,-1.11518,-1.11834,-1.10806,-1.11663,-1.10945,-1.10746,-1.1228,-1.11281,-1.10528,-1.11658,-1.11523,-1.10763,-1.10166,-1.11899,-1.10791,-1.11271,-1.10244,-1.11162,-1.11363,-1.11366,-1.0951,-1.12508,-1.11075,-1.10493,-1.10877,-1.11388,-1.10522,-1.11926,-1.09694,-1.11658,-1.11622,-1.10694,-1.10031,-1.11993,-1.10828,-1.11195,-1.10342,-1.11903]
32×26 Array{Float64,2}:
 -0.00323313   0.126976   -0.0992289    -0.167479    -0.110219     0.0246603    -0.0832838    0.100332     0.0244777     0.0605398   0.0645718   -0.0485615    0.0452842    0.0158742   -0.0139617    -0.0554733    0.00521622  -0.0228849    0.0619301   -0.0829362    -0.0489097   -0.00604683  -0.00647376    0.0571584   0.0249102    0.0300986 
  0.0128055    0.0186011   0.0398795    -0.0721209   -0.0676095    0.045477     -0.131104     0.171893    -0.03552      -0.019767    0.251811     0.108412     0.0149779    0.0491628    0.0573826     0.110814    -0.0724701   -0.042244    -0.0243045    0.10626       0.0391251   -0.166248     0.0868823     0.0399748  -0.122901    -0.157912  
  0.0168136   -0.115474    0.109224      0.0672664    0.0577305    0.0979234    -0.0608239   -0.189385    -0.194858     -0.0508537   0.0153248    0.102808    -0.0829054   -0.0520995    0.0283404     0.0106955   -0.0112488   -0.0676947    0.0865498    0.194445     -0.0559265   -0.0800907   -0.0958566     0.0522844   0.0526285    0.0852133 
  0.0936172   -0.0499371  -0.219062      0.0632347   -0.300098    -0.0968274     0.150332    -0.0243205   -0.00591375    0.0274858   0.0106749   -0.133188    -0.0550764    0.0894741   -0.0955802    -0.0457358   -0.095105    -0.135986     0.0320853    0.0583534     0.180809     0.009861    -0.0291711     0.0882574   0.0510461   -0.0995637 
 -0.0502747    0.0407823   0.000765772  -0.0942381    0.0302996    0.0108871    -0.135107     0.0634665   -0.0719806    -0.142848   -0.0597088    0.009615    -0.0688032   -0.0156027    0.0473787    -0.0788811   -0.0485456   -0.0531609    0.0325216   -0.0733985     0.0435423    0.0326619    0.0501557    -0.0148063   0.0472896    0.0022838 
  0.0504276   -0.0644631  -0.0170253    -0.107367     0.00306166  -0.0531967    -0.113878    -0.0279921   -0.0294476     0.0984562   0.0177092    0.114323     0.0311645   -0.0392794   -0.050126      0.174351    -0.0246927   -0.0160589   -0.0408292    0.032345     -0.0777773    0.0555179    0.0231405    -0.0524635  -0.00598088  -0.163273  
 -0.119061    -0.0615833   0.032647     -0.119803     0.0757003    0.0400357     0.0206982   -0.0732325   -0.138551     -0.0154612   0.213309     0.086254    -0.157693     0.0787563    0.10137       0.0653108    0.1215      -0.132289     0.0170376   -0.00640871   -0.0813169   -0.122163     0.281401      0.167111    0.0588257   -0.0355274 
  0.092373     0.235913    0.0491828     0.0844088   -0.222795     0.0793936     0.103638     0.0320218   -0.118002      0.0423429   0.0237859   -0.0417636   -0.0374026   -0.0150851    0.0139167     0.0527925   -0.00172096   0.0809343   -0.00425826  -0.0861141    -0.0851429    0.0270415    0.000513137   0.0528126  -0.0312128   -0.162393  
  0.141144    -0.140771    0.141767      0.0108921    0.0929865    0.121387     -0.0370619    0.0156021   -0.0339834     0.370394    0.138908     0.168687     0.134508    -0.0404455   -0.00194415   -0.0543282   -0.0556876    0.0916597   -0.135258    -0.080138     -0.0542752   -0.336599     0.484955     -0.0812953  -0.0229243   -0.0219879 
 -0.106757    -0.0449965   0.121795     -0.0769989    0.0932773    0.00741504    0.0110032    0.0161529   -0.0125448    -0.496161   -0.105069     0.164793    -0.0379187   -0.0542256    0.0456376    -0.0618601   -0.0588481   -0.224456    -0.133444     0.0606091    -0.00708554   0.11197     -0.277944     -0.0799889  -0.0166435   -0.015483  
 -0.0575685    0.0213071  -0.0307227    -0.118306    -0.0966272   -0.000684376   0.0972058    0.0987247   -0.0409332    -0.0434998   0.0548223   -0.0572263    0.0863832   -0.104822    -0.0904614    -0.00823226  -0.0377971   -0.137261     0.0279298   -0.0674766     0.0534066    0.0710198   -0.099003      0.0656458  -0.0260584    0.053662  
 -0.0678563   -0.0626892  -0.0623953     0.0336963    0.0181882    0.00554383    0.0487166    0.0166455    0.0165319     0.0376591   0.0108656    0.00882095  -0.0482136    0.0307031   -0.075346     -0.0390305   -0.0278355    0.00178647   0.102093     0.0046608    -0.076318    -0.0266078    0.0643159     0.0290357  -0.0436383    0.00383933
 -0.0750842   -0.0408139  -0.0147217    -0.0393316   -0.14196      0.0548058    -0.0272143    0.0126299   -0.0774601    -0.0362689   0.0507224    0.0787132    0.125985    -0.11748     -0.034512     -0.043123     0.0367459    0.106722     0.113412     0.022847      0.0949939   -0.0129848    0.0174275    -0.099106   -0.0426235   -0.0948569 
  0.0200094    0.0668855   0.151235      0.0102675    0.0208155    0.00868092    0.00278012   0.108684     0.0334087     0.0940422  -0.233016    -0.0997136    0.072056    -0.0297296   -0.0647466    -0.0289939    0.00386334   0.0366433    0.110929     0.174457     -0.0394215    0.0797921   -0.028537      0.0296501   0.0291084    0.111489  
 -0.0762616    0.102672   -0.00560017   -0.0242782   -0.0413617   -0.0783702     0.06765     -0.0532945   -0.0955715     0.0277771   0.255866    -0.038784     0.00381119  -0.202091    -0.0258749     0.0224581   -0.0252934    0.146331    -0.163203    -0.0513431     0.121196    -0.155094    -0.155169     -0.083604   -0.133069    -0.247922  
 -0.0576293    0.106457    0.0681537     0.067753     0.0628509   -0.0586592     0.0576527   -0.0668631    0.0365624    -0.0313469   0.0914502   -0.130897     0.0477756   -0.0490473    0.00915487    0.0605996    0.083362     0.00616811  -0.0326612    0.0493634     0.120884    -0.133901    -0.185177      0.030606   -0.0193769    0.0387903 
  0.0912633   -0.201331   -0.444074      0.00759938  -0.189652     0.0159477    -0.347802     0.066007     0.0487899     0.016375   -0.260996     0.138202    -0.16146      0.0836558   -0.1249        0.00269031  -0.0246445   -0.130379    -0.108277     0.10944       0.0657803   -1.66075     -0.0285346    -0.0706582  -0.0568904   -0.0515616 
  0.272795    -0.175108   -0.125291      0.0576779    0.173245     0.018854     -0.623021     0.0497741    0.0259034     0.18277    -0.258765     0.164424    -0.132819     0.147145    -0.121505     -0.0227245   -0.0666196   -0.168949    -0.0823554    0.432229      0.270235     1.28019     -0.0357308    -0.0849332  -0.222776     0.0126434 
  0.148632     0.0453381  -0.0955214     0.012392     0.0405013    0.0178208     0.953004     0.0376087    0.0467932     0.318185   -0.177505     0.0398074   -0.152368    -0.176271    -0.135958     -0.0345547   -0.0430968    0.206435    -0.0842673    0.37384       0.169634    -0.996852    -0.0247065    -0.0675886  -0.185719    -0.133394  
  0.135209    -0.0361297  -0.10507      -0.0246576   -0.0926673    0.0164648    -0.0202781    0.0319148    0.0489549     0.0448885  -0.122936     0.0141534   -0.0858658    0.374983    -0.130824     -0.0418049   -0.0390771    0.112208    -0.085468    -0.267364      0.133856     0.697172    -0.0155723    -0.0613085  -0.126552     0.0694834 
  0.195336     0.1334     -0.0460568     0.0308031    0.152949    -0.132545      0.147171    -0.0191442   -0.064259     -0.109296    0.104213     0.0059956   -0.368512    -0.0789826    0.190506      0.166491    -0.19609     -0.120391     0.139582    -0.0259436     0.0526604   -0.0256965    0.0669603    -0.0327667  -0.100088     0.00275091
  0.0802896    0.101609   -0.155466      0.0145014    0.030819    -0.0972459     0.194344     0.00740697  -0.0991095    -0.188878    0.0866698   -0.0226154    0.416542     0.00599614   0.182633      0.144784     0.0505147   -0.220592     0.134251     0.126253     -0.0616      -0.0681202    0.0653166    -0.0362842  -0.0333445    0.0109188 
 -0.0195154   -0.0558243   0.00380955    0.0103694   -0.0744753    0.224803     -0.14893      0.0406729    0.124487     -0.062207    0.0780197    0.0261771   -0.0475724   -0.110356    -0.0577046    -0.0645236   -0.0676815   -0.126808     0.0251674   -0.264721      0.083909     0.0148382   -0.0609385     0.16897     0.14754     -0.101587  
  0.25232      0.240154    0.0697703     0.0123895    0.0213407    0.0115518    -0.0628297   -0.0374223    0.000823869  -0.0335394   0.0730229    0.0755932   -0.136802     0.0273104   -0.0383886    -0.200671    -0.044972     0.0607006    0.00408069  -0.000815047  -0.119002     0.0457305    0.0475957     0.0196913   0.0616933    0.0929316 
  0.00011503  -0.105836   -0.0264877     0.0228861   -0.0107512    0.000452987   0.0387906   -0.0620501   -0.121087      0.0137093   0.172898    -0.00327804   0.0013509   -0.00850599  -0.000623976   0.0975331    0.162577     0.0373416   -0.0869781   -0.0543813    -0.0410467    0.0211934    0.0110383     0.0325853  -0.020353     0.0104738 
  0.00915721   0.161536   -0.0986769    -0.223409    -0.0611996    0.142852      0.0999823    0.200251    -0.0359909     0.0939872  -0.0143609    0.156939    -0.0909616    0.0379385   -0.00603326   -0.0636287   -0.108545     0.0260923    0.0858516    0.152329      0.109668     0.103736    -0.0302369     0.0177658  -0.0513813    0.0846873 
  0.142519    -0.0957473   0.0698834     0.0186053    0.0960405   -0.0063234     0.0390844    0.0715066    0.00700794   -0.0852623   0.149046    -0.0539305    0.0385491    0.0692205   -0.0200486     0.0688439    0.0187033   -0.0257341    0.0740726   -0.0237968    -0.106891    -0.0378188    0.12095      -0.085089    0.0691891    0.0311699 
 -0.0425286   -0.275508    0.00482565   -0.0705587    0.0553467    0.0575794    -0.00891017   0.176398    -0.0670356     0.189857    0.00843265  -0.0725376   -0.00781774   0.128624    -0.0770277    -0.0412722   -0.0207206    0.0739374    0.0188678    0.00404777   -0.0612938   -0.0620084    0.160944     -0.099788   -0.157275     0.131516  
  0.118447    -0.0929553   0.129471     -0.206066    -0.00541458  -0.0526907    -0.055275    -0.099313    -0.533676     -0.0202373  -0.025454    -0.235289     0.0644945    0.111447    -0.0693431     0.102572    -0.286557    -0.0171515   -0.00341608   0.287254     -0.0827492   -0.00700475  -0.0340975     0.279731    0.0968241    0.0427287 
  0.0462113   -0.0424659   0.0379586    -0.126081    -0.0234947   -0.0403992    -0.0780913   -0.0849055    0.281965     -0.024115    0.163262    -0.143742     0.0606594    0.0670565    0.0209181    -0.0822454   -0.159596    -0.0228811   -0.174838     0.274765     -0.0963818   -0.050875    -0.00848318    0.113448    0.0826619    0.0709171 
 -0.132435    -0.0285819   0.0702101     0.0517002    0.07051     -0.173243     -0.110627     0.100162    -0.0472837    -0.0459727   0.0727122    0.075851    -0.0234361    0.00261471  -0.182105      0.0600098    0.0185458   -0.0643075    0.0469235    0.0184775     0.0665354    0.191942    -0.300453     -0.103784   -0.0594811    0.194747  
  0.098456    -0.0396173   0.0221514    -0.00243358   0.0605986   -0.183948     -0.12101      0.00201568   0.0821555     0.100929    0.067011     0.074391    -0.0277928    0.0212466    0.0704652     0.0926711   -0.0583895   -0.0753299   -0.209557     0.0450888     0.142409     0.166645     0.147712     -0.124798   -0.044045    -0.227316  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 4 9 10 14 24 28
INFO: iteration 1, average log likelihood -1.107922
WARNING: Variances had to be floored 4 7 9 10 14 24 25 28
INFO: iteration 2, average log likelihood -1.093896
WARNING: Variances had to be floored 4 9 10 14 24 25 27 28
INFO: iteration 3, average log likelihood -1.096994
WARNING: Variances had to be floored 4 7 9 10 14 24 28
INFO: iteration 4, average log likelihood -1.100866
WARNING: Variances had to be floored 4 9 10 14 24 25 28
INFO: iteration 5, average log likelihood -1.100311
WARNING: Variances had to be floored 4 7 9 10 14 24 25 27 28
INFO: iteration 6, average log likelihood -1.089567
WARNING: Variances had to be floored 4 9 10 14 24 28
INFO: iteration 7, average log likelihood -1.108208
WARNING: Variances had to be floored 4 7 9 10 14 24 25 28
INFO: iteration 8, average log likelihood -1.093034
WARNING: Variances had to be floored 4 9 10 14 24 25 27 28
INFO: iteration 9, average log likelihood -1.096908
WARNING: Variances had to be floored 4 7 9 10 14 24 28
INFO: iteration 10, average log likelihood -1.100855
INFO: EM with 100000 data points 10 iterations avll -1.100855
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.325711e+05
      1       7.010748e+05      -2.314963e+05 |       32
      2       6.708089e+05      -3.026585e+04 |       32
      3       6.547664e+05      -1.604255e+04 |       32
      4       6.441462e+05      -1.062015e+04 |       32
      5       6.372108e+05      -6.935399e+03 |       32
      6       6.312623e+05      -5.948581e+03 |       32
      7       6.257817e+05      -5.480575e+03 |       32
      8       6.226151e+05      -3.166606e+03 |       32
      9       6.213726e+05      -1.242510e+03 |       32
     10       6.208580e+05      -5.146011e+02 |       32
     11       6.206140e+05      -2.439282e+02 |       32
     12       6.204931e+05      -1.209551e+02 |       32
     13       6.204317e+05      -6.139415e+01 |       32
     14       6.204022e+05      -2.947585e+01 |       32
     15       6.203828e+05      -1.941239e+01 |       31
     16       6.203681e+05      -1.466365e+01 |       32
     17       6.203545e+05      -1.358844e+01 |       31
     18       6.203437e+05      -1.083759e+01 |       31
     19       6.203350e+05      -8.686406e+00 |       29
     20       6.203270e+05      -7.984181e+00 |       31
     21       6.203194e+05      -7.659899e+00 |       31
     22       6.203114e+05      -8.016464e+00 |       30
     23       6.203042e+05      -7.124839e+00 |       27
     24       6.202994e+05      -4.785783e+00 |       26
     25       6.202960e+05      -3.469495e+00 |       21
     26       6.202925e+05      -3.443022e+00 |       23
     27       6.202891e+05      -3.452584e+00 |       22
     28       6.202853e+05      -3.811675e+00 |       17
     29       6.202821e+05      -3.190359e+00 |       25
     30       6.202787e+05      -3.347533e+00 |       20
     31       6.202745e+05      -4.219600e+00 |       22
     32       6.202693e+05      -5.214758e+00 |       26
     33       6.202592e+05      -1.012230e+01 |       30
     34       6.202496e+05      -9.575230e+00 |       27
     35       6.202360e+05      -1.357942e+01 |       26
     36       6.202177e+05      -1.831798e+01 |       30
     37       6.201942e+05      -2.351540e+01 |       29
     38       6.201689e+05      -2.526452e+01 |       29
     39       6.201384e+05      -3.051714e+01 |       31
     40       6.201089e+05      -2.954296e+01 |       31
     41       6.200664e+05      -4.242124e+01 |       32
     42       6.200251e+05      -4.133040e+01 |       31
     43       6.199889e+05      -3.624736e+01 |       32
     44       6.199499e+05      -3.896254e+01 |       32
     45       6.198987e+05      -5.116545e+01 |       31
     46       6.198482e+05      -5.049180e+01 |       31
     47       6.197858e+05      -6.243357e+01 |       32
     48       6.197229e+05      -6.292508e+01 |       32
     49       6.196753e+05      -4.763154e+01 |       31
     50       6.196421e+05      -3.314044e+01 |       30
K-means terminated without convergence after 50 iterations (objv = 619642.1162496088)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.341277
INFO: iteration 2, average log likelihood -1.314214
INFO: iteration 3, average log likelihood -1.288816
INFO: iteration 4, average log likelihood -1.257357
INFO: iteration 5, average log likelihood -1.214131
WARNING: Variances had to be floored 5
INFO: iteration 6, average log likelihood -1.154700
WARNING: Variances had to be floored 6 9 26 27 32
INFO: iteration 7, average log likelihood -1.114710
WARNING: Variances had to be floored 8 10 14 19 23
INFO: iteration 8, average log likelihood -1.109169
WARNING: Variances had to be floored 5 22
INFO: iteration 9, average log likelihood -1.121983
WARNING: Variances had to be floored 30 31
INFO: iteration 10, average log likelihood -1.110410
WARNING: Variances had to be floored 9 26 27 32
INFO: iteration 11, average log likelihood -1.086895
WARNING: Variances had to be floored 5 10 19 23
INFO: iteration 12, average log likelihood -1.093544
WARNING: Variances had to be floored 14 22
INFO: iteration 13, average log likelihood -1.113035
WARNING: Variances had to be floored 8
INFO: iteration 14, average log likelihood -1.089730
WARNING: Variances had to be floored 6 9 15 26 30 32
INFO: iteration 15, average log likelihood -1.054093
WARNING: Variances had to be floored 5 19 23 27
INFO: iteration 16, average log likelihood -1.074637
WARNING: Variances had to be floored 10 14 22
INFO: iteration 17, average log likelihood -1.092230
WARNING: Variances had to be floored 8
INFO: iteration 18, average log likelihood -1.084161
WARNING: Variances had to be floored 9 26 30 32
INFO: iteration 19, average log likelihood -1.057379
WARNING: Variances had to be floored 5 19 22 23
INFO: iteration 20, average log likelihood -1.078544
WARNING: Variances had to be floored 10 14 27
INFO: iteration 21, average log likelihood -1.093218
WARNING: Variances had to be floored 6 8
INFO: iteration 22, average log likelihood -1.082971
WARNING: Variances had to be floored 9 15 26 32
INFO: iteration 23, average log likelihood -1.062903
WARNING: Variances had to be floored 5 19 22 23 27 30
INFO: iteration 24, average log likelihood -1.064689
WARNING: Variances had to be floored 10 14
INFO: iteration 25, average log likelihood -1.119374
WARNING: Variances had to be floored 8
INFO: iteration 26, average log likelihood -1.089640
WARNING: Variances had to be floored 9 26 32
INFO: iteration 27, average log likelihood -1.052057
WARNING: Variances had to be floored 5 6 14 19 22 23 27
INFO: iteration 28, average log likelihood -1.053023
WARNING: Variances had to be floored 10 30
INFO: iteration 29, average log likelihood -1.118896
WARNING: Variances had to be floored 15
INFO: iteration 30, average log likelihood -1.097275
WARNING: Variances had to be floored 9 26 32
INFO: iteration 31, average log likelihood -1.054895
WARNING: Variances had to be floored 5 8 14 19 22 23 27
INFO: iteration 32, average log likelihood -1.053054
WARNING: Variances had to be floored 10 30
INFO: iteration 33, average log likelihood -1.117018
INFO: iteration 34, average log likelihood -1.092148
WARNING: Variances had to be floored 6 9 15 26 32
INFO: iteration 35, average log likelihood -1.040128
WARNING: Variances had to be floored 5 19 22 23 30
INFO: iteration 36, average log likelihood -1.069650
WARNING: Variances had to be floored 10 27
INFO: iteration 37, average log likelihood -1.111589
WARNING: Variances had to be floored 8 14
INFO: iteration 38, average log likelihood -1.086400
WARNING: Variances had to be floored 9 26 32
INFO: iteration 39, average log likelihood -1.059172
WARNING: Variances had to be floored 5 15 19 22 23 27 30
INFO: iteration 40, average log likelihood -1.058802
WARNING: Variances had to be floored 6 10
INFO: iteration 41, average log likelihood -1.123534
WARNING: Variances had to be floored 8
INFO: iteration 42, average log likelihood -1.089388
WARNING: Variances had to be floored 9 26 32
INFO: iteration 43, average log likelihood -1.057918
WARNING: Variances had to be floored 5 6 14 19 22 23 27
INFO: iteration 44, average log likelihood -1.060584
WARNING: Variances had to be floored 10 15
INFO: iteration 45, average log likelihood -1.114823
WARNING: Variances had to be floored 8
INFO: iteration 46, average log likelihood -1.081602
WARNING: Variances had to be floored 9 26 30 32
INFO: iteration 47, average log likelihood -1.050242
WARNING: Variances had to be floored 5 14 19 22 23 27
INFO: iteration 48, average log likelihood -1.073820
WARNING: Variances had to be floored 10 15
INFO: iteration 49, average log likelihood -1.114041
WARNING: Variances had to be floored 8
INFO: iteration 50, average log likelihood -1.082030
INFO: EM with 100000 data points 50 iterations avll -1.082030
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.00153147  -0.0447324   -0.145725    -0.102719     -0.154906    0.00307069   0.281033     0.00370116  -0.0199228    -0.0781961    0.00114531  -0.0272002    0.0452165   -0.132868    -0.113528    -0.0639028    0.0384142  -0.175122    -0.0875835    -0.14934       0.0807213   0.0849976  -0.10779      0.0870987   -0.10139     0.0782799 
  0.0928611    0.235933     0.0532257    0.0846938    -0.223641    0.0797546    0.104031     0.0307905   -0.121077      0.0435411    0.0245004   -0.0430635   -0.0453178   -0.0138255    0.0141847    0.0530379   -0.001342    0.0833683   -0.00314704   -0.0862748    -0.0826826   0.0255117   0.00294544   0.0526425   -0.0324815  -0.160016  
 -0.0194634   -0.0523322    0.00369417   0.0105173    -0.0742219   0.222529    -0.146264     0.0406199    0.124527     -0.0620557    0.0787625    0.0262021   -0.0473858   -0.107655    -0.0564938   -0.0659153   -0.0684781  -0.126079     0.0255991    -0.264185      0.0834686   0.015772   -0.0619904    0.168641     0.146813   -0.10179   
  0.0132188    0.018807     0.0399607   -0.0767608    -0.0681374   0.0454529   -0.130389     0.171624    -0.0358536    -0.0186672    0.251077     0.10761      0.0151645    0.049014     0.0581287    0.110352    -0.0725623  -0.0408405   -0.0253195     0.106633      0.0411773  -0.166998    0.0858464    0.0389158   -0.12207    -0.158471  
 -0.114092    -0.0535609    0.00665169  -0.110848      0.0286902   0.0392423    0.0165314   -0.0617696   -0.117555     -0.0139624    0.205112     0.0800077   -0.131564     0.0778652    0.0964332    0.065832     0.0982432  -0.156625     0.0285025     0.0141353    -0.0462368  -0.1052      0.262635     0.133572     0.0457353  -0.0611635 
  0.0147984   -0.260351     0.00366227  -0.0478031     0.0663717   0.045342    -0.0414944    0.0111894   -0.0356596    -0.0560136   -0.00641517   0.139528     0.0629765    0.0256485    0.0263649   -0.156971    -0.0675092  -0.132667    -0.104241      0.000828934  -0.0267968  -0.0300726  -0.00510195  -0.0688235   -0.0243801  -0.00260747
  0.0181365   -0.119039     0.102372     0.0629132     0.0505346   0.0941025   -0.0575243   -0.179767    -0.192734     -0.0508664    0.0136078    0.0952297   -0.0797292   -0.0469646    0.0331255    0.0114879   -0.0128509  -0.073499     0.0875032     0.190699     -0.0517825  -0.082535   -0.0962772    0.0523111    0.0493754   0.0819715 
  0.159426    -0.056001     0.0417181   -0.000837052   0.0963455  -0.0266929    0.0655216    0.0309485    0.0272993    -0.140395     0.183252    -0.0251052    0.0535539    0.0533624    0.0113494    0.0902776    0.030234   -0.0244843    0.0862918    -0.0296989    -0.124523   -0.0233475   0.0998184   -0.0705085    0.0969694  -0.0182741 
 -0.0173225   -0.250058     0.00933062  -0.0597833     0.0595082   0.0465878   -0.00730523   0.163829    -0.0566106     0.15719      0.0286399   -0.0758981   -0.00226432   0.115215    -0.0635383   -0.0237087   -0.0158749   0.0570343    0.025408      0.00394864   -0.0698228  -0.0496809   0.149857    -0.0990555   -0.125114    0.116234  
 -0.0925763   -0.0422874   -0.0573188    0.00223152   -0.148169   -0.0418247   -0.0340617    0.0127545   -0.050059      0.0144061    0.177142    -0.00122597   0.169598     0.0550244   -0.0178289   -0.0668366   -0.0454813  -0.0410592    0.091698      0.208796      0.115545    0.0721076   0.0193638   -0.0850944   -0.0164331  -0.164186  
  0.00780374   0.111375    -0.0278385   -0.0656781     0.0683039  -0.0293158    0.0254634    0.00220031  -0.151443     -0.109639     0.0582073   -0.0478765   -0.0894791   -0.024665     0.139241     0.0748848   -0.0726676  -0.128443     0.116624      0.0725319     0.0379105  -0.0404312   0.0944726   -0.0145247   -0.0269908  -0.0108686 
 -0.0040582   -0.033351     0.0416069    0.0179557     0.0593701  -0.179982    -0.120698     0.0456392    0.0238312     0.0410287    0.065642     0.0726343   -0.0275416    0.0132656   -0.0412069    0.0820102   -0.0211551  -0.0756963   -0.103672      0.0345462     0.114379    0.184364   -0.0549971   -0.114998    -0.0532535  -0.0431009 
  0.160652    -0.0869305   -0.185086     0.0116776    -0.0177012   0.0172856    0.0104138    0.0457074    0.042881      0.141221    -0.200677     0.0848488   -0.131377     0.112119    -0.1285      -0.0251806   -0.0434754   0.0148991   -0.0897183     0.148716      0.158863   -0.143675   -0.0254439   -0.0707991   -0.15       -0.0236237 
 -0.0481821    0.247682    -0.140596    -0.225717     -0.0978976   0.0440488   -0.117884     0.204204    -0.032287      0.0889195    0.0296221   -0.108538     0.0964137    0.0360336    0.00684617  -0.0357046   -0.0836876  -0.123489    -0.00167448    0.0436448    -0.0506477  -0.0856134   0.0129149    0.0879914    0.154005   -0.0316733 
 -0.061822     0.111328    -0.115057    -0.209606     -0.0740742   0.0630017   -0.121345     0.109916    -0.133087      0.0804421    0.0641855   -0.0886874    0.0832213    0.0303134    0.0140516    0.00286671  -0.0854355  -0.137636     0.000360187   0.0868887    -0.112399   -0.0431299   0.00305198   0.0353883    0.197588   -0.065814  
  0.0723514    0.00279401  -0.00783282  -0.106895      0.127308   -0.0592363   -0.0604537   -0.0272067    0.000280557   0.0212369   -0.0513221    0.066792     0.0723634   -0.00451069  -0.0561454    0.214701    -0.125478   -0.0569269   -0.0336204    -0.0350339    -0.0977834   0.132663   -0.0797098   -0.0755006    0.0340042  -0.108227  
  0.0127236   -0.13804     -0.021969    -0.108815     -0.142644   -0.0408777   -0.167652    -0.0370378   -0.0697974     0.175358     0.120319     0.14413     -0.005841    -0.0733258   -0.0409538    0.0998659    0.102726    0.0315544   -0.0500923     0.114516     -0.0768256  -0.0190152   0.142475    -0.0194574   -0.0421371  -0.225696  
  0.00081448   0.150573    -0.0952971   -0.206669     -0.0733924   0.135796     0.0912011    0.183422    -0.0402412     0.0920222   -0.0134214    0.144166    -0.0821972    0.0383875   -0.0109135   -0.0649299   -0.105797    0.0336797    0.085437      0.170897      0.119691    0.10111    -0.029252     0.0116614   -0.051977    0.0565718 
 -0.0221173   -0.0951624   -0.0327628    0.0250454    -0.0259299   0.00261186   0.0414185   -0.0669704   -0.143502      0.0219264    0.159927    -0.00808552  -0.00159949  -0.0246616   -0.0041368    0.10524      0.189783    0.0439947   -0.106853     -0.0827399    -0.0324402   0.043446   -0.00775218   0.0365817   -0.0219267   0.0391626 
 -0.0282707   -0.0866272   -0.110742     0.0121391    -0.0126763   0.0791391    0.173296    -0.0726056    0.0273962     0.038611     0.0538168   -0.082472    -0.0234019    0.0610209   -0.112757    -0.0553263   -0.0303239   0.05843      0.16579      -0.0275417    -0.0731922   0.0304048   0.0251585    0.0368337   -0.0353865   0.0791127 
 -0.0578271    0.107035     0.0682303    0.0591244     0.0625822  -0.0588346    0.0568759   -0.069497     0.0375985    -0.0316018    0.0920342   -0.130504     0.0462391   -0.0526862    0.0110631    0.058638     0.0901467   0.00668865  -0.0320602     0.0541968     0.124179   -0.12734    -0.18509      0.0311532   -0.0238742   0.0370856 
  0.103145    -0.0603223   -0.215462     0.0862161    -0.30463    -0.0891253    0.20599     -0.0716049   -0.00300273    0.0345389    0.0396461   -0.123061    -0.0599255    0.0827452   -0.16346     -0.0423839   -0.0834578  -0.188288     0.0319528     0.0723211     0.16303     0.0357883  -0.0368535    0.0860752    0.0394345  -0.115354  
  0.00700183   0.0650482    0.153995     0.0104443     0.0167461  -2.99277e-5   0.00523883   0.106643     0.053837      0.0863192   -0.237245    -0.104188     0.0976624   -0.0116655   -0.063705    -0.0572541   -0.0167612   0.0373704    0.146325      0.233651     -0.0182068   0.0771658  -0.0403779    0.0214506    0.0218472   0.0716218 
 -0.128575     0.0871944    0.0851179   -0.120335     -0.0630589   0.00676908  -0.0813349    0.192372    -0.0702125    -0.00637749   0.110877    -0.0862722    0.109972    -0.0831037   -0.0737888    0.0567603   -0.11689    -0.115779     0.145432      0.0101391     0.0425449   0.068762   -0.0998786    0.0538933    0.040092    0.0295015 
 -0.0775003    0.0999036   -0.0062252   -0.0228661    -0.0446917  -0.0796747    0.0655753   -0.0539111   -0.103724      0.030241     0.255665    -0.0364462    0.00896668  -0.192795    -0.0213668    0.0167284   -0.0277269   0.14163     -0.150254     -0.0514525     0.123549   -0.148502   -0.155041    -0.0857897   -0.129613   -0.247442  
 -0.00214844  -0.0012477   -0.045382    -0.0200999    -0.0265829  -0.034323    -0.161067     0.0993589    0.0374714    -0.165603    -0.196354     0.0837607   -0.00435498  -0.024747    -0.0124923   -0.152688    -0.0721393  -0.0366092   -0.0337583    -0.214875      0.0677397   0.0443328  -0.0224919   -0.0180976    0.0372861   0.0290843 
  0.0224345    0.137493    -0.0646364   -0.140224     -0.14083    -0.01745     -0.042191     0.0878014    0.109603      0.0397579    0.0862237    0.0109434    0.0185194    0.0119823    0.00745077  -0.092896     0.0850424   0.111882     0.131613     -0.261069      0.0207872   0.0830591  -0.0159184    0.0440945   -0.101216    0.088697  
 -0.0540786   -0.0731855   -0.00503176  -0.0527505    -0.147188    0.136086    -0.0512112    0.0184753   -0.109843     -0.0747981   -0.0588536    0.119054     0.141736    -0.296558    -0.062075    -0.0520307    0.102533    0.290682     0.113993     -0.0714942     0.0930915  -0.0932955  -0.00784953  -0.0994728   -0.0584996   0.0295124 
  0.0813022   -0.0668329    0.0835861   -0.173738     -0.014153   -0.0485448   -0.0682859   -0.0926122   -0.11736      -0.02216      0.0714998   -0.190189     0.063707     0.0888073   -0.02534      0.00864136  -0.225045   -0.0179264   -0.0934787     0.283739     -0.0896803  -0.0278164  -0.0216593    0.19873      0.0893488   0.0596847 
 -0.0072597   -0.014441     0.16181     -0.0325983     0.0896399   0.070258    -0.0106536    0.0148268   -0.0219748    -0.0611107    0.00713153   0.172279     0.0480039   -0.0647138    0.0174899   -0.0665094   -0.0621342  -0.0720781   -0.13604      -0.00306252   -0.025028   -0.109055    0.120688    -0.0787755   -0.0236712  -0.0387572 
 -0.14002     -0.0458515    0.0126486    0.0907735     0.0792741  -0.158683    -0.208215     0.196279     0.0227844     0.0337616   -0.0711348    0.198229    -0.0617602   -0.0260358   -0.0026396    0.0102417   -0.0292363  -0.101339    -0.0280804     0.0571375    -0.0792971  -0.155885    0.125836     0.00521288  -0.0629925  -0.144686  
  0.25124      0.24688      0.0599862    0.0180178     0.0232119   0.0058758   -0.0551218   -0.0371476    0.000210857  -0.0322655    0.0740297    0.0747145   -0.141531     0.0348526   -0.0336379   -0.198492    -0.0517599   0.0576914    0.00924622   -0.00385952   -0.110424    0.048216    0.0440722    0.020353     0.0585901   0.0920988 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 9 26 30 32
INFO: iteration 1, average log likelihood -1.050400
WARNING: Variances had to be floored 5 9 14 19 22 23 26 27 30 32
INFO: iteration 2, average log likelihood -1.019783
WARNING: Variances had to be floored 5 6 9 10 15 26 30 32
INFO: iteration 3, average log likelihood -1.025793
WARNING: Variances had to be floored 8 9 14 19 22 23 26 27 30 32
INFO: iteration 4, average log likelihood -1.018504
WARNING: Variances had to be floored 5 9 26 30 32
INFO: iteration 5, average log likelihood -1.037687
WARNING: Variances had to be floored 5 6 9 10 14 19 22 23 26 27 30 32
INFO: iteration 6, average log likelihood -1.016785
WARNING: Variances had to be floored 9 15 26 30 32
INFO: iteration 7, average log likelihood -1.037586
WARNING: Variances had to be floored 5 6 8 9 14 19 22 23 26 27 30 32
INFO: iteration 8, average log likelihood -1.008691
WARNING: Variances had to be floored 5 9 10 26 30 32
INFO: iteration 9, average log likelihood -1.037432
WARNING: Variances had to be floored 9 14 19 22 23 26 27 30 32
INFO: iteration 10, average log likelihood -1.028365
INFO: EM with 100000 data points 10 iterations avll -1.028365
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.00712578   0.0644383    0.0724622   -0.10289      0.030078     0.104157     0.1127       0.302164    0.0249902   -0.0798904    0.0588958     0.0428569   -0.0487241   -0.136641     0.123574     0.0777127     0.0508471    0.038038     -0.0665568    0.0293823    -0.135143    -0.11948     -0.0730183   -0.0410131    0.0589357   -0.0708902 
  0.0271381    0.103007     0.0711847   -0.0352675   -0.13142     -0.0918331   -0.00907328   0.18347     0.164941     0.0197138    0.124665     -0.0400962    0.11218     -0.101861     0.00223938  -0.0453728     0.170866    -0.0447302    -0.0370056   -0.108921      0.0419981   -0.0650536    0.133175     0.0573232    0.0232638    0.188145  
 -0.122429    -0.136177     0.0155646    0.199046     0.0294997   -0.170109    -0.100726    -0.121254    0.0727291   -0.120484    -0.0133194     0.124934    -0.0308143    0.160288    -0.108496    -0.00140834   -0.0401693    0.159195      0.157687     0.178667      0.0762886   -0.0395994   -0.0831228   -0.0693288   -0.0543568    0.0861551 
  0.120642    -0.0862834    0.0365691    0.0837109   -0.00594727   0.193712     0.0329709    0.131502   -0.0417572    0.0135808   -0.218101     -0.0462365    0.034295    -0.137939    -0.00882379  -0.0489746    -0.0488021    0.0237013    -0.126049    -0.0200783     0.0439444    0.107035    -0.00955129   0.103235     0.147289     0.0109482 
  0.197377    -0.105101    -0.0378238   -0.0909862    0.104899    -0.136551     0.153374     0.0341637   0.0326958   -0.135307     0.00573923   -0.123518     0.0229254   -0.0415527   -0.0790235    0.0961811    -0.233177    -0.133203      0.026761     0.0485343    -0.0966316    0.0080725    0.104572    -0.00554918   0.0700858   -0.176498  
 -0.128545    -0.0271737    0.00855418  -0.00228655   0.0421149    0.0812077    0.00521513  -0.0701821   0.0394383    0.0544915    0.0721889     0.0667967   -0.0552384    0.0615072   -0.0300158   -0.000447432  -0.0544907   -0.136091     -0.146511     0.107862      0.162872    -0.0601532   -0.053979    -0.123962     0.0204042   -0.135839  
  0.114862     0.0179746   -0.0277933   -0.0672879   -0.0326749   -0.0508233    0.117943    -0.133422   -0.00232981   0.0996603    0.0817094    -0.0522801   -0.0408261   -0.0174135    0.0852806   -0.00358813    0.0699329   -0.136435     -0.0950562    0.00809573   -0.0528316    0.0646547    0.151303    -0.103106     0.0173809   -0.0953231 
 -0.0223063   -0.0118715    0.0111847   -0.0446535   -0.144326    -0.0850298    0.0671251    0.0584635   0.0495607    0.225959     0.00108775    0.0566251    0.0544895    0.00812299   0.0176189    0.0298158     0.014684     0.00316054    0.0270618   -0.0666712    -0.0395318    0.078736     0.127404    -0.094933    -0.00519504   0.145131  
  0.0226594   -0.073565     0.0333147   -0.0510336   -0.0131587   -0.14278     -0.0451306    0.0314381   0.0643933    0.00919226  -0.0498034     0.0613566   -0.0748722   -0.058654     0.0317089   -0.0145554     0.0286018   -0.0571352     0.123041     0.123001     -0.025384     0.228002    -0.0945523    0.127246     0.0946925    0.0291223 
 -0.0262237   -0.209292    -0.198196    -0.0940403    0.00097609   0.0131242   -0.174297     0.0152935   0.0407545    0.0738987    0.0440234    -0.0504809    0.125362     0.0507679    0.146677     0.0863845    -0.00128765   0.0551799     0.00682285   0.0277661    -0.101713    -0.170622     0.0409837   -0.118667     0.189575    -0.0956776 
 -0.0229324   -0.0199967   -0.0591046   -0.0426467   -0.014662     0.0336007    0.121455     0.0267981   0.0495089    0.0466294    0.166491      0.0439627    0.0965008   -0.0444028    0.19181     -0.0387031     0.0197431    0.110823      0.0638945    0.127437     -0.0105109    0.210388     0.128424     0.051946    -0.116966    -0.0181561 
 -0.148331     0.0101556   -0.0828317    0.0856344   -0.10787      0.14988     -0.0235403    0.0660328  -0.0237564   -0.0265522   -0.0103507     0.188439    -0.0299069   -0.116687    -0.0575477   -0.117455      0.0963498   -0.0956569     0.115297    -0.0464198     0.0237568   -0.012719     0.0140499    0.00948999  -0.0417713   -0.121903  
 -0.0235682    0.0180399    0.0164495   -0.160795    -0.0810671    0.112231    -0.222077     0.0276121  -0.132112    -0.155701     0.0246912    -0.0371849    0.175901    -0.106868    -0.00115317  -0.0452938     0.135203     0.0904853     0.156412     0.0778872     0.0179054    0.00308844  -0.111509     0.099173    -0.144055     0.0257288 
 -0.122169    -0.175779     0.052249     0.058003     0.0150784    0.0821381   -0.233479     0.0899934   0.209007    -0.0816614    0.149269     -0.0499839    0.162371     0.0302204    0.110371    -0.133943      0.0116954    0.000871227   0.135176    -0.0534974     0.144242    -0.0552811    0.0503054    0.161959    -0.00201038   0.124015  
  0.0768544   -0.0623328   -0.0203948   -0.0756915   -0.0189922    0.0485585    0.0128107   -0.0709599   0.143988     0.112781     0.147591     -0.143028     0.0745391   -0.00860835  -0.00968343  -0.157221      0.175473     0.0609013    -0.096143    -0.0112828     0.108826     0.0201326    0.0223786   -0.0245612   -0.10569     -0.0161112 
  0.102335    -0.0825248   -0.0317745   -0.100748     0.169877     0.086835     0.0329575   -0.0274941  -0.0289348    0.00897949   0.0179304     0.116985     0.0894874    0.0448481   -0.0138553    0.0208496     0.0759538   -0.0984401    -0.0566145   -0.0805398    -0.193463     0.0361622    0.193515    -0.0473727    0.0558396    0.115673  
 -0.0871701   -0.00591369   0.117064    -0.0941497   -0.0590394   -0.110996    -0.0886089    0.153884   -0.0508323    0.23467     -0.0791577    -0.0519094   -0.0354432   -0.0765958   -0.0216031    0.04364      -0.125383     0.00187995    0.029933    -0.200408      0.0157781    0.0183051   -0.225031     0.0058016    0.0547719   -0.123079  
 -0.0964155    0.0323128   -0.0460519    0.0805962   -0.0909044    0.0649701    0.0600935    0.101105   -0.0389599   -0.0808725   -0.0690884     0.0452515   -0.0434999   -0.187334    -0.0946664    0.00128958   -0.0092812   -0.0826746    -0.106074    -0.000320576   0.161892     0.0543475   -0.120115    -0.120626    -0.118733    -0.0171686 
  0.0918303    0.0401723   -0.125466    -0.195976    -0.216958    -0.0175669    0.128865    -0.100132    0.0143116    0.0750948    0.00949723    0.00260244  -0.0809056   -0.0506168    0.102896     0.138915      0.152479    -0.238455      0.118175     0.00288752    0.0282621    0.0308281    0.193249    -0.101967    -0.00270832   0.180745  
  0.0878243    0.0101476   -0.0391778    0.0643943    0.0484874    0.109782    -0.115616     0.104148    0.117989    -0.0117974   -0.0612002     0.00688739   0.040451    -0.13505      0.0869845   -0.0538149    -0.0459843    0.154885     -0.112923    -0.0500021     0.0627049    0.0329449   -0.168841    -0.101886     0.0540658   -0.00326198
 -0.0624873   -0.0949083   -0.0609013    0.130232    -0.031795     0.048262    -0.0381126    0.0711981   0.184481     0.0737438    0.00254587   -0.108079     0.0085837   -0.164208    -0.0697402    0.00529516    0.103671     0.10173       0.0231285    0.0718838    -0.208581     0.00856207  -0.144841     0.099328     0.0455166   -0.0523381 
  0.0943477   -0.116875    -0.0759516   -0.0115277   -0.117085     0.0768881    0.171528    -0.0192702   0.0479259   -0.0221662    0.192736      0.0966676    0.136219     0.0291554   -0.162289     0.0193624     0.00142686  -0.00660285    0.0215884    0.0297765    -0.131212     0.0837936   -0.139641     0.104199    -0.0111723   -0.141852  
  0.0709904   -0.0726783    0.166709     0.152073     0.0313477    0.116589    -0.197093    -0.104552    0.0910046    0.120355    -0.0895884     0.0568389    0.113772    -0.045615    -0.0504586   -0.0649089    -0.0876756    0.00983925    0.0393609   -0.0666232    -0.0568073   -0.0773381   -0.0850528   -0.0186022   -0.0736206    0.149498  
 -0.0636462    0.00578358   0.0306239    0.0255998    0.00306152  -0.0511184   -0.0746666   -0.0774453  -0.130662    -0.117242    -0.0247878     0.056974     0.087047    -0.0408331   -0.086568    -0.0773693    -0.00266735  -0.230941      0.172279    -0.0849201    -0.0233114    0.107568    -0.024494    -0.00062239  -0.0466241    0.0266699 
  0.0579304   -0.0936108   -0.0328016    0.0979489   -0.108628    -0.088614    -0.125196    -0.013436   -0.0499124   -0.0533526   -0.0841827    -0.135149    -0.0517636    0.123223    -0.102759    -0.0750236     0.0526401    0.00726455    0.0680302    0.0525758     0.0498136    0.0401756    0.0309573   -0.0322756   -0.169707    -0.0771457 
 -0.0935443    0.11289     -0.114975    -0.0504882   -0.200504     0.0321507    0.125176     0.0143693   0.116185     0.181563     0.00993885    0.124419     0.0236389    0.0416874   -0.10281      0.0409754     0.050553    -0.019979     -0.184349     0.0279157    -0.158437     0.162405    -0.114987    -0.075235    -0.041097    -0.0645463 
  0.0507227    0.120018     0.0182339    0.0656979    0.0203916    0.144096    -0.0286925    0.0138732  -0.127861     0.222714    -0.0917809     0.0336471   -0.133811    -0.106263    -0.0428184   -0.0597488    -0.110773    -0.0459148    -0.083014    -0.105724     -0.0700089   -0.0103657   -0.0919703    0.013072     0.0894105    0.12293   
 -0.0658169    0.0117976   -0.0999857    0.222169     0.0992766    0.0127746   -0.0377475    0.016497    0.0313806   -0.0811094   -0.0435561    -0.104752     0.119481    -0.149451    -0.0198127   -0.106007     -0.0829866   -0.109643      0.296204    -0.0460183     0.118773    -0.114002     0.106853     0.193316    -0.141577     0.0215641 
 -0.0925541    0.0731772   -0.126768    -0.0279095    0.0639399   -0.0366298    0.147533     0.222659    0.119028    -0.0567645   -0.0411827     0.140293     0.157809    -0.0246752   -0.127139    -0.0259206     0.164636     0.133789      0.0292415    0.0609855    -0.0740157   -0.137456    -0.0775981    0.0516598   -0.0221299   -0.0444971 
 -0.0398764    0.00984658  -0.0608285    0.044576    -0.0462783   -0.0213333   -0.0555979    0.0865582  -0.0347116   -0.142121    -0.0155699    -0.0753971   -0.106364     0.149019     0.17297     -0.0812586     0.0538686    0.077798     -0.189002     0.0350158    -0.151576     0.142136     0.155717    -0.173732     0.0605528   -0.0285735 
 -0.0532865   -0.0393552   -0.0102937   -0.0447843    0.075758     0.00708326  -0.0975696   -0.0753784  -0.0519028   -0.0715331   -0.0271945    -0.113622     0.0858149   -0.0996772   -0.0904473   -0.0668404    -0.0706552   -0.019063      0.141408     0.0334508    -0.146565    -0.0568468   -0.123659    -0.174159     0.0798283    0.275534  
 -0.0377377   -0.00151415   0.0650714   -0.0558611   -0.00400326  -0.167113    -0.0339911    0.0375999  -0.0157306   -0.00332311  -0.000423498   0.113993     0.00792718   0.171154     0.0196342    0.114435     -0.0370233    0.0527868     0.188349     0.0124074    -0.00915946  -0.192534    -0.204771    -0.0157593    0.0475219    0.0720338 kind full, method split
0: avll = -1.422543576915618
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422562
INFO: iteration 2, average log likelihood -1.422508
INFO: iteration 3, average log likelihood -1.422470
INFO: iteration 4, average log likelihood -1.422423
INFO: iteration 5, average log likelihood -1.422354
INFO: iteration 6, average log likelihood -1.422231
INFO: iteration 7, average log likelihood -1.421971
INFO: iteration 8, average log likelihood -1.421396
INFO: iteration 9, average log likelihood -1.420328
INFO: iteration 10, average log likelihood -1.418972
INFO: iteration 11, average log likelihood -1.417940
INFO: iteration 12, average log likelihood -1.417445
INFO: iteration 13, average log likelihood -1.417260
INFO: iteration 14, average log likelihood -1.417193
INFO: iteration 15, average log likelihood -1.417169
INFO: iteration 16, average log likelihood -1.417159
INFO: iteration 17, average log likelihood -1.417155
INFO: iteration 18, average log likelihood -1.417153
INFO: iteration 19, average log likelihood -1.417152
INFO: iteration 20, average log likelihood -1.417152
INFO: iteration 21, average log likelihood -1.417151
INFO: iteration 22, average log likelihood -1.417151
INFO: iteration 23, average log likelihood -1.417151
INFO: iteration 24, average log likelihood -1.417151
INFO: iteration 25, average log likelihood -1.417150
INFO: iteration 26, average log likelihood -1.417150
INFO: iteration 27, average log likelihood -1.417150
INFO: iteration 28, average log likelihood -1.417150
INFO: iteration 29, average log likelihood -1.417150
INFO: iteration 30, average log likelihood -1.417150
INFO: iteration 31, average log likelihood -1.417150
INFO: iteration 32, average log likelihood -1.417150
INFO: iteration 33, average log likelihood -1.417150
INFO: iteration 34, average log likelihood -1.417150
INFO: iteration 35, average log likelihood -1.417149
INFO: iteration 36, average log likelihood -1.417149
INFO: iteration 37, average log likelihood -1.417149
INFO: iteration 38, average log likelihood -1.417149
INFO: iteration 39, average log likelihood -1.417149
INFO: iteration 40, average log likelihood -1.417149
INFO: iteration 41, average log likelihood -1.417149
INFO: iteration 42, average log likelihood -1.417149
INFO: iteration 43, average log likelihood -1.417149
INFO: iteration 44, average log likelihood -1.417149
INFO: iteration 45, average log likelihood -1.417149
INFO: iteration 46, average log likelihood -1.417149
INFO: iteration 47, average log likelihood -1.417149
INFO: iteration 48, average log likelihood -1.417149
INFO: iteration 49, average log likelihood -1.417149
INFO: iteration 50, average log likelihood -1.417149
INFO: EM with 100000 data points 50 iterations avll -1.417149
952.4 data points per parameter
1: avll = [-1.42256,-1.42251,-1.42247,-1.42242,-1.42235,-1.42223,-1.42197,-1.4214,-1.42033,-1.41897,-1.41794,-1.41744,-1.41726,-1.41719,-1.41717,-1.41716,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417167
INFO: iteration 2, average log likelihood -1.417111
INFO: iteration 3, average log likelihood -1.417073
INFO: iteration 4, average log likelihood -1.417029
INFO: iteration 5, average log likelihood -1.416976
INFO: iteration 6, average log likelihood -1.416915
INFO: iteration 7, average log likelihood -1.416849
INFO: iteration 8, average log likelihood -1.416780
INFO: iteration 9, average log likelihood -1.416716
INFO: iteration 10, average log likelihood -1.416660
INFO: iteration 11, average log likelihood -1.416613
INFO: iteration 12, average log likelihood -1.416574
INFO: iteration 13, average log likelihood -1.416541
INFO: iteration 14, average log likelihood -1.416512
INFO: iteration 15, average log likelihood -1.416486
INFO: iteration 16, average log likelihood -1.416461
INFO: iteration 17, average log likelihood -1.416436
INFO: iteration 18, average log likelihood -1.416411
INFO: iteration 19, average log likelihood -1.416385
INFO: iteration 20, average log likelihood -1.416359
INFO: iteration 21, average log likelihood -1.416332
INFO: iteration 22, average log likelihood -1.416305
INFO: iteration 23, average log likelihood -1.416278
INFO: iteration 24, average log likelihood -1.416251
INFO: iteration 25, average log likelihood -1.416227
INFO: iteration 26, average log likelihood -1.416204
INFO: iteration 27, average log likelihood -1.416183
INFO: iteration 28, average log likelihood -1.416165
INFO: iteration 29, average log likelihood -1.416149
INFO: iteration 30, average log likelihood -1.416135
INFO: iteration 31, average log likelihood -1.416122
INFO: iteration 32, average log likelihood -1.416112
INFO: iteration 33, average log likelihood -1.416103
INFO: iteration 34, average log likelihood -1.416095
INFO: iteration 35, average log likelihood -1.416088
INFO: iteration 36, average log likelihood -1.416081
INFO: iteration 37, average log likelihood -1.416075
INFO: iteration 38, average log likelihood -1.416070
INFO: iteration 39, average log likelihood -1.416065
INFO: iteration 40, average log likelihood -1.416061
INFO: iteration 41, average log likelihood -1.416057
INFO: iteration 42, average log likelihood -1.416053
INFO: iteration 43, average log likelihood -1.416049
INFO: iteration 44, average log likelihood -1.416046
INFO: iteration 45, average log likelihood -1.416043
INFO: iteration 46, average log likelihood -1.416039
INFO: iteration 47, average log likelihood -1.416036
INFO: iteration 48, average log likelihood -1.416034
INFO: iteration 49, average log likelihood -1.416031
INFO: iteration 50, average log likelihood -1.416028
INFO: EM with 100000 data points 50 iterations avll -1.416028
473.9 data points per parameter
2: avll = [-1.41717,-1.41711,-1.41707,-1.41703,-1.41698,-1.41692,-1.41685,-1.41678,-1.41672,-1.41666,-1.41661,-1.41657,-1.41654,-1.41651,-1.41649,-1.41646,-1.41644,-1.41641,-1.41639,-1.41636,-1.41633,-1.4163,-1.41628,-1.41625,-1.41623,-1.4162,-1.41618,-1.41616,-1.41615,-1.41613,-1.41612,-1.41611,-1.4161,-1.41609,-1.41609,-1.41608,-1.41608,-1.41607,-1.41607,-1.41606,-1.41606,-1.41605,-1.41605,-1.41605,-1.41604,-1.41604,-1.41604,-1.41603,-1.41603,-1.41603]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416036
INFO: iteration 2, average log likelihood -1.415978
INFO: iteration 3, average log likelihood -1.415927
INFO: iteration 4, average log likelihood -1.415869
INFO: iteration 5, average log likelihood -1.415800
INFO: iteration 6, average log likelihood -1.415719
INFO: iteration 7, average log likelihood -1.415631
INFO: iteration 8, average log likelihood -1.415541
INFO: iteration 9, average log likelihood -1.415455
INFO: iteration 10, average log likelihood -1.415377
INFO: iteration 11, average log likelihood -1.415308
INFO: iteration 12, average log likelihood -1.415246
INFO: iteration 13, average log likelihood -1.415190
INFO: iteration 14, average log likelihood -1.415139
INFO: iteration 15, average log likelihood -1.415093
INFO: iteration 16, average log likelihood -1.415051
INFO: iteration 17, average log likelihood -1.415012
INFO: iteration 18, average log likelihood -1.414975
INFO: iteration 19, average log likelihood -1.414941
INFO: iteration 20, average log likelihood -1.414908
INFO: iteration 21, average log likelihood -1.414876
INFO: iteration 22, average log likelihood -1.414844
INFO: iteration 23, average log likelihood -1.414814
INFO: iteration 24, average log likelihood -1.414784
INFO: iteration 25, average log likelihood -1.414757
INFO: iteration 26, average log likelihood -1.414730
INFO: iteration 27, average log likelihood -1.414706
INFO: iteration 28, average log likelihood -1.414684
INFO: iteration 29, average log likelihood -1.414664
INFO: iteration 30, average log likelihood -1.414646
INFO: iteration 31, average log likelihood -1.414630
INFO: iteration 32, average log likelihood -1.414616
INFO: iteration 33, average log likelihood -1.414603
INFO: iteration 34, average log likelihood -1.414592
INFO: iteration 35, average log likelihood -1.414582
INFO: iteration 36, average log likelihood -1.414573
INFO: iteration 37, average log likelihood -1.414564
INFO: iteration 38, average log likelihood -1.414557
INFO: iteration 39, average log likelihood -1.414550
INFO: iteration 40, average log likelihood -1.414543
INFO: iteration 41, average log likelihood -1.414537
INFO: iteration 42, average log likelihood -1.414532
INFO: iteration 43, average log likelihood -1.414526
INFO: iteration 44, average log likelihood -1.414521
INFO: iteration 45, average log likelihood -1.414516
INFO: iteration 46, average log likelihood -1.414512
INFO: iteration 47, average log likelihood -1.414507
INFO: iteration 48, average log likelihood -1.414503
INFO: iteration 49, average log likelihood -1.414499
INFO: iteration 50, average log likelihood -1.414495
INFO: EM with 100000 data points 50 iterations avll -1.414495
236.4 data points per parameter
3: avll = [-1.41604,-1.41598,-1.41593,-1.41587,-1.4158,-1.41572,-1.41563,-1.41554,-1.41546,-1.41538,-1.41531,-1.41525,-1.41519,-1.41514,-1.41509,-1.41505,-1.41501,-1.41498,-1.41494,-1.41491,-1.41488,-1.41484,-1.41481,-1.41478,-1.41476,-1.41473,-1.41471,-1.41468,-1.41466,-1.41465,-1.41463,-1.41462,-1.4146,-1.41459,-1.41458,-1.41457,-1.41456,-1.41456,-1.41455,-1.41454,-1.41454,-1.41453,-1.41453,-1.41452,-1.41452,-1.41451,-1.41451,-1.4145,-1.4145,-1.4145]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414500
INFO: iteration 2, average log likelihood -1.414443
INFO: iteration 3, average log likelihood -1.414392
INFO: iteration 4, average log likelihood -1.414334
INFO: iteration 5, average log likelihood -1.414263
INFO: iteration 6, average log likelihood -1.414178
INFO: iteration 7, average log likelihood -1.414079
INFO: iteration 8, average log likelihood -1.413971
INFO: iteration 9, average log likelihood -1.413859
INFO: iteration 10, average log likelihood -1.413749
INFO: iteration 11, average log likelihood -1.413646
INFO: iteration 12, average log likelihood -1.413551
INFO: iteration 13, average log likelihood -1.413465
INFO: iteration 14, average log likelihood -1.413388
INFO: iteration 15, average log likelihood -1.413318
INFO: iteration 16, average log likelihood -1.413256
INFO: iteration 17, average log likelihood -1.413200
INFO: iteration 18, average log likelihood -1.413149
INFO: iteration 19, average log likelihood -1.413102
INFO: iteration 20, average log likelihood -1.413059
INFO: iteration 21, average log likelihood -1.413019
INFO: iteration 22, average log likelihood -1.412982
INFO: iteration 23, average log likelihood -1.412947
INFO: iteration 24, average log likelihood -1.412913
INFO: iteration 25, average log likelihood -1.412882
INFO: iteration 26, average log likelihood -1.412851
INFO: iteration 27, average log likelihood -1.412822
INFO: iteration 28, average log likelihood -1.412794
INFO: iteration 29, average log likelihood -1.412768
INFO: iteration 30, average log likelihood -1.412742
INFO: iteration 31, average log likelihood -1.412718
INFO: iteration 32, average log likelihood -1.412695
INFO: iteration 33, average log likelihood -1.412674
INFO: iteration 34, average log likelihood -1.412653
INFO: iteration 35, average log likelihood -1.412634
INFO: iteration 36, average log likelihood -1.412616
INFO: iteration 37, average log likelihood -1.412599
INFO: iteration 38, average log likelihood -1.412583
INFO: iteration 39, average log likelihood -1.412567
INFO: iteration 40, average log likelihood -1.412553
INFO: iteration 41, average log likelihood -1.412539
INFO: iteration 42, average log likelihood -1.412525
INFO: iteration 43, average log likelihood -1.412513
INFO: iteration 44, average log likelihood -1.412500
INFO: iteration 45, average log likelihood -1.412489
INFO: iteration 46, average log likelihood -1.412477
INFO: iteration 47, average log likelihood -1.412466
INFO: iteration 48, average log likelihood -1.412456
INFO: iteration 49, average log likelihood -1.412445
INFO: iteration 50, average log likelihood -1.412435
INFO: EM with 100000 data points 50 iterations avll -1.412435
118.1 data points per parameter
4: avll = [-1.4145,-1.41444,-1.41439,-1.41433,-1.41426,-1.41418,-1.41408,-1.41397,-1.41386,-1.41375,-1.41365,-1.41355,-1.41346,-1.41339,-1.41332,-1.41326,-1.4132,-1.41315,-1.4131,-1.41306,-1.41302,-1.41298,-1.41295,-1.41291,-1.41288,-1.41285,-1.41282,-1.41279,-1.41277,-1.41274,-1.41272,-1.4127,-1.41267,-1.41265,-1.41263,-1.41262,-1.4126,-1.41258,-1.41257,-1.41255,-1.41254,-1.41253,-1.41251,-1.4125,-1.41249,-1.41248,-1.41247,-1.41246,-1.41245,-1.41244]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412433
INFO: iteration 2, average log likelihood -1.412365
INFO: iteration 3, average log likelihood -1.412298
INFO: iteration 4, average log likelihood -1.412218
INFO: iteration 5, average log likelihood -1.412118
INFO: iteration 6, average log likelihood -1.411992
INFO: iteration 7, average log likelihood -1.411840
INFO: iteration 8, average log likelihood -1.411669
INFO: iteration 9, average log likelihood -1.411489
INFO: iteration 10, average log likelihood -1.411312
INFO: iteration 11, average log likelihood -1.411147
INFO: iteration 12, average log likelihood -1.410999
INFO: iteration 13, average log likelihood -1.410869
INFO: iteration 14, average log likelihood -1.410755
INFO: iteration 15, average log likelihood -1.410656
INFO: iteration 16, average log likelihood -1.410569
INFO: iteration 17, average log likelihood -1.410493
INFO: iteration 18, average log likelihood -1.410427
INFO: iteration 19, average log likelihood -1.410368
INFO: iteration 20, average log likelihood -1.410315
INFO: iteration 21, average log likelihood -1.410268
INFO: iteration 22, average log likelihood -1.410226
INFO: iteration 23, average log likelihood -1.410188
INFO: iteration 24, average log likelihood -1.410153
INFO: iteration 25, average log likelihood -1.410121
INFO: iteration 26, average log likelihood -1.410091
INFO: iteration 27, average log likelihood -1.410062
INFO: iteration 28, average log likelihood -1.410036
INFO: iteration 29, average log likelihood -1.410010
INFO: iteration 30, average log likelihood -1.409986
INFO: iteration 31, average log likelihood -1.409962
INFO: iteration 32, average log likelihood -1.409940
INFO: iteration 33, average log likelihood -1.409918
INFO: iteration 34, average log likelihood -1.409897
INFO: iteration 35, average log likelihood -1.409877
INFO: iteration 36, average log likelihood -1.409857
INFO: iteration 37, average log likelihood -1.409838
INFO: iteration 38, average log likelihood -1.409820
INFO: iteration 39, average log likelihood -1.409802
INFO: iteration 40, average log likelihood -1.409785
INFO: iteration 41, average log likelihood -1.409768
INFO: iteration 42, average log likelihood -1.409752
INFO: iteration 43, average log likelihood -1.409736
INFO: iteration 44, average log likelihood -1.409721
INFO: iteration 45, average log likelihood -1.409707
INFO: iteration 46, average log likelihood -1.409693
INFO: iteration 47, average log likelihood -1.409680
INFO: iteration 48, average log likelihood -1.409667
INFO: iteration 49, average log likelihood -1.409654
INFO: iteration 50, average log likelihood -1.409642
INFO: EM with 100000 data points 50 iterations avll -1.409642
59.0 data points per parameter
5: avll = [-1.41243,-1.41236,-1.4123,-1.41222,-1.41212,-1.41199,-1.41184,-1.41167,-1.41149,-1.41131,-1.41115,-1.411,-1.41087,-1.41075,-1.41066,-1.41057,-1.41049,-1.41043,-1.41037,-1.41032,-1.41027,-1.41023,-1.41019,-1.41015,-1.41012,-1.41009,-1.41006,-1.41004,-1.41001,-1.40999,-1.40996,-1.40994,-1.40992,-1.4099,-1.40988,-1.40986,-1.40984,-1.40982,-1.4098,-1.40978,-1.40977,-1.40975,-1.40974,-1.40972,-1.40971,-1.40969,-1.40968,-1.40967,-1.40965,-1.40964]
[-1.42254,-1.42256,-1.42251,-1.42247,-1.42242,-1.42235,-1.42223,-1.42197,-1.4214,-1.42033,-1.41897,-1.41794,-1.41744,-1.41726,-1.41719,-1.41717,-1.41716,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41715,-1.41717,-1.41711,-1.41707,-1.41703,-1.41698,-1.41692,-1.41685,-1.41678,-1.41672,-1.41666,-1.41661,-1.41657,-1.41654,-1.41651,-1.41649,-1.41646,-1.41644,-1.41641,-1.41639,-1.41636,-1.41633,-1.4163,-1.41628,-1.41625,-1.41623,-1.4162,-1.41618,-1.41616,-1.41615,-1.41613,-1.41612,-1.41611,-1.4161,-1.41609,-1.41609,-1.41608,-1.41608,-1.41607,-1.41607,-1.41606,-1.41606,-1.41605,-1.41605,-1.41605,-1.41604,-1.41604,-1.41604,-1.41603,-1.41603,-1.41603,-1.41604,-1.41598,-1.41593,-1.41587,-1.4158,-1.41572,-1.41563,-1.41554,-1.41546,-1.41538,-1.41531,-1.41525,-1.41519,-1.41514,-1.41509,-1.41505,-1.41501,-1.41498,-1.41494,-1.41491,-1.41488,-1.41484,-1.41481,-1.41478,-1.41476,-1.41473,-1.41471,-1.41468,-1.41466,-1.41465,-1.41463,-1.41462,-1.4146,-1.41459,-1.41458,-1.41457,-1.41456,-1.41456,-1.41455,-1.41454,-1.41454,-1.41453,-1.41453,-1.41452,-1.41452,-1.41451,-1.41451,-1.4145,-1.4145,-1.4145,-1.4145,-1.41444,-1.41439,-1.41433,-1.41426,-1.41418,-1.41408,-1.41397,-1.41386,-1.41375,-1.41365,-1.41355,-1.41346,-1.41339,-1.41332,-1.41326,-1.4132,-1.41315,-1.4131,-1.41306,-1.41302,-1.41298,-1.41295,-1.41291,-1.41288,-1.41285,-1.41282,-1.41279,-1.41277,-1.41274,-1.41272,-1.4127,-1.41267,-1.41265,-1.41263,-1.41262,-1.4126,-1.41258,-1.41257,-1.41255,-1.41254,-1.41253,-1.41251,-1.4125,-1.41249,-1.41248,-1.41247,-1.41246,-1.41245,-1.41244,-1.41243,-1.41236,-1.4123,-1.41222,-1.41212,-1.41199,-1.41184,-1.41167,-1.41149,-1.41131,-1.41115,-1.411,-1.41087,-1.41075,-1.41066,-1.41057,-1.41049,-1.41043,-1.41037,-1.41032,-1.41027,-1.41023,-1.41019,-1.41015,-1.41012,-1.41009,-1.41006,-1.41004,-1.41001,-1.40999,-1.40996,-1.40994,-1.40992,-1.4099,-1.40988,-1.40986,-1.40984,-1.40982,-1.4098,-1.40978,-1.40977,-1.40975,-1.40974,-1.40972,-1.40971,-1.40969,-1.40968,-1.40967,-1.40965,-1.40964]
32×26 Array{Float64,2}:
 -0.433989    -0.222213   -0.286526    0.0491272   -0.254689    0.399161    -0.33962    -0.103143     -0.223887   -0.471411    0.114273   -0.181058      0.51808      0.197965    -0.101809    -0.0169169  -0.0624191  -0.629259     0.310053    0.666383    -0.115318    -0.0623891   -0.130749    0.160722    -0.0115029    0.0910788
 -0.88899     -0.783991   -0.116445   -0.384179     0.443758   -0.171123    -0.149226    0.225418      0.0725872   0.796059    0.361846   -0.0454689     0.389411     0.157373    -0.090622     0.172034    0.0883449  -0.12863      0.378016    0.511571     0.0390417   -0.175034    -0.283142   -0.152258     0.0599776    0.0874129
 -0.17499     -0.455727    0.038952    0.597151     0.0994594  -0.326007     0.0743611   0.196571      0.0811111  -0.0282634   0.356898   -0.286008     -0.153113    -0.190283     0.00734082   0.255936    0.391405    0.613976    -0.226907   -0.00318478   0.378597     0.506964     0.269808   -0.14836     -0.184784     0.17447  
  0.10678     -0.468305    0.083993    0.247239    -0.224       0.125292    -0.387728    0.239397     -0.268302   -0.242198    0.178596   -0.126531     -0.325273     0.191489     0.262005    -0.015573    0.357484   -0.164982     0.247004    0.241377     0.233802    -0.378153     0.747362   -0.226956    -0.36304     -0.17357  
  0.00460731  -0.247827    0.336394    0.163941    -0.0226848   0.515852    -0.117626   -0.146302     -0.296671    0.180352   -0.0244188   0.226615      0.164465    -0.520685    -0.29078     -0.259422    0.415815   -0.135311     0.234075   -0.588449    -0.0736997    0.00302833  -0.111134   -0.193358     0.035575    -0.332445 
  0.0591047   -0.227249   -0.148579    0.268184     0.0933383   0.102386    -0.0268936  -0.158719      0.530119    0.0911175  -0.0278789  -0.039192      0.188622     0.298459     0.037723    -0.549614   -0.290544   -0.00563167   0.412489   -0.0842118    0.378783    -0.0527999   -0.412539   -0.132405    -0.0500207   -0.0803756
  0.425121    -0.0466111   0.142465    0.518038     0.201393   -0.102659     0.519813    0.597346     -0.0868677  -0.365147   -0.43371     0.283824      0.18308     -0.0974523    0.0124422   -0.257376   -0.186999    0.475739    -0.391549   -0.0521146    0.00397728  -0.124882    -0.210521   -0.779272     0.118534    -0.300046 
 -0.391618    -0.108703    0.318594    0.521053     0.373023    0.11626      0.346603    0.658554     -0.298667    0.0064396  -0.0572805  -0.0954539    -0.0932789   -0.0774853   -0.462796    -0.103723   -0.49917    -0.574159     0.216647    0.215285     0.326463     0.223628    -0.0455608  -0.252712     0.677299    -0.219675 
 -0.0819787    0.813685    0.0436208   0.00157684  -0.21002    -0.250839     0.267186    0.0125349    -0.187282   -0.40627     0.0246608   0.300586     -0.319796    -0.254001    -0.0373958   -0.161095    0.280439    0.0355592    0.0558882  -0.127728     0.236686     0.595403    -0.0529367   0.170843     0.00335836   0.461942 
 -0.33764      0.0708299  -0.537779   -0.238127    -0.28967    -0.293696    -0.0132724   0.0459263     0.553577    0.725478    0.28578     0.0195107    -0.220544    -0.36958      0.429101    -0.0833954   0.271385    0.147825     0.475523   -0.342942    -0.177413     0.315035    -0.0176069   0.334285    -0.525259     0.765926 
 -0.0572407    0.137624   -0.122101   -0.171027     0.190097   -0.19268      0.204656    0.128471      0.193391   -0.0161623   0.112691    0.000769709   0.00257784  -0.11355      0.205028     0.17181    -0.162787    0.0836804   -0.0397136   0.0798665   -0.348339    -0.0793275    0.0305408   0.0251041   -0.275549    -0.110024 
  0.0822083    0.0633635  -0.0288201  -0.173228    -0.214026    0.0786985   -0.138249   -0.0584601    -0.180948    0.0586194  -0.127278    0.022418      0.0723371   -0.0227376   -0.104698    -0.0573919   0.0428872  -0.0183623   -0.169755   -0.088489    -0.0990868   -0.00441487  -0.0720418   0.0289982    0.259813     0.0820397
 -0.0400591   -0.0967378   0.125116   -0.348867    -0.223054    0.161193     0.187257   -0.000945571  -0.457213    0.0512288  -0.296579    0.136804      0.210523     0.163568     0.685537     0.556629   -0.450939    0.157452     0.200858   -0.686551     0.11095      0.443288    -0.167309   -0.013806     0.199261    -0.151432 
  0.109632    -0.139682   -0.678527   -0.450186     0.171092    0.300836    -0.14868     0.245612     -0.580104   -0.178407   -0.2329      0.245326      0.0888502   -0.0882003    0.514737     0.304601    0.875819    0.306751    -0.310193    0.166405    -0.0296355    0.227306    -0.230006   -0.38757      0.168715    -0.222881 
  0.285401     0.148905    0.711422   -0.214149    -0.14907     0.0758122   -0.271103   -0.15594      -0.238116   -0.125757    0.110289    0.472325      0.391183     0.307382    -0.508521     0.253323    0.70904    -0.267876    -0.555318    0.169783     0.304346    -0.218077     0.301654    0.0669833   -0.0675821    0.0681685
  0.50014      0.268202    0.524117   -0.0921371    0.0830958   0.364309    -0.112219   -0.117884     -0.640638   -0.683093   -0.386767   -0.316012     -0.0408707    0.322262    -0.959895     0.226537   -0.417823    0.117626    -0.676401    0.38052      0.0156125    0.00891821  -0.0357015   0.0570872    0.933092    -0.757598 
  0.160124    -0.602892    0.0349936  -0.0151064    0.274116   -0.00298872   0.161071   -0.738453      0.212475    0.15726     0.209342   -0.111418      0.328139    -0.00496536  -0.151793    -0.166059    0.168006   -0.0531353    0.198733   -0.454623     0.101479     0.269376    -0.199735   -0.74813      0.012859    -0.175962 
  0.176784     0.161222   -0.0467258   0.970153     0.179716   -0.12439      0.0164253  -0.0140665     0.205269   -0.0646532   0.0732103   0.94721       0.389467    -0.523981     0.0440917   -0.763907    0.57468     0.357783     0.688498   -0.155767     0.368481    -0.0746214   -0.24544    -0.283739    -0.342663     0.0542806
 -0.0479       0.377731   -0.158238   -0.497684     0.202187   -1.13283      0.0912569  -0.0831176     0.272001    0.0705858   0.159984    0.267457      0.129816     0.285908     0.56565      0.327848   -0.0833696   0.748656    -0.285145    0.424366    -0.003503    -0.0121221   -0.543837    0.345744    -0.0190833    0.208444 
 -0.121334     0.204232   -0.149477   -0.320502     0.0378304   0.155609     0.354116   -0.649135      0.0562482   0.390921    0.0186485   0.273094      0.749168    -0.0166112   -0.337793     0.0903377  -0.0395663   0.30446     -0.573599   -0.0529838   -0.539886     0.504666    -0.882028    0.476673     0.368767     0.208802 
 -0.122375    -0.0992564  -0.0836428   0.336309     0.114058   -0.392908    -0.0470223   0.102861      0.95265     0.17686     0.269348   -0.330181     -0.172977    -0.0979199   -0.552901    -0.303976   -0.223798    0.00713484   0.259002    0.46283     -0.112321    -0.589972     0.142996    0.210801    -0.0273264    0.331863 
  0.160699    -0.138527   -0.193852    0.244296     0.067053   -0.337517     0.0893686   0.839925      0.359495   -0.0148114  -0.0776286  -0.0418206    -0.863911    -0.242199     0.823521    -0.234797   -0.311408    0.0243313    0.47745     0.041173     0.240977    -0.670639     0.849374   -0.185       -0.123652    -0.182652 
 -0.0099967    0.523919    0.135778   -0.145418     0.236642   -0.505192    -0.0419247   0.410131     -0.292969   -0.0565929  -0.045056    0.00623956   -0.410908    -1.14059     -0.171755     0.136665    0.267241    0.401058    -0.477005   -0.298529    -0.690156     0.106665     0.0802876   0.0613352   -0.264569     0.25061  
 -0.193817    -0.162843    0.495026   -0.327642     0.242363   -0.467074    -0.28197     0.225129     -0.106608   -0.0946509   0.584699    0.0626939     0.337288     0.333629     0.416161     0.195557   -0.0831985  -0.265114    -0.526093   -0.281082    -0.284665    -0.234447     0.364752   -0.264938    -0.959735    -0.582583 
  0.795649     0.396995   -0.389973   -0.292313    -0.328631    0.241051     0.264303   -0.290269      0.241818    0.0724031  -0.415132   -0.0337789    -0.0915518    0.105329     0.0658483    0.0177378  -0.230661    0.169265    -0.0652475  -0.444436    -0.788962    -0.381488     0.0892118   0.0592423    0.215194    -0.153367 
  0.66956      0.451013   -0.161895   -0.136811    -0.402269    0.248033    -0.633114   -0.55765       0.408378   -0.0724742  -0.604081   -0.340211     -0.0620502    0.205111    -0.368867    -0.620103    0.397992    0.458365    -0.010253   -0.354239     0.611383     0.180523    -0.263058    0.418997    -0.0979588    0.45329  
  0.00963199   0.0804604   0.258147    0.964726    -0.504739    0.0885016   -0.0322477   0.057302      0.21837    -0.620321   -0.0370282  -0.5681       -0.413973    -0.121516    -0.700637    -0.648365   -0.1303     -0.347822    -0.101688    0.0590431    0.178003     0.15527      0.852048   -0.035377    -0.49932      0.128484 
  0.338174    -0.822793   -0.202779    0.890445    -0.555677    1.1633      -0.285727    0.319478     -0.0278822   0.217235   -0.631086   -0.5688       -0.00877953   0.0806307   -0.563798    -0.0933138   0.424365   -0.228353    -0.216785   -0.302623     0.460416     0.0668453    0.370147   -0.305934     0.568974     0.140299 
 -0.172037     0.190668    0.0526531  -0.895705    -0.2114      0.0206365   -0.0458687  -0.0703765     0.127971    0.344174   -0.0485148  -0.664874     -0.424442     0.330629    -0.131906     0.655528   -0.693231   -0.692105    -0.282831    0.0334751   -0.205252     0.111601     0.257792    0.469489     0.140639     0.158429 
 -0.176498     0.250239    0.046242   -0.693222     0.156239    0.516552    -0.663006   -0.45664       0.151496   -0.0564655  -0.0205254   0.466176     -0.124435     0.0423241    0.137248    -0.359617   -0.375911   -0.728567     1.0114      0.159071    -0.152877    -0.338606    -0.346563    0.47646      0.110835    -0.209612 
 -0.45904     -0.398791   -0.0426733   0.242678     0.201628    0.181133     0.599794    0.901533     -0.450022    0.383438    0.496772    0.264475      0.183211    -0.435104     0.442938     0.683177   -0.473377   -0.266558    -0.190816    0.685888    -0.693249     0.00648159   0.19974    -0.00921955   0.308331    -0.293445 
 -0.271011    -0.115751   -0.0834979   0.187319    -0.278248    0.258235    -0.139626    0.253202      0.023196    0.0405513   0.0902758  -0.099268      0.0461241    0.179958    -0.0207302    0.0695735  -0.0614704  -0.243527     0.146326    0.443226     0.282449     0.0629306   -0.0915891   0.313266     0.144606     0.158761 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409630
INFO: iteration 2, average log likelihood -1.409619
INFO: iteration 3, average log likelihood -1.409608
INFO: iteration 4, average log likelihood -1.409597
INFO: iteration 5, average log likelihood -1.409587
INFO: iteration 6, average log likelihood -1.409577
INFO: iteration 7, average log likelihood -1.409568
INFO: iteration 8, average log likelihood -1.409558
INFO: iteration 9, average log likelihood -1.409549
INFO: iteration 10, average log likelihood -1.409540
INFO: EM with 100000 data points 10 iterations avll -1.409540
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.373738e+05
      1       7.074769e+05      -2.298969e+05 |       32
      2       6.938686e+05      -1.360827e+04 |       32
      3       6.888227e+05      -5.045850e+03 |       32
      4       6.862900e+05      -2.532692e+03 |       32
      5       6.846177e+05      -1.672362e+03 |       32
      6       6.833769e+05      -1.240740e+03 |       32
      7       6.823906e+05      -9.863643e+02 |       32
      8       6.815758e+05      -8.148088e+02 |       32
      9       6.809276e+05      -6.482074e+02 |       32
     10       6.803902e+05      -5.373517e+02 |       32
     11       6.799070e+05      -4.832358e+02 |       32
     12       6.794431e+05      -4.638950e+02 |       32
     13       6.790058e+05      -4.372988e+02 |       32
     14       6.786025e+05      -4.032772e+02 |       32
     15       6.782441e+05      -3.583648e+02 |       32
     16       6.779581e+05      -2.860433e+02 |       32
     17       6.777056e+05      -2.524973e+02 |       32
     18       6.774776e+05      -2.280039e+02 |       32
     19       6.772414e+05      -2.362248e+02 |       32
     20       6.770183e+05      -2.230983e+02 |       32
     21       6.768065e+05      -2.117873e+02 |       32
     22       6.766001e+05      -2.064089e+02 |       32
     23       6.764162e+05      -1.838448e+02 |       32
     24       6.762584e+05      -1.578118e+02 |       32
     25       6.761145e+05      -1.439413e+02 |       32
     26       6.759862e+05      -1.282474e+02 |       32
     27       6.758656e+05      -1.206553e+02 |       32
     28       6.757536e+05      -1.119659e+02 |       32
     29       6.756417e+05      -1.118751e+02 |       32
     30       6.755341e+05      -1.076581e+02 |       32
     31       6.754374e+05      -9.672477e+01 |       32
     32       6.753402e+05      -9.717515e+01 |       32
     33       6.752628e+05      -7.733910e+01 |       32
     34       6.752019e+05      -6.096610e+01 |       32
     35       6.751501e+05      -5.178008e+01 |       32
     36       6.751010e+05      -4.905280e+01 |       32
     37       6.750579e+05      -4.313232e+01 |       32
     38       6.750130e+05      -4.487549e+01 |       32
     39       6.749738e+05      -3.922712e+01 |       32
     40       6.749432e+05      -3.060849e+01 |       32
     41       6.749127e+05      -3.050393e+01 |       32
     42       6.748855e+05      -2.716331e+01 |       32
     43       6.748593e+05      -2.620264e+01 |       32
     44       6.748357e+05      -2.360898e+01 |       32
     45       6.748149e+05      -2.079620e+01 |       32
     46       6.747940e+05      -2.089295e+01 |       32
     47       6.747725e+05      -2.149278e+01 |       32
     48       6.747535e+05      -1.905812e+01 |       32
     49       6.747392e+05      -1.427980e+01 |       32
     50       6.747245e+05      -1.465855e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 674724.5397045574)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421179
INFO: iteration 2, average log likelihood -1.416350
INFO: iteration 3, average log likelihood -1.415072
INFO: iteration 4, average log likelihood -1.414107
INFO: iteration 5, average log likelihood -1.413052
INFO: iteration 6, average log likelihood -1.412048
INFO: iteration 7, average log likelihood -1.411348
INFO: iteration 8, average log likelihood -1.410954
INFO: iteration 9, average log likelihood -1.410732
INFO: iteration 10, average log likelihood -1.410587
INFO: iteration 11, average log likelihood -1.410480
INFO: iteration 12, average log likelihood -1.410393
INFO: iteration 13, average log likelihood -1.410320
INFO: iteration 14, average log likelihood -1.410255
INFO: iteration 15, average log likelihood -1.410198
INFO: iteration 16, average log likelihood -1.410147
INFO: iteration 17, average log likelihood -1.410101
INFO: iteration 18, average log likelihood -1.410058
INFO: iteration 19, average log likelihood -1.410018
INFO: iteration 20, average log likelihood -1.409982
INFO: iteration 21, average log likelihood -1.409948
INFO: iteration 22, average log likelihood -1.409917
INFO: iteration 23, average log likelihood -1.409888
INFO: iteration 24, average log likelihood -1.409861
INFO: iteration 25, average log likelihood -1.409836
INFO: iteration 26, average log likelihood -1.409813
INFO: iteration 27, average log likelihood -1.409791
INFO: iteration 28, average log likelihood -1.409771
INFO: iteration 29, average log likelihood -1.409753
INFO: iteration 30, average log likelihood -1.409735
INFO: iteration 31, average log likelihood -1.409719
INFO: iteration 32, average log likelihood -1.409704
INFO: iteration 33, average log likelihood -1.409690
INFO: iteration 34, average log likelihood -1.409677
INFO: iteration 35, average log likelihood -1.409664
INFO: iteration 36, average log likelihood -1.409652
INFO: iteration 37, average log likelihood -1.409641
INFO: iteration 38, average log likelihood -1.409631
INFO: iteration 39, average log likelihood -1.409621
INFO: iteration 40, average log likelihood -1.409612
INFO: iteration 41, average log likelihood -1.409603
INFO: iteration 42, average log likelihood -1.409594
INFO: iteration 43, average log likelihood -1.409586
INFO: iteration 44, average log likelihood -1.409578
INFO: iteration 45, average log likelihood -1.409570
INFO: iteration 46, average log likelihood -1.409563
INFO: iteration 47, average log likelihood -1.409556
INFO: iteration 48, average log likelihood -1.409549
INFO: iteration 49, average log likelihood -1.409542
INFO: iteration 50, average log likelihood -1.409536
INFO: EM with 100000 data points 50 iterations avll -1.409536
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.742578    -0.152047    -0.682032   -0.220447    -0.289882     -0.33359    -0.335761     0.141213     0.488562    0.83262      0.490245    -0.185971     -0.047667   -0.325571     0.103912     0.000125235   0.544497    -0.110458     0.53885    -0.106093     -0.156286     0.297413    -0.0154387    0.442439    -0.472367    1.02536  
 -0.0461511   -0.323193    -0.089843    0.13056      0.220783      0.395704   -0.207529     0.856583     0.303515    0.480146     0.153426    -0.266483     -0.106441   -0.222333    -0.451462    -0.165595     -0.341747    -0.607079     0.21707     0.503293     -0.317676    -0.564936    -0.0239299   -0.106556     0.250872    0.222429 
 -0.110805    -0.162563    -0.0406293   0.206775     0.2425       -0.161338    0.511082    -0.149132     0.271583    0.340706    -0.0726652    0.120329      0.399306   -0.25171     -0.557282    -0.351011      0.00186555   0.218448    -0.176852   -0.25103      -0.0330418    0.375202    -0.761007    -0.385321     0.438551    0.156052 
  0.200045    -0.352369    -0.256597   -0.154929     0.250543     -0.0669005   0.310161    -0.24169      0.0367209   0.663427    -0.0606556   -0.175531     -0.406429   -0.27246      0.880807    -0.164976     -0.474552     0.37433      0.597897   -1.05052      -0.155876     0.350673     0.1661      -0.467553     0.0320308  -0.0353157
 -0.457664    -0.0443984    0.426411    0.699065    -0.0373517     0.137036    0.180391     0.334191    -0.476975   -0.340594     0.467347     0.125965     -0.193732   -0.199557    -0.332327    -0.0144405     0.280342    -0.390521     0.0610419   0.176129      0.46369      0.533165     0.351682    -0.134652    -0.0907092   0.0500301
  0.370555     0.279832     0.485277   -0.00276809   0.0697011     0.360421   -0.0444093   -0.055377    -0.697383   -0.723155    -0.356473    -0.167033      0.134341    0.239879    -0.902807     0.148419     -0.330915    -0.0312943   -0.570932    0.411621      0.0337069    0.00535863  -0.139666     0.0536172    0.890141   -0.550866 
 -0.0561763    0.591723    -0.071761    0.301708    -0.0157207    -0.689977   -0.150983     0.203172     0.272457   -0.606208     0.0834532   -0.613859     -0.49159    -0.481206    -0.590567    -0.493904     -0.161991     0.0929692   -0.143464    0.140225     -0.23099      0.0182741    0.387367     0.215922    -0.445768    0.235394 
 -0.342134     0.376971     0.259403   -0.71733      0.312095     -0.82176     0.159481     0.151468    -0.0298368   0.121799     0.611385     0.597314      0.245743   -0.0863967    0.405692     0.329305     -0.144897     0.224599    -0.137792    0.140303     -0.59572     -0.0447628   -0.268621     0.0608157   -0.268532    0.183439 
 -0.02357      0.00262386   0.0357503  -0.0522842   -0.000537662   0.0032088  -0.0332688    0.0562673   -0.0836483  -0.0151575   -0.00971988   0.00194869    0.0406279  -0.08004     -0.0368362   -0.033763     -0.0240274   -0.0533522   -0.0287145  -0.0388378    -0.108243    -0.0204114    0.00668947  -0.0448628    0.0620017  -0.0611988
 -0.319252    -0.194561    -0.0393631   0.171633    -0.182135      0.212491   -0.0401297    0.378034    -0.198812    0.0677278    0.0669939   -0.0448082     0.126349   -0.0255824   -0.061434     0.197334     -0.0846278   -0.222333     0.0361622   0.269348      0.0792036    0.165984    -0.0962116    0.0742585    0.142993    0.107003 
  0.243796     0.115944    -0.0659153   0.948845     0.118537     -0.231144   -0.11894     -0.0134311    0.32792    -0.0371771    0.101076     0.744073      0.376281   -0.397391    -0.0961421   -0.961801      0.392734     0.452015     0.582197   -0.0821        0.363889    -0.229613    -0.349341    -0.240195    -0.281439    0.154715 
 -0.052671     0.0335258    0.396046   -0.159943    -0.0789526     0.543638   -0.0602379   -0.349591    -0.231299    0.663189     0.451475     0.00183823    0.328703   -0.128981    -0.34148      0.262347     -0.281203    -0.269543    -0.294262   -0.22827      -0.457634     0.293804    -0.190093     0.581225     0.255801   -0.268985 
  0.746743     0.374811    -0.114841   -0.0776293   -0.207083      0.0473622   0.5622      -0.30026      0.344579   -0.0721676    0.170596    -0.114459     -0.368917    0.27377     -0.196466     0.181897     -0.667506    -0.153254     0.0204009  -0.260938     -0.897294    -0.519454     0.520789    -0.300616     0.184134   -0.488713 
  0.143153     0.166547     0.0745431  -0.138877    -0.356858      0.347889   -0.182256    -0.318689    -0.183847   -0.0833552   -0.0746218    0.400115      0.166277    0.0629119   -0.260989     0.167627      0.868836    -0.0116666   -0.203367    0.0134221     0.254253     0.0389654    0.176864     0.106942    -0.0199594   0.24676  
 -0.307623     0.225908     0.159048   -1.02687     -0.0521976     0.0799249  -0.107026     0.173246    -0.270198    0.171767    -0.369048    -0.511807     -0.357855    0.123105     0.091034     0.974515     -0.396642    -0.47146     -0.399707   -0.192324     -0.358568     0.0621071    0.61335      0.445832    -0.0659604  -0.0444748
 -0.0518207    0.822935    -0.330771   -0.174422    -0.248309     -0.432495    0.28666     -0.111166     0.404989   -0.0202486   -0.0016324   -0.135153     -0.0354632   0.409038     0.506407     0.414712     -0.472982     0.184024    -0.233235    0.340668      0.00712584   0.241298    -0.430215     0.756369    -0.0582333   0.167796 
 -0.46403     -0.729076    -0.305438    0.263596     0.454952     -0.328511    0.277054     0.661649    -0.356905    0.0825901    0.513574    -0.236678      0.12668    -0.418416     0.552195     0.551439     -0.213475     0.138054    -0.331706    0.508652     -0.486161     0.0433737    0.154622    -0.195932     0.10578    -0.343786 
  0.00338893  -0.0561914   -0.700853   -0.443548    -0.224612      0.172769    0.0787364   -0.228119    -0.0756085  -0.100657    -0.332987     0.0110032     0.505058    0.0948975    0.336921     0.037755     -0.1676       0.0396846   -0.108571   -0.000420178  -0.476016    -0.0423194   -0.527867    -0.0356658    0.109543   -0.0436675
  0.091195     0.0676099   -0.018423    0.2616       0.140953     -0.476078    0.154444     0.724341     0.662171    0.0092811    0.0990382    0.122218     -0.662072   -0.173046     0.493533    -0.296076     -0.332473     0.0642283    0.192911    0.195778      0.142634    -0.668215     0.639074     0.115143    -0.184069   -0.0676267
  0.467486    -0.491392    -0.012164    0.867015    -0.381364      0.762196   -0.335493    -0.0314197    0.105902   -0.18305     -0.775276    -0.65016      -0.0764452  -0.0269134   -0.648254    -0.535196      0.237748    -0.126811    -0.0483557  -0.318177      0.47383     -0.0323456    0.364682    -0.27754      0.293416    0.0228527
  0.39733      0.254311    -0.207537   -0.555229    -0.0625613     0.103168   -0.111478    -1.05242      0.350542    0.0829719   -0.170743     0.000712693   0.274095    0.111048    -0.184488    -0.111092      0.331276     0.351276    -0.1025     -0.295663     -0.00147238   0.260783    -0.537595     0.297082     0.019401    0.23889  
 -0.12777     -0.504385    -0.10046     0.218647     0.193104      0.387133    0.532785     0.659624    -0.0253154  -0.00954543  -0.504804     0.286772      0.459908    0.4613       0.293511     0.153012     -0.303571    -0.00578813   0.134704    0.43373       0.528362    -0.0603169   -0.184061    -0.294083     0.249088   -0.419131 
  0.349536     0.303431    -0.710969   -0.445942     0.361198     -0.0518324  -0.0713415    0.463097    -0.34309    -0.160301    -0.0950971    0.348052     -0.337931   -0.52212      0.412563     0.332082      0.846623     0.377501    -0.167759    0.0530269    -0.312251     0.345223    -0.432138    -0.238014     0.216177    0.0448697
 -0.00667191   0.0587944   -0.209037    0.300101     0.0663279    -0.11271     0.159049     0.00873079   0.45477     0.0510227    0.137378     0.0103499    -0.193572    0.0317736    0.289221    -0.148612      0.0358564    0.376284     0.358816   -0.0960531     0.414907     0.289946    -0.130637     0.00459143  -0.296915    0.165875 
 -0.409616    -0.118001    -0.0497448  -0.199868     0.366062      0.41541    -0.499966    -0.616957     0.0240881  -0.266634     0.220951     0.245935      0.20337    -0.0132767    0.102267    -0.39902      -0.305209    -0.688113     1.20252     0.241547     -0.103535    -0.174485    -0.310367     0.119872     0.0145411  -0.270545 
  0.108083     0.468343    -0.233777   -0.484132    -0.546987      0.24136    -0.492413    -0.0715186    0.223521    0.00118773  -0.850396    -0.0146241    -0.327849    0.231561    -0.0526803   -0.54164      -0.242706    -0.169624     0.367623   -0.0906478     0.176941    -0.14001     -0.214397     0.531926     0.291319    0.361574 
  0.360105     0.553316     0.352473    0.181314    -0.281084     -0.238463    0.51786      0.115008    -0.370333   -0.250772    -0.672978     0.351011     -0.0762206  -0.53274      0.114138     0.00431279    0.120499     0.527849    -0.44561    -0.849652     -0.263447     0.0953551    0.141496    -0.160618    -0.126904    0.0290427
  0.547828    -0.248201     0.636632   -0.31211      0.127407     -0.511283   -0.686888     0.340528    -0.127822    0.15159      0.0311728   -0.0687821     0.109181    0.390035    -0.00892337  -0.0755736     0.212299     0.537121    -0.716824   -0.0179995     0.539148     0.21602     -0.00132639  -0.317656    -0.0180306   0.130376 
 -0.208473    -0.422839     0.354696   -0.00992523  -0.361511     -0.0945885  -0.187542    -0.541664     0.74676     0.417746     0.489743    -0.487286      0.185673    0.596954    -0.531015    -0.129123     -0.316373    -0.595705     0.348032    0.381912      0.31963     -0.145281     5.44315e-5   0.0870009    0.198265    0.209192 
 -0.0923137   -0.663808     0.242854   -0.0106332    0.205269      0.285352   -0.00549184  -0.256953    -0.273605   -0.0740338    0.242939     0.292589      0.513042   -0.041885     0.131918     0.0317184     0.615069    -0.291376    -0.0506585  -0.428915      0.0839498   -0.0254016    0.0513791   -0.779455    -0.409417   -0.721308 
 -0.0850552   -0.576892     0.0611338   0.272329     0.172977     -0.230864   -0.0375498    0.115893     0.54239     0.126165     0.306859    -0.124199     -0.0147143   0.00112575  -0.0988124   -0.0241002     0.189854     0.100406     0.101745    0.139471     -0.352234    -0.708613     0.600711    -0.205479    -0.499338   -0.0916882
  0.10374     -0.457936    -0.0675497   0.0729987   -0.393155      0.106467   -0.611562     0.333712    -0.504848   -0.409261     0.176223    -0.363891     -0.316297    0.283518     0.603757     0.0854917     0.165417    -0.193845     0.337553    0.291457      0.348468    -0.314795     0.649629    -0.14766     -0.409471   -0.142232 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409529
INFO: iteration 2, average log likelihood -1.409523
INFO: iteration 3, average log likelihood -1.409517
INFO: iteration 4, average log likelihood -1.409511
INFO: iteration 5, average log likelihood -1.409505
INFO: iteration 6, average log likelihood -1.409499
INFO: iteration 7, average log likelihood -1.409493
INFO: iteration 8, average log likelihood -1.409488
INFO: iteration 9, average log likelihood -1.409482
INFO: iteration 10, average log likelihood -1.409477
INFO: EM with 100000 data points 10 iterations avll -1.409477
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
