>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.10.2
INFO: Installing FileIO v0.1.2
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.4.2
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.5
INFO: Installing StatsBase v0.10.0
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-96-generic #143-Ubuntu SMP Mon Aug 29 20:15:20 UTC 2016 x86_64 x86_64
Memory: 2.4471054077148438 GB (660.03125 MB free)
Uptime: 25326.0 sec
Load Avg:  1.01611328125  1.02001953125  1.04541015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1367448 s        945 s     127284 s     736423 s         50 s
#2  3499 MHz     648500 s       4031 s      81123 s    1677215 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.7.1
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.10.2
 - FileIO                        0.1.2
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.4.2
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.5
 - StatsBase                     0.10.0
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-7.012294605255129e6,[55729.1,44270.9],
[-11643.6 6830.26 30246.8; 11949.6 -6589.51 -30304.5],

Array{Float64,2}[
[66356.2 -2659.5 -6516.03; -2659.5 56119.7 1458.81; -6516.03 1458.81 53080.6],

[34156.0 3022.53 5893.37; 3022.53 43815.1 -1345.84; 5893.37 -1345.84 47419.1]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.387560e+03
      1       1.022193e+03      -3.653665e+02 |        8
      2       9.563652e+02      -6.582794e+01 |        3
      3       9.250851e+02      -3.128014e+01 |        0
      4       9.250851e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 925.085059905809)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.075762
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.726451
INFO: iteration 2, lowerbound -3.566419
INFO: iteration 3, lowerbound -3.404689
INFO: iteration 4, lowerbound -3.239704
INFO: iteration 5, lowerbound -3.088964
INFO: dropping number of Gaussions to 7
INFO: iteration 6, lowerbound -2.957089
INFO: iteration 7, lowerbound -2.858900
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.791510
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.729343
INFO: iteration 10, lowerbound -2.688978
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.651743
INFO: iteration 12, lowerbound -2.609379
INFO: iteration 13, lowerbound -2.567011
INFO: iteration 14, lowerbound -2.523326
INFO: iteration 15, lowerbound -2.480853
INFO: iteration 16, lowerbound -2.441416
INFO: iteration 17, lowerbound -2.405522
INFO: iteration 18, lowerbound -2.372712
INFO: iteration 19, lowerbound -2.343195
INFO: iteration 20, lowerbound -2.319951
INFO: iteration 21, lowerbound -2.308191
INFO: dropping number of Gaussions to 2
INFO: iteration 22, lowerbound -2.303080
INFO: iteration 23, lowerbound -2.299263
INFO: iteration 24, lowerbound -2.299257
INFO: iteration 25, lowerbound -2.299255
INFO: iteration 26, lowerbound -2.299254
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Fri 30 Sep 2016 11:33:04 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Fri 30 Sep 2016 11:33:06 AM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Fri 30 Sep 2016 11:33:08 AM UTC: EM with 272 data points 0 iterations avll -2.075762
5.8 data points per parameter
,Fri 30 Sep 2016 11:33:09 AM UTC: GMM converted to Variational GMM
,Fri 30 Sep 2016 11:33:11 AM UTC: iteration 1, lowerbound -3.726451
,Fri 30 Sep 2016 11:33:11 AM UTC: iteration 2, lowerbound -3.566419
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 3, lowerbound -3.404689
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 4, lowerbound -3.239704
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 5, lowerbound -3.088964
,Fri 30 Sep 2016 11:33:12 AM UTC: dropping number of Gaussions to 7
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 6, lowerbound -2.957089
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 7, lowerbound -2.858900
,Fri 30 Sep 2016 11:33:12 AM UTC: dropping number of Gaussions to 5
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 8, lowerbound -2.791510
,Fri 30 Sep 2016 11:33:12 AM UTC: dropping number of Gaussions to 4
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 9, lowerbound -2.729343
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 10, lowerbound -2.688978
,Fri 30 Sep 2016 11:33:12 AM UTC: dropping number of Gaussions to 3
,Fri 30 Sep 2016 11:33:12 AM UTC: iteration 11, lowerbound -2.651743
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 12, lowerbound -2.609379
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 13, lowerbound -2.567011
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 14, lowerbound -2.523326
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 15, lowerbound -2.480853
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 16, lowerbound -2.441416
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 17, lowerbound -2.405522
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 18, lowerbound -2.372712
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 19, lowerbound -2.343195
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 20, lowerbound -2.319951
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 21, lowerbound -2.308191
,Fri 30 Sep 2016 11:33:13 AM UTC: dropping number of Gaussions to 2
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 22, lowerbound -2.303080
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 23, lowerbound -2.299263
,Fri 30 Sep 2016 11:33:13 AM UTC: iteration 24, lowerbound -2.299257
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 25, lowerbound -2.299255
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 26, lowerbound -2.299254
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 27, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 28, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 29, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 30, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 31, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 32, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 33, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 34, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 35, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 36, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 37, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:14 AM UTC: iteration 38, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 39, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 40, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 41, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 42, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 43, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 44, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 45, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 46, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 47, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 48, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 49, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: iteration 50, lowerbound -2.299253
,Fri 30 Sep 2016 11:33:15 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000006
avll from stats: -0.9925937313508174
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9925937313508167
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9925937313508167
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9428347632741694
avll from llpg:  -0.9428347632741694
avll direct:     -0.9428347632741694
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0385262    0.0986495   0.102906    -0.0492394  -0.162271     0.148255     0.109597    -0.015868     0.0714082   -0.0424232   -0.167127    -0.0209876    0.121802   -0.218518     0.00310892   0.0747849    -0.0527914   -0.0291529    0.110289    -0.0208811    0.121397     -0.0615229   -0.0961057    0.0409653    0.149811     0.00417279
 -0.0270153    0.109963   -0.0719647    0.1585      0.0362258   -0.0122337   -0.0699277    0.0642888    0.145001    -0.10217      0.073467     0.0324009   -0.0010046  -0.126549    -0.110105     0.145354     -0.0982307   -0.00592742  -0.0818239   -0.129135     0.066576      0.0419743    0.126087     0.0514834   -0.0286384   -0.0209428 
 -0.30601     -0.0943153  -0.0632888   -0.0346452  -0.131854     0.00810772  -0.0219367    0.0771118   -0.230968    -0.0383745    0.0746067   -0.0897486   -0.0849055  -0.0684219   -0.0683353    0.0785671     0.199077    -0.0912835    0.0639333    0.0749694    0.0635838     0.0582285    0.0585444   -0.00632639  -0.0119059    0.0436881 
  0.137404     0.0143014   0.0912095   -0.137276    0.0186364   -0.139215     0.0236284   -0.00266117   0.0555152   -0.0472451    0.0797426    0.0506688   -0.153037    0.166274    -0.154103     0.0831478     0.0528171   -0.0178804    0.0628352   -0.0233937    0.0659116     0.135086    -0.0015771   -0.0355706    0.0398821   -0.0547139 
  0.0380658   -0.0354754  -0.0902548    0.150377    0.0679315    0.0378079    0.0200414   -0.0523955    0.0283223    0.021326     0.0547701   -0.121435    -0.0752395  -0.0824791    0.135377     0.0818657    -0.149123    -0.0490762   -0.0242365   -0.0399827   -0.0475057     0.0630135    0.0876611    0.024017    -0.00594773   0.189802  
 -0.0296299    0.165283    0.159023    -0.0353778   0.0189482    0.023858    -0.140527     0.00821609  -0.134911    -0.0929592    0.0333635    0.0538954    0.0799157   0.0912382   -0.111639     0.0820337    -0.00737087   0.228568     0.0319091    0.126496    -0.120891      0.0245025   -0.022602    -0.079387     0.0138098   -0.0457582 
 -0.0513483    0.0492598  -0.105751     0.0209442   0.130657    -0.0870414   -0.14522     -0.0837136    0.00830019  -0.077788     0.0732842    0.0760965    0.0785062   0.00312552  -0.0130664   -0.000725631   0.0341481   -0.16009     -0.0146289    0.073907     0.0223825    -0.127959     0.187917    -0.0923346    0.0191197    0.0142848 
  0.00392117  -0.0135035  -0.100905    -0.0200424  -0.144736     0.122009    -0.151032     0.0446183    0.00335018  -0.0221017    0.00176249   0.0723446   -0.0721206   0.0442094   -0.0859749   -0.0455997    -0.0619326   -0.00316605  -0.017282     0.0110153   -0.0172248    -0.00134887   0.00413024   0.0176522    0.0985877   -0.240506  
 -0.0366693    0.0124329  -0.0650117    0.0453173   0.0890539   -0.162231    -0.0706297    0.0304221    0.153886    -0.0195986    0.0306195   -0.00251299   0.0272358  -0.0752685   -0.136951    -0.0605289    -0.0996749    0.052026    -0.024254    -0.246027    -0.0834885     0.0519113   -0.071248    -0.101173    -0.00711705  -0.0409135 
  0.253134    -0.0074015   0.0560207    0.299972    0.111781    -0.16094     -0.00229577   0.0343062   -0.0974808   -0.0158945   -0.128145     0.0716099   -0.186207   -0.0232146   -0.0872134    0.0635981    -0.115048    -0.00801457   0.00595822  -0.0581869    0.144551      0.182154     0.0951292    0.113826    -0.0840164    0.0266554 
 -0.050823    -0.0557628   0.00101101   0.267747   -0.0753109    0.190015    -0.0420054   -0.00213564  -0.00118896   0.0386537   -0.245325    -0.237819     0.0689565  -0.136433    -0.0672456    0.00214129   -0.0142826    0.0437069   -0.0216545    0.0251612    0.132087      0.0716864   -0.171714    -0.0777405    0.00547053   0.0682813 
  0.146311     0.090789   -0.227904    -0.0128044   0.0975635    0.0936964    0.0530301    0.0677854    0.183997     0.304717     0.0047005   -0.0806148    0.113714   -0.108768     0.13704     -0.0246389     0.106778     0.217951     0.00923135   0.02808      0.130932     -0.154674     0.0701799   -0.0675312    0.058009     0.137785  
  0.131936    -0.0323735  -0.0183425    0.243999    0.00315928   0.0791533   -0.0878882   -0.0610321   -0.233716     0.194329     0.138697     0.196824    -0.0120079  -0.162889     0.167815    -0.117606      0.018568     0.0309873   -0.0825183   -0.100021     0.0429593     0.00288386   0.117413     0.0643381   -0.0389056    0.0209933 
  0.00613526   0.153339   -0.036176    -0.0606661  -0.0988933   -0.00912771   0.129743     0.0245152   -0.00443837   0.0200353    0.0321991    0.0676066    0.0642926  -0.0665438    0.0368663   -0.124196     -0.0826492   -0.011396    -0.20998     -0.0324524   -0.139398     -0.00686456   0.0992399   -0.100096    -0.138355    -0.0161774 
  0.0423998   -0.0587905  -0.0507069   -0.114375    0.0371902   -0.0656658    0.069168     0.0931583   -0.175565     0.116743    -0.171021    -0.0336295    0.0800873  -0.0793232    0.0283733    0.224248      0.255367    -0.0791142    0.0414143    0.100389     0.104121     -0.00162846   0.103886     0.0921465   -0.149573     0.0274612 
 -0.125896     0.0109823   0.100784     0.0148454  -0.120792    -0.0470819    0.0430899   -0.128609    -0.11651     -0.0550194   -0.0836914    0.0255782   -0.0566086   0.0215618   -0.247117     0.00236455   -0.00374421   0.0287832    0.102175    -0.0460575    0.0402122    -0.0546193   -0.0811606    0.0937624    0.0650502   -0.0579386 
 -0.0722043   -0.0118854   0.00943444   0.0682396   0.0869228   -0.0076976   -0.0617884   -0.0486936   -0.145361    -0.0499864    0.0961517   -0.0233751    0.0907985   0.149212     0.168474    -0.128426     -0.0839131    0.0572833   -0.0407878   -0.0049147   -0.102835      0.00947749  -0.0877354    0.0662673   -0.00112367  -0.0268115 
  0.0468315    0.110188    0.0266228   -0.0565898  -0.0510582    0.0155329   -0.141447    -0.0731288    0.262331     0.194851    -0.00256675   0.144596     0.0225729   0.234025    -0.106327    -0.0264651    -0.140066    -0.00629652  -0.114891    -0.0562249   -0.0858605    -0.0602083    0.123393    -0.186813     0.042175    -0.0173602 
 -0.085803     0.0533931  -0.0251895   -0.0273379  -0.060704    -0.0580897   -0.097468     0.107876    -0.00535067   0.0355287    0.0123676    0.0911157    0.223319    0.085711    -0.044565     0.0203197     0.120306     0.0548887   -0.0450837    0.0124015    0.0200099     0.044731    -0.0613088    0.290335     0.0281139    0.0640694 
 -0.00850948   0.0299513   0.112722     0.0768198  -0.0121028   -0.0918531    0.0100251   -0.0161897   -0.129798     0.132854    -0.0522255    0.0190372    0.0332127  -0.292389    -0.0733983    0.0261731     0.0452975   -0.0961067    0.091559    -0.132938     0.0741229     0.0827412   -0.11335      0.186115     0.0428651   -0.00273717
 -0.0817892   -0.112369   -0.226197    -0.0584596   0.200046     0.091072     0.024774     0.0661056   -0.0184225    0.0114535    0.0259537    0.15075      0.142712    0.0296653    0.0555832   -0.0174021     0.0652085   -0.111423     0.182937    -0.0575103   -0.120265     -0.0136243   -0.046883    -0.143197    -0.0206322    0.0102866 
 -0.0679674    0.0329616  -0.0172961    0.0712952  -0.227556    -0.018525    -0.10949     -0.104708    -0.0552876    0.193395     0.158392     0.0933168    0.0502827   0.141371    -0.0518707   -0.00995978    0.0128069    0.081128    -0.115699     0.184143     0.0453925     0.0024807   -0.049401     0.119554    -0.00550913   0.0166199 
 -0.0164657    0.0795576  -0.0189368    0.0194348   0.180964    -0.0737251   -0.143409     0.020544     0.00704715  -0.00192072  -0.0152408    0.184006    -0.124347    0.0482117   -0.18606      0.135876      0.0277024    0.103383     0.00578448   0.0313432    0.000433646  -0.059593     0.025306    -0.0307835    0.00727093   0.0463841 
 -0.0894641    0.0818767   0.0508952   -0.131117    0.00100123   0.0659955   -0.0612538    0.0840174   -0.193451    -0.106044     0.0685605   -0.150835     0.119883    0.0274808    0.11284     -0.165844     -0.0998766   -0.0394498    0.0558708   -0.00955624  -0.16597      -0.0418811    0.0255055    0.0589245    0.0379041    0.0305302 
 -0.0201509   -0.0955432  -0.00549158   0.0214643  -0.119598     0.196207     0.0365622   -0.077763     0.0579205   -0.0684788    0.0479739    0.0607643    0.0568093   0.0682093   -0.121991     0.0971129    -0.151786    -0.0413063    0.00321331   0.0778933   -0.046689      0.0222748    0.120381     0.0997336    0.125064    -0.0167992 
  0.0865309    0.0781929  -0.130594     0.0652628  -0.136938     0.23103     -0.100961     0.0883247   -0.216561     0.268308     0.0184226    0.0543554    0.0429352  -0.0511065   -0.120311    -0.0663962     0.207507     0.00194858   0.200469    -0.203762     0.0380488     0.227483    -0.0798513   -0.0772858   -0.0316392    0.0592123 
 -0.0983716   -0.0040213   0.114348    -0.0343682   0.133537    -0.019413    -0.00180087   0.150591    -0.193329     0.12565     -0.0561229    0.0962021   -0.0269278   0.0810345    0.0767836    0.0935052    -0.107769    -0.185908    -0.172582    -0.0563056    0.0910848     0.0488612   -0.100761     0.12902     -0.0763709   -0.0164452 
  0.0262446   -0.0289885  -0.128658     0.0193706   0.150546    -0.100217     0.129608     0.217516    -0.0710633   -0.0229041    0.0955529    0.0883678    0.0557517  -0.04831      0.146432     0.0801693    -0.0133461   -0.206877     0.0505153    0.107297    -0.0804502    -0.0542817   -0.179282     0.106017     0.242388     0.0625492 
  0.121943    -0.0714593  -0.0311711   -0.132734   -0.0127413    0.0917952    0.0289243    0.0375073    0.0455981    0.0580528   -0.00186124  -0.0196962    0.0390139  -0.0768321    0.25071     -0.0975945     0.069919    -0.165868     0.0705493   -0.119512     0.164049     -0.211218    -0.119194    -0.103953    -0.0649457   -0.123962  
  0.076898     0.167957    0.0414841    0.0659985   0.0253112    0.021531     0.0615489    0.0858683    0.0364123    0.0728476    0.177883    -0.0193096    0.0193113  -0.163204    -0.0330514    0.013792      0.0749743    0.0150693    0.199201    -0.154676    -0.109372      0.0585629   -0.0568523   -0.0168413    0.103647     0.0290953 
  0.0423919   -0.063977    0.183272    -0.0270091   0.0517423   -0.1115       0.00638207  -0.0828963    0.164378    -0.212838     0.208011    -0.125498     0.0210366   0.170043     0.0357233   -0.028002      0.0488005   -0.0457961   -0.0642012    0.0180914   -0.0627172     0.0798965   -0.050463    -0.0276002   -0.188856    -0.122824  
  0.127852    -0.0890147   0.0788797   -0.0534504   0.0386186   -0.0778226    0.10978     -0.0272218   -0.0367652   -0.0814634    0.0723721   -0.0307686   -0.081525   -0.163088     0.0131339    0.0183485    -0.125775     0.0615165    0.0396609   -0.0753881   -0.0930187     0.251944    -0.158508     0.0339361   -0.0245879    0.0866832 kind diag, method split
0: avll = -1.4362466936587084
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.436305
INFO: iteration 2, average log likelihood -1.436241
INFO: iteration 3, average log likelihood -1.435583
INFO: iteration 4, average log likelihood -1.427873
INFO: iteration 5, average log likelihood -1.412059
INFO: iteration 6, average log likelihood -1.407575
INFO: iteration 7, average log likelihood -1.407060
INFO: iteration 8, average log likelihood -1.406871
INFO: iteration 9, average log likelihood -1.406767
INFO: iteration 10, average log likelihood -1.406698
INFO: iteration 11, average log likelihood -1.406643
INFO: iteration 12, average log likelihood -1.406590
INFO: iteration 13, average log likelihood -1.406535
INFO: iteration 14, average log likelihood -1.406475
INFO: iteration 15, average log likelihood -1.406407
INFO: iteration 16, average log likelihood -1.406331
INFO: iteration 17, average log likelihood -1.406247
INFO: iteration 18, average log likelihood -1.406156
INFO: iteration 19, average log likelihood -1.406056
INFO: iteration 20, average log likelihood -1.405946
INFO: iteration 21, average log likelihood -1.405827
INFO: iteration 22, average log likelihood -1.405699
INFO: iteration 23, average log likelihood -1.405566
INFO: iteration 24, average log likelihood -1.405431
INFO: iteration 25, average log likelihood -1.405295
INFO: iteration 26, average log likelihood -1.405155
INFO: iteration 27, average log likelihood -1.405009
INFO: iteration 28, average log likelihood -1.404856
INFO: iteration 29, average log likelihood -1.404693
INFO: iteration 30, average log likelihood -1.404514
INFO: iteration 31, average log likelihood -1.404336
INFO: iteration 32, average log likelihood -1.404188
INFO: iteration 33, average log likelihood -1.404073
INFO: iteration 34, average log likelihood -1.403984
INFO: iteration 35, average log likelihood -1.403910
INFO: iteration 36, average log likelihood -1.403845
INFO: iteration 37, average log likelihood -1.403783
INFO: iteration 38, average log likelihood -1.403726
INFO: iteration 39, average log likelihood -1.403680
INFO: iteration 40, average log likelihood -1.403644
INFO: iteration 41, average log likelihood -1.403615
INFO: iteration 42, average log likelihood -1.403592
INFO: iteration 43, average log likelihood -1.403572
INFO: iteration 44, average log likelihood -1.403555
INFO: iteration 45, average log likelihood -1.403541
INFO: iteration 46, average log likelihood -1.403529
INFO: iteration 47, average log likelihood -1.403518
INFO: iteration 48, average log likelihood -1.403508
INFO: iteration 49, average log likelihood -1.403500
INFO: iteration 50, average log likelihood -1.403492
INFO: EM with 100000 data points 50 iterations avll -1.403492
952.4 data points per parameter
1: avll = [-1.43631,-1.43624,-1.43558,-1.42787,-1.41206,-1.40757,-1.40706,-1.40687,-1.40677,-1.4067,-1.40664,-1.40659,-1.40654,-1.40647,-1.40641,-1.40633,-1.40625,-1.40616,-1.40606,-1.40595,-1.40583,-1.4057,-1.40557,-1.40543,-1.40529,-1.40516,-1.40501,-1.40486,-1.40469,-1.40451,-1.40434,-1.40419,-1.40407,-1.40398,-1.40391,-1.40384,-1.40378,-1.40373,-1.40368,-1.40364,-1.40362,-1.40359,-1.40357,-1.40356,-1.40354,-1.40353,-1.40352,-1.40351,-1.4035,-1.40349]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.403617
INFO: iteration 2, average log likelihood -1.403501
INFO: iteration 3, average log likelihood -1.403093
INFO: iteration 4, average log likelihood -1.398082
INFO: iteration 5, average log likelihood -1.382662
INFO: iteration 6, average log likelihood -1.371288
INFO: iteration 7, average log likelihood -1.366808
INFO: iteration 8, average log likelihood -1.364837
INFO: iteration 9, average log likelihood -1.363636
INFO: iteration 10, average log likelihood -1.362775
INFO: iteration 11, average log likelihood -1.362124
INFO: iteration 12, average log likelihood -1.361634
INFO: iteration 13, average log likelihood -1.361270
INFO: iteration 14, average log likelihood -1.361007
INFO: iteration 15, average log likelihood -1.360812
INFO: iteration 16, average log likelihood -1.360656
INFO: iteration 17, average log likelihood -1.360518
INFO: iteration 18, average log likelihood -1.360384
INFO: iteration 19, average log likelihood -1.360242
INFO: iteration 20, average log likelihood -1.360080
INFO: iteration 21, average log likelihood -1.359891
INFO: iteration 22, average log likelihood -1.359663
INFO: iteration 23, average log likelihood -1.359388
INFO: iteration 24, average log likelihood -1.359072
INFO: iteration 25, average log likelihood -1.358733
INFO: iteration 26, average log likelihood -1.358394
INFO: iteration 27, average log likelihood -1.358065
INFO: iteration 28, average log likelihood -1.357756
INFO: iteration 29, average log likelihood -1.357482
INFO: iteration 30, average log likelihood -1.357260
INFO: iteration 31, average log likelihood -1.357090
INFO: iteration 32, average log likelihood -1.356960
INFO: iteration 33, average log likelihood -1.356861
INFO: iteration 34, average log likelihood -1.356783
INFO: iteration 35, average log likelihood -1.356722
INFO: iteration 36, average log likelihood -1.356673
INFO: iteration 37, average log likelihood -1.356632
INFO: iteration 38, average log likelihood -1.356597
INFO: iteration 39, average log likelihood -1.356568
INFO: iteration 40, average log likelihood -1.356544
INFO: iteration 41, average log likelihood -1.356524
INFO: iteration 42, average log likelihood -1.356509
INFO: iteration 43, average log likelihood -1.356495
INFO: iteration 44, average log likelihood -1.356484
INFO: iteration 45, average log likelihood -1.356475
INFO: iteration 46, average log likelihood -1.356466
INFO: iteration 47, average log likelihood -1.356459
INFO: iteration 48, average log likelihood -1.356453
INFO: iteration 49, average log likelihood -1.356447
INFO: iteration 50, average log likelihood -1.356442
INFO: EM with 100000 data points 50 iterations avll -1.356442
473.9 data points per parameter
2: avll = [-1.40362,-1.4035,-1.40309,-1.39808,-1.38266,-1.37129,-1.36681,-1.36484,-1.36364,-1.36277,-1.36212,-1.36163,-1.36127,-1.36101,-1.36081,-1.36066,-1.36052,-1.36038,-1.36024,-1.36008,-1.35989,-1.35966,-1.35939,-1.35907,-1.35873,-1.35839,-1.35807,-1.35776,-1.35748,-1.35726,-1.35709,-1.35696,-1.35686,-1.35678,-1.35672,-1.35667,-1.35663,-1.3566,-1.35657,-1.35654,-1.35652,-1.35651,-1.3565,-1.35648,-1.35647,-1.35647,-1.35646,-1.35645,-1.35645,-1.35644]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.356622
INFO: iteration 2, average log likelihood -1.356460
INFO: iteration 3, average log likelihood -1.356236
INFO: iteration 4, average log likelihood -1.354475
INFO: iteration 5, average log likelihood -1.345324
INFO: iteration 6, average log likelihood -1.328567
INFO: iteration 7, average log likelihood -1.318077
INFO: iteration 8, average log likelihood -1.313904
INFO: iteration 9, average log likelihood -1.311939
INFO: iteration 10, average log likelihood -1.310792
INFO: iteration 11, average log likelihood -1.310053
INFO: iteration 12, average log likelihood -1.309581
INFO: iteration 13, average log likelihood -1.309273
INFO: iteration 14, average log likelihood -1.309059
INFO: iteration 15, average log likelihood -1.308904
INFO: iteration 16, average log likelihood -1.308786
INFO: iteration 17, average log likelihood -1.308689
INFO: iteration 18, average log likelihood -1.308605
INFO: iteration 19, average log likelihood -1.308526
INFO: iteration 20, average log likelihood -1.308447
INFO: iteration 21, average log likelihood -1.308363
INFO: iteration 22, average log likelihood -1.308276
INFO: iteration 23, average log likelihood -1.308189
INFO: iteration 24, average log likelihood -1.308105
INFO: iteration 25, average log likelihood -1.308024
INFO: iteration 26, average log likelihood -1.307949
INFO: iteration 27, average log likelihood -1.307877
INFO: iteration 28, average log likelihood -1.307808
INFO: iteration 29, average log likelihood -1.307741
INFO: iteration 30, average log likelihood -1.307678
INFO: iteration 31, average log likelihood -1.307620
INFO: iteration 32, average log likelihood -1.307567
INFO: iteration 33, average log likelihood -1.307518
INFO: iteration 34, average log likelihood -1.307471
INFO: iteration 35, average log likelihood -1.307422
INFO: iteration 36, average log likelihood -1.307370
INFO: iteration 37, average log likelihood -1.307310
INFO: iteration 38, average log likelihood -1.307238
INFO: iteration 39, average log likelihood -1.307151
INFO: iteration 40, average log likelihood -1.307048
INFO: iteration 41, average log likelihood -1.306928
INFO: iteration 42, average log likelihood -1.306792
INFO: iteration 43, average log likelihood -1.306642
INFO: iteration 44, average log likelihood -1.306489
INFO: iteration 45, average log likelihood -1.306349
INFO: iteration 46, average log likelihood -1.306238
INFO: iteration 47, average log likelihood -1.306159
INFO: iteration 48, average log likelihood -1.306105
INFO: iteration 49, average log likelihood -1.306069
INFO: iteration 50, average log likelihood -1.306045
INFO: EM with 100000 data points 50 iterations avll -1.306045
236.4 data points per parameter
3: avll = [-1.35662,-1.35646,-1.35624,-1.35448,-1.34532,-1.32857,-1.31808,-1.3139,-1.31194,-1.31079,-1.31005,-1.30958,-1.30927,-1.30906,-1.3089,-1.30879,-1.30869,-1.3086,-1.30853,-1.30845,-1.30836,-1.30828,-1.30819,-1.3081,-1.30802,-1.30795,-1.30788,-1.30781,-1.30774,-1.30768,-1.30762,-1.30757,-1.30752,-1.30747,-1.30742,-1.30737,-1.30731,-1.30724,-1.30715,-1.30705,-1.30693,-1.30679,-1.30664,-1.30649,-1.30635,-1.30624,-1.30616,-1.30611,-1.30607,-1.30605]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.306227
INFO: iteration 2, average log likelihood -1.306013
INFO: iteration 3, average log likelihood -1.305530
INFO: iteration 4, average log likelihood -1.299981
INFO: iteration 5, average log likelihood -1.271954
INFO: iteration 6, average log likelihood -1.241967
WARNING: Variances had to be floored 7
INFO: iteration 7, average log likelihood -1.226363
WARNING: Variances had to be floored 8
INFO: iteration 8, average log likelihood -1.227650
INFO: iteration 9, average log likelihood -1.231340
INFO: iteration 10, average log likelihood -1.220913
WARNING: Variances had to be floored 7
INFO: iteration 11, average log likelihood -1.215186
WARNING: Variances had to be floored 8 13
INFO: iteration 12, average log likelihood -1.220888
INFO: iteration 13, average log likelihood -1.231271
INFO: iteration 14, average log likelihood -1.217590
WARNING: Variances had to be floored 7
INFO: iteration 15, average log likelihood -1.211179
WARNING: Variances had to be floored 8
INFO: iteration 16, average log likelihood -1.216804
INFO: iteration 17, average log likelihood -1.220839
INFO: iteration 18, average log likelihood -1.211215
WARNING: Variances had to be floored 7
INFO: iteration 19, average log likelihood -1.206730
INFO: iteration 20, average log likelihood -1.213909
WARNING: Variances had to be floored 8 13
INFO: iteration 21, average log likelihood -1.206903
INFO: iteration 22, average log likelihood -1.221845
WARNING: Variances had to be floored 7
INFO: iteration 23, average log likelihood -1.209737
INFO: iteration 24, average log likelihood -1.216264
WARNING: Variances had to be floored 8
INFO: iteration 25, average log likelihood -1.208503
INFO: iteration 26, average log likelihood -1.215494
WARNING: Variances had to be floored 7
INFO: iteration 27, average log likelihood -1.206785
WARNING: Variances had to be floored 13
INFO: iteration 28, average log likelihood -1.213929
INFO: iteration 29, average log likelihood -1.212525
WARNING: Variances had to be floored 8
INFO: iteration 30, average log likelihood -1.205071
WARNING: Variances had to be floored 7
INFO: iteration 31, average log likelihood -1.213448
INFO: iteration 32, average log likelihood -1.216842
INFO: iteration 33, average log likelihood -1.208496
WARNING: Variances had to be floored 8
INFO: iteration 34, average log likelihood -1.203681
WARNING: Variances had to be floored 7 13
INFO: iteration 35, average log likelihood -1.211029
INFO: iteration 36, average log likelihood -1.221397
INFO: iteration 37, average log likelihood -1.209632
WARNING: Variances had to be floored 8
INFO: iteration 38, average log likelihood -1.204360
WARNING: Variances had to be floored 7
INFO: iteration 39, average log likelihood -1.211821
INFO: iteration 40, average log likelihood -1.215756
INFO: iteration 41, average log likelihood -1.207253
WARNING: Variances had to be floored 8 13
INFO: iteration 42, average log likelihood -1.202330
WARNING: Variances had to be floored 7
INFO: iteration 43, average log likelihood -1.216554
INFO: iteration 44, average log likelihood -1.217094
INFO: iteration 45, average log likelihood -1.208296
WARNING: Variances had to be floored 8
INFO: iteration 46, average log likelihood -1.203556
WARNING: Variances had to be floored 7
INFO: iteration 47, average log likelihood -1.211368
INFO: iteration 48, average log likelihood -1.215343
WARNING: Variances had to be floored 13
INFO: iteration 49, average log likelihood -1.206816
WARNING: Variances had to be floored 8
INFO: iteration 50, average log likelihood -1.208479
INFO: EM with 100000 data points 50 iterations avll -1.208479
118.1 data points per parameter
4: avll = [-1.30623,-1.30601,-1.30553,-1.29998,-1.27195,-1.24197,-1.22636,-1.22765,-1.23134,-1.22091,-1.21519,-1.22089,-1.23127,-1.21759,-1.21118,-1.2168,-1.22084,-1.21121,-1.20673,-1.21391,-1.2069,-1.22185,-1.20974,-1.21626,-1.2085,-1.21549,-1.20679,-1.21393,-1.21253,-1.20507,-1.21345,-1.21684,-1.2085,-1.20368,-1.21103,-1.2214,-1.20963,-1.20436,-1.21182,-1.21576,-1.20725,-1.20233,-1.21655,-1.21709,-1.2083,-1.20356,-1.21137,-1.21534,-1.20682,-1.20848]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 13 14
INFO: iteration 1, average log likelihood -1.212995
WARNING: Variances had to be floored 13 14
INFO: iteration 2, average log likelihood -1.206077
WARNING: Variances had to be floored 13 14
INFO: iteration 3, average log likelihood -1.201445
WARNING: Variances had to be floored 13 14 16
INFO: iteration 4, average log likelihood -1.175186
WARNING: Variances had to be floored 11 13 14 15 25 30
INFO: iteration 5, average log likelihood -1.125226
WARNING: Variances had to be floored 13 14 27 29
INFO: iteration 6, average log likelihood -1.124089
WARNING: Variances had to be floored 3 11 13 14 26 30 32
INFO: iteration 7, average log likelihood -1.106160
WARNING: Variances had to be floored 13 14 15 18
INFO: iteration 8, average log likelihood -1.118719
WARNING: Variances had to be floored 13 14 27 29 30
INFO: iteration 9, average log likelihood -1.111995
WARNING: Variances had to be floored 11 13 14
INFO: iteration 10, average log likelihood -1.105781
WARNING: Variances had to be floored 3 10 13 14 15 26 32
INFO: iteration 11, average log likelihood -1.091739
WARNING: Variances had to be floored 13 14 18 27 29 30
INFO: iteration 12, average log likelihood -1.098992
WARNING: Variances had to be floored 11 13 14
INFO: iteration 13, average log likelihood -1.104926
WARNING: Variances had to be floored 13 14 15 32
INFO: iteration 14, average log likelihood -1.094291
WARNING: Variances had to be floored 3 13 14 26 27 29 30
INFO: iteration 15, average log likelihood -1.082773
WARNING: Variances had to be floored 11 13 14 18
INFO: iteration 16, average log likelihood -1.096229
WARNING: Variances had to be floored 10 13 14 15 32
INFO: iteration 17, average log likelihood -1.100508
WARNING: Variances had to be floored 13 14 26 27 29 30
INFO: iteration 18, average log likelihood -1.094527
WARNING: Variances had to be floored 3 11 13 14
INFO: iteration 19, average log likelihood -1.091053
WARNING: Variances had to be floored 13 14 15 18 32
INFO: iteration 20, average log likelihood -1.092789
WARNING: Variances had to be floored 13 14 26 27 29 30
INFO: iteration 21, average log likelihood -1.092356
WARNING: Variances had to be floored 11 13 14
INFO: iteration 22, average log likelihood -1.088465
WARNING: Variances had to be floored 3 13 14 15
INFO: iteration 23, average log likelihood -1.079219
WARNING: Variances had to be floored 10 11 13 14 18 26 27 29 30 32
INFO: iteration 24, average log likelihood -1.071563
WARNING: Variances had to be floored 13 14
INFO: iteration 25, average log likelihood -1.118610
WARNING: Variances had to be floored 11 13 14
INFO: iteration 26, average log likelihood -1.078962
WARNING: Variances had to be floored 3 13 14 15 26 27 29 30 32
INFO: iteration 27, average log likelihood -1.065309
WARNING: Variances had to be floored 11 13 14 18
INFO: iteration 28, average log likelihood -1.101220
WARNING: Variances had to be floored 13 14
INFO: iteration 29, average log likelihood -1.094117
WARNING: Variances had to be floored 10 11 13 14 15 26 27 29 30 32
INFO: iteration 30, average log likelihood -1.057293
WARNING: Variances had to be floored 3 13 14
INFO: iteration 31, average log likelihood -1.109672
WARNING: Variances had to be floored 11 13 14 18
INFO: iteration 32, average log likelihood -1.080269
WARNING: Variances had to be floored 13 14 15 26 27 29 30 32
INFO: iteration 33, average log likelihood -1.079177
WARNING: Variances had to be floored 11 13 14
INFO: iteration 34, average log likelihood -1.099539
WARNING: Variances had to be floored 3 13 14
INFO: iteration 35, average log likelihood -1.078532
WARNING: Variances had to be floored 10 11 13 14 15 18 26 27 29 30 32
INFO: iteration 36, average log likelihood -1.056800
WARNING: Variances had to be floored 13 14
INFO: iteration 37, average log likelihood -1.123941
WARNING: Variances had to be floored 11 13 14
INFO: iteration 38, average log likelihood -1.080734
WARNING: Variances had to be floored 3 13 14 15 26 27 29 30 32
INFO: iteration 39, average log likelihood -1.064964
WARNING: Variances had to be floored 11 13 14 18
INFO: iteration 40, average log likelihood -1.100938
WARNING: Variances had to be floored 13 14
INFO: iteration 41, average log likelihood -1.093762
WARNING: Variances had to be floored 10 11 13 14 15 26 27 29 30 32
INFO: iteration 42, average log likelihood -1.056772
WARNING: Variances had to be floored 3 13 14
INFO: iteration 43, average log likelihood -1.109621
WARNING: Variances had to be floored 11 13 14 18
INFO: iteration 44, average log likelihood -1.080202
WARNING: Variances had to be floored 13 14 15 26 27 29 30 32
INFO: iteration 45, average log likelihood -1.079116
WARNING: Variances had to be floored 11 13 14
INFO: iteration 46, average log likelihood -1.099520
WARNING: Variances had to be floored 3 13 14
INFO: iteration 47, average log likelihood -1.078502
WARNING: Variances had to be floored 10 11 13 14 15 18 26 27 29 30 32
INFO: iteration 48, average log likelihood -1.056766
WARNING: Variances had to be floored 13 14
INFO: iteration 49, average log likelihood -1.123935
WARNING: Variances had to be floored 11 13 14
INFO: iteration 50, average log likelihood -1.080725
INFO: EM with 100000 data points 50 iterations avll -1.080725
59.0 data points per parameter
5: avll = [-1.21299,-1.20608,-1.20144,-1.17519,-1.12523,-1.12409,-1.10616,-1.11872,-1.11199,-1.10578,-1.09174,-1.09899,-1.10493,-1.09429,-1.08277,-1.09623,-1.10051,-1.09453,-1.09105,-1.09279,-1.09236,-1.08847,-1.07922,-1.07156,-1.11861,-1.07896,-1.06531,-1.10122,-1.09412,-1.05729,-1.10967,-1.08027,-1.07918,-1.09954,-1.07853,-1.0568,-1.12394,-1.08073,-1.06496,-1.10094,-1.09376,-1.05677,-1.10962,-1.0802,-1.07912,-1.09952,-1.0785,-1.05677,-1.12393,-1.08073]
[-1.43625,-1.43631,-1.43624,-1.43558,-1.42787,-1.41206,-1.40757,-1.40706,-1.40687,-1.40677,-1.4067,-1.40664,-1.40659,-1.40654,-1.40647,-1.40641,-1.40633,-1.40625,-1.40616,-1.40606,-1.40595,-1.40583,-1.4057,-1.40557,-1.40543,-1.40529,-1.40516,-1.40501,-1.40486,-1.40469,-1.40451,-1.40434,-1.40419,-1.40407,-1.40398,-1.40391,-1.40384,-1.40378,-1.40373,-1.40368,-1.40364,-1.40362,-1.40359,-1.40357,-1.40356,-1.40354,-1.40353,-1.40352,-1.40351,-1.4035,-1.40349,-1.40362,-1.4035,-1.40309,-1.39808,-1.38266,-1.37129,-1.36681,-1.36484,-1.36364,-1.36277,-1.36212,-1.36163,-1.36127,-1.36101,-1.36081,-1.36066,-1.36052,-1.36038,-1.36024,-1.36008,-1.35989,-1.35966,-1.35939,-1.35907,-1.35873,-1.35839,-1.35807,-1.35776,-1.35748,-1.35726,-1.35709,-1.35696,-1.35686,-1.35678,-1.35672,-1.35667,-1.35663,-1.3566,-1.35657,-1.35654,-1.35652,-1.35651,-1.3565,-1.35648,-1.35647,-1.35647,-1.35646,-1.35645,-1.35645,-1.35644,-1.35662,-1.35646,-1.35624,-1.35448,-1.34532,-1.32857,-1.31808,-1.3139,-1.31194,-1.31079,-1.31005,-1.30958,-1.30927,-1.30906,-1.3089,-1.30879,-1.30869,-1.3086,-1.30853,-1.30845,-1.30836,-1.30828,-1.30819,-1.3081,-1.30802,-1.30795,-1.30788,-1.30781,-1.30774,-1.30768,-1.30762,-1.30757,-1.30752,-1.30747,-1.30742,-1.30737,-1.30731,-1.30724,-1.30715,-1.30705,-1.30693,-1.30679,-1.30664,-1.30649,-1.30635,-1.30624,-1.30616,-1.30611,-1.30607,-1.30605,-1.30623,-1.30601,-1.30553,-1.29998,-1.27195,-1.24197,-1.22636,-1.22765,-1.23134,-1.22091,-1.21519,-1.22089,-1.23127,-1.21759,-1.21118,-1.2168,-1.22084,-1.21121,-1.20673,-1.21391,-1.2069,-1.22185,-1.20974,-1.21626,-1.2085,-1.21549,-1.20679,-1.21393,-1.21253,-1.20507,-1.21345,-1.21684,-1.2085,-1.20368,-1.21103,-1.2214,-1.20963,-1.20436,-1.21182,-1.21576,-1.20725,-1.20233,-1.21655,-1.21709,-1.2083,-1.20356,-1.21137,-1.21534,-1.20682,-1.20848,-1.21299,-1.20608,-1.20144,-1.17519,-1.12523,-1.12409,-1.10616,-1.11872,-1.11199,-1.10578,-1.09174,-1.09899,-1.10493,-1.09429,-1.08277,-1.09623,-1.10051,-1.09453,-1.09105,-1.09279,-1.09236,-1.08847,-1.07922,-1.07156,-1.11861,-1.07896,-1.06531,-1.10122,-1.09412,-1.05729,-1.10967,-1.08027,-1.07918,-1.09954,-1.07853,-1.0568,-1.12394,-1.08073,-1.06496,-1.10094,-1.09376,-1.05677,-1.10962,-1.0802,-1.07912,-1.09952,-1.0785,-1.05677,-1.12393,-1.08073]
32×26 Array{Float64,2}:
  0.162229    -0.0881776    0.0840687   -0.0438977    0.0758832    -0.0895933    0.0935158    -0.0260111    -0.0167329   -0.0774195   0.0757846  -0.0329031   -0.0814542   -0.168142     0.017355   -0.0333007    -0.125568      0.0546392     0.040423    -0.0349165    -0.113471     0.258981    -0.162292     0.0492404  -0.0251887    0.117856  
 -0.0332531    0.165534     0.159493    -0.0371474    0.00264654    0.00513628  -0.141933      0.01511      -0.130521    -0.106421    0.028924    0.06203      0.0807638    0.0339285   -0.10934     0.079425      0.00357189    0.260793      0.0442219    0.130316     -0.122334    -0.0110338   -0.0227663   -0.0572887   0.00934229  -0.050555  
  0.0688111   -0.018043    -0.0906763    0.150885     0.0722377     0.0368914    0.0443859    -0.0237296    -0.00831793   0.028869    0.0679177  -0.139054    -0.0675463   -0.0792917    0.121362    0.0857918    -0.142549     -0.0593467    -0.0230469   -0.039188     -0.0384394    0.0584601    0.0566167    0.0197246  -0.00885368   0.17919   
  0.0929198    0.076207    -0.130408     0.0619859   -0.129484      0.215197    -0.0897042     0.178612     -0.215927     0.275449    0.0136859   0.0403183    0.03746     -0.0498815   -0.120628   -0.0960021     0.208077      0.00643211    0.191969    -0.215814      0.0522573    0.190408    -0.0817931   -0.0617083  -0.0359272    0.0301201 
  0.0636563    0.0136123   -0.0318958   -0.0986066   -0.0511611     0.0447631    0.0961017     0.0300087    -0.00884325   0.0605819   0.015397    0.041725     0.0543266   -0.0837491    0.145463   -0.0815059    -0.00856878   -0.0895885    -0.0620975   -0.0757202     0.0350171   -0.110487    -0.0114076   -0.0803886  -0.118541    -0.0765585 
  0.112527    -0.00514179   0.0835289    0.165652     0.11401      -0.117989    -0.000449945   0.0796467    -0.138236     0.0468172  -0.101368    0.104789    -0.107957     0.0267953   -0.0206281   0.0603547    -0.0967674    -0.0679058    -0.0823661   -0.0714012     0.110619     0.0954769    0.0129804    0.125606   -0.0726008    0.00931416
  0.0501443    0.178932    -0.0684981   -0.101171    -0.112935      0.159278     0.103962      0.0696743     0.221445     0.264605   -0.0727943  -0.0728106    0.125406    -0.0564173    0.141823   -0.0337382     0.144121      0.202488      0.0797628    0.0408138    -0.14971     -0.157849     0.0810937   -0.140009    0.0576232    0.103161  
  0.218415    -0.0316577   -0.304688     0.0593075    0.294255      0.0237412    0.0312306     0.0671921     0.0560961    0.522188   -0.0293153  -0.0840807    0.144462    -0.173583     0.128144   -0.0318175    -0.0338874     0.242944     -0.044209    -0.0495482     0.686308    -0.149932     0.0528237    0.0253048   0.0574429    0.169285  
  0.0520701    0.0434477    0.0268633   -0.0812709   -0.115135      0.0638584    0.12045       0.000963036  -0.0145386    0.0383345  -0.14955    -0.0332383    0.107733    -0.159482    -0.0229059   0.132321      0.0846597    -0.0697558     0.0923203    0.0439345     0.131674    -0.0444605    0.0283251    0.0489068   0.0184064    0.0223915 
 -0.0537559   -0.051074     0.00644395   0.0206779    0.0819664    -0.0290788   -0.0813875    -0.0245697    -0.150689    -0.0164557   0.0369281  -0.0361571    0.065654     0.127798     0.176829   -0.0598252    -0.0210298     0.0125166    -0.0312618    0.0227209    -0.0813451    0.00517802  -0.0821513    0.0351814  -0.0325997   -0.0271122 
 -0.0867145    0.09319      0.0895086   -0.128719     0.00125595    0.0668836   -0.0685689     0.0990256    -0.171554    -0.107174    0.0759292  -0.152963     0.127284     0.0357539    0.112946   -0.156288     -0.0917911    -0.0288461     0.0546809   -0.00910272   -0.156367    -0.0420531    0.0118205    0.0200089   0.0305536    0.049242  
  0.0237026   -0.0197679   -0.0999652   -0.0132657   -0.129936      0.116836    -0.142115      0.0518704     0.0100392   -0.0148634   0.0023138   0.109731    -0.0708268    0.0524648   -0.0717201  -0.0459763    -0.0177246    -0.0213271    -0.0159946   -0.000682026  -0.0195805   -0.00139528  -0.00777292   0.0418793   0.099866    -0.240297  
  0.0320016    0.110767     0.167635    -0.0529518   -0.0656391     0.0978273   -0.107496     -0.609318      0.255709     0.396009    0.125265    0.157273     0.0227001    0.310467    -0.0946483   0.00647857   -0.374596      0.0179584    -0.0932263   -0.0521229    -0.0857635   -0.0742041    0.0966559   -0.183208    0.229633    -0.0332383 
  0.083357     0.113906    -0.14459     -0.0460868   -0.0345968    -0.0397481   -0.106298      0.428065      0.250071     0.0591822  -0.182453    0.16079      0.0225824    0.166808    -0.119303   -0.0430659     0.0247631    -0.0152151    -0.128266    -0.0789992    -0.0799335   -0.0522438    0.154651    -0.186012   -0.163894    -0.00537112
 -0.0951601    0.016452    -0.0291156   -0.0297694   -0.0568076    -0.0585622   -0.0805468     0.112901     -0.0128068    0.0399029   0.0250888   0.0643946    0.208353     0.0911852   -0.0708097   0.0172806     0.106481      0.0289193    -0.0512167    0.047535      0.0228782    0.044141    -0.0598326    0.287014    0.0233303    0.0584077 
 -0.0435316   -0.0481221   -0.0118178    0.273756    -0.0881284     0.185114    -0.0297033    -0.0310148    -0.0248856    0.0544149  -0.241051   -0.237962     0.0771148   -0.153832    -0.0652914   0.0045871    -0.0145284     0.0908295    -0.0223222   -0.0653348     0.131645     0.0732524   -0.173791    -0.0648525   0.00206672   0.0732513 
  0.0044193   -0.0719771   -0.00401637  -0.0395983    0.131602      0.00539185   0.0211047    -0.0378931     0.0963245   -0.0951088   0.120197    0.0187866    0.112282     0.114169     0.0510636  -0.054475      0.0590718    -0.0874668     0.0570573   -0.0364596    -0.09811      0.0454234   -0.0392912   -0.0828208  -0.10237     -0.06631   
 -0.267856    -0.084001    -0.0569832   -0.0365748   -0.0908263     0.00903271  -0.0168957     0.0883449    -0.208072    -0.0269428   0.0735916  -0.061361    -0.0588642   -0.0477472   -0.0354676   0.0246406     0.152781     -0.0509789     0.0701605    0.0599427     0.0714486    0.0753701    0.0498987   -0.0115999  -0.0196098    0.0303408 
 -0.0683198    0.0885763   -0.116179     0.0301786    0.128423     -0.0873917   -0.145498     -0.0876519     0.0096253   -0.0810289   0.0536697   0.0952021    0.0737992    0.00834291  -0.0140737  -0.00629104    0.0317951    -0.164443     -0.0305011    0.0574479     0.036021    -0.145115     0.188094    -0.0904499   0.0454061    0.0132863 
  0.0242974    0.166947     0.0342997    0.066099     0.0180858     0.0208754    0.0616383     0.0908155     0.0601983    0.0724254   0.200736   -0.0179296    0.0220487   -0.161554    -0.0221656   0.0728679     0.10632      -0.0186187     0.22844     -0.16637      -0.108662     0.0746505   -0.0429033   -0.0179626   0.0926391    0.0287813 
 -0.0453942    0.0525414   -0.11474      0.0144215    0.0456897    -0.180927    -0.0926788     0.0397008     0.107233    -0.0108903   0.0382209  -0.276601     0.0245289   -0.0573389   -0.141998   -0.0682473    -0.0422526    -0.0877015     0.0821336   -0.207557     -0.014739     0.066653    -0.0892017   -0.0480649   0.0252426   -0.0605748 
 -0.0917318   -0.0837367   -0.0491692    0.0616948    0.0925838    -0.111559    -0.0494514     0.0221574     0.169222    -0.0264672   0.0408636   0.292789     0.0219932   -0.0821627   -0.112185   -0.0555971    -0.131648      0.119173     -0.0811249   -0.27012      -0.229292     0.0207197   -0.0215844   -0.153541   -0.065562    -0.02545   
 -0.0707408    0.0472042   -0.090868     0.0627421   -0.263242     -0.0190829   -0.0931696    -0.423732     -0.0416774    0.199805    0.124654    0.181627     0.0476216    0.134191    -0.0431686  -0.0177699     0.0340194     0.000396218  -0.0996458    0.174547      0.0405011   -0.0335083    0.0666277    0.130205   -0.182909     0.106216  
 -0.0590595   -0.0300991    0.0334818    0.0906436   -0.195004     -0.0220312   -0.136525      0.257984     -0.134563     0.191334    0.166599   -0.00655145   0.0360589    0.158992    -0.067983    0.000787609  -0.0211419     0.169194     -0.157285     0.175044      0.0524753    0.0369403   -0.171589     0.11417     0.13739     -0.0698366 
 -0.026376    -0.0904079   -0.0101754    0.0336215   -0.114682      0.175024     0.0367609    -0.0795006     0.0696997   -0.0678647   0.0707772   0.0635824    0.0889645    0.0693902   -0.129881    0.0999167    -0.154044     -0.0379865     0.0207256    0.0493601    -0.0571317    0.0377863    0.112553     0.0992832   0.129335    -0.0399966 
 -0.125334     0.022131     0.0919123    0.0262246   -0.0886915    -0.0162155    0.0496433    -0.116562     -0.136837    -0.0668298  -0.0646019   0.0109481   -0.0619139    0.0357553   -0.241126   -0.00164554   -0.000213929   0.0238144     0.107123    -0.0477882     0.0581132    0.0228378   -0.114924     0.0857872   0.0687946   -0.0621337 
  0.0359404   -0.0497117   -0.150334    -0.0028818    0.160558     -0.100036     0.125319      0.207643     -0.0696983   -0.0388414   0.083747    0.0755732    0.0625402   -0.052167     0.14536     0.0847842    -0.0292018    -0.194198      0.0489922    0.116535     -0.0809472   -0.016522    -0.184448     0.0941597   0.234879     0.0709356 
 -0.012372     0.103783    -0.0828002    0.140651     0.012277      0.0222152   -0.0722024     0.0221327     0.136894    -0.117875    0.0716223   0.0330698   -0.00632771  -0.13072     -0.115594    0.152125     -0.106496     -0.0237953    -0.0708218   -0.140414      0.0609156    0.0386154    0.116327     0.0493204  -0.0245787   -0.0275071 
 -0.032548     0.0252889   -0.0343817    0.00999064   0.173237     -0.0808084   -0.143504      0.0247291    -0.0067495   -0.0189245  -0.0212221   0.192566    -0.120965     0.0493726   -0.219611    0.138063      0.0385084     0.108242      0.00356153   0.0267593     0.00154668  -0.0689954    0.053879    -0.0479365   0.0240339    0.0699698 
  0.134684    -0.0169628   -0.0358445    0.236056     0.00611492    0.0820257   -0.0895711    -0.0595459    -0.25059      0.218591    0.138772    0.186563    -0.0272228   -0.177926     0.130583   -0.113151      0.00978545    0.031548     -0.0687953   -0.1016        0.0429751    0.00483009   0.145394     0.0717311  -0.0288982    0.0233171 
 -0.00558702  -0.00139736   0.102283     0.074487    -0.000688549  -0.0983814    0.0239536    -0.0195841    -0.167454     0.151035   -0.0627697   0.0138978    0.0218404   -0.290261    -0.0673097   0.016208     -0.0135122    -0.107532      0.0903427   -0.133019      0.0171887    0.0797086   -0.11999      0.212052    0.0252317    0.00588919
  0.143699     0.0140155    0.0869256   -0.135772     0.0205236    -0.118696     0.0281934     0.010643      0.0405932   -0.0337819   0.081072    0.0585672   -0.141594     0.168412    -0.102349    0.0659663     0.071718     -0.0342899     0.0650416   -0.0272459     0.0986942    0.218192    -0.00817057  -0.048169    0.0368162   -0.0342559 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 13 14 15 26 27 29 30 32
INFO: iteration 1, average log likelihood -1.064956
WARNING: Variances had to be floored 3 11 13 14 15 18 26 27 29 30 32
INFO: iteration 2, average log likelihood -1.052074
WARNING: Variances had to be floored 3 13 14 15 26 27 29 30 32
INFO: iteration 3, average log likelihood -1.062636
WARNING: Variances had to be floored 3 10 11 13 14 15 18 26 27 29 30 32
INFO: iteration 4, average log likelihood -1.048274
WARNING: Variances had to be floored 3 13 14 15 26 27 29 30 32
INFO: iteration 5, average log likelihood -1.064882
WARNING: Variances had to be floored 3 11 13 14 15 18 26 27 29 30 32
INFO: iteration 6, average log likelihood -1.050752
WARNING: Variances had to be floored 3 10 13 14 15 26 27 29 30 32
INFO: iteration 7, average log likelihood -1.061136
WARNING: Variances had to be floored 3 11 13 14 15 18 26 27 29 30 32
INFO: iteration 8, average log likelihood -1.052547
WARNING: Variances had to be floored 3 13 14 15 26 27 29 30 32
INFO: iteration 9, average log likelihood -1.062837
WARNING: Variances had to be floored 3 10 11 13 14 15 18 26 27 29 30 32
INFO: iteration 10, average log likelihood -1.048501
INFO: EM with 100000 data points 10 iterations avll -1.048501
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.645413e+05
      1       7.164363e+05      -2.481050e+05 |       32
      2       6.873750e+05      -2.906127e+04 |       32
      3       6.683748e+05      -1.900025e+04 |       32
      4       6.564539e+05      -1.192090e+04 |       32
      5       6.503886e+05      -6.065217e+03 |       32
      6       6.463311e+05      -4.057593e+03 |       32
      7       6.431255e+05      -3.205532e+03 |       32
      8       6.406027e+05      -2.522807e+03 |       32
      9       6.383985e+05      -2.204205e+03 |       32
     10       6.369633e+05      -1.435228e+03 |       32
     11       6.358613e+05      -1.101972e+03 |       32
     12       6.348185e+05      -1.042808e+03 |       32
     13       6.339541e+05      -8.644144e+02 |       32
     14       6.333327e+05      -6.213515e+02 |       32
     15       6.330036e+05      -3.291611e+02 |       32
     16       6.328742e+05      -1.293225e+02 |       32
     17       6.327998e+05      -7.441471e+01 |       32
     18       6.327438e+05      -5.607170e+01 |       32
     19       6.327127e+05      -3.103763e+01 |       31
     20       6.326987e+05      -1.402890e+01 |       31
     21       6.326890e+05      -9.670837e+00 |       29
     22       6.326828e+05      -6.246994e+00 |       27
     23       6.326766e+05      -6.220895e+00 |       25
     24       6.326711e+05      -5.490045e+00 |       27
     25       6.326669e+05      -4.162560e+00 |       28
     26       6.326633e+05      -3.567543e+00 |       22
     27       6.326609e+05      -2.476698e+00 |       23
     28       6.326584e+05      -2.435644e+00 |       19
     29       6.326571e+05      -1.275947e+00 |       17
     30       6.326561e+05      -1.044888e+00 |       16
     31       6.326552e+05      -9.392868e-01 |       15
     32       6.326547e+05      -4.996537e-01 |       11
     33       6.326542e+05      -5.046205e-01 |       10
     34       6.326537e+05      -4.864733e-01 |        7
     35       6.326533e+05      -3.307150e-01 |        6
     36       6.326531e+05      -2.503956e-01 |        7
     37       6.326529e+05      -1.669515e-01 |        8
     38       6.326526e+05      -3.507244e-01 |       11
     39       6.326522e+05      -3.587459e-01 |        7
     40       6.326518e+05      -3.923692e-01 |       11
     41       6.326514e+05      -3.823143e-01 |       10
     42       6.326511e+05      -3.349110e-01 |       13
     43       6.326507e+05      -3.726665e-01 |        7
     44       6.326505e+05      -2.407314e-01 |        5
     45       6.326504e+05      -8.366654e-02 |        2
     46       6.326503e+05      -7.544493e-02 |        4
     47       6.326503e+05      -6.328797e-02 |        6
     48       6.326501e+05      -1.403303e-01 |        5
     49       6.326500e+05      -1.677017e-01 |        8
     50       6.326497e+05      -2.316727e-01 |        8
K-means terminated without convergence after 50 iterations (objv = 632649.7315602272)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.351365
INFO: iteration 2, average log likelihood -1.324717
INFO: iteration 3, average log likelihood -1.299982
INFO: iteration 4, average log likelihood -1.272947
INFO: iteration 5, average log likelihood -1.238875
INFO: iteration 6, average log likelihood -1.192129
WARNING: Variances had to be floored 1 31
INFO: iteration 7, average log likelihood -1.134384
WARNING: Variances had to be floored 7 13 15 24 25
INFO: iteration 8, average log likelihood -1.110039
WARNING: Variances had to be floored 5 10 27
INFO: iteration 9, average log likelihood -1.130171
WARNING: Variances had to be floored 6 22
INFO: iteration 10, average log likelihood -1.113979
WARNING: Variances had to be floored 1 8 15 24 31
INFO: iteration 11, average log likelihood -1.079358
WARNING: Variances had to be floored 2 7 25
INFO: iteration 12, average log likelihood -1.114359
WARNING: Variances had to be floored 10 27
INFO: iteration 13, average log likelihood -1.103310
WARNING: Variances had to be floored 5 22 24
INFO: iteration 14, average log likelihood -1.088194
WARNING: Variances had to be floored 1 8 15
INFO: iteration 15, average log likelihood -1.082310
WARNING: Variances had to be floored 7 13 25 31
INFO: iteration 16, average log likelihood -1.078293
WARNING: Variances had to be floored 10 22 24 27
INFO: iteration 17, average log likelihood -1.094218
INFO: iteration 18, average log likelihood -1.111620
WARNING: Variances had to be floored 1 5 6 8 15
INFO: iteration 19, average log likelihood -1.065683
WARNING: Variances had to be floored 7 24 31
INFO: iteration 20, average log likelihood -1.096866
WARNING: Variances had to be floored 10 13 22 25 27
INFO: iteration 21, average log likelihood -1.105174
INFO: iteration 22, average log likelihood -1.126368
WARNING: Variances had to be floored 1 2 24
INFO: iteration 23, average log likelihood -1.075508
WARNING: Variances had to be floored 5 7 8 10 15 22 31
INFO: iteration 24, average log likelihood -1.071153
WARNING: Variances had to be floored 27
INFO: iteration 25, average log likelihood -1.119209
WARNING: Variances had to be floored 13 24 25
INFO: iteration 26, average log likelihood -1.091904
WARNING: Variances had to be floored 1
INFO: iteration 27, average log likelihood -1.100477
WARNING: Variances had to be floored 5 6 7 10 22 27
INFO: iteration 28, average log likelihood -1.065294
WARNING: Variances had to be floored 2 8 15 24
INFO: iteration 29, average log likelihood -1.110633
WARNING: Variances had to be floored 25 31
INFO: iteration 30, average log likelihood -1.115263
WARNING: Variances had to be floored 1
INFO: iteration 31, average log likelihood -1.101500
WARNING: Variances had to be floored 13 22 27
INFO: iteration 32, average log likelihood -1.081092
WARNING: Variances had to be floored 2 5 7 10 24
INFO: iteration 33, average log likelihood -1.078043
WARNING: Variances had to be floored 8 15
INFO: iteration 34, average log likelihood -1.103406
WARNING: Variances had to be floored 1 6 25
INFO: iteration 35, average log likelihood -1.083054
WARNING: Variances had to be floored 22 24 27 31
INFO: iteration 36, average log likelihood -1.089529
WARNING: Variances had to be floored 5 7 10 13
INFO: iteration 37, average log likelihood -1.105053
WARNING: Variances had to be floored 8 15
INFO: iteration 38, average log likelihood -1.116700
WARNING: Variances had to be floored 1 24
INFO: iteration 39, average log likelihood -1.094715
WARNING: Variances had to be floored 22 25 27
INFO: iteration 40, average log likelihood -1.085849
WARNING: Variances had to be floored 6 7 10 31
INFO: iteration 41, average log likelihood -1.073778
WARNING: Variances had to be floored 5 8 13 15 24
INFO: iteration 42, average log likelihood -1.097518
WARNING: Variances had to be floored 1
INFO: iteration 43, average log likelihood -1.133075
WARNING: Variances had to be floored 22 27
INFO: iteration 44, average log likelihood -1.108522
WARNING: Variances had to be floored 2 24 25
INFO: iteration 45, average log likelihood -1.082743
WARNING: Variances had to be floored 5 7 8 10 15
INFO: iteration 46, average log likelihood -1.070862
WARNING: Variances had to be floored 1 6 13 31
INFO: iteration 47, average log likelihood -1.096816
WARNING: Variances had to be floored 22 24 27
INFO: iteration 48, average log likelihood -1.121410
INFO: iteration 49, average log likelihood -1.122473
WARNING: Variances had to be floored 7 10 25
INFO: iteration 50, average log likelihood -1.067427
INFO: EM with 100000 data points 50 iterations avll -1.067427
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0931357    0.0181487   -0.0337616   -0.0281485   -0.060374     -0.0583444   -0.0771409   0.112879   -0.00565706   0.0348126    0.026725     0.0573402   0.221987     0.0857742   -0.0750273    0.0132507    0.112575      0.0370895   -0.0490454     0.0595802     0.0213318   0.0439799    -0.0554218    0.289418     0.0296977    0.0610729 
  0.0817912   -0.0218789    0.0492608   -0.096525    -0.131034      0.142002     0.13142     0.0691121   0.0828521   -0.0447676    0.0626092   -0.0195261   0.11713     -0.199347     0.00260296   0.0284511    0.0485029    -0.0281478    0.103401      0.0342115     0.127435   -0.0468994    -0.123252     0.0741316    0.236351     0.01153   
 -0.0437167   -0.047353    -0.012179     0.272672    -0.088746      0.185495    -0.029927   -0.0310963  -0.0262352    0.0541462   -0.241084    -0.23769     0.0766534   -0.153082    -0.0652914    0.00432532  -0.0137835     0.0924682   -0.0222097    -0.0656545     0.131602    0.0732188    -0.174584    -0.0640634    0.002653     0.073828  
 -0.0267072   -0.0906351   -0.00937852   0.0333668   -0.114545      0.174755     0.0365224  -0.0795479   0.0698117   -0.0671199    0.0713958    0.0634431   0.0885632    0.0694238   -0.129867     0.100388    -0.153938     -0.0383166    0.0208913     0.047945     -0.0574723   0.0377745     0.11283      0.0996275    0.129415    -0.0397766 
 -0.0275014    0.0206195   -0.0375285    0.0165076    0.17009      -0.0900159   -0.14303     0.0206272  -0.0168188   -0.0107831   -0.0174303    0.191276   -0.121425     0.0479897   -0.205431     0.136464     0.0372808     0.1098       0.00627221    0.0256111     0.0022159  -0.0651345     0.0541549   -0.0436893    0.0221934    0.0691533 
  0.0456616    0.145001     0.0903054   -0.0253398   -0.147291      0.143342     0.12988    -0.0953919   0.084117    -0.0320308   -0.176033    -0.0167555   0.111736    -0.208988     0.00436443   0.0675991   -0.0707059    -0.0319858    0.100786      0.000510479   0.107801   -0.0775017    -0.0476046   -0.0113494    0.118466     0.00255338
  0.139942    -0.0167741   -0.0340641    0.237797     0.00488346    0.0839419   -0.0886915  -0.0606238  -0.251804     0.216799     0.1393       0.191982   -0.0219747   -0.176829     0.128132    -0.117159     0.0141801     0.0332568   -0.0708252    -0.105026      0.0426058   0.00678923    0.14773      0.0720196   -0.0273082    0.0252959 
  0.0398378   -0.048432    -0.150265    -0.00217979   0.159311     -0.0988874    0.125007    0.207979   -0.0672937   -0.0352387    0.0813261    0.075361    0.0631794   -0.0530895    0.143813     0.0852488   -0.0317421    -0.188546     0.0493337     0.122844     -0.0811308  -0.0212113    -0.178936     0.0930835    0.234142     0.0693781 
 -0.0329665    0.165587     0.159445    -0.0371223    0.00278285    0.00555222  -0.141774    0.0148366  -0.130511    -0.106031     0.0286901    0.0620719   0.0807249    0.0341107   -0.109413     0.078943     0.0035828     0.25995      0.0440063     0.129858     -0.122355   -0.0109981    -0.0227584   -0.0576932    0.00976266  -0.0501521 
 -0.293395    -0.0850042   -0.0867035   -0.0327161   -0.114901     -0.00200414  -0.0163392   0.0889903  -0.229942    -0.0265512    0.0715815   -0.071048   -0.0634887   -0.0608221   -0.0486681    0.0238371    0.159378     -0.0382653    0.0725601     0.0644427     0.0780354   0.0820101     0.0693848   -0.00759757  -0.0111695    0.0444023 
  0.0545343   -0.0346712    0.202961    -0.0278175    0.0541735    -0.0807871    0.013761   -0.107305    0.17109     -0.189712     0.203595    -0.126396    0.0191573    0.166808     0.0450671   -0.1051       0.0524279    -0.035397    -0.0670568     0.016804     -0.0573117   0.0706805    -0.0454526   -0.0279814   -0.169914    -0.0856291 
 -0.0687249    0.0883846   -0.116636     0.0301952    0.127708     -0.0879753   -0.145647   -0.0882186   0.00970586  -0.0816651    0.0535864    0.094881    0.0735472    0.00831241  -0.0122087   -0.0063465    0.031743     -0.163976    -0.0299602     0.0567339     0.0353859  -0.145248      0.188467    -0.090354     0.0455988    0.0132555 
  0.147504     0.0176201    0.0878182   -0.135205     0.0205583    -0.123391     0.0246936   0.0081429   0.0462326   -0.0375956    0.0840263    0.0606946  -0.149593     0.170723    -0.103546     0.0618454    0.0723545    -0.0324949    0.0671769    -0.0284337     0.0999403   0.227639     -0.0137688   -0.05111      0.0388286   -0.0394447 
  0.0224697   -0.0190703   -0.100219    -0.013479    -0.129902      0.116758    -0.142255    0.0521028   0.0102141   -0.0150674    0.00242987   0.110381   -0.0700194    0.0525416   -0.0700614   -0.0460384   -0.0175835    -0.0203596   -0.0151829    -0.00120493   -0.0199989  -0.00160482   -0.00948149   0.0410612    0.099806    -0.240263  
 -0.125539     0.020422     0.0892534    0.0271692   -0.0945599    -0.0235416    0.0548397  -0.118634   -0.133268    -0.0678173   -0.0647932    0.0112454  -0.0624943    0.0364931   -0.240421    -0.00065436  -0.000834933   0.0260362    0.109316     -0.0526788     0.0574233   0.0194095    -0.112854     0.0870001    0.0733071   -0.0642033 
 -0.0704466   -0.0195271   -0.0810903    0.0386881    0.0717481    -0.147175    -0.0689904   0.028737    0.143884    -0.020558     0.0390534    0.023039    0.0242987   -0.0712823   -0.129203    -0.0614845   -0.0894347     0.0266278   -0.00265338   -0.244631     -0.132286    0.0422086    -0.0517636   -0.106457    -0.0217316   -0.0412524 
  0.0228138    0.166591     0.0345918    0.0659932    0.0183339     0.0213104    0.0615758   0.0905906   0.0616577    0.0727652    0.20145     -0.0183205   0.0219727   -0.16102     -0.021592     0.0731557    0.106278     -0.0197078    0.228309     -0.16647      -0.10887     0.0751002    -0.0417606   -0.0178451    0.092643     0.0299504 
  0.134078     0.0734419   -0.186504    -0.0211316    0.0911569     0.0912354    0.0679695   0.0684648   0.137796     0.393178    -0.051713    -0.0788943   0.134569    -0.115791     0.135007    -0.0330211    0.0540758     0.223102     0.0180442    -0.00429435    0.269397   -0.153625      0.0669761   -0.0571414    0.057564     0.136408  
  0.0962633    0.072542    -0.130159     0.0706123   -0.131408      0.214584    -0.0887915   0.174745   -0.218168     0.273604     0.0192481    0.0419669   0.0356287   -0.0500666   -0.124685    -0.0989696    0.202903      0.00973113   0.192758     -0.213602      0.049184    0.190351     -0.0799875   -0.0626401   -0.0362857    0.0363784 
  0.162263    -0.0881346    0.0839297   -0.0436062    0.076041     -0.0888792    0.0938874  -0.0255805  -0.0163435   -0.0782676    0.0758353   -0.032786   -0.0805633   -0.167901     0.0165739   -0.0336171   -0.12557       0.0546575    0.0404108    -0.0349449    -0.113589    0.25903      -0.162486     0.0495977   -0.0251461    0.119135  
 -0.065085     0.0113592   -0.0306353    0.0761894   -0.229604     -0.020878    -0.114208   -0.0881785  -0.0853442    0.196185     0.146405     0.0910667   0.0422121    0.14654     -0.0554161   -0.00867783   0.00685794    0.083128    -0.128975      0.174947      0.0461257   0.000522557  -0.0479236    0.121319    -0.0298196    0.0205988 
 -0.0704849   -0.121785    -0.21118     -0.047274     0.19337       0.125279     0.0203204   0.0448804  -0.016856     0.0233361    0.025783     0.206119    0.272554     0.0339406    0.0386294    0.00649043   0.0867825    -0.139559     0.23156      -0.0841765    -0.126182    0.00563218   -0.0452454   -0.128125    -0.0173029   -0.182024  
 -0.0925812   -0.0101551    0.123744    -0.0422567    0.128121     -0.00821652  -0.0254529   0.14705    -0.188876     0.120925    -0.0536491    0.131883   -0.0323124    0.101534     0.0680188    0.0984747   -0.0971862    -0.181075    -0.1757       -0.0554037     0.0860838   0.0495316    -0.0912649    0.127553    -0.0749681   -0.018165  
  0.063508     0.111039     0.00851735  -0.0507457   -0.0469405     0.0254833   -0.114569   -0.102739    0.256375     0.247872    -0.0239838    0.161228    0.0236908    0.278472    -0.105964    -0.0175969   -0.183355     -0.00117191  -0.116863     -0.0723135    -0.0849731  -0.0621087     0.130792    -0.183731     0.0299521   -0.0197757 
 -0.0721347   -0.0496469    0.0119491    0.0514158    0.0818215    -0.0122799   -0.103728   -0.0481972  -0.147081    -0.0492739    0.0779388   -0.0280643   0.0710039    0.164391     0.165779    -0.135614    -0.105972      0.0567583   -0.0394763     0.000406034  -0.114356    0.00994445   -0.0826989    0.0248087   -0.00577794  -0.0379947 
  0.012473    -0.037888    -0.0379282   -0.128043    -0.0144064    -0.0467758    0.0400071   0.0473579  -0.156757     0.0817118   -0.102341    -0.0707395   0.088858    -0.0634302    0.0315684    0.141333     0.241188     -0.105842     0.0445592     0.0775517     0.0578771  -0.0113564     0.083448     0.0841996   -0.105222     0.057786  
  0.0638107   -0.0131361   -0.105947     0.129411     0.0873165     0.043049     0.0496726  -0.0102362  -0.00934466   0.0261201    0.0627061   -0.156632   -0.120362    -0.0783406    0.119532     0.086011    -0.1355       -0.0693765   -0.0250659    -0.0402676    -0.0424173   0.0587946     0.0590021   -0.00214376  -0.00990749   0.2709    
 -0.0130158    0.103538    -0.0832153    0.141557     0.0125055     0.020735    -0.072163    0.0249166   0.135913    -0.119542     0.0719058    0.0331237  -0.00625027  -0.130445    -0.115003     0.15065     -0.107373     -0.0247893   -0.0702883    -0.140665      0.0605349   0.0412174     0.117556     0.049006    -0.0265256   -0.0277265 
  0.066683     0.0189665   -0.0345135   -0.0988389   -0.0566684     0.0391594    0.0897154   0.0297262  -0.00448401   0.0565629    0.0116738    0.0425936   0.0561732   -0.087779     0.143242    -0.09392     -0.00549371   -0.0843713   -0.0655674    -0.0786329     0.0354007  -0.113056     -0.00379386  -0.0842508   -0.125251    -0.0710638 
  0.277228    -0.00214015   0.0450736    0.338761     0.107065     -0.178228     0.0118894   0.0383266  -0.0973046   -0.00687945  -0.122743     0.0719345  -0.173654    -0.0286434   -0.0905418    0.0426778   -0.0919353    -0.00664037   0.000445738  -0.0874236     0.140178    0.130692      0.0794074    0.122988    -0.0767762    0.0262299 
 -0.0910402    0.0943903    0.121818    -0.121463    -0.000534813   0.0641624   -0.0614727   0.120362   -0.160543    -0.108203     0.0798581   -0.148074    0.123811     0.0636904    0.132948    -0.164446    -0.177901     -0.00304882   0.0539423    -0.0120138    -0.178051   -0.0440135     0.0112497   -0.00519133   0.0324452    0.0254714 
 -0.00540019  -0.00196973   0.100254     0.0740144   -0.00242835   -0.100149     0.0248189  -0.018878   -0.16468      0.145193    -0.0630595    0.0174367   0.017897    -0.287345    -0.0633823    0.019315    -0.00960459   -0.10782      0.0899332    -0.132295      0.0154253   0.0764766    -0.120147     0.210838     0.0236218    0.00235401INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 5 8 15 22 24 31
INFO: iteration 1, average log likelihood -1.063380
WARNING: Variances had to be floored 1 5 6 8 13 15 22 24 27 31
INFO: iteration 2, average log likelihood -1.045195
WARNING: Variances had to be floored 1 2 5 8 10 13 15 22 24 31
INFO: iteration 3, average log likelihood -1.033118
WARNING: Variances had to be floored 1 5 6 7 8 13 15 22 24 25 27 31
INFO: iteration 4, average log likelihood -1.030659
WARNING: Variances had to be floored 1 5 8 15 22 24 31
INFO: iteration 5, average log likelihood -1.055497
WARNING: Variances had to be floored 1 2 5 8 10 13 15 22 24 27 31
INFO: iteration 6, average log likelihood -1.035799
WARNING: Variances had to be floored 1 5 6 7 8 13 15 22 24 31
INFO: iteration 7, average log likelihood -1.037865
WARNING: Variances had to be floored 1 5 8 15 22 24 25 27 31
INFO: iteration 8, average log likelihood -1.038548
WARNING: Variances had to be floored 1 5 6 8 10 13 15 22 24 31
INFO: iteration 9, average log likelihood -1.043491
WARNING: Variances had to be floored 1 2 5 7 8 13 15 22 24 27 31
INFO: iteration 10, average log likelihood -1.040171
INFO: EM with 100000 data points 10 iterations avll -1.040171
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0878911    0.102972    -0.0305384   -0.0635109    0.180471    0.0969955    0.082557     0.0945177    0.238357     0.0574736   -0.231664     -0.10548     -0.0385683    0.088781    -0.148088    -0.131367     0.103161    0.11797     -0.0336352    0.139389    -0.156693    -0.083802     0.157002     0.170957     0.0332116   0.133335 
 -0.0392464   -0.0805125   -0.11263      0.0182757    0.0756707  -0.034156    -0.00985992  -0.0520409    0.0537728    0.093341     0.00585979    0.160582     0.0551282   -0.0362269    0.103005     0.151777     0.0672406   0.0866726    0.11        -0.154946     0.123395    -0.11512     -0.0898447    0.189383    -0.0686178   0.0285569
  0.199868     0.0412576   -0.032064    -0.261645     0.0696787  -0.0270031   -0.0762452    0.00300789   0.0291593    0.0708551   -0.257519      0.0343068   -0.0520726   -0.0536011   -0.0770176    0.00283791   0.167762    0.125467    -0.0124072    0.0120133    0.030312    -0.0250518    0.0730272   -0.00380896  -0.116332   -0.0673453
  0.0508017   -0.0212418   -0.12527     -0.0442094    0.0174901   0.0766777    0.0631069    0.00478271  -0.00626486   0.0299507    0.216062     -0.0476749   -0.0394564   -0.0188541   -0.0888317    0.160265    -0.0154463   0.0808017    0.0159171   -0.0602587   -0.00384202  -0.133919    -0.0306521   -0.0441063   -0.071344    0.0592953
 -0.01638      0.0555597    0.0995004    0.0488023    0.0534321   0.0621306   -0.0527026   -0.0203504    0.00317901  -0.101536    -0.0237257    -0.209188     0.00326246   0.0712848    0.143143    -0.0588583   -0.158038   -0.150773     0.0994991    0.102288    -0.0778416    0.025922    -0.018897     0.0456534   -0.146521   -0.241287 
  0.22616     -0.293632    -0.0499103    0.00143413  -0.0373587  -0.128978    -0.0524248   -0.117214     0.156364    -0.166607     0.024918     -0.0825225   -0.0469034   -0.0217507    0.0416875    0.115733    -0.0879221  -0.077924     0.0375511    0.0572317   -0.168162     0.196022     0.0482286    0.123194    -0.107515   -0.0512768
 -0.0372927    0.00290632   0.0466191    0.0851843    0.0104641  -0.00627094   0.0304644    0.0789014   -0.0182246    0.095622     0.0498179    -0.0397722   -0.055811     0.12694      0.0120758    0.0190172   -0.134422   -0.0951992   -0.079175    -0.0646091   -0.106328    -0.157983    -0.0598407    0.0360091    0.0983237  -0.067474 
 -0.0897228    0.0863585    0.187614     0.107517     0.152519    0.007878    -0.0660526   -0.11072     -0.0433853   -0.0225966   -0.0662535     0.108065    -0.0439474    0.172233    -0.0432018    0.0261679   -0.033005   -0.131925    -0.0913031   -0.137756     0.0496175   -0.00108374  -0.00978112   0.0853826    0.0231774   0.0657138
 -0.0359609    0.0998727    0.0242022    0.0140016   -0.0327546  -0.0893483    0.0883675   -0.214762     0.00254641   0.276782    -0.191939      0.1101       0.00111777  -0.204733     0.158613    -0.11764      0.192171    0.102407     0.00362402  -0.0266168   -0.00576729  -0.0146108    0.0969319    0.13743      0.0511811  -0.114704 
  0.023293    -0.0289055   -0.150476     0.104372    -0.024596    0.026429    -0.0986198    0.1598      -0.2033       0.00873641  -0.0297076     0.156208     0.0601116   -0.124214    -0.00723114   0.0303175    0.120827    0.0902973   -0.0361824   -0.166599    -0.059551     0.0753787    0.216119     0.0638702   -0.0147958   0.128866 
  0.0678109    0.00666784  -0.0210492   -0.041262    -0.0643819   0.0539012   -0.142997     0.00305475  -0.138104     0.00733932  -0.09438       0.0595565   -0.161251    -0.226187     0.0716471    0.226421     0.0270199  -0.15884     -0.0259732    0.13178     -0.0403865   -0.180147    -0.0332298    0.0167189   -0.168449   -0.042421 
  0.0646669   -0.216255    -0.173129    -0.0512003   -0.0304021   0.144563    -0.0183061    0.0057472    0.0983717   -0.119662    -0.0769582     0.00408568  -0.0949825   -0.0953009    0.0519396    0.0250758    0.074309    0.113474    -0.0647951   -0.0464834   -0.0435928    0.0387712    0.0900958    0.201888     0.223781    0.104184 
  0.0608172   -0.0447944    0.0575868    0.0789687    0.116654    0.0535743    0.0847353   -0.0454669   -0.152014     0.156524    -0.0435921     0.100123     0.176101     0.0825702    0.0480505   -0.0831584   -0.0476064   0.237766     0.011137    -0.0684571   -0.208507    -0.0322475    0.0999034    0.120319    -0.0718943   0.0117309
  0.0270069   -0.345366     0.125531    -0.0391099    0.112903    0.0432655   -0.0298264   -0.106701    -0.136086    -0.0745181   -0.0374183     0.00284263  -0.0653527    0.142276    -0.0762274    0.0403417    0.174045    0.1096      -0.101778    -0.154801    -0.12535      0.097644     0.00506423  -0.0735895   -0.0388104   0.0966462
  0.027308    -0.106595    -0.186515     0.0237801   -0.163141   -0.157894     0.0320385   -0.0234627   -0.0156381   -0.0189066   -0.169769      0.0984844   -0.00835829   0.0334358    0.0785877    0.0832582    0.093052    0.0822546   -0.151878    -0.00486337   0.0654921    0.068704    -0.0704942   -0.140797     0.17285     0.0650612
  0.0471629   -0.0529641   -0.035687    -0.0391415    0.0167721   0.062758    -0.152016    -0.0747587    0.0598156    0.265453     0.0972338    -0.00894616   0.00577035  -0.0504039   -0.0324889    0.0387866   -0.0412599  -0.0893627   -0.00752174   0.00762074  -0.171892    -0.0570551    0.0844384    0.11138      0.0707625  -0.100926 
 -0.0667177   -0.0324573   -0.0951403    0.0793497   -0.173619    0.078732    -0.163685     0.0229055    0.0493445   -0.0661931   -0.0560911     0.0261598    0.0373151   -0.222828     0.131901    -0.0480527    0.0885259   0.00150229   0.0355205    0.010171     0.115078    -0.134517    -0.0891023   -0.00534293  -0.0751731  -0.166169 
  0.00800812  -0.115395     0.0561947   -0.0517673    0.0398904  -0.0403754   -0.0465462    0.0467983   -0.136926     0.120432    -0.103856     -0.00425737   0.00133609   0.00142821   0.0273807    0.0498957   -0.0988306   0.132263    -0.125706     0.0854527   -0.178151    -0.0515426   -0.0753177    0.0211045   -0.29333    -0.0103775
 -0.0780909   -0.0701614   -0.0877547    0.00559286  -0.108546   -0.0292286   -0.117275     0.0254041    0.201574    -0.0210281   -0.0341275     0.0642095    0.00276932   0.0382784   -0.147552     0.133296     0.0111101   0.0391384   -0.0292446    0.0256014    0.0433525    0.0151256   -0.101331    -0.0083412    0.0799856   0.136241 
  0.00634502   0.106566    -0.0484695    0.0212178    0.0284877   0.00867456  -0.0630964   -0.213861    -0.197658     0.0114017   -0.118697      0.06759     -0.0806602    0.12084      0.0251235   -0.0168698   -0.0951696   0.121468     0.0717117   -0.0671552    0.122153     0.00280858   0.260831     0.134251    -0.0382563  -0.16219  
 -0.0998971    0.105338    -0.0905275    0.0376192   -0.0244597   0.153884     0.206609     0.0352965   -0.020656     0.0765219    0.162344      0.167576     0.0667352    0.0914691   -0.0938175    0.0544509    0.0127993   0.0173821   -0.147136    -0.0370327   -0.014891    -0.0913266   -0.212146    -0.238256    -0.0718072   0.0468247
 -0.0731069    0.117426    -0.0622556   -0.101584    -0.0116648   0.0874596   -0.0747226   -0.145221    -0.142898    -0.0453999    0.220826      0.167707    -0.0797538    0.0721415    0.0483335   -0.123781     0.121578    0.103825    -0.164652    -0.0151489   -0.115001    -0.0954864    0.0248972   -0.0856061   -0.0447127   0.110196 
 -0.13342      0.0367187    0.114686     0.00667581   0.0740405  -0.0665308   -0.0174831    0.0231165    0.168763    -0.157338    -0.000616633  -0.150993     0.0368913   -0.131125     0.040068    -0.131714     0.0752867  -0.0011596    0.0422126    0.188896    -0.00452351  -0.0393413   -0.0400226   -0.16074     -0.121185   -0.139514 
 -0.0477027   -0.0625275   -0.00180588  -0.15324      0.157581    0.0352437   -0.192049     0.0363232   -0.0493456    0.015534     0.0445527    -0.113183     0.017704     0.036341     0.0112265    0.126328    -0.0878467   0.087004     0.048118    -0.0668661    0.0951761    0.00586549  -0.0434292   -0.0630301   -0.135236    0.0134928
 -0.110472    -0.134852    -0.113188    -0.0439907    0.0783016  -0.00530297   0.171709     0.0890428   -0.089033     0.143144    -0.123032      0.134807     0.0681332   -0.110437     0.10397     -0.104868     0.144251    0.0326415    0.0626868    0.0215991   -0.0715674    0.0676994   -0.180892     0.0363278    0.138545   -0.0864751
  0.021472    -0.108856    -0.0548691    0.0737886   -0.122105    0.116482    -0.083961     0.0578614    0.136992     0.0766346   -0.0527969    -0.161723    -0.136464    -0.0250445    0.00306369  -0.0855508    0.0870622   0.124384    -0.0546402   -0.184673     0.0384061   -0.130742    -0.105065     0.0713621   -0.0519414   0.0138204
 -0.125196    -0.0884324   -0.154585    -0.0197785    0.138261   -0.0697571    0.124722     0.18475      0.0745293   -0.0134453   -0.0966162     0.0835134   -0.004547    -0.0266741    0.0733354   -0.0475865   -0.0539256  -0.16731      0.0701587   -0.130481    -0.00389014   0.0999428   -0.0584326   -0.0244265    0.037809    0.11823  
 -0.0237038    0.0516258   -0.00982537   0.059989     0.0348119  -0.0742137    0.00697305   0.125545     0.121329    -0.0330462    0.00100603    0.200538    -0.00354887   0.12502      0.00326448  -0.0346174    0.0132323   0.0789115   -0.0210797   -0.0450363    0.00782922   0.101793     0.225246    -0.0539223    0.0330838   0.121408 
  0.0401897   -0.0752048   -0.0595655   -0.0787007   -0.132149    0.0506888   -0.0357994    0.0700431   -0.0405472    0.0485958    0.159089      0.0165664   -0.00949695  -0.0100527   -0.167283    -0.0558253   -0.0212946   0.124873    -0.0766402   -0.0254974    0.0213593   -0.0645518   -0.00896415  -0.12551      0.107888   -0.140335 
 -0.00496618  -0.0394881   -0.089077    -0.181796    -0.013982    0.0489066    0.0681623   -0.0548758   -0.109327     0.0506182    0.0150169     0.149594    -0.136651     0.0605497    0.0306945    0.0689532   -0.143209   -0.0354974   -0.0988322    0.110106     0.05574      0.053639     0.158064     0.146512    -0.151876   -0.184237 
  0.0518511   -0.12593     -0.24337     -0.0929066    0.187939    0.0164081    0.129913    -0.0520372    0.144686    -0.140137    -0.151206     -0.0290844    0.00144357   0.0487943   -0.109634    -0.0858956   -0.0339569  -0.133689    -0.157705    -0.00143343   0.135213     0.00136169   0.0371757   -0.0482556    0.0827229   0.10115  
  0.00039758   0.167135     0.140036    -0.0556961    0.0331975  -0.221493    -0.0620549    0.033225    -0.0169376   -0.176415     0.0762856     0.0935768    0.00551292   0.064069     0.0957228   -0.231915     0.100755   -0.0118249   -0.119294    -0.0670786    0.0601109    0.0758201   -0.0188241    0.128071     0.11972    -0.0253152kind full, method split
0: avll = -1.4376938869875695
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.437714
INFO: iteration 2, average log likelihood -1.437639
INFO: iteration 3, average log likelihood -1.437581
INFO: iteration 4, average log likelihood -1.437513
INFO: iteration 5, average log likelihood -1.437433
INFO: iteration 6, average log likelihood -1.437340
INFO: iteration 7, average log likelihood -1.437233
INFO: iteration 8, average log likelihood -1.437101
INFO: iteration 9, average log likelihood -1.436907
INFO: iteration 10, average log likelihood -1.436569
INFO: iteration 11, average log likelihood -1.435965
INFO: iteration 12, average log likelihood -1.435020
INFO: iteration 13, average log likelihood -1.433900
INFO: iteration 14, average log likelihood -1.432989
INFO: iteration 15, average log likelihood -1.432477
INFO: iteration 16, average log likelihood -1.432253
INFO: iteration 17, average log likelihood -1.432164
INFO: iteration 18, average log likelihood -1.432129
INFO: iteration 19, average log likelihood -1.432115
INFO: iteration 20, average log likelihood -1.432110
INFO: iteration 21, average log likelihood -1.432107
INFO: iteration 22, average log likelihood -1.432106
INFO: iteration 23, average log likelihood -1.432105
INFO: iteration 24, average log likelihood -1.432105
INFO: iteration 25, average log likelihood -1.432105
INFO: iteration 26, average log likelihood -1.432104
INFO: iteration 27, average log likelihood -1.432104
INFO: iteration 28, average log likelihood -1.432104
INFO: iteration 29, average log likelihood -1.432104
INFO: iteration 30, average log likelihood -1.432104
INFO: iteration 31, average log likelihood -1.432104
INFO: iteration 32, average log likelihood -1.432103
INFO: iteration 33, average log likelihood -1.432103
INFO: iteration 34, average log likelihood -1.432103
INFO: iteration 35, average log likelihood -1.432103
INFO: iteration 36, average log likelihood -1.432103
INFO: iteration 37, average log likelihood -1.432103
INFO: iteration 38, average log likelihood -1.432103
INFO: iteration 39, average log likelihood -1.432103
INFO: iteration 40, average log likelihood -1.432103
INFO: iteration 41, average log likelihood -1.432103
INFO: iteration 42, average log likelihood -1.432103
INFO: iteration 43, average log likelihood -1.432103
INFO: iteration 44, average log likelihood -1.432103
INFO: iteration 45, average log likelihood -1.432103
INFO: iteration 46, average log likelihood -1.432103
INFO: iteration 47, average log likelihood -1.432103
INFO: iteration 48, average log likelihood -1.432103
INFO: iteration 49, average log likelihood -1.432103
INFO: iteration 50, average log likelihood -1.432102
INFO: EM with 100000 data points 50 iterations avll -1.432102
952.4 data points per parameter
1: avll = [-1.43771,-1.43764,-1.43758,-1.43751,-1.43743,-1.43734,-1.43723,-1.4371,-1.43691,-1.43657,-1.43596,-1.43502,-1.4339,-1.43299,-1.43248,-1.43225,-1.43216,-1.43213,-1.43212,-1.43211,-1.43211,-1.43211,-1.43211,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.432119
INFO: iteration 2, average log likelihood -1.432036
INFO: iteration 3, average log likelihood -1.431962
INFO: iteration 4, average log likelihood -1.431873
INFO: iteration 5, average log likelihood -1.431766
INFO: iteration 6, average log likelihood -1.431646
INFO: iteration 7, average log likelihood -1.431523
INFO: iteration 8, average log likelihood -1.431409
INFO: iteration 9, average log likelihood -1.431313
INFO: iteration 10, average log likelihood -1.431236
INFO: iteration 11, average log likelihood -1.431178
INFO: iteration 12, average log likelihood -1.431137
INFO: iteration 13, average log likelihood -1.431109
INFO: iteration 14, average log likelihood -1.431091
INFO: iteration 15, average log likelihood -1.431080
INFO: iteration 16, average log likelihood -1.431073
INFO: iteration 17, average log likelihood -1.431068
INFO: iteration 18, average log likelihood -1.431064
INFO: iteration 19, average log likelihood -1.431062
INFO: iteration 20, average log likelihood -1.431060
INFO: iteration 21, average log likelihood -1.431058
INFO: iteration 22, average log likelihood -1.431057
INFO: iteration 23, average log likelihood -1.431055
INFO: iteration 24, average log likelihood -1.431054
INFO: iteration 25, average log likelihood -1.431053
INFO: iteration 26, average log likelihood -1.431051
INFO: iteration 27, average log likelihood -1.431050
INFO: iteration 28, average log likelihood -1.431049
INFO: iteration 29, average log likelihood -1.431048
INFO: iteration 30, average log likelihood -1.431047
INFO: iteration 31, average log likelihood -1.431046
INFO: iteration 32, average log likelihood -1.431045
INFO: iteration 33, average log likelihood -1.431044
INFO: iteration 34, average log likelihood -1.431043
INFO: iteration 35, average log likelihood -1.431042
INFO: iteration 36, average log likelihood -1.431041
INFO: iteration 37, average log likelihood -1.431040
INFO: iteration 38, average log likelihood -1.431039
INFO: iteration 39, average log likelihood -1.431038
INFO: iteration 40, average log likelihood -1.431037
INFO: iteration 41, average log likelihood -1.431036
INFO: iteration 42, average log likelihood -1.431035
INFO: iteration 43, average log likelihood -1.431034
INFO: iteration 44, average log likelihood -1.431033
INFO: iteration 45, average log likelihood -1.431032
INFO: iteration 46, average log likelihood -1.431031
INFO: iteration 47, average log likelihood -1.431029
INFO: iteration 48, average log likelihood -1.431028
INFO: iteration 49, average log likelihood -1.431027
INFO: iteration 50, average log likelihood -1.431026
INFO: EM with 100000 data points 50 iterations avll -1.431026
473.9 data points per parameter
2: avll = [-1.43212,-1.43204,-1.43196,-1.43187,-1.43177,-1.43165,-1.43152,-1.43141,-1.43131,-1.43124,-1.43118,-1.43114,-1.43111,-1.43109,-1.43108,-1.43107,-1.43107,-1.43106,-1.43106,-1.43106,-1.43106,-1.43106,-1.43106,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.431037
INFO: iteration 2, average log likelihood -1.430977
INFO: iteration 3, average log likelihood -1.430925
INFO: iteration 4, average log likelihood -1.430865
INFO: iteration 5, average log likelihood -1.430790
INFO: iteration 6, average log likelihood -1.430698
INFO: iteration 7, average log likelihood -1.430590
INFO: iteration 8, average log likelihood -1.430472
INFO: iteration 9, average log likelihood -1.430353
INFO: iteration 10, average log likelihood -1.430242
INFO: iteration 11, average log likelihood -1.430145
INFO: iteration 12, average log likelihood -1.430063
INFO: iteration 13, average log likelihood -1.429995
INFO: iteration 14, average log likelihood -1.429940
INFO: iteration 15, average log likelihood -1.429895
INFO: iteration 16, average log likelihood -1.429858
INFO: iteration 17, average log likelihood -1.429827
INFO: iteration 18, average log likelihood -1.429801
INFO: iteration 19, average log likelihood -1.429780
INFO: iteration 20, average log likelihood -1.429762
INFO: iteration 21, average log likelihood -1.429746
INFO: iteration 22, average log likelihood -1.429732
INFO: iteration 23, average log likelihood -1.429720
INFO: iteration 24, average log likelihood -1.429709
INFO: iteration 25, average log likelihood -1.429699
INFO: iteration 26, average log likelihood -1.429691
INFO: iteration 27, average log likelihood -1.429683
INFO: iteration 28, average log likelihood -1.429675
INFO: iteration 29, average log likelihood -1.429668
INFO: iteration 30, average log likelihood -1.429662
INFO: iteration 31, average log likelihood -1.429656
INFO: iteration 32, average log likelihood -1.429650
INFO: iteration 33, average log likelihood -1.429645
INFO: iteration 34, average log likelihood -1.429640
INFO: iteration 35, average log likelihood -1.429635
INFO: iteration 36, average log likelihood -1.429630
INFO: iteration 37, average log likelihood -1.429626
INFO: iteration 38, average log likelihood -1.429622
INFO: iteration 39, average log likelihood -1.429617
INFO: iteration 40, average log likelihood -1.429613
INFO: iteration 41, average log likelihood -1.429609
INFO: iteration 42, average log likelihood -1.429605
INFO: iteration 43, average log likelihood -1.429601
INFO: iteration 44, average log likelihood -1.429597
INFO: iteration 45, average log likelihood -1.429593
INFO: iteration 46, average log likelihood -1.429589
INFO: iteration 47, average log likelihood -1.429585
INFO: iteration 48, average log likelihood -1.429581
INFO: iteration 49, average log likelihood -1.429578
INFO: iteration 50, average log likelihood -1.429574
INFO: EM with 100000 data points 50 iterations avll -1.429574
236.4 data points per parameter
3: avll = [-1.43104,-1.43098,-1.43092,-1.43086,-1.43079,-1.4307,-1.43059,-1.43047,-1.43035,-1.43024,-1.43015,-1.43006,-1.43,-1.42994,-1.42989,-1.42986,-1.42983,-1.4298,-1.42978,-1.42976,-1.42975,-1.42973,-1.42972,-1.42971,-1.4297,-1.42969,-1.42968,-1.42968,-1.42967,-1.42966,-1.42966,-1.42965,-1.42964,-1.42964,-1.42964,-1.42963,-1.42963,-1.42962,-1.42962,-1.42961,-1.42961,-1.42961,-1.4296,-1.4296,-1.42959,-1.42959,-1.42959,-1.42958,-1.42958,-1.42957]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.429579
INFO: iteration 2, average log likelihood -1.429514
INFO: iteration 3, average log likelihood -1.429453
INFO: iteration 4, average log likelihood -1.429384
INFO: iteration 5, average log likelihood -1.429301
INFO: iteration 6, average log likelihood -1.429201
INFO: iteration 7, average log likelihood -1.429087
INFO: iteration 8, average log likelihood -1.428962
INFO: iteration 9, average log likelihood -1.428836
INFO: iteration 10, average log likelihood -1.428713
INFO: iteration 11, average log likelihood -1.428599
INFO: iteration 12, average log likelihood -1.428498
INFO: iteration 13, average log likelihood -1.428409
INFO: iteration 14, average log likelihood -1.428333
INFO: iteration 15, average log likelihood -1.428269
INFO: iteration 16, average log likelihood -1.428215
INFO: iteration 17, average log likelihood -1.428169
INFO: iteration 18, average log likelihood -1.428130
INFO: iteration 19, average log likelihood -1.428097
INFO: iteration 20, average log likelihood -1.428068
INFO: iteration 21, average log likelihood -1.428041
INFO: iteration 22, average log likelihood -1.428017
INFO: iteration 23, average log likelihood -1.427995
INFO: iteration 24, average log likelihood -1.427974
INFO: iteration 25, average log likelihood -1.427954
INFO: iteration 26, average log likelihood -1.427934
INFO: iteration 27, average log likelihood -1.427915
INFO: iteration 28, average log likelihood -1.427896
INFO: iteration 29, average log likelihood -1.427877
INFO: iteration 30, average log likelihood -1.427859
INFO: iteration 31, average log likelihood -1.427841
INFO: iteration 32, average log likelihood -1.427823
INFO: iteration 33, average log likelihood -1.427806
INFO: iteration 34, average log likelihood -1.427789
INFO: iteration 35, average log likelihood -1.427772
INFO: iteration 36, average log likelihood -1.427756
INFO: iteration 37, average log likelihood -1.427740
INFO: iteration 38, average log likelihood -1.427725
INFO: iteration 39, average log likelihood -1.427711
INFO: iteration 40, average log likelihood -1.427696
INFO: iteration 41, average log likelihood -1.427683
INFO: iteration 42, average log likelihood -1.427670
INFO: iteration 43, average log likelihood -1.427658
INFO: iteration 44, average log likelihood -1.427646
INFO: iteration 45, average log likelihood -1.427634
INFO: iteration 46, average log likelihood -1.427623
INFO: iteration 47, average log likelihood -1.427612
INFO: iteration 48, average log likelihood -1.427602
INFO: iteration 49, average log likelihood -1.427592
INFO: iteration 50, average log likelihood -1.427582
INFO: EM with 100000 data points 50 iterations avll -1.427582
118.1 data points per parameter
4: avll = [-1.42958,-1.42951,-1.42945,-1.42938,-1.4293,-1.4292,-1.42909,-1.42896,-1.42884,-1.42871,-1.4286,-1.4285,-1.42841,-1.42833,-1.42827,-1.42821,-1.42817,-1.42813,-1.4281,-1.42807,-1.42804,-1.42802,-1.428,-1.42797,-1.42795,-1.42793,-1.42791,-1.4279,-1.42788,-1.42786,-1.42784,-1.42782,-1.42781,-1.42779,-1.42777,-1.42776,-1.42774,-1.42773,-1.42771,-1.4277,-1.42768,-1.42767,-1.42766,-1.42765,-1.42763,-1.42762,-1.42761,-1.4276,-1.42759,-1.42758]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.427583
INFO: iteration 2, average log likelihood -1.427513
INFO: iteration 3, average log likelihood -1.427446
INFO: iteration 4, average log likelihood -1.427366
INFO: iteration 5, average log likelihood -1.427265
INFO: iteration 6, average log likelihood -1.427137
INFO: iteration 7, average log likelihood -1.426983
INFO: iteration 8, average log likelihood -1.426809
INFO: iteration 9, average log likelihood -1.426625
INFO: iteration 10, average log likelihood -1.426442
INFO: iteration 11, average log likelihood -1.426269
INFO: iteration 12, average log likelihood -1.426112
INFO: iteration 13, average log likelihood -1.425974
INFO: iteration 14, average log likelihood -1.425853
INFO: iteration 15, average log likelihood -1.425748
INFO: iteration 16, average log likelihood -1.425657
INFO: iteration 17, average log likelihood -1.425578
INFO: iteration 18, average log likelihood -1.425508
INFO: iteration 19, average log likelihood -1.425446
INFO: iteration 20, average log likelihood -1.425390
INFO: iteration 21, average log likelihood -1.425340
INFO: iteration 22, average log likelihood -1.425294
INFO: iteration 23, average log likelihood -1.425253
INFO: iteration 24, average log likelihood -1.425214
INFO: iteration 25, average log likelihood -1.425178
INFO: iteration 26, average log likelihood -1.425144
INFO: iteration 27, average log likelihood -1.425112
INFO: iteration 28, average log likelihood -1.425081
INFO: iteration 29, average log likelihood -1.425052
INFO: iteration 30, average log likelihood -1.425024
INFO: iteration 31, average log likelihood -1.424997
INFO: iteration 32, average log likelihood -1.424971
INFO: iteration 33, average log likelihood -1.424946
INFO: iteration 34, average log likelihood -1.424922
INFO: iteration 35, average log likelihood -1.424899
INFO: iteration 36, average log likelihood -1.424877
INFO: iteration 37, average log likelihood -1.424856
INFO: iteration 38, average log likelihood -1.424836
INFO: iteration 39, average log likelihood -1.424817
INFO: iteration 40, average log likelihood -1.424798
INFO: iteration 41, average log likelihood -1.424781
INFO: iteration 42, average log likelihood -1.424764
INFO: iteration 43, average log likelihood -1.424749
INFO: iteration 44, average log likelihood -1.424734
INFO: iteration 45, average log likelihood -1.424719
INFO: iteration 46, average log likelihood -1.424706
INFO: iteration 47, average log likelihood -1.424693
INFO: iteration 48, average log likelihood -1.424680
INFO: iteration 49, average log likelihood -1.424668
INFO: iteration 50, average log likelihood -1.424657
INFO: EM with 100000 data points 50 iterations avll -1.424657
59.0 data points per parameter
5: avll = [-1.42758,-1.42751,-1.42745,-1.42737,-1.42726,-1.42714,-1.42698,-1.42681,-1.42663,-1.42644,-1.42627,-1.42611,-1.42597,-1.42585,-1.42575,-1.42566,-1.42558,-1.42551,-1.42545,-1.42539,-1.42534,-1.42529,-1.42525,-1.42521,-1.42518,-1.42514,-1.42511,-1.42508,-1.42505,-1.42502,-1.425,-1.42497,-1.42495,-1.42492,-1.4249,-1.42488,-1.42486,-1.42484,-1.42482,-1.4248,-1.42478,-1.42476,-1.42475,-1.42473,-1.42472,-1.42471,-1.42469,-1.42468,-1.42467,-1.42466]
[-1.43769,-1.43771,-1.43764,-1.43758,-1.43751,-1.43743,-1.43734,-1.43723,-1.4371,-1.43691,-1.43657,-1.43596,-1.43502,-1.4339,-1.43299,-1.43248,-1.43225,-1.43216,-1.43213,-1.43212,-1.43211,-1.43211,-1.43211,-1.43211,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.4321,-1.43212,-1.43204,-1.43196,-1.43187,-1.43177,-1.43165,-1.43152,-1.43141,-1.43131,-1.43124,-1.43118,-1.43114,-1.43111,-1.43109,-1.43108,-1.43107,-1.43107,-1.43106,-1.43106,-1.43106,-1.43106,-1.43106,-1.43106,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43105,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43104,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43103,-1.43104,-1.43098,-1.43092,-1.43086,-1.43079,-1.4307,-1.43059,-1.43047,-1.43035,-1.43024,-1.43015,-1.43006,-1.43,-1.42994,-1.42989,-1.42986,-1.42983,-1.4298,-1.42978,-1.42976,-1.42975,-1.42973,-1.42972,-1.42971,-1.4297,-1.42969,-1.42968,-1.42968,-1.42967,-1.42966,-1.42966,-1.42965,-1.42964,-1.42964,-1.42964,-1.42963,-1.42963,-1.42962,-1.42962,-1.42961,-1.42961,-1.42961,-1.4296,-1.4296,-1.42959,-1.42959,-1.42959,-1.42958,-1.42958,-1.42957,-1.42958,-1.42951,-1.42945,-1.42938,-1.4293,-1.4292,-1.42909,-1.42896,-1.42884,-1.42871,-1.4286,-1.4285,-1.42841,-1.42833,-1.42827,-1.42821,-1.42817,-1.42813,-1.4281,-1.42807,-1.42804,-1.42802,-1.428,-1.42797,-1.42795,-1.42793,-1.42791,-1.4279,-1.42788,-1.42786,-1.42784,-1.42782,-1.42781,-1.42779,-1.42777,-1.42776,-1.42774,-1.42773,-1.42771,-1.4277,-1.42768,-1.42767,-1.42766,-1.42765,-1.42763,-1.42762,-1.42761,-1.4276,-1.42759,-1.42758,-1.42758,-1.42751,-1.42745,-1.42737,-1.42726,-1.42714,-1.42698,-1.42681,-1.42663,-1.42644,-1.42627,-1.42611,-1.42597,-1.42585,-1.42575,-1.42566,-1.42558,-1.42551,-1.42545,-1.42539,-1.42534,-1.42529,-1.42525,-1.42521,-1.42518,-1.42514,-1.42511,-1.42508,-1.42505,-1.42502,-1.425,-1.42497,-1.42495,-1.42492,-1.4249,-1.42488,-1.42486,-1.42484,-1.42482,-1.4248,-1.42478,-1.42476,-1.42475,-1.42473,-1.42472,-1.42471,-1.42469,-1.42468,-1.42467,-1.42466]
32×26 Array{Float64,2}:
 -0.753885    -0.125135   -0.168644    -0.329291    0.395738    0.176648      0.333419    0.390335    0.458732     0.147749   -0.905405    0.305322    0.559547    -0.814855    0.252857    0.347697    -0.431226   -1.07108      0.468104    0.605333     0.223347   -0.329644    0.107795     -0.177316   -0.0371842   -0.652109  
 -0.647695    -0.294012   -0.161419     0.339732    0.291886   -0.129586     -1.1782      0.291502    0.150106     0.391994   -0.0819051  -0.131864    0.0906437   -0.112123    0.235185   -0.266554     0.74154    -0.0255667    0.404862    0.0417498    0.400339   -0.017831    0.147358     -0.561354    0.0851989   -1.4921    
 -0.468181    -0.327897   -0.00495228  -0.580227    0.311495    0.166651     -0.197973    0.162548   -0.0556804   -0.0895604  -0.103154   -0.116777   -0.245815    -0.591915    0.306806   -0.232323     0.281067    0.0540918   -0.305304   -0.0743456   -0.0626278   0.226758    0.215007      0.0442727   0.474408    -0.0244181 
  0.1618      -0.233163   -0.157036     0.308439   -0.331606   -0.0701534     0.0764047   0.148721    0.223074    -0.166098   -0.133704    0.0541187   0.0735567   -0.0487708  -0.106315    0.290032     0.0596416   0.0755075   -0.0815168   0.0842549    0.0691728  -0.0050781  -0.000706368  -0.294726   -0.318621     0.202125  
 -0.324097     0.126116   -0.352833     0.291327   -0.0130149   0.247285      0.0376064  -0.117431   -1.03051      0.551038    0.219328   -0.32924    -0.822384    -0.980393    0.11274     0.308572     0.260227   -0.103279    -0.0824722   0.0110843   -0.131433   -0.310896    0.449474      0.228515    0.066603    -0.752563  
  0.084736    -0.207536   -0.0948828   -0.185142    0.571232   -0.000184874   0.273094    0.308157   -0.182578     0.695783    0.778539   -0.294317   -0.264362    -0.41042     0.0505703   0.265792     0.193974    0.127356    -0.660387    0.031028    -0.199959   -0.824768   -0.293039      0.606346    0.310734    -0.138333  
 -0.15495     -0.747765   -0.56834     -0.450088    0.177574    0.241512     -0.851703   -0.564361   -0.0064803    0.184023   -0.213863   -0.186731   -0.509823     0.945875    0.116146   -0.123075    -0.0547003  -0.10998     -0.0708381  -0.00363569  -0.0583711  -0.297601    0.109517      0.549535   -0.232911    -0.257021  
 -0.293284     0.180358   -0.797188    -0.152457   -0.0788646   0.161096     -0.311897    0.739465    0.176909     0.0209972  -0.192919    0.210436    0.00807731   0.499959    0.445902    0.0462702   -0.242844    0.639542    -0.21799    -0.0862413   -0.179617   -0.200581    0.572767      0.0895341   0.0931864   -0.63456   
  0.824635     0.460385   -0.0914062   -0.433403   -0.508007    0.38426       0.899831    0.0824631  -0.413287    -0.379267    0.187157   -0.0417578  -0.108713    -0.131896   -0.492485   -0.0414455   -0.263314    0.0953431   -0.427772   -0.106387    -0.0865678   0.0891959  -0.443647      0.228205    0.208278     1.21151   
  0.19362      0.174587   -0.00591594  -0.186728    0.182353   -0.248222      0.477187   -0.0276684   0.886056    -0.349259    0.0103962  -0.0127734   0.332867     0.447824    0.297488   -0.481145    -0.518332    0.204178    -0.609739    0.071634     0.0522771  -0.653958   -0.114124      0.398292   -0.313292     0.662894  
  0.35106      0.366758    0.646371     0.523128    0.51274    -0.124847      0.397524   -0.451314    0.124462    -0.150147    0.365565   -0.215026   -0.259591    -0.211141    0.0251722  -0.469265    -0.149552   -0.140498     0.411124    0.485525     0.480984   -0.284304   -0.540752     -0.084404    0.0452481   -0.110124  
  0.122255     0.446377    0.0011119   -0.0620722   0.337953   -0.16205      -0.155697    0.357711    0.600231     0.592252    0.321169   -0.281577    0.215865     0.291023   -0.216982   -0.333112    -0.234657    0.223339     0.440575    0.294905    -0.216338   -0.0737247  -0.275139      0.494853    0.770962    -0.0448175 
  0.656133    -0.0942003   0.0281924    0.148575   -0.125565   -0.649146     -0.526118   -0.176673   -0.225495    -0.0979925   0.411148   -0.295172   -0.473237     0.623462   -0.188475    0.039324     0.37706     0.620026    -0.357777   -0.585189    -0.204148    0.0608465  -0.238383      0.279981   -0.111782     0.678845  
  0.224886    -0.41611     0.060504    -0.177076    0.170022   -0.46014       0.278841   -0.62908    -0.227907     0.629566   -0.0516641   0.462619    0.0479319    0.0485977  -0.232228    0.512568    -0.395626   -0.157465     0.142947   -0.273651    -0.296534    0.289015    0.113803      0.73524    -0.443274     0.978935  
 -0.142967     0.60504    -0.19959     -0.362188   -0.852522    0.713839      0.0660819  -0.743139   -0.418869    -0.158882    0.10398     0.592656   -0.275787     0.993123    0.0194164  -0.12182     -0.386938   -0.217215    -0.196066   -0.667948     0.24165     0.322296   -0.191826     -0.199548   -0.228447    -0.352529  
  0.346671     0.481322   -0.028374    -0.150685    0.411305    0.290469      0.0951902  -0.0838802  -0.74234      0.213691    0.321215    0.467412    0.645392     0.331519   -0.0724313   0.216285    -0.600282   -0.141486     0.169022   -0.432231    -0.425313   -0.131409    0.361451      0.0660757   0.0440195   -0.371705  
  0.0574162    0.0765965   0.065306    -0.729662    0.0126905   0.198809     -0.119242    0.218388    0.256297    -0.74993    -0.408254   -0.145764    0.669294    -0.190632   -0.173657   -0.0696932    0.0200579  -0.296031    -0.163996   -0.574414     0.267769    0.205418   -0.355867     -0.199874    0.435655     0.0109177 
 -0.231868     0.102358   -0.0888784    0.101094   -0.620483    0.376787      0.317347    0.390132   -0.322905     0.429956   -0.385021    0.104403    0.507333    -0.517352   -0.584119   -0.13391      0.172559    0.151142     0.098572   -0.343332     0.327971    0.512191   -0.288005      0.0268594   0.420812    -0.141108  
  0.0896642   -0.445275   -0.126852    -0.0721872  -0.0311117   0.176134     -0.147502   -0.203164   -0.858138    -0.0584345  -0.270141    0.0907938  -0.160945     0.54088    -0.359182   -0.229415     0.466779   -0.475373    -0.248729    0.222924    -0.686149   -0.355347   -0.277501     -0.623016   -0.995266     0.336096  
  0.114415     0.171116    0.0276259    0.226721   -0.317415   -0.00707522   -0.304321    0.459447    0.0561808    0.181103   -0.150037   -0.214439   -0.324347    -0.0159301  -0.653425    0.238388     0.753908   -0.210131     0.254491    0.263       -0.0332716  -0.0608754  -0.337272     -0.495779   -0.082015    -0.220194  
  0.214777     0.128453    0.0417276   -0.155981    0.170236    0.183331     -0.251989   -0.148584   -0.42641      0.243738    0.16943     0.270348    0.0961136    0.387975   -0.0962219  -0.142455    -0.15249    -0.00941297  -0.0566892  -0.284726    -0.557974    0.0113631   0.0418654     0.18251    -0.127242    -0.0209636 
 -0.00422106  -0.0544011   0.0104953    0.0173766   0.0629738  -0.0425232    -0.0361864   0.044975    0.147789    -0.0665716  -0.091272   -0.0141402   0.00713069  -0.0482386   0.0230357   0.00782297   0.0438662   0.0894021    0.0272373  -0.0271285    0.0645772  -0.016066    0.0222805    -0.0210233  -0.0183922   -0.0108953 
  0.0285003    0.34829    -0.361082    -0.117326   -0.0463197   0.18417       0.329654   -0.0230511  -0.175443     0.263782   -0.0148613   0.109147   -0.187792     0.0647112  -0.215831    0.132957     0.029436   -0.286537    -0.367276   -0.0403777    0.434829   -0.22107    -0.228543     -0.0200991   0.373918     0.174335  
  0.39519      0.0586676  -0.522022     0.11854     0.0381892   0.0678091    -0.104504   -0.562397   -0.0707279    0.231357    0.0224292   0.197318   -0.12902      0.235512   -0.183486    0.453923    -0.384154    0.130373     0.500378    0.141963     0.781956   -0.0861306  -0.0569765     0.44409     0.252069    -0.00951367
 -0.340429    -0.682913   -0.228578    -0.573398    0.100863   -0.0594004    -0.140412    0.289151    0.00753077  -0.364544   -0.044161   -0.372268   -0.287317    -0.196587    0.0405896   0.121115    -0.256678   -0.00515697  -0.210247    0.141627    -0.4788      0.384587    0.0559284     0.0726424  -0.0822637   -0.0236256 
 -0.28947     -0.847872    0.112554    -0.215745    0.213076   -0.162811     -0.10301    -0.128789   -0.223455     0.0260568  -0.473272    0.470913    0.337674    -0.43391     0.555425    0.0139824    0.0797051   0.181521    -0.184421   -0.180502    -0.343939    0.24306     0.766435     -0.102193   -0.177542    -0.133259  
 -0.309318    -0.711687   -0.223531     0.129147   -0.427331    0.0121047     0.212056   -0.0410532   0.684492    -0.503242   -0.199939   -0.436494   -0.323197    -0.444943    0.237978   -0.0759005    0.317372    0.155204    -0.298742    0.0607648    0.542186    0.156687    0.144114     -0.0574001  -0.146812     0.376793  
  0.122712    -0.341691    0.0285648    0.655692    0.492158   -0.45943       0.0464974  -0.169708    0.609729     0.102911   -0.0692664   0.177762   -0.322517    -0.17263     0.35073     0.489771     0.302918    0.0505155    0.0651943   0.490964     0.34718    -0.420468    0.376882      0.033853   -0.0563558    0.263872  
  0.0511596    0.130778    0.602611     0.324821   -0.235127   -0.0341323    -0.608516   -0.212071    0.120844    -0.511074   -0.617267    0.400804    0.445659     0.419438   -0.303456   -0.229671    -0.128505    0.190631     0.732333   -0.0186728   -0.350937    0.380303   -0.0150814    -0.516456   -0.149524     0.435976  
 -0.188982     0.0540477   0.778698     0.386287   -0.0884729  -0.270463      0.273391   -0.581764   -0.106493    -0.309232   -0.515688    0.0245775   0.0178697   -0.097067    0.114283   -0.205818     0.459758    0.144499     0.227868   -0.0487864   -0.182239    0.364633   -0.0644773     0.203164   -0.652936    -0.392415  
  0.0343847    0.609046   -0.0277389    0.335247   -0.408437    0.0197528     0.408391    0.580137    0.0835285   -0.227946    0.388088    0.357564    0.207248    -0.240123    0.442845   -0.151383     0.0141228  -0.246395    -0.404693    0.20411      0.0691292  -0.248414    0.283427     -0.495206   -0.19828     -0.197662  
 -0.222066     0.0457788   0.167035     0.0839192   0.378625   -0.0454373    -0.0206978  -0.143947    0.233748     0.125813    0.738302   -0.0743347  -0.273203    -0.0899579   0.775814   -0.213658    -0.115416    0.628846     0.140735    0.187838    -0.373243    0.151878    0.130992      0.241142    0.00209612  -0.309646  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.424646
INFO: iteration 2, average log likelihood -1.424635
INFO: iteration 3, average log likelihood -1.424625
INFO: iteration 4, average log likelihood -1.424615
INFO: iteration 5, average log likelihood -1.424606
INFO: iteration 6, average log likelihood -1.424596
INFO: iteration 7, average log likelihood -1.424587
INFO: iteration 8, average log likelihood -1.424579
INFO: iteration 9, average log likelihood -1.424570
INFO: iteration 10, average log likelihood -1.424562
INFO: EM with 100000 data points 10 iterations avll -1.424562
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.011522e+06
      1       7.275072e+05      -2.840144e+05 |       30
      2       7.153406e+05      -1.216662e+04 |       30
      3       7.107783e+05      -4.562218e+03 |       30
      4       7.083314e+05      -2.446948e+03 |       31
      5       7.066308e+05      -1.700572e+03 |       31
      6       7.053891e+05      -1.241762e+03 |       31
      7       7.041204e+05      -1.268677e+03 |       31
      8       7.029231e+05      -1.197278e+03 |       31
      9       7.020002e+05      -9.229244e+02 |       31
     10       7.013760e+05      -6.241668e+02 |       31
     11       7.008690e+05      -5.070246e+02 |       31
     12       7.004244e+05      -4.446340e+02 |       31
     13       7.000088e+05      -4.155893e+02 |       31
     14       6.996579e+05      -3.508483e+02 |       31
     15       6.993692e+05      -2.886810e+02 |       31
     16       6.991392e+05      -2.300364e+02 |       31
     17       6.989458e+05      -1.933604e+02 |       31
     18       6.987777e+05      -1.681257e+02 |       31
     19       6.986395e+05      -1.382346e+02 |       31
     20       6.985076e+05      -1.318591e+02 |       31
     21       6.983778e+05      -1.297991e+02 |       31
     22       6.982482e+05      -1.296510e+02 |       31
     23       6.981066e+05      -1.415764e+02 |       31
     24       6.979749e+05      -1.317203e+02 |       31
     25       6.978519e+05      -1.230184e+02 |       31
     26       6.977530e+05      -9.890380e+01 |       31
     27       6.976643e+05      -8.868582e+01 |       31
     28       6.975718e+05      -9.249844e+01 |       31
     29       6.974846e+05      -8.721139e+01 |       31
     30       6.973920e+05      -9.251461e+01 |       31
     31       6.973109e+05      -8.116524e+01 |       31
     32       6.972359e+05      -7.495349e+01 |       31
     33       6.971768e+05      -5.914757e+01 |       31
     34       6.971198e+05      -5.698392e+01 |       31
     35       6.970738e+05      -4.598192e+01 |       31
     36       6.970278e+05      -4.598884e+01 |       31
     37       6.969755e+05      -5.231118e+01 |       31
     38       6.969294e+05      -4.606601e+01 |       31
     39       6.968799e+05      -4.958715e+01 |       31
     40       6.968300e+05      -4.983761e+01 |       31
     41       6.967725e+05      -5.752615e+01 |       31
     42       6.967038e+05      -6.866124e+01 |       31
     43       6.966385e+05      -6.537217e+01 |       31
     44       6.965612e+05      -7.724010e+01 |       31
     45       6.964869e+05      -7.433505e+01 |       31
     46       6.964086e+05      -7.831365e+01 |       31
     47       6.963409e+05      -6.764067e+01 |       31
     48       6.962813e+05      -5.959566e+01 |       31
     49       6.962375e+05      -4.387216e+01 |       31
     50       6.961977e+05      -3.981372e+01 |       31
K-means terminated without convergence after 50 iterations (objv = 696197.6544987424)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: 26 pathological elements normalized
WARNING: 26 pathological elements normalized
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.436947
WARNING: Variances had to be floored 10
INFO: iteration 2, average log likelihood -1.431919
INFO: iteration 3, average log likelihood -1.430560
WARNING: Variances had to be floored 10
INFO: iteration 4, average log likelihood -1.429527
INFO: iteration 5, average log likelihood -1.428492
WARNING: Variances had to be floored 10
INFO: iteration 6, average log likelihood -1.427516
INFO: iteration 7, average log likelihood -1.426862
WARNING: Variances had to be floored 10
INFO: iteration 8, average log likelihood -1.426444
INFO: iteration 9, average log likelihood -1.426218
WARNING: Variances had to be floored 10
INFO: iteration 10, average log likelihood -1.426032
INFO: iteration 11, average log likelihood -1.425923
WARNING: Variances had to be floored 10
INFO: iteration 12, average log likelihood -1.425801
INFO: iteration 13, average log likelihood -1.425732
WARNING: Variances had to be floored 10
INFO: iteration 14, average log likelihood -1.425639
INFO: iteration 15, average log likelihood -1.425590
WARNING: Variances had to be floored 10
INFO: iteration 16, average log likelihood -1.425513
INFO: iteration 17, average log likelihood -1.425477
WARNING: Variances had to be floored 10
INFO: iteration 18, average log likelihood -1.425410
INFO: iteration 19, average log likelihood -1.425383
WARNING: Variances had to be floored 10
INFO: iteration 20, average log likelihood -1.425325
INFO: iteration 21, average log likelihood -1.425304
WARNING: Variances had to be floored 10
INFO: iteration 22, average log likelihood -1.425251
INFO: iteration 23, average log likelihood -1.425236
WARNING: Variances had to be floored 10
INFO: iteration 24, average log likelihood -1.425188
INFO: iteration 25, average log likelihood -1.425178
WARNING: Variances had to be floored 10
INFO: iteration 26, average log likelihood -1.425134
INFO: iteration 27, average log likelihood -1.425127
WARNING: Variances had to be floored 10
INFO: iteration 28, average log likelihood -1.425087
INFO: iteration 29, average log likelihood -1.425083
WARNING: Variances had to be floored 10
INFO: iteration 30, average log likelihood -1.425045
INFO: iteration 31, average log likelihood -1.425044
WARNING: Variances had to be floored 10
INFO: iteration 32, average log likelihood -1.425009
INFO: iteration 33, average log likelihood -1.425009
WARNING: Variances had to be floored 10
INFO: iteration 34, average log likelihood -1.424976
INFO: iteration 35, average log likelihood -1.424978
WARNING: Variances had to be floored 10
INFO: iteration 36, average log likelihood -1.424946
INFO: iteration 37, average log likelihood -1.424950
WARNING: Variances had to be floored 10
INFO: iteration 38, average log likelihood -1.424919
INFO: iteration 39, average log likelihood -1.424923
WARNING: Variances had to be floored 10
INFO: iteration 40, average log likelihood -1.424893
INFO: iteration 41, average log likelihood -1.424899
WARNING: Variances had to be floored 10
INFO: iteration 42, average log likelihood -1.424869
INFO: iteration 43, average log likelihood -1.424875
WARNING: Variances had to be floored 10
INFO: iteration 44, average log likelihood -1.424846
INFO: iteration 45, average log likelihood -1.424853
WARNING: Variances had to be floored 10
INFO: iteration 46, average log likelihood -1.424825
INFO: iteration 47, average log likelihood -1.424832
WARNING: Variances had to be floored 10
INFO: iteration 48, average log likelihood -1.424804
INFO: iteration 49, average log likelihood -1.424811
WARNING: Variances had to be floored 10
INFO: iteration 50, average log likelihood -1.424784
INFO: EM with 100000 data points 50 iterations avll -1.424784
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.189136     0.0634193    0.678044     0.336488    -0.228941   -0.159658    -0.0834158   -0.466245    -0.197107    -0.471738   -0.583636    0.315535     0.166027      0.276226    0.000381753  -0.248269     0.0913453     0.174914     0.37871     -0.138431    -0.402246     0.404435    0.0347456   -0.195889    -0.733279    -0.0852962
 -0.21078     -0.240413    -1.09057     -0.225639     0.0155316   0.601212    -0.863225     0.0435367   -0.0871455    0.373182   -0.598788    0.240487    -0.390313      0.925288    0.224688      0.151539    -0.123198      0.24799     -0.293215     0.0246457   -0.0995803   -0.391332    0.634992     0.494308     0.0134762   -0.33852  
  0.211002    -0.143907    -0.253697     0.0366146   -0.653617    0.302246     0.840956     0.221279     0.248138    -0.359456   -0.176609   -0.013191     0.000533682  -0.251596    0.0985172    -0.271981     0.251975     -0.00124423  -0.667071    -0.15285      0.314667     0.083506   -0.0918403   -0.221875    -0.224459     0.681844 
 -0.114089    -0.193358    -0.174602    -0.117304     0.550138   -0.0292878    0.0349645   -0.00485105   0.587296     0.0103698  -0.140923    0.220473    -0.337863     -0.0889772   0.0321565     0.061824    -0.369122     -0.0587999    0.132004     0.648297     0.78836     -0.414846   -0.271464     0.0899264    0.14671      0.280285 
  0.835776     0.281405     0.153354     0.108049     0.0554309   0.20517     -0.742981     0.258091    -0.721758     0.332057    0.154103    0.605361     0.408883      0.675063   -0.132292     -0.24186     -0.0136724    -0.232001     0.00923892   0.145153    -1.01019     -0.0992047   0.363891    -0.146763    -0.443962     0.200693 
  0.121909     0.0480907    0.267935     0.00392591   0.24396    -0.00441269  -0.205096    -0.118003    -0.103437     0.0179882   0.0694385   0.0971229    0.0405243     0.0510131   0.0683061    -0.132781     0.0734564     0.130526     0.075048    -0.162286    -0.222671    -0.0140434   0.00123445   0.102266    -0.140452    -0.160693 
 -0.353011    -0.574982     0.0123601   -0.803131     0.380956   -0.281832    -0.0240506   -0.0212306   -0.0557451    0.137089   -0.500688    0.29969      0.219974     -0.34851     0.364161     -0.0253147   -0.288356      0.123572    -0.22179     -0.233018    -0.284204     0.209376    0.553259     0.49644      0.0905853    0.186876 
 -0.039885    -0.295366    -0.0352879   -0.458209     0.0786275   0.0429822   -0.186479    -0.175576     0.633333    -1.35133    -0.0766613  -0.479808    -0.0362516     0.0783904   0.426376     -0.225436    -0.0257505    -0.0222088   -0.368278     0.00412223   0.392616     0.0257291  -0.152277    -0.36324     -0.00822643   0.0383092
  0.0117122   -0.118918     0.584376     0.799798     0.206621   -0.343554     0.47927     -0.4259       0.436185    -0.230607   -0.0580131   0.0484501   -0.13662      -0.55097     0.444648      0.140692     0.567503      0.263936     0.148013     0.366772     0.313946     0.0109818   0.231683     0.0772738   -0.057178    -0.0834582
 -0.290528     2.69038      0.592054     0.195225    -0.070241    1.70048      0.0968897    0.310508    -0.453953     1.045       1.00299    -1.17024      1.29042       1.71884    -1.05153      -0.145887    -3.05022      -1.86078     -0.0320363   -1.34133     -2.01737      0.0340936   1.94247     -4.68034      0.64342     -1.23121  
 -0.0683237    0.305168    -0.163778     0.346263    -0.0355075  -0.43074     -0.361888     0.637063     0.86764      0.112213    0.0332988  -0.259262    -0.0233037     0.199714    0.137163     -0.29015      0.0422255     0.332332     0.304425     0.412415     0.139096    -0.215153    0.00489163  -0.12005     -0.141851    -0.65207  
  0.299499    -0.642859    -0.0576018    0.738751     0.286662   -0.93186     -0.310743     0.213031     0.0936904    0.56921     0.0937203  -0.241561    -0.37046       0.0479178  -0.0979039     0.549005     0.72176      -0.0444919   -0.292599     0.219853    -0.252394    -0.688042    0.250047     0.0677427   -0.180694     0.442764 
 -0.288978    -0.438415    -0.13935     -0.285971     0.0360615   0.0375091   -0.0380745    0.250662     0.198866    -0.374109   -0.42563    -0.187482     0.09071      -0.221081    0.0902577    -0.0156157   -0.00553168   -0.0107896   -0.0586624   -0.0600707   -0.0445629    0.144912    0.0803539   -0.220858     0.0744739   -0.145913 
  0.682535     0.259112    -0.14061      0.0573053   -0.216624    0.0662597   -0.248564    -0.66976     -0.21227      0.119387   -0.252156    0.0735487   -0.176393      0.395464   -0.616172      0.374578     0.0364541    -0.0738016    0.567456    -0.188452     0.765983    -0.0943029  -0.59068      0.229409     0.0889959    0.24833  
  0.564156     0.0301883   -0.0521454   -0.0179519   -0.209572   -0.463632    -0.505575     0.0601341   -0.200726    -0.269711    0.679836   -0.26856     -0.672953      0.65956    -0.112791     -0.149909     0.308151      0.955422    -0.376736    -0.558368    -0.303675     0.173204   -0.23571      0.30949     -0.0806961    0.490705 
 -0.0114969   -0.00217965  -0.232956    -0.162899     0.477659    0.0358317    0.177616     0.153866     0.0997873    0.659257    0.706758   -0.41373     -0.20007      -0.397671    0.275918      0.0321485   -0.179265      0.441106    -0.124045     0.0202385   -0.0488262   -0.257806   -0.0494726    0.72002      0.673292    -0.340439 
 -0.632539    -0.568172    -0.175453     0.0173268    0.252821    0.0378451   -0.58888      0.175437    -0.0608736    0.286217   -0.112302    0.0519388   -0.0433145    -0.257021    0.431121     -0.0917659    0.401207     -0.0807717    0.13548      0.138055     0.00244442   0.121089    0.408254    -0.394695     0.0745909   -1.01625  
 -0.0160306   -0.0467701   -0.420536    -0.683203    -0.22516     0.194901    -0.204307    -0.268216    -0.498092     0.0172168  -0.122714    0.043232     0.757748     -0.0467545  -0.278227      0.628006     0.286263     -0.178012    -0.401578    -1.52073     -0.306009     0.438934    0.282011     0.0871294    0.222664    -0.0200013
  0.338408     0.222259    -0.0290676   -0.392781     0.0229998   0.0973929    0.358705     0.200614    -0.216455     0.140162    0.317053   -0.0402799   -0.0158507     0.14441    -0.477901     -0.041122    -0.229287     -0.182131    -0.452664     0.0284122   -0.337088    -0.321228   -0.522607     0.389755     0.189166     0.567877 
  0.109469     0.251335     0.527957    -0.0492008    0.247806   -0.113579    -0.497801     0.0672079    0.546761    -0.180752   -0.294641   -0.0728629    0.957162      0.510653   -0.471582     -0.344681    -0.0938397     0.247904     0.542031    -0.155843    -0.543113     0.143956   -0.295074    -0.197302     0.694953     0.424366 
  0.00188422  -0.459086    -0.0560463    0.0262487   -0.417347    0.285066    -0.243487     0.636487     0.03501      0.0345043  -0.49474     0.0409534   -0.0221203    -0.482505   -0.877665      0.206501     0.495033      0.170361     0.355317     0.237021    -0.121784     0.606934   -0.341239    -0.613524    -0.0752642    0.256945 
 -0.170359     0.54223     -0.229465    -0.588997    -0.358949    0.911358     0.193538    -0.264214    -0.528884    -0.0465085   0.120479    0.607658     0.0657589     0.584152   -0.0148787    -0.489278    -0.595616      0.141822    -0.0386022   -0.572938     0.0679908    0.340339   -0.290256    -0.00834583   0.0394316   -0.404885 
 -0.189744    -0.178523    -0.176784    -0.112149     0.141501    0.0881806    0.0458213    0.291115     0.268433    -0.218823    0.691127    0.00344331  -0.206106     -0.187659    0.734164     -0.00601074  -0.102898      0.195104    -0.284188     0.300413    -0.443645    -0.104668    0.319953    -0.0557794   -0.276408    -0.067969 
  0.073365     0.328298    -0.337546     0.0489125   -0.208401    0.148726     0.198847    -0.0126351   -0.0504492    0.152747   -0.0361051   0.143799    -0.0362432     0.156095   -0.164789      0.123565    -0.0290725    -0.101719    -0.0965598   -0.0339447    0.303293    -0.0305513  -0.0429549   -0.0960936    0.243145     0.0912908
 -0.163908     0.5984       0.0267322    0.0623716    0.0264624   0.134821    -0.084635     0.165846    -0.492548     0.324525    0.103212   -0.217566    -0.66385      -0.255088   -0.117257      0.0511008    0.647182     -0.346593    -0.117718     0.0436299    0.089621    -0.221531   -0.27571     -0.225406     0.155856    -0.67495  
 -0.17908      0.190171     0.341651    -0.0150408   -0.311199    0.159353     0.294891     0.370158     0.012652     0.0759889  -0.325956   -0.0309636    0.833842     -0.667081   -0.373756     -0.230365     0.230721     -0.451882     0.189943    -0.155976     0.522679     0.0577807  -0.417881    -0.0160286    0.455271    -0.146329 
  0.00974694  -0.986706    -0.067257    -0.429937     0.0600829   0.0600516   -0.161728    -0.612983    -0.513275     0.0741209  -0.172402   -0.443134    -0.542415      0.328116   -0.345016     -0.242373     0.193361     -0.549584    -0.161845     0.221099    -0.642469     0.088      -0.231511     0.0952982   -0.694084     0.197164 
  0.206717     0.171419    -0.00627567  -0.00987976  -0.0537628   0.0606399    0.330385    -0.0489657   -0.00453028  -0.0230248   0.0922388   0.0103363   -0.0892952     0.0201242  -0.164466     -0.0348321   -0.214724      0.106816    -0.139843    -0.0412317    0.0130438   -0.156904   -0.228025     0.229868     0.0048713    0.378538 
 -0.0759703    0.490615    -0.391871     0.350798    -0.414432    0.0656693    0.471481     0.569239    -0.00155029  -0.25683    -0.0893526   0.825415     0.848258     -0.0916764   0.264505      0.294266    -0.378416     -0.256624    -0.194562     0.044234     0.195285    -0.509644    0.567671    -0.547175    -0.163846    -0.376795 
 -0.153703    -0.604571    -0.262727     0.49235      0.122223    0.346207    -0.4245      -0.216469    -0.42112      0.511968    0.121464    0.208939    -0.178761     -0.367406   -0.0357765     0.317783     0.000366874   0.184445     0.373097     0.0271485   -0.0471071    0.0871068   0.546777    -0.00560567  -0.170321    -0.331984 
  0.277249     0.548978     0.206491     0.189923     0.381798   -0.286        0.503106    -0.738662    -0.0945513    0.295807    0.524426    0.234223     0.0929732     0.329163    0.167784      0.253626    -0.861689     -0.196057     0.164843    -0.0865117   -0.0754536   -0.230541    0.0846378    0.449922    -0.225363     0.33768  
  0.0222266   -0.53678     -0.183664     0.223571    -0.105171   -0.551392    -0.00737735  -0.433174     0.0876126    0.0526643  -0.256336   -0.0513973   -0.301066      0.0613324   0.148696      0.449504     0.122295      0.0877238   -0.313734    -0.0451375    0.144767     0.2317      0.317299     0.349092    -0.321674     0.627465 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.424792
WARNING: Variances had to be floored 10
INFO: iteration 2, average log likelihood -1.424764
INFO: iteration 3, average log likelihood -1.424773
WARNING: Variances had to be floored 10
INFO: iteration 4, average log likelihood -1.424746
INFO: iteration 5, average log likelihood -1.424754
WARNING: Variances had to be floored 10
INFO: iteration 6, average log likelihood -1.424727
INFO: iteration 7, average log likelihood -1.424736
WARNING: Variances had to be floored 10
INFO: iteration 8, average log likelihood -1.424710
INFO: iteration 9, average log likelihood -1.424719
WARNING: Variances had to be floored 10
INFO: iteration 10, average log likelihood -1.424693
INFO: EM with 100000 data points 10 iterations avll -1.424693
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
