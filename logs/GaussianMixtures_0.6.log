>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1162
Commit d95591e (2016-10-31 16:52 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (653.796875 MB free)
Uptime: 21658.0 sec
Load Avg:  1.01025390625  1.01953125  1.04541015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1360566 s       1244 s     143797 s     416004 s         60 s
#2  3499 MHz     449851 s       5506 s      74483 s    1575170 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-2.143453425694401e7,[80493.1,19506.9],
[-4923.08 -21492.8 6832.61; 4656.21 22022.2 -7035.74],

Array{Float64,2}[
[84812.4 342.098 -856.124; 342.098 67475.7 3349.32; -856.124 3349.32 79381.9],

[15403.0 -2.49017 1312.88; -2.49017 31865.5 -3526.66; 1312.88 -3526.66 20219.8]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.548230e+03
      1       1.238770e+03      -3.094594e+02 |        8
      2       1.082986e+03      -1.557843e+02 |        5
      3       9.467048e+02      -1.362814e+02 |        5
      4       8.928751e+02      -5.382970e+01 |        0
      5       8.928751e+02       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 892.8750592587726)
INFO: K-means with 272 data points using 5 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.068890
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.832005
INFO: iteration 2, lowerbound -3.712044
INFO: iteration 3, lowerbound -3.572289
INFO: iteration 4, lowerbound -3.401005
INFO: iteration 5, lowerbound -3.216066
INFO: iteration 6, lowerbound -3.038639
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.878221
INFO: dropping number of Gaussions to 6
INFO: iteration 8, lowerbound -2.726268
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.584554
INFO: iteration 10, lowerbound -2.473123
INFO: dropping number of Gaussions to 4
INFO: iteration 11, lowerbound -2.392250
INFO: iteration 12, lowerbound -2.337584
INFO: iteration 13, lowerbound -2.315738
INFO: dropping number of Gaussions to 2
INFO: iteration 14, lowerbound -2.306767
INFO: iteration 15, lowerbound -2.299270
INFO: iteration 16, lowerbound -2.299261
INFO: iteration 17, lowerbound -2.299256
INFO: iteration 18, lowerbound -2.299255
INFO: iteration 19, lowerbound -2.299254
INFO: iteration 20, lowerbound -2.299253
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: 49 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Tue 01 Nov 2016 10:30:35 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Tue 01 Nov 2016 10:30:37 AM UTC: K-means with 272 data points using 5 iterations
11.3 data points per parameter
,Tue 01 Nov 2016 10:30:38 AM UTC: EM with 272 data points 0 iterations avll -2.068890
5.8 data points per parameter
,Tue 01 Nov 2016 10:30:39 AM UTC: GMM converted to Variational GMM
,Tue 01 Nov 2016 10:30:40 AM UTC: iteration 1, lowerbound -3.832005
,Tue 01 Nov 2016 10:30:40 AM UTC: iteration 2, lowerbound -3.712044
,Tue 01 Nov 2016 10:30:40 AM UTC: iteration 3, lowerbound -3.572289
,Tue 01 Nov 2016 10:30:40 AM UTC: iteration 4, lowerbound -3.401005
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 5, lowerbound -3.216066
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 6, lowerbound -3.038639
,Tue 01 Nov 2016 10:30:41 AM UTC: dropping number of Gaussions to 7
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 7, lowerbound -2.878221
,Tue 01 Nov 2016 10:30:41 AM UTC: dropping number of Gaussions to 6
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 8, lowerbound -2.726268
,Tue 01 Nov 2016 10:30:41 AM UTC: dropping number of Gaussions to 5
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 9, lowerbound -2.584554
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 10, lowerbound -2.473123
,Tue 01 Nov 2016 10:30:41 AM UTC: dropping number of Gaussions to 4
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 11, lowerbound -2.392250
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 12, lowerbound -2.337584
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 13, lowerbound -2.315738
,Tue 01 Nov 2016 10:30:41 AM UTC: dropping number of Gaussions to 2
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 14, lowerbound -2.306767
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 15, lowerbound -2.299270
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 16, lowerbound -2.299261
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 17, lowerbound -2.299256
,Tue 01 Nov 2016 10:30:41 AM UTC: iteration 18, lowerbound -2.299255
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 19, lowerbound -2.299254
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 20, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 21, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 22, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 23, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 24, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 25, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 26, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 27, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 28, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 29, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 30, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 31, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:42 AM UTC: iteration 32, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 33, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 34, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 35, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 36, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 37, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 38, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 39, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 40, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 41, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 42, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 43, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 44, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 45, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 46, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 47, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: iteration 48, lowerbound -2.299253
,Tue 01 Nov 2016 10:30:43 AM UTC: 49 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999991
avll from stats: -0.9827122060329032
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9827122060329033
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9827122060329033
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
WARNING: is is deprecated, use === instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in is(::Array{Float64,2}, ::Vararg{Array{Float64,2},N}) at ./deprecated.jl:30
 in _rcopy!(::Array{Float64,2}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/utils.jl:10
 in unwhiten!(::Array{Float64,2}, ::PDMats.PDMat{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/pdmat.jl:60
 in rand(::Distributions.MvNormal{Float64,PDMats.PDMat{Float64,Array{Float64,2}},Array{Float64,1}}, ::Int64) at /home/vagrant/.julia/v0.6/Distributions/src/multivariates.jl:18
 in rand(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:60
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9546069699641619
avll from llpg:  -0.9546069699641622
avll direct:     -0.9546069699641622
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0281747    0.03804       0.040348   -0.0635752   -0.110693     0.000541657  -0.229579   -0.0607444    0.0334403  -0.274366    -0.0604804   -0.31405      0.0976883     0.128533    -0.131512     0.101447      0.246225    0.0950555   -0.162115     0.032756     0.0352617   -0.311991     0.0365226     0.237276      0.0367585   0.116078 
  0.199161    -0.000577926   0.201967    0.076425     0.114446     0.157281     -0.0394663   0.0411257   -0.0184495  -0.0121952   -0.0135636    0.0843536    0.0954929     0.0397397   -0.0294564    0.0658516     0.0581133  -0.00829816   0.123254    -0.0134396   -0.0766023    0.0221954   -0.087016      0.183934      0.0651886  -0.0961432
  0.0246515    0.00229477   -0.0163838  -0.0236325   -0.005792     0.0396174    -0.0291029  -0.16925     -0.0732151   0.00619214  -0.036778    -0.102693    -0.0391176    -0.186669     0.00868739  -0.0339361    -0.0560939  -0.0566321   -0.108231     0.0577464    0.0395826   -0.0841318    0.0991382     0.014919      0.0379836  -0.0117712
  0.0107061    0.127562      0.202786    0.0147347    0.100531     0.182616      0.0545438  -0.100932     0.0456499   0.266461     0.154432    -0.00401378   0.000188798  -0.104207    -0.113415     0.0536101    -0.111181    0.0790673    0.0460786    0.05654      0.13883      0.0746895   -0.0775251    -0.0294196    -0.0707822  -0.0212338
  0.114151    -0.137375      0.0810153   0.102418    -0.0542352    0.0250985     0.196029   -0.203912    -0.0960178  -0.102802     0.065524     0.205957    -0.0871795    -0.0719114    0.288516     0.00639377   -0.12915     0.117824    -0.131853    -0.0365035   -0.128376     0.0297149    0.0533062     0.177556     -0.0664733  -0.108028 
  0.0456962    0.0115136    -0.194624   -0.0269288   -0.0968489    0.0127133     0.0799615   0.0171322    0.0658307   0.14045      0.0675931    0.0819969   -0.0576525    -0.133888    -0.0220232    0.0449889     0.141106   -0.1208      -0.121481    -0.03703     -0.0639888   -0.0464192   -0.0992354    -0.0385417    -0.0496508  -0.100689 
 -0.003902     0.0683347     0.0433634   0.0841454   -0.0307523   -0.15867      -0.0898641  -0.0563822    0.0464915   0.00982943  -0.226149     0.0178643    0.138937     -0.0220937    0.0482288    0.00632639    0.139144   -0.0049637   -0.0195049    0.243822     0.107833    -0.169784    -0.132635     -0.0114881     0.0364542   0.123109 
 -0.148859     0.0694014     0.206947   -0.0676422   -0.0414261    0.269351      0.0885626   0.0454777   -0.0571498   0.0820403    0.148731     0.0200027   -0.00915808   -0.115865     0.0206691    0.0721649    -0.0848369   0.0154642   -0.0351817    0.136065    -0.0402301    0.107205     0.0126607     0.161492      0.134773    0.15162  
  0.0745929    0.167377     -0.188818    0.100518     0.0576473    0.0312149     0.0110576  -0.0379929    0.143347    0.0789105    0.00986949   0.0740096   -0.105449     -0.12018      0.128306     0.108824     -0.0159712  -0.142438    -0.144485     0.0127913    0.0373051    0.0282637   -0.166236      0.101937      0.0583435   0.161049 
  0.0053905    0.0596831     0.0607302  -0.118808    -0.140751    -0.0189217     0.0359252  -0.0984461   -0.136362   -0.0218031    0.0456684   -0.00996227   0.00382829    0.116665    -0.078907     0.00764843    0.0404064   0.113953     0.0143228   -0.168766     0.0423147    0.0613111   -0.121181     -0.236793     -0.0918695  -0.0414229
  0.0545272   -0.0201166    -0.0182502   0.0877339    0.0667998   -0.101663     -0.0478522   0.0631825   -0.196101    0.0759013    0.0288006   -0.144874     0.0084001     0.0374575   -0.0542149    0.00421884   -0.0828127   0.156635     0.00361046  -0.0863554    0.0475628   -0.0442134    0.128433      0.0558768     0.144352   -0.0919332
  0.141886    -0.0656839    -0.0782291  -0.125009     0.0659348    0.121049     -0.0316079  -0.0654468   -0.0706077   0.0489567   -0.153887     0.0750251    0.0235179     0.0750424   -0.101808    -0.0202624    -0.0848247  -0.032894     0.0331731   -0.0305366   -0.0729836    0.167302     0.159255      0.0410957     0.0505451   0.0369817
 -0.181201    -0.107155     -0.136082   -0.106029    -0.107503    -0.0195023     0.0618749  -0.0723382    0.050544   -0.011319     0.0336185    0.0406051    0.0323726    -0.0217137    0.0657126   -0.0729018     0.0668451   0.0423587    0.0566827    0.182313    -0.0216912   -0.085061     0.163294      0.0331552    -0.0611273  -0.0387617
 -0.0468988    0.0152707    -0.0393673   0.0525196    0.04116      0.000667001   0.0182669  -0.0729207    0.340873    0.121942     0.123961     0.0172817   -0.00946272   -0.0750678   -0.012277     0.0283431    -0.0378347  -0.0982209    0.122267     0.0520187   -0.0762489   -0.0389917   -0.158115      0.000979551  -0.0605556  -0.201583 
 -0.110406     0.00947178    0.126437   -0.0506838   -0.0301827    0.0450431    -0.166423   -0.0856024   -0.217803    0.0121582    0.0791913    0.0282465   -0.11391       0.0362701   -0.0257017    0.0473583    -0.13055     0.0195164    0.0930043    0.0906624   -0.10437      0.0149575   -0.0464264    -0.00458367   -0.109136   -0.0924177
  0.0803759    0.108852      0.108897    0.0947637   -0.00556217   0.0590608    -0.0154424   0.0580132   -0.0367888   0.0839542    0.00495716  -0.0540917   -0.173078     -0.0595215   -0.105301     0.000924717  -0.0286948   0.119094    -0.0388818   -0.0511973   -0.129376     0.00736958  -0.124378     -0.141611      0.0457763  -0.178053 
  0.0447043    0.0550547     0.0563414  -0.0843474   -0.131388    -0.0578389     0.0968174   0.111076     0.100745    0.00516915  -0.0390445   -0.138898    -0.153827      0.168067     0.0225031    0.00149996    0.0363429  -0.029597     0.0199498   -0.0283128    0.0438225   -0.00378241  -0.17755       0.0152397     0.192922    0.115008 
 -0.0728938    0.0541419     0.158439    0.108967     0.0146229   -0.10172      -0.0457205  -0.00119775  -0.0108973   0.0438576   -0.0207228   -0.0523222   -0.0249689    -0.04155     -0.015602    -0.0957731    -0.0225777  -0.0914767    0.118545     0.163008     0.0922751   -0.166497     0.353641      0.0749947    -0.0814984  -0.127839 
 -0.0321381    0.011746      0.100923   -0.0798906    0.0398586    0.0461871     0.0328557  -0.162233     0.0167236   0.0185571   -0.0538253   -0.00663181   0.017395      0.067136     0.00620198   0.00269918   -0.0984599   0.012427     0.00868624   0.158689    -0.0195741   -0.0371283   -0.127035      0.0372169     0.0758804   0.058704 
 -0.0124753    0.0185097     0.0567783   0.0645848   -0.0576378   -0.0770309    -0.0453731   0.046099    -0.159283    0.0348795    0.143187    -0.068759     0.0388419    -0.0380364   -0.0217502    0.0993452     0.249884    0.00451202  -0.166859    -0.0268786    0.0613325    0.144279    -0.157384     -0.147367      0.0549033  -0.0214577
 -0.0693984   -0.0136318     0.141236    0.0122459   -0.150214     0.134529      0.0456052  -0.0252766    0.013974   -0.0585137   -0.13296      0.00612373  -0.149109     -0.131139    -0.0479006    0.0715543     0.0888204  -0.0309877    0.16877     -0.111082     0.0148074   -0.0370646    0.0471981    -0.145967     -0.0493532  -0.0508212
 -0.150973     0.0320488     0.0235568  -0.00537292  -0.0130959    0.224652      0.0659339  -0.0943338    0.174754   -0.177363    -0.0651229   -0.0360213    0.0113761     0.0746622   -0.106634     0.12939       0.0591723  -0.0794802    0.140111    -0.0157188    0.0846794    0.0601891   -0.0806215    -0.105607     -0.0812042   0.0595728
 -0.118967    -0.159244     -0.0610195   0.0216634    0.0361325    0.0494749    -0.165975   -0.0319222    0.149227    0.035364    -0.0311122   -0.161465    -0.222665     -0.0249118   -0.0742763   -0.0851104     0.0276395   0.0256635   -0.102415     0.149111     0.0807232    0.0596044    0.128735      0.0456937    -0.129678   -0.0950986
  0.0663988   -0.0153648     0.144732   -0.188783     0.168022    -0.14564       0.0205087   0.0352339   -0.041959    0.00855223  -0.0275503   -0.132951    -0.0601929    -0.0824227    0.022832    -0.0319807     0.0160234   0.136268    -0.0609524    0.197375    -0.0143314    0.0295576   -0.0687127    -0.0618864    -0.128372   -0.239297 
  0.111763    -0.0484133     0.0460433   0.0405442   -0.120227    -0.0954369    -0.126828    0.0168143   -0.0191573  -0.150965     0.0685013    0.0547238    0.231361     -0.143539    -0.0325628   -0.0943324     0.134409    0.025933     0.162539     0.0718894    0.016009    -0.0752451   -0.0631242     0.155435     -0.188085    0.0285705
 -0.168246    -0.194011      0.100396    0.213084     0.0884589   -0.0468635    -0.0030648  -0.043168    -0.14181     0.0172966   -0.081882    -0.0721127   -0.0564224    -0.0901482    0.143254     0.135244      0.0820535   0.00937871   0.120515    -0.124824     0.149812     0.162906     0.161835      0.251142      0.0487252   0.0500711
 -0.0330035   -0.0197766    -0.225188    0.0875489    0.0115814    0.0535191    -0.0351191  -0.00834193  -0.0545763   0.131172     0.0664493    0.0300481   -0.104279      0.00668172  -0.156671     0.0343116     0.0134851  -0.0559526   -0.1917       0.0114177   -0.0215501   -0.0635853    0.000849151  -0.0773481    -0.0977273   0.235559 
 -0.00564928  -0.134505     -0.236075    0.109611     0.103377     0.0247628     0.173226   -0.132067     0.194067    0.0545536    0.120871     0.113149     0.0411486     0.0219042    0.122592     0.0434157    -0.0162407  -0.00873179   0.09187     -0.223624     0.249161     0.142401     0.046481     -0.035244     -0.128697    0.165503 
 -0.0162349   -0.0633212     0.0649524  -0.213197    -0.0526987   -0.0305628    -0.103402    0.0284815    0.135727    0.0183919   -0.0351514   -0.162931    -0.00423719   -0.147656     0.108205    -0.0113158    -0.0236555   0.125341     0.107916     0.0856984   -0.0495771    0.052486     0.0533556     0.107749     -0.0212094   0.0055689
  0.0369298    0.0811619     0.146642    0.122459    -0.0386725    0.065055      0.0180011   0.0966299    0.0290295   0.0376455   -0.0582715    0.15267      0.0398077    -0.0859528    0.141891    -0.124164     -0.126957   -0.185271    -0.158107    -0.118114    -0.12863      0.0888452    0.00562374    0.0827118     0.213937    0.15316  
 -0.0920703   -0.00587813   -0.0597833  -0.0240469   -0.186325     0.0532848    -0.0585105  -0.0761946   -0.139626   -0.11329      0.0456096   -0.0149091    0.0387991    -0.00969685   0.0122979    0.0614427    -0.147817   -0.109846    -0.0679478    0.00368721  -0.00881168   0.0178807    0.0687507     0.0989395    -0.0820255   0.0229918
  0.0494459    0.00322451   -0.0581246  -0.00654617  -0.0661003    0.157033      0.162779    0.0207463    0.20041    -0.196471    -0.0144489   -0.0318812    0.0418832     0.0728993   -0.0303724   -0.14738      -0.0912548   0.0442653    0.118871     0.070111    -0.171843    -0.0938435    0.170165     -0.134896     -0.138864   -0.0066107kind diag, method split
0: avll = -1.4116636271491259
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.411725
INFO: iteration 2, average log likelihood -1.411654
INFO: iteration 3, average log likelihood -1.411176
INFO: iteration 4, average log likelihood -1.405815
INFO: iteration 5, average log likelihood -1.390215
INFO: iteration 6, average log likelihood -1.382901
INFO: iteration 7, average log likelihood -1.381572
INFO: iteration 8, average log likelihood -1.380922
INFO: iteration 9, average log likelihood -1.380376
INFO: iteration 10, average log likelihood -1.379809
INFO: iteration 11, average log likelihood -1.379189
INFO: iteration 12, average log likelihood -1.378506
INFO: iteration 13, average log likelihood -1.377735
INFO: iteration 14, average log likelihood -1.376968
INFO: iteration 15, average log likelihood -1.376321
INFO: iteration 16, average log likelihood -1.375817
INFO: iteration 17, average log likelihood -1.375429
INFO: iteration 18, average log likelihood -1.375123
INFO: iteration 19, average log likelihood -1.374857
INFO: iteration 20, average log likelihood -1.374594
INFO: iteration 21, average log likelihood -1.374323
INFO: iteration 22, average log likelihood -1.374071
INFO: iteration 23, average log likelihood -1.373845
INFO: iteration 24, average log likelihood -1.373630
INFO: iteration 25, average log likelihood -1.373412
INFO: iteration 26, average log likelihood -1.373188
INFO: iteration 27, average log likelihood -1.372977
INFO: iteration 28, average log likelihood -1.372801
INFO: iteration 29, average log likelihood -1.372661
INFO: iteration 30, average log likelihood -1.372549
INFO: iteration 31, average log likelihood -1.372452
INFO: iteration 32, average log likelihood -1.372354
INFO: iteration 33, average log likelihood -1.372215
INFO: iteration 34, average log likelihood -1.371969
INFO: iteration 35, average log likelihood -1.371591
INFO: iteration 36, average log likelihood -1.371155
INFO: iteration 37, average log likelihood -1.370700
INFO: iteration 38, average log likelihood -1.370251
INFO: iteration 39, average log likelihood -1.369821
INFO: iteration 40, average log likelihood -1.369401
INFO: iteration 41, average log likelihood -1.368991
INFO: iteration 42, average log likelihood -1.368600
INFO: iteration 43, average log likelihood -1.368220
INFO: iteration 44, average log likelihood -1.367844
INFO: iteration 45, average log likelihood -1.367476
INFO: iteration 46, average log likelihood -1.367153
INFO: iteration 47, average log likelihood -1.366916
INFO: iteration 48, average log likelihood -1.366760
INFO: iteration 49, average log likelihood -1.366653
INFO: iteration 50, average log likelihood -1.366574
INFO: EM with 100000 data points 50 iterations avll -1.366574
952.4 data points per parameter
1: avll = [-1.41172,-1.41165,-1.41118,-1.40582,-1.39022,-1.3829,-1.38157,-1.38092,-1.38038,-1.37981,-1.37919,-1.37851,-1.37774,-1.37697,-1.37632,-1.37582,-1.37543,-1.37512,-1.37486,-1.37459,-1.37432,-1.37407,-1.37384,-1.37363,-1.37341,-1.37319,-1.37298,-1.3728,-1.37266,-1.37255,-1.37245,-1.37235,-1.37221,-1.37197,-1.37159,-1.37116,-1.3707,-1.37025,-1.36982,-1.3694,-1.36899,-1.3686,-1.36822,-1.36784,-1.36748,-1.36715,-1.36692,-1.36676,-1.36665,-1.36657]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.366633
INFO: iteration 2, average log likelihood -1.366454
INFO: iteration 3, average log likelihood -1.365575
INFO: iteration 4, average log likelihood -1.357996
INFO: iteration 5, average log likelihood -1.339266
INFO: iteration 6, average log likelihood -1.328163
INFO: iteration 7, average log likelihood -1.323389
INFO: iteration 8, average log likelihood -1.320692
INFO: iteration 9, average log likelihood -1.319187
INFO: iteration 10, average log likelihood -1.318240
INFO: iteration 11, average log likelihood -1.317563
INFO: iteration 12, average log likelihood -1.317056
INFO: iteration 13, average log likelihood -1.316662
INFO: iteration 14, average log likelihood -1.316362
INFO: iteration 15, average log likelihood -1.316137
INFO: iteration 16, average log likelihood -1.315973
INFO: iteration 17, average log likelihood -1.315860
INFO: iteration 18, average log likelihood -1.315786
INFO: iteration 19, average log likelihood -1.315738
INFO: iteration 20, average log likelihood -1.315705
INFO: iteration 21, average log likelihood -1.315680
INFO: iteration 22, average log likelihood -1.315661
INFO: iteration 23, average log likelihood -1.315644
INFO: iteration 24, average log likelihood -1.315629
INFO: iteration 25, average log likelihood -1.315615
INFO: iteration 26, average log likelihood -1.315601
INFO: iteration 27, average log likelihood -1.315588
INFO: iteration 28, average log likelihood -1.315574
INFO: iteration 29, average log likelihood -1.315561
INFO: iteration 30, average log likelihood -1.315547
INFO: iteration 31, average log likelihood -1.315532
INFO: iteration 32, average log likelihood -1.315518
INFO: iteration 33, average log likelihood -1.315502
INFO: iteration 34, average log likelihood -1.315486
INFO: iteration 35, average log likelihood -1.315469
INFO: iteration 36, average log likelihood -1.315450
INFO: iteration 37, average log likelihood -1.315429
INFO: iteration 38, average log likelihood -1.315407
INFO: iteration 39, average log likelihood -1.315384
INFO: iteration 40, average log likelihood -1.315362
INFO: iteration 41, average log likelihood -1.315342
INFO: iteration 42, average log likelihood -1.315323
INFO: iteration 43, average log likelihood -1.315308
INFO: iteration 44, average log likelihood -1.315294
INFO: iteration 45, average log likelihood -1.315282
INFO: iteration 46, average log likelihood -1.315273
INFO: iteration 47, average log likelihood -1.315264
INFO: iteration 48, average log likelihood -1.315258
INFO: iteration 49, average log likelihood -1.315252
INFO: iteration 50, average log likelihood -1.315248
INFO: EM with 100000 data points 50 iterations avll -1.315248
473.9 data points per parameter
2: avll = [-1.36663,-1.36645,-1.36558,-1.358,-1.33927,-1.32816,-1.32339,-1.32069,-1.31919,-1.31824,-1.31756,-1.31706,-1.31666,-1.31636,-1.31614,-1.31597,-1.31586,-1.31579,-1.31574,-1.3157,-1.31568,-1.31566,-1.31564,-1.31563,-1.31561,-1.3156,-1.31559,-1.31557,-1.31556,-1.31555,-1.31553,-1.31552,-1.3155,-1.31549,-1.31547,-1.31545,-1.31543,-1.31541,-1.31538,-1.31536,-1.31534,-1.31532,-1.31531,-1.31529,-1.31528,-1.31527,-1.31526,-1.31526,-1.31525,-1.31525]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.315420
INFO: iteration 2, average log likelihood -1.315239
INFO: iteration 3, average log likelihood -1.314602
INFO: iteration 4, average log likelihood -1.308428
INFO: iteration 5, average log likelihood -1.288606
INFO: iteration 6, average log likelihood -1.273651
INFO: iteration 7, average log likelihood -1.268543
INFO: iteration 8, average log likelihood -1.266129
INFO: iteration 9, average log likelihood -1.264521
INFO: iteration 10, average log likelihood -1.263368
INFO: iteration 11, average log likelihood -1.262357
INFO: iteration 12, average log likelihood -1.261434
INFO: iteration 13, average log likelihood -1.260696
INFO: iteration 14, average log likelihood -1.260166
INFO: iteration 15, average log likelihood -1.259746
INFO: iteration 16, average log likelihood -1.259346
INFO: iteration 17, average log likelihood -1.258932
INFO: iteration 18, average log likelihood -1.258471
INFO: iteration 19, average log likelihood -1.257932
INFO: iteration 20, average log likelihood -1.257341
INFO: iteration 21, average log likelihood -1.256747
INFO: iteration 22, average log likelihood -1.256192
INFO: iteration 23, average log likelihood -1.255686
INFO: iteration 24, average log likelihood -1.255255
INFO: iteration 25, average log likelihood -1.254931
INFO: iteration 26, average log likelihood -1.254730
INFO: iteration 27, average log likelihood -1.254592
INFO: iteration 28, average log likelihood -1.254488
INFO: iteration 29, average log likelihood -1.254409
INFO: iteration 30, average log likelihood -1.254352
INFO: iteration 31, average log likelihood -1.254310
INFO: iteration 32, average log likelihood -1.254278
INFO: iteration 33, average log likelihood -1.254251
INFO: iteration 34, average log likelihood -1.254227
INFO: iteration 35, average log likelihood -1.254203
INFO: iteration 36, average log likelihood -1.254178
INFO: iteration 37, average log likelihood -1.254149
INFO: iteration 38, average log likelihood -1.254115
INFO: iteration 39, average log likelihood -1.254076
INFO: iteration 40, average log likelihood -1.254028
INFO: iteration 41, average log likelihood -1.253971
INFO: iteration 42, average log likelihood -1.253900
INFO: iteration 43, average log likelihood -1.253810
INFO: iteration 44, average log likelihood -1.253689
INFO: iteration 45, average log likelihood -1.253525
INFO: iteration 46, average log likelihood -1.253300
INFO: iteration 47, average log likelihood -1.252991
INFO: iteration 48, average log likelihood -1.252577
WARNING: Variances had to be floored 6
INFO: iteration 49, average log likelihood -1.251996
INFO: iteration 50, average log likelihood -1.262780
INFO: EM with 100000 data points 50 iterations avll -1.262780
236.4 data points per parameter
3: avll = [-1.31542,-1.31524,-1.3146,-1.30843,-1.28861,-1.27365,-1.26854,-1.26613,-1.26452,-1.26337,-1.26236,-1.26143,-1.2607,-1.26017,-1.25975,-1.25935,-1.25893,-1.25847,-1.25793,-1.25734,-1.25675,-1.25619,-1.25569,-1.25526,-1.25493,-1.25473,-1.25459,-1.25449,-1.25441,-1.25435,-1.25431,-1.25428,-1.25425,-1.25423,-1.2542,-1.25418,-1.25415,-1.25412,-1.25408,-1.25403,-1.25397,-1.2539,-1.25381,-1.25369,-1.25353,-1.2533,-1.25299,-1.25258,-1.252,-1.26278]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.258428
INFO: iteration 2, average log likelihood -1.255223
INFO: iteration 3, average log likelihood -1.252721
INFO: iteration 4, average log likelihood -1.236180
WARNING: Variances had to be floored 2
INFO: iteration 5, average log likelihood -1.203929
INFO: iteration 6, average log likelihood -1.188981
WARNING: Variances had to be floored 2 12
INFO: iteration 7, average log likelihood -1.171744
WARNING: Variances had to be floored 7
INFO: iteration 8, average log likelihood -1.174903
WARNING: Variances had to be floored 2
INFO: iteration 9, average log likelihood -1.174074
INFO: iteration 10, average log likelihood -1.168852
WARNING: Variances had to be floored 2
INFO: iteration 11, average log likelihood -1.161527
INFO: iteration 12, average log likelihood -1.163106
WARNING: Variances had to be floored 2 13
INFO: iteration 13, average log likelihood -1.155868
INFO: iteration 14, average log likelihood -1.166664
WARNING: Variances had to be floored 2 7
INFO: iteration 15, average log likelihood -1.156174
WARNING: Variances had to be floored 12
INFO: iteration 16, average log likelihood -1.166225
WARNING: Variances had to be floored 2 13
INFO: iteration 17, average log likelihood -1.160204
INFO: iteration 18, average log likelihood -1.167550
WARNING: Variances had to be floored 2
INFO: iteration 19, average log likelihood -1.157273
INFO: iteration 20, average log likelihood -1.159197
WARNING: Variances had to be floored 2 13
INFO: iteration 21, average log likelihood -1.152853
INFO: iteration 22, average log likelihood -1.164085
WARNING: Variances had to be floored 2 7
INFO: iteration 23, average log likelihood -1.154579
INFO: iteration 24, average log likelihood -1.166793
WARNING: Variances had to be floored 2 12
INFO: iteration 25, average log likelihood -1.155149
INFO: iteration 26, average log likelihood -1.163687
WARNING: Variances had to be floored 2
INFO: iteration 27, average log likelihood -1.154153
WARNING: Variances had to be floored 14
INFO: iteration 28, average log likelihood -1.154388
WARNING: Variances had to be floored 2 13
INFO: iteration 29, average log likelihood -1.158623
INFO: iteration 30, average log likelihood -1.166398
WARNING: Variances had to be floored 2
INFO: iteration 31, average log likelihood -1.155518
INFO: iteration 32, average log likelihood -1.157345
WARNING: Variances had to be floored 2 7
INFO: iteration 33, average log likelihood -1.151070
WARNING: Variances had to be floored 12
INFO: iteration 34, average log likelihood -1.163231
WARNING: Variances had to be floored 2 14
INFO: iteration 35, average log likelihood -1.157584
WARNING: Variances had to be floored 13
INFO: iteration 36, average log likelihood -1.166835
WARNING: Variances had to be floored 2
INFO: iteration 37, average log likelihood -1.163157
INFO: iteration 38, average log likelihood -1.160616
WARNING: Variances had to be floored 2
INFO: iteration 39, average log likelihood -1.154534
WARNING: Variances had to be floored 7
INFO: iteration 40, average log likelihood -1.157015
WARNING: Variances had to be floored 2
INFO: iteration 41, average log likelihood -1.161199
WARNING: Variances had to be floored 13
INFO: iteration 42, average log likelihood -1.156634
WARNING: Variances had to be floored 2 12 14
INFO: iteration 43, average log likelihood -1.154580
INFO: iteration 44, average log likelihood -1.173022
WARNING: Variances had to be floored 2
INFO: iteration 45, average log likelihood -1.159046
INFO: iteration 46, average log likelihood -1.159725
WARNING: Variances had to be floored 2
INFO: iteration 47, average log likelihood -1.153738
INFO: iteration 48, average log likelihood -1.155127
WARNING: Variances had to be floored 2 14
INFO: iteration 49, average log likelihood -1.147305
INFO: iteration 50, average log likelihood -1.164967
INFO: EM with 100000 data points 50 iterations avll -1.164967
118.1 data points per parameter
4: avll = [-1.25843,-1.25522,-1.25272,-1.23618,-1.20393,-1.18898,-1.17174,-1.1749,-1.17407,-1.16885,-1.16153,-1.16311,-1.15587,-1.16666,-1.15617,-1.16622,-1.1602,-1.16755,-1.15727,-1.1592,-1.15285,-1.16408,-1.15458,-1.16679,-1.15515,-1.16369,-1.15415,-1.15439,-1.15862,-1.1664,-1.15552,-1.15735,-1.15107,-1.16323,-1.15758,-1.16684,-1.16316,-1.16062,-1.15453,-1.15701,-1.1612,-1.15663,-1.15458,-1.17302,-1.15905,-1.15973,-1.15374,-1.15513,-1.1473,-1.16497]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 4 13 14
INFO: iteration 1, average log likelihood -1.153435
WARNING: Variances had to be floored 3 4 13 14 23 24
INFO: iteration 2, average log likelihood -1.149585
WARNING: Variances had to be floored 3 4 11 13 14 25 26
INFO: iteration 3, average log likelihood -1.145742
WARNING: Variances had to be floored 3 4 11 13 14 23 24
INFO: iteration 4, average log likelihood -1.131698
WARNING: Variances had to be floored 3 4 11 13 14 24 25 26 27
INFO: iteration 5, average log likelihood -1.087639
WARNING: Variances had to be floored 3 4 11 13 14 23 25 26 28
INFO: iteration 6, average log likelihood -1.075501
WARNING: Variances had to be floored 3 4 10 11 13 14 24 27
INFO: iteration 7, average log likelihood -1.066805
WARNING: Variances had to be floored 3 4 9 11 13 14 19 23 25 26
INFO: iteration 8, average log likelihood -1.065695
WARNING: Variances had to be floored 3 4 11 13 14 16 24 25 26 27 28
INFO: iteration 9, average log likelihood -1.064325
WARNING: Variances had to be floored 3 4 8 10 11 13 14 23
INFO: iteration 10, average log likelihood -1.073943
WARNING: Variances had to be floored 3 4 11 13 14 19 24 25 26
INFO: iteration 11, average log likelihood -1.081098
WARNING: Variances had to be floored 3 4 9 11 13 14 16 23 25 26 27
INFO: iteration 12, average log likelihood -1.063585
WARNING: Variances had to be floored 3 4 10 11 13 14 24 25 26 28
INFO: iteration 13, average log likelihood -1.070755
WARNING: Variances had to be floored 3 4 11 13 14 23 27
INFO: iteration 14, average log likelihood -1.072167
WARNING: Variances had to be floored 3 4 8 11 13 14 16 19 24 25 26
INFO: iteration 15, average log likelihood -1.057536
WARNING: Variances had to be floored 3 4 10 11 13 14 23 25 28
INFO: iteration 16, average log likelihood -1.071470
WARNING: Variances had to be floored 3 4 9 11 13 14 23 24 26 27
INFO: iteration 17, average log likelihood -1.068841
WARNING: Variances had to be floored 3 4 11 13 14 16 23 26
INFO: iteration 18, average log likelihood -1.070740
WARNING: Variances had to be floored 3 4 10 11 13 14 19 24 26 27 28
INFO: iteration 19, average log likelihood -1.053734
WARNING: Variances had to be floored 3 4 8 11 13 14 23
INFO: iteration 20, average log likelihood -1.081539
WARNING: Variances had to be floored 3 4 11 13 14 16 23 24 25 26
INFO: iteration 21, average log likelihood -1.066280
WARNING: Variances had to be floored 3 4 9 10 11 13 14 23 27
INFO: iteration 22, average log likelihood -1.065835
WARNING: Variances had to be floored 3 4 11 13 14 19 24 26
INFO: iteration 23, average log likelihood -1.072417
WARNING: Variances had to be floored 3 4 11 13 14 16 23 26 27 28
INFO: iteration 24, average log likelihood -1.062970
WARNING: Variances had to be floored 3 4 8 11 13 14 23 24
INFO: iteration 25, average log likelihood -1.074430
WARNING: Variances had to be floored 3 4 10 11 13 14 19 23 26
INFO: iteration 26, average log likelihood -1.066359
WARNING: Variances had to be floored 3 4 9 11 13 14 16 22 24 25 26 27
INFO: iteration 27, average log likelihood -1.065523
WARNING: Variances had to be floored 3 4 11 13 14 23
INFO: iteration 28, average log likelihood -1.083733
WARNING: Variances had to be floored 3 4 10 11 13 14 24 26
INFO: iteration 29, average log likelihood -1.052504
WARNING: Variances had to be floored 3 4 8 11 13 14 16 19 23 26 27 28
INFO: iteration 30, average log likelihood -1.050659
WARNING: Variances had to be floored 3 4 11 13 14 23 24
INFO: iteration 31, average log likelihood -1.093855
WARNING: Variances had to be floored 3 4 9 10 11 13 14 23 26
INFO: iteration 32, average log likelihood -1.065061
WARNING: Variances had to be floored 3 4 11 13 14 16 24 26 27
INFO: iteration 33, average log likelihood -1.062300
WARNING: Variances had to be floored 3 4 8 11 13 14 19 23 26 28
INFO: iteration 34, average log likelihood -1.056874
WARNING: Variances had to be floored 3 4 10 11 13 14 22 23 24 27
INFO: iteration 35, average log likelihood -1.065194
WARNING: Variances had to be floored 3 4 9 11 13 14 16 19 23 24 26
INFO: iteration 36, average log likelihood -1.052154
WARNING: Variances had to be floored 3 4 11 13 14 23 24 26 27 28
INFO: iteration 37, average log likelihood -1.054181
WARNING: Variances had to be floored 3 4 8 9 10 11 13 14 23 24
INFO: iteration 38, average log likelihood -1.049912
WARNING: Variances had to be floored 3 4 11 13 14 16 19 23 24 26
INFO: iteration 39, average log likelihood -1.064613
WARNING: Variances had to be floored 3 4 11 13 14 23 24 26 27
INFO: iteration 40, average log likelihood -1.058170
WARNING: Variances had to be floored 3 4 9 10 11 13 14 19 23 24 26 28
INFO: iteration 41, average log likelihood -1.040007
WARNING: Variances had to be floored 3 4 8 11 13 14 16 23 24 27
INFO: iteration 42, average log likelihood -1.063740
WARNING: Variances had to be floored 3 4 11 13 14 23 24 26
INFO: iteration 43, average log likelihood -1.066274
WARNING: Variances had to be floored 3 4 9 10 11 13 14 19 23 24 26 27 28
INFO: iteration 44, average log likelihood -1.035130
WARNING: Variances had to be floored 3 4 11 13 14 16 23 24
INFO: iteration 45, average log likelihood -1.071721
WARNING: Variances had to be floored 3 4 8 11 13 14 19 23 24 26
INFO: iteration 46, average log likelihood -1.055462
WARNING: Variances had to be floored 3 4 9 10 11 13 14 16 23 24 26 27
INFO: iteration 47, average log likelihood -1.050472
WARNING: Variances had to be floored 3 4 11 13 14 23 24 26 28
INFO: iteration 48, average log likelihood -1.064355
WARNING: Variances had to be floored 3 4 11 13 14 16 19 23 24 27
INFO: iteration 49, average log likelihood -1.051585
WARNING: Variances had to be floored 3 4 8 9 10 11 13 14 23 24 26
INFO: iteration 50, average log likelihood -1.051172
INFO: EM with 100000 data points 50 iterations avll -1.051172
59.0 data points per parameter
5: avll = [-1.15343,-1.14959,-1.14574,-1.1317,-1.08764,-1.0755,-1.0668,-1.0657,-1.06433,-1.07394,-1.0811,-1.06359,-1.07076,-1.07217,-1.05754,-1.07147,-1.06884,-1.07074,-1.05373,-1.08154,-1.06628,-1.06583,-1.07242,-1.06297,-1.07443,-1.06636,-1.06552,-1.08373,-1.0525,-1.05066,-1.09386,-1.06506,-1.0623,-1.05687,-1.06519,-1.05215,-1.05418,-1.04991,-1.06461,-1.05817,-1.04001,-1.06374,-1.06627,-1.03513,-1.07172,-1.05546,-1.05047,-1.06436,-1.05159,-1.05117]
[-1.41166,-1.41172,-1.41165,-1.41118,-1.40582,-1.39022,-1.3829,-1.38157,-1.38092,-1.38038,-1.37981,-1.37919,-1.37851,-1.37774,-1.37697,-1.37632,-1.37582,-1.37543,-1.37512,-1.37486,-1.37459,-1.37432,-1.37407,-1.37384,-1.37363,-1.37341,-1.37319,-1.37298,-1.3728,-1.37266,-1.37255,-1.37245,-1.37235,-1.37221,-1.37197,-1.37159,-1.37116,-1.3707,-1.37025,-1.36982,-1.3694,-1.36899,-1.3686,-1.36822,-1.36784,-1.36748,-1.36715,-1.36692,-1.36676,-1.36665,-1.36657,-1.36663,-1.36645,-1.36558,-1.358,-1.33927,-1.32816,-1.32339,-1.32069,-1.31919,-1.31824,-1.31756,-1.31706,-1.31666,-1.31636,-1.31614,-1.31597,-1.31586,-1.31579,-1.31574,-1.3157,-1.31568,-1.31566,-1.31564,-1.31563,-1.31561,-1.3156,-1.31559,-1.31557,-1.31556,-1.31555,-1.31553,-1.31552,-1.3155,-1.31549,-1.31547,-1.31545,-1.31543,-1.31541,-1.31538,-1.31536,-1.31534,-1.31532,-1.31531,-1.31529,-1.31528,-1.31527,-1.31526,-1.31526,-1.31525,-1.31525,-1.31542,-1.31524,-1.3146,-1.30843,-1.28861,-1.27365,-1.26854,-1.26613,-1.26452,-1.26337,-1.26236,-1.26143,-1.2607,-1.26017,-1.25975,-1.25935,-1.25893,-1.25847,-1.25793,-1.25734,-1.25675,-1.25619,-1.25569,-1.25526,-1.25493,-1.25473,-1.25459,-1.25449,-1.25441,-1.25435,-1.25431,-1.25428,-1.25425,-1.25423,-1.2542,-1.25418,-1.25415,-1.25412,-1.25408,-1.25403,-1.25397,-1.2539,-1.25381,-1.25369,-1.25353,-1.2533,-1.25299,-1.25258,-1.252,-1.26278,-1.25843,-1.25522,-1.25272,-1.23618,-1.20393,-1.18898,-1.17174,-1.1749,-1.17407,-1.16885,-1.16153,-1.16311,-1.15587,-1.16666,-1.15617,-1.16622,-1.1602,-1.16755,-1.15727,-1.1592,-1.15285,-1.16408,-1.15458,-1.16679,-1.15515,-1.16369,-1.15415,-1.15439,-1.15862,-1.1664,-1.15552,-1.15735,-1.15107,-1.16323,-1.15758,-1.16684,-1.16316,-1.16062,-1.15453,-1.15701,-1.1612,-1.15663,-1.15458,-1.17302,-1.15905,-1.15973,-1.15374,-1.15513,-1.1473,-1.16497,-1.15343,-1.14959,-1.14574,-1.1317,-1.08764,-1.0755,-1.0668,-1.0657,-1.06433,-1.07394,-1.0811,-1.06359,-1.07076,-1.07217,-1.05754,-1.07147,-1.06884,-1.07074,-1.05373,-1.08154,-1.06628,-1.06583,-1.07242,-1.06297,-1.07443,-1.06636,-1.06552,-1.08373,-1.0525,-1.05066,-1.09386,-1.06506,-1.0623,-1.05687,-1.06519,-1.05215,-1.05418,-1.04991,-1.06461,-1.05817,-1.04001,-1.06374,-1.06627,-1.03513,-1.07172,-1.05546,-1.05047,-1.06436,-1.05159,-1.05117]
32×26 Array{Float64,2}:
  0.0513615     0.00664154   -0.0140664  -0.0410688   -0.108297     0.0663004   -0.0840516   -0.187143    -0.0546046     0.0104789   -0.067769    -0.654076    -0.0584094   -0.229314     0.00879423  -0.304613     0.0672134   -0.0540457  -0.189609     0.0904377   0.106196   -0.114542     0.00135856    0.0116608   -0.0102973     0.00434587
 -0.000339295  -0.00705269   -0.0145778   0.0277193    0.137913     0.0155252    0.0537609   -0.232101    -0.102832     -0.0382692   -0.030334     0.237888    -0.0268399   -0.183628     0.00449292   0.347881    -0.223407    -0.0682484  -0.039795     0.025664    0.0298262  -0.0503435    0.156116      0.0176411    0.0882986    -0.00307469
  0.0602449    -0.13702       0.055735   -0.0239453   -0.117666    -0.0531227    0.191804     0.167962     0.0965203    -0.207109    -0.146522    -0.290781    -0.196602     0.149134     0.00205445   0.387942     0.0596588   -0.0451321  -0.604203    -0.18653     0.0254324  -0.00416967  -0.0759863     0.120557     0.231827      0.119159  
  0.0575081     0.283755      0.059659   -0.0708102   -0.14267     -0.0536791    0.0228898    0.0684198    0.0973476     0.183112     0.00178481   0.0786217   -0.12548      0.185484     0.0482227   -0.370409    -0.0326023   -0.0160928   0.54816      0.0796902   0.0722623  -0.00450882  -0.19744      -0.10643      0.022465      0.113075  
  0.0393728     0.0454495     0.113107   -0.215064     0.169329    -0.13909      0.0251588    0.0695467   -0.0675346    -0.00956615  -0.0295681   -1.07429     -0.112691    -0.0835727    0.0547494   -0.0549837   -0.0389529    0.0847729  -0.108676     0.222219   -0.0240429   0.045573    -0.0531144    -0.0798129   -0.20175      -0.203659  
  0.150544     -0.104086      0.16713    -0.142838     0.171761    -0.151067     0.0172709    0.00979912  -0.0085684     0.0175107   -0.0265147    0.99514     -0.0549687   -0.0778759    0.0371369   -0.0191896    0.144394     0.154984    0.038433     0.201283    0.010206    0.00617885  -0.0943458    -0.0517397   -0.123764     -0.255346  
  0.00556981   -0.0215632    -0.0567936   0.0057171   -0.0253969    0.0981676   -0.00480634  -0.0291525   -0.0251232     0.0531529   -0.0571651    0.0384213   -0.0866064   -0.0184458   -0.0994627    0.0533702    0.019208    -0.0393667   0.00183913  -0.0583307  -0.0237107  -0.00466352   0.0626417    -0.074308    -0.0332444     0.0821167 
  0.184313      0.00500608    0.22116     0.0732538    0.107049     0.156387    -0.0468566    0.0301798   -0.000826693  -0.00404186  -0.0266804    0.0860464    0.0879614    0.0409095   -0.0226982    0.149568     0.0578135   -0.0248907   0.108971    -0.0222605  -0.0762443   0.0102587   -0.0898915     0.175713     0.0726237    -0.060178  
  0.116106     -0.135955      0.0832032   0.112345    -0.0966371    0.0239125    0.187065    -0.215826    -0.0807417    -0.10748      0.0236263    0.22085     -0.0715351   -0.0683112    0.302912    -0.0131087   -0.134817     0.125306   -0.128923    -0.0259235  -0.126854    0.0532136    0.0509068     0.166554    -0.0660968    -0.127658  
  0.0602077     0.109408      0.105594    0.0896037   -0.012158     0.0699031   -0.00246218   0.0438979   -0.0355186     0.0816938   -0.00566165  -0.0410108   -0.171972    -0.0528593   -0.102496     0.0234601    0.0360444    0.114762   -0.0601224   -0.0539623  -0.128993    0.00745816  -0.128755     -0.117526     0.0472022    -0.173225  
 -0.170468     -0.0918387    -0.146243   -0.104652    -0.0901016   -0.0345919    0.0623066   -0.0743903    0.0580955    -0.00883883   0.032853     0.0260359    0.0110537   -0.0211964    0.075879    -0.079881     0.0591626    0.0419996   0.0557378    0.145691   -0.0177259  -0.0788598    0.15434       0.0368425   -0.056647      0.031337  
 -0.148152      0.0534592     0.206685   -0.0784698    0.00662951   0.274369     0.0888245    0.0500503   -0.053145      0.0796993    0.147877     0.0130311   -0.00803612  -0.11672      0.021504     0.0456326   -0.0838641    0.0164658  -0.0342167    0.137226   -0.0432973   0.12298      0.00718721    0.167265     0.152625      0.203925  
 -0.148169     -0.068321      0.108008    0.212144     0.0906257    0.00246281  -0.0147169    0.118434    -0.141908     -0.247798     0.287501    -0.0689105   -0.0167051    0.0364209    0.120273    -0.200939     0.10289      0.212299    0.102034    -0.111442    0.143738    0.154706     0.161164      0.402466     0.0387166     0.0524283 
 -0.132078     -0.425499      0.127474    0.212542     0.105777    -0.116849     0.00243267  -0.219228    -0.141979      0.254202    -0.421271    -0.0758312   -0.141015    -0.220291     0.156612     0.461416     0.0626766   -0.216945    0.146268    -0.139437    0.163872    0.169818     0.161075      0.1669       0.0671512     0.13      
 -0.0724875    -0.0661821    -0.157276    0.0355289   -0.0593342    0.043804     0.0268337   -0.0996051    0.00151418   -0.0454707    0.0854856    0.0256873    0.0457228    0.00594706   0.0719891    0.0680317   -0.101715    -0.0641415   0.005394    -0.0971469   0.0873107   0.060764     0.063283      0.0573736   -0.109126      0.0710579 
  0.00709661   -0.000418112   0.0704369   0.074045    -0.0507553   -0.134185    -0.0927799   -0.0503806    0.0354143     0.00961919  -0.202725     0.0197262    0.130216     0.00115892   0.016194    -0.00576319   0.136052    -0.0108355  -0.0144547    0.225381    0.11541    -0.156224    -0.126691      0.0214346   -0.000797812   0.0949748 
 -0.153019      0.0232847     0.0579167   0.00910314   0.00143215   0.220056     0.0602126   -0.0712731    0.17805      -0.155546    -0.128599    -0.0169143    8.90644e-6   0.0849081   -0.0928286    0.12815      0.0592162   -0.0522838   0.142984    -0.0281459   0.0941941   0.0588353   -0.0806447    -0.115848    -0.0906145     0.0390906 
 -0.0515374     0.0134211    -0.0526789   0.00928206   0.0340073    0.0153131    0.0209105   -0.0792226    0.340528      0.13634      0.139718     0.0230438   -0.0122238   -0.0627639   -0.00382006   0.0272108    0.035131    -0.107869    0.13506      0.0484406  -0.0760387  -0.0373993   -0.14637      -0.00942965  -0.0422461    -0.204879  
  0.0236304     0.0584512     0.0368378  -0.0705765   -0.0794549   -0.0180618    0.0162226   -0.0988576   -0.112787     -0.00611702   0.099889    -0.00218475   0.0208712    0.119866    -0.0908423    0.010632     0.0205808    0.103462    0.0115708   -0.185614    0.0386922   0.0877318   -0.111236     -0.209434    -0.0907708    -0.0338474 
  0.0214726     0.00973891   -0.196446   -0.0198434   -0.102399     0.00712773   0.0837559    0.00501476   0.127521      0.149559     0.0868192    0.0917322   -0.0576679   -0.166544    -0.0251286    0.038179     0.176271    -0.112821   -0.116169    -0.0497382  -0.0561987  -0.0393162   -0.085402     -0.0428025   -0.0518423    -0.0846492 
 -0.0995107    -0.160927     -0.0225531   0.0422991    0.0346275    0.0201688   -0.115896    -0.0302979    0.150348     -0.327477     0.0634532   -0.142532    -0.19481     -0.0799766   -0.048291    -0.0872164    0.0262097   -0.0154941  -0.117873     0.0966551   0.115598    0.0460476    0.0356089     0.00621729  -0.174555     -0.103881  
 -0.124001     -0.157594     -0.0793844   0.00528256   0.0773563    0.055547    -0.24808     -0.0342912    0.15824       0.555205    -0.0562301   -0.186602    -0.213025     0.0244122   -0.100868    -0.0825429    0.0377015    0.0981183  -0.115871     0.173765    0.0505415   0.0763552    0.231043      0.0860189   -0.0630653    -0.0829988 
 -0.243001     -0.000247543  -0.0409768  -0.00745308  -0.0770029    0.141959     0.151699    -0.0291259    0.195537     -0.24159      0.0751703   -0.0324281    0.0392907    0.118373    -0.0400543   -0.148931    -0.0970645    0.0415231   0.111578     0.0512242  -0.20739    -0.12412      0.169383     -0.131187    -0.158899      0.0137393 
  0.998182     -0.00681556   -0.0668363   0.00161359  -0.0443933    0.180424     0.200808     0.200543     0.203439     -0.0818193   -0.497038    -0.0326276    0.0464621   -0.0361938    0.00887061  -0.109807    -0.0800789    0.0499412   0.0725908    0.16349    -0.0350545  -0.0140023    0.164313     -0.14427     -0.0763079    -0.10542   
 -0.0119252    -0.0656781     0.0474131  -0.188714    -0.0522275   -0.0447282   -0.056897     0.0260708    0.126103      0.0210864   -0.022863    -0.155961    -0.00778306  -0.133188     0.140862     0.0062222   -0.0265213    0.108506    0.117185     0.0685887  -0.0125051   0.0464178    0.0505458     0.0865695   -0.0282193     0.0293677 
  0.0169773     0.0230388    -0.0220573   0.102589    -0.0994556   -0.0944757   -0.0406024    0.0624522   -0.150014      0.0349129    0.149015    -0.0305677    0.0385923   -0.0116188   -0.0216454    0.0990606    0.247534    -0.0292568  -0.168266    -0.0324309   0.0326392   0.143037    -0.188388     -0.133223     0.055398      0.0166018 
  0.0336655     0.0298465    -0.0107813  -0.0675388   -0.102246    -0.0310346   -0.223196    -0.0643722    0.0238543    -0.238686    -0.0651908   -0.293477     0.105625     0.125435    -0.13159      0.109623     0.225419     0.0566975  -0.158147     0.0279385   0.0161756  -0.272249     0.000146223   0.237939     0.0361072     0.113284  
 -0.0705653     0.0452542     0.181527    0.095446     0.0230306   -0.0663179   -0.0467362   -0.0337199   -0.00798372    0.0436248   -0.0194511   -0.0617861   -0.0141178   -0.0382806   -0.00826144  -0.0998018   -0.0393057   -0.0795633   0.0956435    0.162627    0.0906455  -0.162404     0.33535       0.0851476   -0.0752178    -0.132029  
  0.0545643     0.121691     -0.0223349   0.138986    -0.00593965   0.0354227    0.014644     0.0274051    0.0758073     0.0552929   -0.0294633    0.145676    -0.0252251   -0.110189     0.127241    -0.0116123   -0.0757856   -0.16253    -0.142409    -0.0260688  -0.0409035   0.00473798  -0.0794484     0.0873708    0.119993      0.139041  
  0.0529819    -0.0289954    -0.0179064   0.0910239    0.0682699   -0.102343    -0.0485182    0.0820041   -0.165613      0.0739662   -0.00158388  -0.143322     0.0162642    0.0620629   -0.0581975    0.00671315  -0.0692341    0.151307   -0.0286232   -0.0851446   0.0418031  -0.0542404    0.133305     -0.00163439   0.143359     -0.164207  
 -0.0579903     0.0784299     0.143615   -0.0124616    0.0291488    0.114682    -0.0686018   -0.104505    -0.11467       0.146245     0.111845     0.0281485   -0.0992556   -0.0326383   -0.0814839    0.0548257   -0.12728      0.0569215   0.0630181    0.0708689   0.0192364   0.0467664   -0.0616603    -0.0406526   -0.103324     -0.0574985 
  0.0394205    -0.018803      0.0735798   0.00846685  -0.0304347   -0.0350631   -0.034243    -0.0643583   -0.0332493    -0.0589468   -0.00788965   0.0178058    0.125714    -0.0456876   -0.0395146   -0.0458636    0.00528561   0.0136267   0.0758172    0.060288   -0.0305474  -0.0486659   -0.0824552     0.128373    -0.046152      0.0334158 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 4 11 13 14 23 24 26 27 28
INFO: iteration 1, average log likelihood -1.060499
WARNING: Variances had to be floored 3 4 11 13 14 16 19 23 24 26 27 28
INFO: iteration 2, average log likelihood -1.040972
WARNING: Variances had to be floored 3 4 9 10 11 13 14 16 23 24 26 27 28
INFO: iteration 3, average log likelihood -1.033584
WARNING: Variances had to be floored 3 4 8 11 13 14 16 19 23 24 26 27 28
INFO: iteration 4, average log likelihood -1.045568
WARNING: Variances had to be floored 3 4 11 13 14 16 23 24 26 27 28
INFO: iteration 5, average log likelihood -1.052863
WARNING: Variances had to be floored 3 4 9 10 11 13 14 16 19 23 24 26 27 28
INFO: iteration 6, average log likelihood -1.034536
WARNING: Variances had to be floored 3 4 11 13 14 16 23 24 26 27 28
INFO: iteration 7, average log likelihood -1.049972
WARNING: Variances had to be floored 3 4 8 11 13 14 16 19 23 24 26 27 28
INFO: iteration 8, average log likelihood -1.037889
WARNING: Variances had to be floored 3 4 9 10 11 13 14 16 23 24 27 28
INFO: iteration 9, average log likelihood -1.043516
WARNING: Variances had to be floored 3 4 11 13 14 16 19 23 24 26 27 28
INFO: iteration 10, average log likelihood -1.045767
INFO: EM with 100000 data points 10 iterations avll -1.045767
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.626461e+05
      1       6.910926e+05      -1.715534e+05 |       32
      2       6.594399e+05      -3.165277e+04 |       32
      3       6.438759e+05      -1.556399e+04 |       32
      4       6.339221e+05      -9.953801e+03 |       32
      5       6.270213e+05      -6.900815e+03 |       32
      6       6.227834e+05      -4.237838e+03 |       32
      7       6.205073e+05      -2.276087e+03 |       32
      8       6.190150e+05      -1.492390e+03 |       32
      9       6.177127e+05      -1.302281e+03 |       32
     10       6.166396e+05      -1.073117e+03 |       32
     11       6.158517e+05      -7.878843e+02 |       32
     12       6.151838e+05      -6.679024e+02 |       32
     13       6.144308e+05      -7.529295e+02 |       32
     14       6.135236e+05      -9.072869e+02 |       32
     15       6.125808e+05      -9.427071e+02 |       32
     16       6.117058e+05      -8.750642e+02 |       32
     17       6.111748e+05      -5.309450e+02 |       32
     18       6.108479e+05      -3.269368e+02 |       32
     19       6.106084e+05      -2.394963e+02 |       32
     20       6.103935e+05      -2.149399e+02 |       32
     21       6.102085e+05      -1.850106e+02 |       32
     22       6.100496e+05      -1.588125e+02 |       32
     23       6.099284e+05      -1.212221e+02 |       32
     24       6.098486e+05      -7.977948e+01 |       32
     25       6.097960e+05      -5.259219e+01 |       32
     26       6.097616e+05      -3.446793e+01 |       32
     27       6.097377e+05      -2.389419e+01 |       32
     28       6.097187e+05      -1.895292e+01 |       30
     29       6.097017e+05      -1.701049e+01 |       31
     30       6.096908e+05      -1.093104e+01 |       31
     31       6.096823e+05      -8.526467e+00 |       29
     32       6.096755e+05      -6.772898e+00 |       27
     33       6.096682e+05      -7.277302e+00 |       31
     34       6.096590e+05      -9.231430e+00 |       28
     35       6.096491e+05      -9.892978e+00 |       29
     36       6.096386e+05      -1.044263e+01 |       30
     37       6.096299e+05      -8.715455e+00 |       27
     38       6.096185e+05      -1.139353e+01 |       29
     39       6.096022e+05      -1.630730e+01 |       31
     40       6.095725e+05      -2.968066e+01 |       31
     41       6.095321e+05      -4.041774e+01 |       31
     42       6.094648e+05      -6.733800e+01 |       31
     43       6.093714e+05      -9.340289e+01 |       31
     44       6.092376e+05      -1.337779e+02 |       32
     45       6.090711e+05      -1.665065e+02 |       32
     46       6.088556e+05      -2.155221e+02 |       32
     47       6.086718e+05      -1.837510e+02 |       32
     48       6.085487e+05      -1.231161e+02 |       32
     49       6.084951e+05      -5.360199e+01 |       32
     50       6.084676e+05      -2.751686e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 608467.5984468329)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.312404
INFO: iteration 2, average log likelihood -1.274892
INFO: iteration 3, average log likelihood -1.237218
INFO: iteration 4, average log likelihood -1.190599
WARNING: Variances had to be floored 13 20
INFO: iteration 5, average log likelihood -1.139878
WARNING: Variances had to be floored 5
INFO: iteration 6, average log likelihood -1.117574
WARNING: Variances had to be floored 9 10 16
INFO: iteration 7, average log likelihood -1.076776
WARNING: Variances had to be floored 8 13 14 20 22 32
INFO: iteration 8, average log likelihood -1.062912
WARNING: Variances had to be floored 2 5 6
INFO: iteration 9, average log likelihood -1.093529
WARNING: Variances had to be floored 27
INFO: iteration 10, average log likelihood -1.071490
WARNING: Variances had to be floored 8 10 13 20 30
INFO: iteration 11, average log likelihood -1.012850
WARNING: Variances had to be floored 5 9 14 16 22 25
INFO: iteration 12, average log likelihood -1.039253
WARNING: Variances had to be floored 2
INFO: iteration 13, average log likelihood -1.085587
WARNING: Variances had to be floored 8 27 30 32
INFO: iteration 14, average log likelihood -1.041365
WARNING: Variances had to be floored 5 10 13 20
INFO: iteration 15, average log likelihood -1.037140
WARNING: Variances had to be floored 9 14 22 25
INFO: iteration 16, average log likelihood -1.045023
WARNING: Variances had to be floored 2 8 16
INFO: iteration 17, average log likelihood -1.061799
WARNING: Variances had to be floored 5 20 32
INFO: iteration 18, average log likelihood -1.048525
WARNING: Variances had to be floored 10 13 27 30
INFO: iteration 19, average log likelihood -1.031095
WARNING: Variances had to be floored 8 9 14 16 20 22 25
INFO: iteration 20, average log likelihood -1.027119
WARNING: Variances had to be floored 2 5
INFO: iteration 21, average log likelihood -1.083519
WARNING: Variances had to be floored 13 32
INFO: iteration 22, average log likelihood -1.063432
WARNING: Variances had to be floored 8 10 20
INFO: iteration 23, average log likelihood -1.021945
WARNING: Variances had to be floored 2 5 9 14 22 25 27 30
INFO: iteration 24, average log likelihood -1.000531
WARNING: Variances had to be floored 13 16
INFO: iteration 25, average log likelihood -1.096058
WARNING: Variances had to be floored 8 10 20 32
INFO: iteration 26, average log likelihood -1.055563
WARNING: Variances had to be floored 5
INFO: iteration 27, average log likelihood -1.056027
WARNING: Variances had to be floored 2 9 13 14 22 25
INFO: iteration 28, average log likelihood -1.010733
WARNING: Variances had to be floored 8 10 16 20 27 30
INFO: iteration 29, average log likelihood -1.052770
WARNING: Variances had to be floored 5 32
INFO: iteration 30, average log likelihood -1.088282
INFO: iteration 31, average log likelihood -1.053492
WARNING: Variances had to be floored 2 5 8 9 10 13 14 20 22 25
INFO: iteration 32, average log likelihood -0.985286
WARNING: Variances had to be floored 16
INFO: iteration 33, average log likelihood -1.105235
WARNING: Variances had to be floored 27 30 32
INFO: iteration 34, average log likelihood -1.056713
WARNING: Variances had to be floored 5 8 13
INFO: iteration 35, average log likelihood -1.042334
WARNING: Variances had to be floored 2 9 10 14 20 22 25
INFO: iteration 36, average log likelihood -1.022164
WARNING: Variances had to be floored 16
INFO: iteration 37, average log likelihood -1.086600
WARNING: Variances had to be floored 5 8 13 32
INFO: iteration 38, average log likelihood -1.043474
WARNING: Variances had to be floored 27 30
INFO: iteration 39, average log likelihood -1.044126
WARNING: Variances had to be floored 2 9 10 13 14 20 22 25
INFO: iteration 40, average log likelihood -0.999950
WARNING: Variances had to be floored 5 8 16
INFO: iteration 41, average log likelihood -1.090880
WARNING: Variances had to be floored 32
INFO: iteration 42, average log likelihood -1.089266
WARNING: Variances had to be floored 13 27
INFO: iteration 43, average log likelihood -1.030425
WARNING: Variances had to be floored 2 5 8 9 10 14 16 20 22 25 30
INFO: iteration 44, average log likelihood -0.987667
INFO: iteration 45, average log likelihood -1.122480
WARNING: Variances had to be floored 13 32
INFO: iteration 46, average log likelihood -1.053917
WARNING: Variances had to be floored 5 8 10 27 30
INFO: iteration 47, average log likelihood -1.019154
WARNING: Variances had to be floored 2 9 14 20 22 25
INFO: iteration 48, average log likelihood -1.033423
WARNING: Variances had to be floored 13 16
INFO: iteration 49, average log likelihood -1.080215
WARNING: Variances had to be floored 5 8 10 32
INFO: iteration 50, average log likelihood -1.052160
INFO: EM with 100000 data points 50 iterations avll -1.052160
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0656694   -0.00436589    0.1418       0.0128646   -0.137908      0.135789      0.0330766   -0.00358114   0.0151413   -0.0462005   -0.13435      0.00794704  -0.141601    -0.119168    -0.0442091    0.121138     0.0921425  -0.0298671    0.156341   -0.129113     0.0217843   -0.0310489    0.0513127   -0.151735    -0.0903434  -0.0512052 
 -0.155538    -0.121875     -0.176787    -0.106957    -0.128154     -0.0577269     0.0612181   -0.0728142    0.0493937   -0.0104071    0.0270647    0.045135     0.0238599   -0.0183285    0.128191    -0.07673      0.0469252   0.030657     0.0292611   0.226767    -0.00936541  -0.0770518    0.144155     0.0787756   -0.0620692   0.0907904 
 -0.0322229    0.0112354     0.107251    -0.0698827    0.0408626    -0.0117787     0.00579392  -0.187927     0.0165756    0.0234411   -0.0694952    0.0027292    0.014139     0.0793137   -0.00656107  -0.0105654   -0.118595    0.00199444   0.028721    0.22532     -0.0158591   -0.0346477   -0.137071     0.040734     0.0822042   0.0556644 
 -0.0440639    0.0126049    -0.217556     0.0887543    0.0150269     0.0523176    -0.0304003   -0.012637    -0.0578925    0.150205     0.0501715    0.0139209   -0.105723     0.00444779  -0.160591     0.0326602    0.0132961  -0.0528361   -0.201126   -7.31506e-6  -0.0327615   -0.092497     0.00147359  -0.0746783   -0.0336899   0.236089  
  0.0598799    0.0692694     0.057038    -0.0492697   -0.129649     -0.056823      0.106617     0.119541     0.0978422   -0.0127326   -0.0711415   -0.114747    -0.161806     0.167369     0.0268665    0.00471824   0.015846   -0.0311657   -0.0333686  -0.0519763    0.048057    -0.004547    -0.139374     0.00854177   0.128258    0.119573  
 -0.0164965   -0.067834      0.0547484   -0.191639    -0.0507857    -0.0438364    -0.0578631    0.0229749    0.123118     0.0183205   -0.0189267   -0.156349    -0.00883401  -0.13175      0.149374     0.004386    -0.0230231   0.112321     0.11258     0.0782597   -0.016231     0.0386463    0.0527211    0.094298    -0.0309637   0.0252479 
  0.0252541    0.0117571    -0.187996    -0.0210945   -0.105795     -0.000274692   0.0780967    0.00765665   0.114216     0.144982     0.0823345    0.0859042   -0.0603759   -0.168459    -0.0298591    0.0385775    0.177836   -0.108756    -0.12237    -0.0415885   -0.0673422   -0.0410645   -0.0853487   -0.0433322   -0.0512906  -0.0938559 
  0.187245     0.00521396    0.222482     0.0736615    0.105595      0.157022     -0.0473994    0.0302956   -0.0021635   -0.00379501  -0.0257672    0.0849492    0.0891774    0.0378915   -0.0230426    0.151146     0.0587493  -0.0256775    0.106073   -0.0221766   -0.0764845    0.0122935   -0.0885418    0.177405     0.0737058  -0.056336  
 -0.0444636   -0.120667     -0.286629     0.0958188    0.104343      0.0201227     0.135322    -0.130811     0.15277      0.0087598    0.109536     0.0789265    0.0421113    0.0217529    0.136411     0.0514218   -0.0349244  -0.00921386   0.0741587  -0.230633     0.195984     0.119298     0.0445728   -0.0446891   -0.131718    0.161734  
 -0.0719805    0.0556963     0.192968     0.0993405    0.0179871    -0.0698462    -0.047432    -0.0229267   -0.00835537   0.0422186   -0.0178509   -0.0669808   -0.0172578   -0.0463053   -0.00512374  -0.0929594   -0.032613   -0.0806316    0.0925435   0.165789     0.0922722   -0.165847     0.336877     0.0861878   -0.0803664  -0.139164  
 -0.00813369   0.137286      0.193815     0.0172766    0.100389      0.175646      0.0197799   -0.0967892    0.0242515    0.259219     0.146138     0.00300376  -0.0554276   -0.0844876   -0.0966423    0.0596547   -0.110746    0.0978181    0.033465    0.0458881    0.134378     0.0664148   -0.0755567   -0.0267877   -0.0704509  -0.0370468 
 -0.154596     0.0232298     0.0596815    0.0109082    0.00350373    0.220943      0.060176    -0.0744253    0.180874    -0.158014    -0.126714    -0.018356     0.00301314   0.0866056   -0.0933604    0.13097      0.0585681  -0.0499451    0.142463   -0.0242331    0.0930135    0.0582796   -0.0803788   -0.115222    -0.0927159   0.0390419 
  0.0376465    0.0327106     0.00556358   0.137257    -0.0760878    -0.0973473    -0.0292323    0.0725906   -0.134448     0.0358085    0.139195    -0.0151365    0.0389926   -0.0149523   -0.0172602    0.0995299    0.234117   -0.0361639   -0.173134   -0.0440879    0.0373639    0.153874    -0.23267     -0.13047      0.0647926  -0.0011905 
  0.0470064    0.107802      0.0907863    0.0655572   -0.00501035    0.0702082     0.00605563   0.0257124   -0.021561     0.0663368    0.00188312  -0.0389955   -0.150058    -0.0494266   -0.112296     0.0223308    0.0286756   0.100973    -0.0502112  -0.0671017   -0.114191    -0.00484273  -0.107158    -0.109854     0.0333618  -0.19652   
 -0.138042     0.0230778     0.0917473   -0.185375    -0.000339032  -0.0697832     0.13845      0.0282474    0.0863244   -0.00743108   0.0367972   -0.0331291   -0.0138422    0.0716188    0.035206     0.0411796   -0.0149216   0.0668364   -0.0671317  -0.345534    -0.0245467   -0.0747973   -0.11382     -0.0207134    0.08167     0.00668464
  0.0736738    0.164904     -0.223428     0.232772     0.0363679     0.0387611     0.00829155  -0.0164219    0.191534     0.0713197   -0.0340764    0.0789866   -0.0924615   -0.284499     0.106428     0.108116    -0.014752   -0.133835    -0.135361    0.0612483    0.0411897   -0.0892837   -0.177471     0.119885     0.0617883   0.248536  
  0.0730886   -0.017319      0.10348      0.12081     -0.0713999     0.0313499     0.0969985   -0.0660153   -0.0225148   -0.0269963   -0.018101     0.219098    -0.0235322   -0.0777151    0.234575    -0.0671582   -0.127526   -0.0374901   -0.136906   -0.0612033   -0.119194     0.045439     0.0236358    0.125373     0.0710501   0.00860492
  0.157291    -0.0623706    -0.0862438   -0.118136     0.0690177     0.119129     -0.0220678   -0.0671502   -0.0361804    0.0456052   -0.131202     0.0837368    0.0130953    0.0787531   -0.0979411    0.00508007  -0.0794054  -0.0318879    0.0703479  -0.0324966   -0.0754993    0.131151     0.162611     0.0308524    0.0493198   0.03858   
 -0.111274    -0.159317     -0.0498012    0.0245547    0.0554811     0.0371677    -0.179169    -0.0322079    0.154352     0.0959446    0.00597076  -0.163472    -0.203946    -0.0302568   -0.0736727   -0.0849071    0.0316999   0.0391813   -0.11747     0.134426     0.0846784    0.0606612    0.129526     0.0442748   -0.120603   -0.0939048 
  0.0245206    0.0304725    -0.00325268  -0.0674845   -0.108995     -0.0391989    -0.220743    -0.061563     0.0109411   -0.226353    -0.0443953   -0.302032     0.0970068    0.132622    -0.122861     0.102429     0.240349    0.0620187   -0.158085    0.0254327    0.0192365   -0.267336     0.00292544   0.219542     0.0370507   0.11808   
  0.0564423   -0.0309096    -0.0177574    0.0967637    0.0686912    -0.102401     -0.049893     0.0826495   -0.168908     0.0662227   -0.00086541  -0.143236     0.0147489    0.0659397   -0.0590392    0.00753261  -0.0684838   0.150021    -0.0316227  -0.0786897    0.0418812   -0.0568034    0.131627    -0.00269328   0.145468   -0.165086  
 -0.114087     0.00438134   -0.0405041   -0.0172445   -0.280742      0.0475156    -0.0621712   -0.0794171   -0.1633      -0.0827466    0.0433641   -0.0064668    0.0544105   -0.0287711    0.0285217    0.0794311   -0.151224   -0.129446    -0.071543    0.044663    -0.00236303   0.0155102    0.0387966    0.139364    -0.0873104  -0.026517  
 -0.148159     0.0539021     0.207168    -0.0781       0.00669345    0.275024      0.0888667    0.0501189   -0.0525618    0.079663     0.147844     0.0128304   -0.00829772  -0.116746     0.0216263    0.0438569   -0.0838969   0.0165517   -0.0338645   0.137241    -0.0436172    0.122046     0.00706027   0.166374     0.152696    0.204332  
  0.0915347   -0.0279612     0.137602    -0.180916     0.170297     -0.145414      0.0210126    0.0404286   -0.0379914    0.00371294  -0.0280923   -0.0542228   -0.08335     -0.080839     0.0469697   -0.0401014    0.0539393   0.120381    -0.0384564   0.212989    -0.00704095   0.026092    -0.0731094   -0.065516    -0.16377    -0.230917  
  0.0107       0.0635046     0.0460238   -0.0653351   -0.0672819    -0.0200722     0.0226602   -0.0966859   -0.130694    -0.00396302   0.110385     0.0100254    0.00692144   0.140303    -0.096387     0.0227555    0.0400109   0.0858513   -0.0063362  -0.178059     0.0305581    0.0878328   -0.129992    -0.193229    -0.0762244  -0.0518929 
  0.139005    -0.0509881     0.0463708    0.104015    -0.115994     -0.0704656    -0.107669     0.0225798   -0.0905024   -0.143018     0.0674007    0.0349901    0.245974    -0.173678    -0.0667919   -0.0888749    0.134698    0.0218733    0.121584   -0.00792214  -0.0468456   -0.0745361   -0.040298     0.224093    -0.18629     0.0282624 
  0.00526731   0.138455      0.0689623    0.0556919   -0.0391817    -0.149616     -0.120104    -0.0564458    0.0704398    0.00936461  -0.225215     0.0237808    0.166118     0.0528147   -0.0100061   -0.0209483    0.128125   -0.0241543   -0.0234036   0.307881     0.111321    -0.145216    -0.121252    -0.0147659   -0.0395379   0.115173  
 -0.111476     0.0310598     0.10214     -0.035136    -0.0294123     0.0728031    -0.173088    -0.1046      -0.249097     0.0192241    0.0745624    0.0492905   -0.148288     0.0133977   -0.0686706    0.050182    -0.150652    0.0116363    0.0901113   0.0911631   -0.0966676    0.0241639   -0.0448485   -0.0400484   -0.117558   -0.0906686 
  0.0245137   -0.00185215   -0.0133828   -0.00460075   0.0171159     0.0399588    -0.011615    -0.210557    -0.0791639   -0.0145181   -0.0485919   -0.193031    -0.0429188   -0.205464     0.00690278   0.0356511   -0.0837252  -0.0618074   -0.115369    0.0560356    0.0649301   -0.0813314    0.0799316    0.0146453    0.0429065  -0.00113119
 -0.110803    -0.258276      0.103942     0.194696     0.0649487    -0.0773007    -0.0129431   -0.0456077   -0.120612     0.00540502  -0.0914537   -0.0530991   -0.0473308   -0.0825117    0.125474     0.103912     0.0892705  -0.0118937    0.0955517  -0.0851796    0.144811     0.0858754    0.103439     0.245366     0.0559238   0.0936215 
 -0.0505978    0.0135021    -0.0539949    0.0080737    0.0279784     0.0142361     0.0200836   -0.0796373    0.340048     0.137875     0.142216     0.0229385   -0.0121773   -0.0620935   -0.006185     0.0270321    0.0357085  -0.106662     0.130397    0.050328    -0.0706288   -0.0366343   -0.146951    -0.0106617   -0.0420826  -0.206491  
  0.0500251   -0.000402578  -0.0475046   -0.00557643  -0.0687999     0.153911      0.164592     0.0252775    0.199076    -0.207913    -0.0582754   -0.0320177    0.0418103    0.0852036   -0.032891    -0.140741    -0.0956972   0.0444844    0.101753    0.0824972   -0.171616    -0.101599     0.16944     -0.134799    -0.138532   -0.0138001 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 20
INFO: iteration 1, average log likelihood -1.047671
WARNING: Variances had to be floored 2 9 13 14 20 22 25 27 30
INFO: iteration 2, average log likelihood -0.975332
WARNING: Variances had to be floored 5 8 10 13 16 20 27 30 32
INFO: iteration 3, average log likelihood -0.988694
WARNING: Variances had to be floored 2 9 14 20 22 25
INFO: iteration 4, average log likelihood -1.015732
WARNING: Variances had to be floored 13 15 20 27 30
INFO: iteration 5, average log likelihood -0.994083
WARNING: Variances had to be floored 2 5 8 9 10 13 14 16 20 22 25 27 30
INFO: iteration 6, average log likelihood -0.968214
WARNING: Variances had to be floored 20 32
INFO: iteration 7, average log likelihood -1.034264
WARNING: Variances had to be floored 2 9 10 13 14 20 22 25 27 30
INFO: iteration 8, average log likelihood -0.976114
WARNING: Variances had to be floored 5 8 16 20 30
INFO: iteration 9, average log likelihood -1.009316
WARNING: Variances had to be floored 2 9 13 14 20 22 25 27 32
INFO: iteration 10, average log likelihood -0.987698
INFO: EM with 100000 data points 10 iterations avll -0.987698
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0121653    0.0737296    0.00211032  -0.186662     -0.137722    -0.0516926    0.0712412   -0.0794164   -0.0622368    -0.166699    -0.00176329  -0.0872475    0.0213931    0.00710688  -0.040932    -0.159795     -0.0768717    0.062972    0.226799     -0.189059     0.0911229   -0.0555422   0.100445      0.0509681    0.0389314  -0.0278322  
  0.144736     0.0332622   -0.0125755   -0.000544009  -0.110453    -0.0570863   -0.075554    -0.242933     0.0210369    -0.0297545   -0.0723274    0.0177143    0.00962103  -0.0595488    0.0846772    0.133106      0.0541322   -0.13135     0.0307125     0.00329462   0.15316      0.0683675   0.174286     -0.110924     0.020231   -0.00145009 
  0.0418639   -0.00407818  -0.285285     0.286397      0.0505848    0.0131372   -0.0673748    0.101219    -0.0317753    -0.0435088    0.0412107    0.0582639    0.0300831    0.0505532    0.188191     0.0380828     0.0340598   -0.0717548  -0.0859251    -0.12939     -0.0873903   -0.0256656  -0.0872438     0.0236182   -0.220622   -0.159989   
  0.11693     -0.00168061  -0.0412092   -0.123605     -0.0598219   -0.0755324    0.0568329   -0.0674215   -0.105842      0.0717755   -0.0702218   -0.0430604   -0.0269635   -0.0707508    0.0505434    0.12723      -0.0216312    0.150735   -0.207313      0.237988     0.0505611    0.247088    0.0565915    -0.0455375    0.0420551   0.235205   
  0.0904768    0.0212653    0.0130569   -0.123549      0.0745792   -0.00652101  -0.0194598   -0.0276163    0.0997217     0.17326      0.0999831   -0.156952    -0.0227081   -0.00792057  -0.10537      0.0669047    -0.207998    -0.0229582  -0.128977     -0.0660748    0.164653     0.140605   -0.0449449     0.00946626   0.0372312   0.0881833  
 -0.031045    -0.0492416    0.129446    -0.16577       0.0550123   -0.119909    -0.0675226   -0.0494203    0.155359      0.111444    -0.0666392   -0.00511389   0.0846628    0.015679    -0.0727883   -0.0380395     0.122002     0.0525251   0.0286413     0.0161304    0.0125182    0.207112   -0.17027       0.0944774   -0.142966    0.0123295  
  0.0422199    0.148461     0.138895     0.0137761     0.0162543   -0.0453921    0.0440471   -0.0830883    0.00452319    0.192476    -0.0599717    0.109889    -0.052774    -0.152573    -0.0427078    0.102775     -0.081951     0.0271368   0.0744837     0.0594918   -0.0888627   -0.0182802  -0.0517053     0.191994     0.185156   -0.013314   
  0.0350321   -0.125062    -0.22993     -0.224445      0.104061     0.149097    -0.0481784    0.0541753   -0.0153952     0.0405068    0.0661663   -0.163485    -0.140051     0.151321     0.146751    -0.0401344     0.0688035   -0.133118    0.00827762    0.0291149    0.122933     0.219864   -0.123551      0.0483587   -0.121722   -0.0442443  
 -0.00983355   0.0198885   -0.157482    -0.0394962    -0.00524354  -0.0374514   -0.0580832   -0.0251958   -0.0313476    -0.0879736   -0.0518949    0.131799    -0.0303838   -0.124095    -0.0675501    0.0269654    -0.1859      -0.0560721  -0.030635      0.0653053   -0.0308573    0.0283149   0.247734      0.00845603   0.121329   -0.116585   
 -0.0130429   -0.110768     0.100778     0.0169099    -0.0394876    0.0799941    0.0440808    0.0307516    0.106401     -0.25658      0.0631274   -0.0324214    0.0214449   -0.0723896   -0.0689362    0.00156702    0.0878054   -0.078236   -0.0181014     0.0407402    0.13433      0.0253782   0.000937728   0.0988217    0.140778   -0.0907882  
 -0.118673    -0.0851598    0.0896221    0.0438228    -0.0394012    0.159411     0.0314299    0.195179    -0.129251      0.182008     0.0452059   -0.0242242   -0.0478398   -0.0706673    0.0442918   -0.0934513    -0.013793    -0.0763466   0.058467      0.0433953   -0.113201    -0.075749    0.159727      0.0897932    0.0756058  -0.00916074 
  0.0624778    0.118613     0.111891     0.115674      0.20165     -0.108614     0.0346951   -0.120921    -0.0167085     0.14882      0.0523456    0.148547    -0.0155847   -0.0888105    0.0887852   -0.144943      0.0876453   -0.0441525  -0.027007      0.0315933   -0.0744473    0.137745   -0.0527332     0.0872959   -0.032438   -0.0657803  
 -0.0398386    0.0371033    0.0643749   -0.185542      0.145376     0.054072     0.125271     0.109818     0.0574771    -0.0411004   -0.0555236    0.0385525    0.080714    -0.105606    -0.127452     0.0164244     0.0433228   -0.183909    0.0105768     0.0726522    0.240304    -0.120229    0.028432     -0.117743     0.125627   -0.177848   
  0.140973     0.0750197    0.102198     0.0743278    -0.00620884   0.121515     0.126399    -0.188598    -0.179819     -0.00393937  -0.119513    -0.120721     0.121019    -0.224133     0.0326771    0.0226627     0.111858     0.0495984  -0.139762     -0.122742     0.0100626   -0.0322401  -0.0415536    -0.0269753    0.0183648   0.0347283  
  0.051346    -0.00659543  -0.102664     0.0187182     0.0919714   -0.0534011    0.0932023   -0.0865554    0.120681     -0.243025     0.0819345   -0.0439387   -0.0834255   -0.158067    -0.0353972    0.0303639     0.200424    -0.0540115   0.0817163     0.0268896   -0.011528     0.0921651  -0.0954715     0.059759     0.181847    0.0639687  
 -0.00284934   0.0037802    0.0605547    0.0732088     0.0800303    0.0512889    0.00548154  -0.0433153    0.0501548     0.0310373    0.0925385    0.0818199    0.0159846   -0.0566261   -0.0148512   -0.0127646     0.119867    -0.0590372  -0.0844585    -0.196492     0.120924     0.0205309   0.0258377     0.216742     0.144345    0.0495358  
 -0.0408495   -0.0221514    0.0373255    0.033955      0.108406     0.279746    -0.143442    -0.0434172    0.102839      0.0869327    0.0112173    0.0899689   -0.018554    -0.124972    -0.0032983   -0.0466622     0.00126241   0.177977   -0.000225286   0.0676022   -0.0586853    0.0114686   0.176481      0.0665545    0.0727898   0.0441712  
  0.00556343   0.159569     0.0806956   -0.0632821     0.0808154    0.0444417   -0.0573629    0.111843    -0.00829578   -0.12368      0.0917326   -0.109732    -0.0671355    0.16064     -0.208494     0.192828     -0.0578904    0.0304603  -0.0387331     0.0874124    0.124948     0.0851886   0.00974611   -0.130886     0.0092692   0.0260111  
  0.0641986   -0.102573    -0.163374     0.0564087    -0.151258    -0.0249892    0.0495706    0.183818     0.049351     -0.227674     0.16806      0.02554      0.0372659   -0.0379668   -0.070315     0.0640127    -0.0593941    0.0653071  -0.0711886     0.109708     0.027765     0.271926    0.118375     -0.0476081   -0.121222    0.147324   
  0.00937062   0.189049     0.197075     0.0233311    -0.119313     0.0614517    0.144576    -0.0281569   -0.0192799    -0.103548    -0.114982    -0.149116     0.0306396   -0.254968    -0.113096     0.0288021    -0.0292307    0.0423759  -0.144368      0.0772833   -0.158103    -0.0527432  -0.0924087     0.0671961   -0.151228   -0.220613   
 -0.0349756   -0.213531    -0.0248496    0.0295118     0.0723253    0.075439     0.0192443   -0.00507937  -0.00427218    0.145364     0.0985738    0.0872554   -0.118866    -0.0780819    0.0399042    0.0572406     0.027505     0.0712796   0.127759      0.0257893    0.0559428    0.104158    0.0426409     0.0283893    0.13404    -0.110281   
  0.00193186  -0.363874     0.021292    -0.0165247     0.105476     0.080276    -0.0738529   -0.0857853    0.00844955    0.102519    -0.0273011    0.126243    -0.04934      0.0507171    0.0709163    0.00927308    0.139484     0.0404684   0.175236     -0.0440967   -0.126819    -0.0551016   0.0361763     0.19255      0.21135    -0.0267442  
 -0.0301257   -0.114447    -0.128586    -0.227501      0.0761572   -0.0675247    0.0325673    0.00693641   0.0312258    -0.0475113    0.143606    -0.217988    -0.038337    -0.00608144  -0.129528     0.188351     -0.0712339   -0.0420458   0.0817141    -0.199053     0.263107    -0.0404443   0.175358     -0.0220314   -0.0436881   0.27408    
 -0.0725215   -0.0642263   -0.157477     0.0516305     0.051469    -0.0650311    0.0444305    0.022953     0.0710845    -0.177938     0.235521     0.0414189    0.0240716   -0.130147     0.0647261   -0.00171161    0.0167524   -0.184212    0.111252      0.0518876   -6.85754e-5  -0.172064    0.0807114    -0.0270651    0.117049    0.163179   
 -0.0854455    0.0386194    0.0583886    0.0387608     0.0840073   -0.00522056  -0.111918    -0.0962265   -0.0917974     0.0257003   -0.0680932    0.055642    -0.233383     0.061533     0.0237923   -0.0068653    -0.126314    -0.0148908   0.098877      0.0137903    0.115898    -0.200481   -0.104045     -0.251734    -0.0859532   0.0171805  
  0.150402    -0.0650047   -0.011751     0.0323773    -0.0229889    0.103697     0.194139     0.0343576   -0.222666     -0.116988    -0.147957     0.0460573    0.00835587   0.0393965   -0.118265    -0.112529      0.00203788  -0.0467195   0.0427387     0.131509    -0.0321734    0.198096    0.0968202     0.063681     0.200336   -0.151657   
 -0.127493    -0.0440994    0.05486      0.137798      0.0198565   -0.0159392    0.12232      0.138186     0.192485      0.0194277    0.0322456    0.0987259    0.189402     0.141797    -0.122389    -0.166212     -0.108518    -0.0671926  -0.160614     -0.0741144    0.150643    -0.0330154  -0.0212998    -0.0173706   -0.296713   -0.062047   
 -0.0591344   -0.157379     0.0154033    0.0325059     0.181259    -0.120468    -0.0106501    0.0190931    0.110504     -0.228676     0.0770897    0.24216     -0.139097     0.238618    -0.0168212   -0.0234896     0.0130134   -0.153637   -0.229613      0.137801    -0.192411    -0.0376674  -0.0649629    -0.0300647    0.143488    0.0808615  
  0.0515402    0.157654    -0.157386     0.00706365    0.0392796   -0.0231623   -0.226484    -0.00693414  -0.000767236  -0.0634943    0.0258538   -0.140482     0.0041271   -0.0912688   -0.230692    -0.0884256    -0.155546    -0.119386   -0.0185402    -0.11138     -0.158376    -0.0532206  -0.0339618     0.172589     0.297911   -0.142946   
  0.0208213    0.0659219   -0.0604874    0.0430266     0.0385043   -0.0671534    0.0609718    0.0681819    0.100014     -0.169072    -0.0757463    0.0456173   -0.0869822    0.0782388    0.00169122  -0.0559247     0.111052    -0.218672   -0.131342     -0.0198913   -0.01641      0.0233725   0.0278591     0.155874     0.269903   -0.000426553
  0.062712     0.0602213    0.0219315    0.013156     -0.151473    -0.127703    -0.102449     0.086025     0.020178      0.220436    -0.0324099   -0.0410352   -0.0662051   -0.112467    -0.0257311    0.000644481  -0.058319    -0.136749   -0.0722118    -0.0615314   -0.0201917   -0.0506899  -0.128413     -0.0449747   -0.163545   -0.0209924  
 -0.111038    -0.132218     0.00857765  -0.0433033    -0.194041     0.00599054  -0.141678     0.134837     0.0123468     0.0991622    0.15388     -0.0516587   -0.0500537    0.0755361    0.198155    -0.0115442    -0.122287     0.196718    0.068572      0.00140984   0.206435    -0.0830883   0.0416106     0.100687    -0.0589151   0.0133278  kind full, method split
0: avll = -1.4233358904159887
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423355
INFO: iteration 2, average log likelihood -1.423269
INFO: iteration 3, average log likelihood -1.423201
INFO: iteration 4, average log likelihood -1.423123
INFO: iteration 5, average log likelihood -1.423028
INFO: iteration 6, average log likelihood -1.422911
INFO: iteration 7, average log likelihood -1.422761
INFO: iteration 8, average log likelihood -1.422552
INFO: iteration 9, average log likelihood -1.422223
INFO: iteration 10, average log likelihood -1.421683
INFO: iteration 11, average log likelihood -1.420873
INFO: iteration 12, average log likelihood -1.419888
INFO: iteration 13, average log likelihood -1.419012
INFO: iteration 14, average log likelihood -1.418461
INFO: iteration 15, average log likelihood -1.418197
INFO: iteration 16, average log likelihood -1.418086
INFO: iteration 17, average log likelihood -1.418041
INFO: iteration 18, average log likelihood -1.418023
INFO: iteration 19, average log likelihood -1.418015
INFO: iteration 20, average log likelihood -1.418012
INFO: iteration 21, average log likelihood -1.418010
INFO: iteration 22, average log likelihood -1.418009
INFO: iteration 23, average log likelihood -1.418008
INFO: iteration 24, average log likelihood -1.418008
INFO: iteration 25, average log likelihood -1.418007
INFO: iteration 26, average log likelihood -1.418007
INFO: iteration 27, average log likelihood -1.418007
INFO: iteration 28, average log likelihood -1.418007
INFO: iteration 29, average log likelihood -1.418006
INFO: iteration 30, average log likelihood -1.418006
INFO: iteration 31, average log likelihood -1.418006
INFO: iteration 32, average log likelihood -1.418006
INFO: iteration 33, average log likelihood -1.418006
INFO: iteration 34, average log likelihood -1.418006
INFO: iteration 35, average log likelihood -1.418005
INFO: iteration 36, average log likelihood -1.418005
INFO: iteration 37, average log likelihood -1.418005
INFO: iteration 38, average log likelihood -1.418005
INFO: iteration 39, average log likelihood -1.418005
INFO: iteration 40, average log likelihood -1.418005
INFO: iteration 41, average log likelihood -1.418005
INFO: iteration 42, average log likelihood -1.418005
INFO: iteration 43, average log likelihood -1.418005
INFO: iteration 44, average log likelihood -1.418005
INFO: iteration 45, average log likelihood -1.418005
INFO: iteration 46, average log likelihood -1.418005
INFO: iteration 47, average log likelihood -1.418005
INFO: iteration 48, average log likelihood -1.418005
INFO: iteration 49, average log likelihood -1.418005
INFO: iteration 50, average log likelihood -1.418005
INFO: EM with 100000 data points 50 iterations avll -1.418005
952.4 data points per parameter
1: avll = [-1.42335,-1.42327,-1.4232,-1.42312,-1.42303,-1.42291,-1.42276,-1.42255,-1.42222,-1.42168,-1.42087,-1.41989,-1.41901,-1.41846,-1.4182,-1.41809,-1.41804,-1.41802,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418020
INFO: iteration 2, average log likelihood -1.417935
INFO: iteration 3, average log likelihood -1.417862
INFO: iteration 4, average log likelihood -1.417776
INFO: iteration 5, average log likelihood -1.417673
INFO: iteration 6, average log likelihood -1.417560
INFO: iteration 7, average log likelihood -1.417449
INFO: iteration 8, average log likelihood -1.417350
INFO: iteration 9, average log likelihood -1.417269
INFO: iteration 10, average log likelihood -1.417205
INFO: iteration 11, average log likelihood -1.417153
INFO: iteration 12, average log likelihood -1.417111
INFO: iteration 13, average log likelihood -1.417078
INFO: iteration 14, average log likelihood -1.417051
INFO: iteration 15, average log likelihood -1.417031
INFO: iteration 16, average log likelihood -1.417015
INFO: iteration 17, average log likelihood -1.417003
INFO: iteration 18, average log likelihood -1.416993
INFO: iteration 19, average log likelihood -1.416986
INFO: iteration 20, average log likelihood -1.416979
INFO: iteration 21, average log likelihood -1.416973
INFO: iteration 22, average log likelihood -1.416968
INFO: iteration 23, average log likelihood -1.416963
INFO: iteration 24, average log likelihood -1.416958
INFO: iteration 25, average log likelihood -1.416954
INFO: iteration 26, average log likelihood -1.416950
INFO: iteration 27, average log likelihood -1.416946
INFO: iteration 28, average log likelihood -1.416942
INFO: iteration 29, average log likelihood -1.416938
INFO: iteration 30, average log likelihood -1.416935
INFO: iteration 31, average log likelihood -1.416931
INFO: iteration 32, average log likelihood -1.416928
INFO: iteration 33, average log likelihood -1.416925
INFO: iteration 34, average log likelihood -1.416922
INFO: iteration 35, average log likelihood -1.416919
INFO: iteration 36, average log likelihood -1.416917
INFO: iteration 37, average log likelihood -1.416915
INFO: iteration 38, average log likelihood -1.416912
INFO: iteration 39, average log likelihood -1.416910
INFO: iteration 40, average log likelihood -1.416908
INFO: iteration 41, average log likelihood -1.416907
INFO: iteration 42, average log likelihood -1.416905
INFO: iteration 43, average log likelihood -1.416903
INFO: iteration 44, average log likelihood -1.416902
INFO: iteration 45, average log likelihood -1.416901
INFO: iteration 46, average log likelihood -1.416899
INFO: iteration 47, average log likelihood -1.416898
INFO: iteration 48, average log likelihood -1.416897
INFO: iteration 49, average log likelihood -1.416896
INFO: iteration 50, average log likelihood -1.416895
INFO: EM with 100000 data points 50 iterations avll -1.416895
473.9 data points per parameter
2: avll = [-1.41802,-1.41794,-1.41786,-1.41778,-1.41767,-1.41756,-1.41745,-1.41735,-1.41727,-1.4172,-1.41715,-1.41711,-1.41708,-1.41705,-1.41703,-1.41701,-1.417,-1.41699,-1.41699,-1.41698,-1.41697,-1.41697,-1.41696,-1.41696,-1.41695,-1.41695,-1.41695,-1.41694,-1.41694,-1.41693,-1.41693,-1.41693,-1.41693,-1.41692,-1.41692,-1.41692,-1.41691,-1.41691,-1.41691,-1.41691,-1.41691,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416906
INFO: iteration 2, average log likelihood -1.416848
INFO: iteration 3, average log likelihood -1.416798
INFO: iteration 4, average log likelihood -1.416739
INFO: iteration 5, average log likelihood -1.416665
INFO: iteration 6, average log likelihood -1.416575
INFO: iteration 7, average log likelihood -1.416471
INFO: iteration 8, average log likelihood -1.416358
INFO: iteration 9, average log likelihood -1.416246
INFO: iteration 10, average log likelihood -1.416142
INFO: iteration 11, average log likelihood -1.416051
INFO: iteration 12, average log likelihood -1.415973
INFO: iteration 13, average log likelihood -1.415906
INFO: iteration 14, average log likelihood -1.415849
INFO: iteration 15, average log likelihood -1.415799
INFO: iteration 16, average log likelihood -1.415756
INFO: iteration 17, average log likelihood -1.415718
INFO: iteration 18, average log likelihood -1.415684
INFO: iteration 19, average log likelihood -1.415654
INFO: iteration 20, average log likelihood -1.415627
INFO: iteration 21, average log likelihood -1.415603
INFO: iteration 22, average log likelihood -1.415581
INFO: iteration 23, average log likelihood -1.415561
INFO: iteration 24, average log likelihood -1.415543
INFO: iteration 25, average log likelihood -1.415526
INFO: iteration 26, average log likelihood -1.415511
INFO: iteration 27, average log likelihood -1.415498
INFO: iteration 28, average log likelihood -1.415486
INFO: iteration 29, average log likelihood -1.415474
INFO: iteration 30, average log likelihood -1.415464
INFO: iteration 31, average log likelihood -1.415455
INFO: iteration 32, average log likelihood -1.415446
INFO: iteration 33, average log likelihood -1.415439
INFO: iteration 34, average log likelihood -1.415431
INFO: iteration 35, average log likelihood -1.415424
INFO: iteration 36, average log likelihood -1.415418
INFO: iteration 37, average log likelihood -1.415412
INFO: iteration 38, average log likelihood -1.415407
INFO: iteration 39, average log likelihood -1.415401
INFO: iteration 40, average log likelihood -1.415396
INFO: iteration 41, average log likelihood -1.415391
INFO: iteration 42, average log likelihood -1.415386
INFO: iteration 43, average log likelihood -1.415382
INFO: iteration 44, average log likelihood -1.415377
INFO: iteration 45, average log likelihood -1.415373
INFO: iteration 46, average log likelihood -1.415369
INFO: iteration 47, average log likelihood -1.415365
INFO: iteration 48, average log likelihood -1.415361
INFO: iteration 49, average log likelihood -1.415357
INFO: iteration 50, average log likelihood -1.415353
INFO: EM with 100000 data points 50 iterations avll -1.415353
236.4 data points per parameter
3: avll = [-1.41691,-1.41685,-1.4168,-1.41674,-1.41667,-1.41658,-1.41647,-1.41636,-1.41625,-1.41614,-1.41605,-1.41597,-1.41591,-1.41585,-1.4158,-1.41576,-1.41572,-1.41568,-1.41565,-1.41563,-1.4156,-1.41558,-1.41556,-1.41554,-1.41553,-1.41551,-1.4155,-1.41549,-1.41547,-1.41546,-1.41545,-1.41545,-1.41544,-1.41543,-1.41542,-1.41542,-1.41541,-1.41541,-1.4154,-1.4154,-1.41539,-1.41539,-1.41538,-1.41538,-1.41537,-1.41537,-1.41536,-1.41536,-1.41536,-1.41535]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415358
INFO: iteration 2, average log likelihood -1.415298
INFO: iteration 3, average log likelihood -1.415242
INFO: iteration 4, average log likelihood -1.415176
INFO: iteration 5, average log likelihood -1.415095
INFO: iteration 6, average log likelihood -1.414997
INFO: iteration 7, average log likelihood -1.414885
INFO: iteration 8, average log likelihood -1.414767
INFO: iteration 9, average log likelihood -1.414650
INFO: iteration 10, average log likelihood -1.414541
INFO: iteration 11, average log likelihood -1.414442
INFO: iteration 12, average log likelihood -1.414353
INFO: iteration 13, average log likelihood -1.414275
INFO: iteration 14, average log likelihood -1.414206
INFO: iteration 15, average log likelihood -1.414144
INFO: iteration 16, average log likelihood -1.414089
INFO: iteration 17, average log likelihood -1.414040
INFO: iteration 18, average log likelihood -1.413997
INFO: iteration 19, average log likelihood -1.413958
INFO: iteration 20, average log likelihood -1.413924
INFO: iteration 21, average log likelihood -1.413893
INFO: iteration 22, average log likelihood -1.413865
INFO: iteration 23, average log likelihood -1.413839
INFO: iteration 24, average log likelihood -1.413814
INFO: iteration 25, average log likelihood -1.413792
INFO: iteration 26, average log likelihood -1.413770
INFO: iteration 27, average log likelihood -1.413750
INFO: iteration 28, average log likelihood -1.413731
INFO: iteration 29, average log likelihood -1.413712
INFO: iteration 30, average log likelihood -1.413694
INFO: iteration 31, average log likelihood -1.413677
INFO: iteration 32, average log likelihood -1.413659
INFO: iteration 33, average log likelihood -1.413643
INFO: iteration 34, average log likelihood -1.413626
INFO: iteration 35, average log likelihood -1.413610
INFO: iteration 36, average log likelihood -1.413595
INFO: iteration 37, average log likelihood -1.413579
INFO: iteration 38, average log likelihood -1.413564
INFO: iteration 39, average log likelihood -1.413549
INFO: iteration 40, average log likelihood -1.413534
INFO: iteration 41, average log likelihood -1.413519
INFO: iteration 42, average log likelihood -1.413505
INFO: iteration 43, average log likelihood -1.413491
INFO: iteration 44, average log likelihood -1.413478
INFO: iteration 45, average log likelihood -1.413464
INFO: iteration 46, average log likelihood -1.413451
INFO: iteration 47, average log likelihood -1.413439
INFO: iteration 48, average log likelihood -1.413427
INFO: iteration 49, average log likelihood -1.413415
INFO: iteration 50, average log likelihood -1.413403
INFO: EM with 100000 data points 50 iterations avll -1.413403
118.1 data points per parameter
4: avll = [-1.41536,-1.4153,-1.41524,-1.41518,-1.4151,-1.415,-1.41489,-1.41477,-1.41465,-1.41454,-1.41444,-1.41435,-1.41428,-1.41421,-1.41414,-1.41409,-1.41404,-1.414,-1.41396,-1.41392,-1.41389,-1.41386,-1.41384,-1.41381,-1.41379,-1.41377,-1.41375,-1.41373,-1.41371,-1.41369,-1.41368,-1.41366,-1.41364,-1.41363,-1.41361,-1.41359,-1.41358,-1.41356,-1.41355,-1.41353,-1.41352,-1.41351,-1.41349,-1.41348,-1.41346,-1.41345,-1.41344,-1.41343,-1.41341,-1.4134]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413402
INFO: iteration 2, average log likelihood -1.413330
INFO: iteration 3, average log likelihood -1.413262
INFO: iteration 4, average log likelihood -1.413182
INFO: iteration 5, average log likelihood -1.413083
INFO: iteration 6, average log likelihood -1.412963
INFO: iteration 7, average log likelihood -1.412820
INFO: iteration 8, average log likelihood -1.412660
INFO: iteration 9, average log likelihood -1.412491
INFO: iteration 10, average log likelihood -1.412324
INFO: iteration 11, average log likelihood -1.412164
INFO: iteration 12, average log likelihood -1.412017
INFO: iteration 13, average log likelihood -1.411884
INFO: iteration 14, average log likelihood -1.411764
INFO: iteration 15, average log likelihood -1.411658
INFO: iteration 16, average log likelihood -1.411562
INFO: iteration 17, average log likelihood -1.411475
INFO: iteration 18, average log likelihood -1.411396
INFO: iteration 19, average log likelihood -1.411324
INFO: iteration 20, average log likelihood -1.411258
INFO: iteration 21, average log likelihood -1.411196
INFO: iteration 22, average log likelihood -1.411138
INFO: iteration 23, average log likelihood -1.411085
INFO: iteration 24, average log likelihood -1.411034
INFO: iteration 25, average log likelihood -1.410987
INFO: iteration 26, average log likelihood -1.410943
INFO: iteration 27, average log likelihood -1.410902
INFO: iteration 28, average log likelihood -1.410863
INFO: iteration 29, average log likelihood -1.410826
INFO: iteration 30, average log likelihood -1.410793
INFO: iteration 31, average log likelihood -1.410761
INFO: iteration 32, average log likelihood -1.410731
INFO: iteration 33, average log likelihood -1.410703
INFO: iteration 34, average log likelihood -1.410677
INFO: iteration 35, average log likelihood -1.410652
INFO: iteration 36, average log likelihood -1.410629
INFO: iteration 37, average log likelihood -1.410607
INFO: iteration 38, average log likelihood -1.410586
INFO: iteration 39, average log likelihood -1.410567
INFO: iteration 40, average log likelihood -1.410548
INFO: iteration 41, average log likelihood -1.410531
INFO: iteration 42, average log likelihood -1.410514
INFO: iteration 43, average log likelihood -1.410498
INFO: iteration 44, average log likelihood -1.410483
INFO: iteration 45, average log likelihood -1.410468
INFO: iteration 46, average log likelihood -1.410454
INFO: iteration 47, average log likelihood -1.410441
INFO: iteration 48, average log likelihood -1.410428
INFO: iteration 49, average log likelihood -1.410415
INFO: iteration 50, average log likelihood -1.410403
INFO: EM with 100000 data points 50 iterations avll -1.410403
59.0 data points per parameter
5: avll = [-1.4134,-1.41333,-1.41326,-1.41318,-1.41308,-1.41296,-1.41282,-1.41266,-1.41249,-1.41232,-1.41216,-1.41202,-1.41188,-1.41176,-1.41166,-1.41156,-1.41148,-1.4114,-1.41132,-1.41126,-1.4112,-1.41114,-1.41108,-1.41103,-1.41099,-1.41094,-1.4109,-1.41086,-1.41083,-1.41079,-1.41076,-1.41073,-1.4107,-1.41068,-1.41065,-1.41063,-1.41061,-1.41059,-1.41057,-1.41055,-1.41053,-1.41051,-1.4105,-1.41048,-1.41047,-1.41045,-1.41044,-1.41043,-1.41042,-1.4104]
[-1.42334,-1.42335,-1.42327,-1.4232,-1.42312,-1.42303,-1.42291,-1.42276,-1.42255,-1.42222,-1.42168,-1.42087,-1.41989,-1.41901,-1.41846,-1.4182,-1.41809,-1.41804,-1.41802,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.41801,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.418,-1.41802,-1.41794,-1.41786,-1.41778,-1.41767,-1.41756,-1.41745,-1.41735,-1.41727,-1.4172,-1.41715,-1.41711,-1.41708,-1.41705,-1.41703,-1.41701,-1.417,-1.41699,-1.41699,-1.41698,-1.41697,-1.41697,-1.41696,-1.41696,-1.41695,-1.41695,-1.41695,-1.41694,-1.41694,-1.41693,-1.41693,-1.41693,-1.41693,-1.41692,-1.41692,-1.41692,-1.41691,-1.41691,-1.41691,-1.41691,-1.41691,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.4169,-1.41691,-1.41685,-1.4168,-1.41674,-1.41667,-1.41658,-1.41647,-1.41636,-1.41625,-1.41614,-1.41605,-1.41597,-1.41591,-1.41585,-1.4158,-1.41576,-1.41572,-1.41568,-1.41565,-1.41563,-1.4156,-1.41558,-1.41556,-1.41554,-1.41553,-1.41551,-1.4155,-1.41549,-1.41547,-1.41546,-1.41545,-1.41545,-1.41544,-1.41543,-1.41542,-1.41542,-1.41541,-1.41541,-1.4154,-1.4154,-1.41539,-1.41539,-1.41538,-1.41538,-1.41537,-1.41537,-1.41536,-1.41536,-1.41536,-1.41535,-1.41536,-1.4153,-1.41524,-1.41518,-1.4151,-1.415,-1.41489,-1.41477,-1.41465,-1.41454,-1.41444,-1.41435,-1.41428,-1.41421,-1.41414,-1.41409,-1.41404,-1.414,-1.41396,-1.41392,-1.41389,-1.41386,-1.41384,-1.41381,-1.41379,-1.41377,-1.41375,-1.41373,-1.41371,-1.41369,-1.41368,-1.41366,-1.41364,-1.41363,-1.41361,-1.41359,-1.41358,-1.41356,-1.41355,-1.41353,-1.41352,-1.41351,-1.41349,-1.41348,-1.41346,-1.41345,-1.41344,-1.41343,-1.41341,-1.4134,-1.4134,-1.41333,-1.41326,-1.41318,-1.41308,-1.41296,-1.41282,-1.41266,-1.41249,-1.41232,-1.41216,-1.41202,-1.41188,-1.41176,-1.41166,-1.41156,-1.41148,-1.4114,-1.41132,-1.41126,-1.4112,-1.41114,-1.41108,-1.41103,-1.41099,-1.41094,-1.4109,-1.41086,-1.41083,-1.41079,-1.41076,-1.41073,-1.4107,-1.41068,-1.41065,-1.41063,-1.41061,-1.41059,-1.41057,-1.41055,-1.41053,-1.41051,-1.4105,-1.41048,-1.41047,-1.41045,-1.41044,-1.41043,-1.41042,-1.4104]
32×26 Array{Float64,2}:
 -0.495701    -0.263543     -0.312048   -0.452547   -0.102752     0.099869     0.472594    -0.110644   -0.606784     0.0599583  -0.156582    0.397425   -0.193562    0.28466    -0.182231     0.48674     0.443468    0.723548    -0.219944   -0.313733    0.22726     -0.851791   -0.126142    -0.0124661   0.295538      0.0631027
  0.0644217   -0.0153005    -0.304546   -0.164506   -0.0774055    0.0492649    0.548527    -0.054431   -0.455891     0.593761   -0.270739    0.311982   -0.268423    0.543073   -0.133421     0.617801    0.185423    0.386288    -0.343325    1.06464     0.464274     0.956878    0.346494    -0.27451     0.374657     -0.0906937
  0.21377      0.0797852    -0.507038   -0.435526   -0.245762     0.827621     0.293855    -0.224762    0.110046    -0.55215     0.350659   -0.281896   -0.926345    0.347968   -0.106589    -0.104316    0.0123592  -0.0914956   -0.307079    0.124003   -0.374801     0.023536   -0.15162     -0.0884833   0.380858     -0.105047 
  0.424955     0.365941      0.0556533  -0.530844    0.124876     0.287921    -0.0744951    0.152976    0.109948     0.453318    0.112214    0.05063    -0.331711    0.641053    0.288483    -0.0119758   0.183353   -0.44407     -0.550426    0.321279    0.0970491   -0.230261   -0.19852      0.239601    0.388853      0.329615 
  0.39155      0.134963     -0.0329465  -0.389745   -0.32494      0.38754     -0.568203     0.178057   -0.583304    -0.123187   -0.292772    0.0303241   0.312291    0.0101591   0.0927092   -0.249322   -0.313878    0.455908     0.213941    0.0639309   0.0168658    0.0705815  -0.21905      0.129933   -0.0128849     0.37447  
  0.527874     0.111923      0.0275604  -0.201549    0.00178823   0.202439    -0.588988     0.359625    0.0684309    0.108048   -0.14072    -0.398074   -0.143102   -0.0990258  -0.128457    -0.148476   -0.155479   -0.249402    -0.0104207   0.101479    0.850637     0.516731   -0.405051    -0.287113   -0.175538      0.447799 
  0.0428054    0.540079     -0.492777    0.197999   -0.347786    -0.249636    -0.689752    -0.347622    0.360356    -0.0860054  -0.0667555   1.19171     0.116639    0.179001   -0.0244116   -0.487364    0.160127    0.0596571   -0.240369   -0.418546    0.0686343   -0.602663    0.137832    -0.112308   -0.13563       0.184202 
 -0.248569     0.173522      0.167598   -0.0157754   0.26498      0.566611    -0.536563    -0.481246   -0.255808     0.545002    0.532242    0.141643   -0.0820206   0.04687     0.118443    -0.644732   -0.120121    0.242994     0.187645   -0.374127    0.282949    -0.69619    -0.0603629   -0.633845    0.946067      0.0400565
 -0.175357    -0.469276     -0.182404   -0.0245556   0.180701     0.00929887   0.0731395   -0.0587032  -0.161963     0.0448243  -0.0800715  -0.314492   -0.249737   -0.0709627  -0.125805     0.175647   -0.217843    0.133179     0.0973267   0.0442651  -0.312099     0.291018    0.37273     -0.0411139  -0.197676      0.174572 
 -0.00183957  -0.000394606  -0.0662416  -0.188244    0.200604    -0.0984239    0.0903535    0.0673185  -0.119396    -0.0557657  -0.108259    0.507017   -0.569593   -0.0281677   0.0215894    0.479807    0.0628686  -0.220901     0.189324   -0.0248286  -0.178423    -0.280054    0.00261068  -0.225079    0.000546637   0.314493 
  0.418194    -0.490352      0.222238    0.200743    0.384184     0.0979394   -0.154321    -0.0384479  -0.169607     0.0283816   0.0422029  -0.110191    0.195062   -0.115645    0.0608036   -0.163051   -0.282852   -0.0915974   -0.774715   -0.105065    0.274778     0.306707    0.338821     0.277931    0.576107     -0.385505 
  0.418631    -0.432446      0.342823    0.319607    0.59632      0.190203     0.0676749    0.142795   -0.213947    -0.130015   -0.416337    0.19079     0.303436   -0.118246    0.00132989   0.0896113   0.452641    0.021709     0.274064    0.508839   -0.0493406   -0.294964   -0.0954075   -0.0326606   0.37682      -0.0279838
  0.168736     0.00958887    0.168762    0.178832    0.105902    -0.788915    -0.00489023   0.34768     0.38217     -0.0804422  -0.0199216   0.0933406   0.13804    -0.0652068  -0.272264     0.350919    0.0489052  -0.418999    -0.154945    0.0778509  -0.0336449    0.460405    0.156534     0.352458   -0.242747     -0.116915 
 -0.859255     0.314696     -0.208842    0.0652895  -0.479764    -0.317657     0.174273     0.273669   -0.202114    -0.100179    0.198156   -0.373616   -0.226287   -0.460321   -0.24232     -0.18053     0.0406278  -0.254936     0.261632   -0.330795   -0.0337033    0.294968   -0.143687     0.330114   -0.138491     -0.0735691
  0.0203915    0.015877     -0.0195938  -0.0438892  -0.00957607  -0.01049     -0.0186725   -0.0685707   0.119588     0.0885976   0.0458532   0.0257987   0.083983   -0.0153178  -0.0287039   -0.113312   -0.0744052  -0.0252185   -0.138963   -0.0914739   0.183643    -0.0119061  -0.0118653    0.0696251   0.152771     -0.0386948
 -0.197282     0.113965      0.0261799   0.177637   -0.27559      0.214899     0.216656    -0.128934    0.179924    -0.251525    0.22146    -0.104132    0.247915    0.0611504   0.300336    -0.207259    0.208768    0.0874393    0.283724   -0.0344515  -0.246185    -0.155052   -0.0469401    0.155124   -0.193083     -0.39345  
 -0.635654    -0.291999      0.218092   -0.458882   -0.101946    -0.40702     -0.148801     0.309923   -1.15217     -0.120269    0.393513    0.0185121  -0.0244028  -0.125284   -0.222768     0.40316    -0.235044    0.496829     0.137444   -0.196778   -0.621923     0.434127    0.210745     0.109369    0.134174      0.106563 
 -0.399552     0.201089      0.0831591   0.379422   -0.055004    -0.459838    -0.0105174    0.110431    0.287519    -0.195666    0.137687    0.175272    0.0496626  -0.309757   -0.0301997   -0.137414    0.245067   -0.0309109    1.10958    -0.0237242  -0.544626    -0.475236    0.192698    -0.104736   -0.25123       0.631739 
  0.154507    -0.576872      0.853271    0.222726    0.911151    -0.534561    -0.333748     0.23171    -0.289685    -0.074757   -0.405666    0.357249   -0.205159   -0.13108    -0.350246     0.248552   -0.217566   -0.0612951    0.109717   -0.21086     0.188328    -0.394643    0.0165646   -0.302774    0.285398      0.775114 
  0.300925    -0.21458       0.542181   -0.495885    0.883483     0.549628     0.0952161   -0.270092   -0.00234028   0.0746912  -0.178445    0.0901715   0.38847    -0.148965    0.467296     0.174808   -0.760333    0.556107    -0.41942     0.280425   -0.245779    -0.370373   -0.171465    -0.652193   -0.194632      0.537252 
  0.0128854    0.0521572     0.460222    0.19381     0.29484      0.072109     0.277233     0.919566   -0.21872     -0.488679    0.354727    0.24279     0.269094   -0.491717    0.149076    -0.051359    0.785496   -0.184416     0.23003     0.0869959   0.304307    -0.310074   -0.310155    -0.0534274  -0.130909     -0.453838 
  0.464468     0.0491044    -0.102114    0.409008   -0.947285    -0.269622    -0.60127      0.513099   -0.575339    -0.283249    0.176271    0.193914   -0.111835    0.0983132  -0.0569556    0.0894291   0.441946   -0.336084     0.0230023   0.199771    0.183022     0.257166   -0.232201     0.767394    0.258401     -0.56129  
  0.33502     -0.211785     -0.101744   -0.0602806   0.0678024    0.204776    -0.146104     0.0427132  -0.11437      0.0353349  -0.2354     -0.078417   -0.0521796   0.0444038  -0.044426    -0.0086125   0.024055    0.00129248  -0.170984    0.154449   -0.00118461   0.0673859   0.0738955    0.020507    0.107065      0.0295901
 -0.0482418    0.650699     -0.190826   -0.61137    -0.0166368   -0.291496    -0.434592     0.278728   -0.0825086    0.266774    0.183163    0.585714   -0.15308    -0.142916   -0.27408      0.551633   -0.249763   -0.685497    -0.577032   -0.385669    0.0958768    0.644168   -0.653635    -0.048535    0.016317      0.76672  
  0.41323      0.869449      0.51251     0.232423   -0.156202    -0.16893     -0.939965     0.177316    0.833209     0.0618102  -0.0222339  -0.262121    0.545934   -0.352493    0.437358    -0.621382   -0.612769   -0.705597     0.159871    0.396667   -0.309711     0.55874    -0.0179212    0.119047   -0.424916     -0.0683369
  0.128778     0.454169     -0.277865    0.388168   -0.0863731    0.318678    -0.451259    -0.901464    0.841908     0.640899   -0.330435    0.0453409   0.208287    0.124221   -0.215742    -0.663321   -0.890357   -0.0628447   -0.0665836  -0.137054    0.651963     0.0195034  -0.0148621    0.127024    0.0696799     0.0268017
 -0.275911     0.0419671    -0.0631868   0.0312653   0.740934    -0.134549     0.719162    -0.529939    0.896437    -0.0235254  -0.0905555  -0.225573    0.296176   -0.634312    0.103647     0.145375   -0.331786    0.403418    -0.103468    0.0689794  -0.121072    -0.375004    0.214675    -0.538325   -0.263476      0.11381  
  0.21952     -0.107034     -0.297178    0.468343    0.0160571    0.207523     0.824358     0.187105    0.578968    -0.35366    -0.112735   -0.482692    0.414734    0.0938846   0.14765     -0.169822    0.437222    0.0126453   -0.162695    0.684145    0.25684      0.399853    0.015506    -0.112884   -0.17019      -0.647411 
 -0.272844     0.0522094    -0.179133   -0.135433   -0.2067       0.372389    -0.00547788  -0.0266476   0.0476097    0.201113    0.359144   -1.42961    -0.0368696  -0.0642507  -0.00945027  -0.137431   -0.513614    0.566691     0.269117   -0.330703    0.145371     0.734862    0.0931643    0.0623283  -0.886        -0.186119 
 -0.324589    -0.433196      0.316606    0.250605   -0.0942076    0.206505     0.152013    -0.952909   -0.00285375  -0.424753    0.0978562   0.0272268   0.499705   -0.107326    0.0402976   -0.16365    -0.187443    0.428417     0.478628   -0.692444    0.0215124   -0.347484    0.24179      0.159326    0.217896     -0.681556 
 -0.28251     -0.102505     -0.337893    0.153688    0.00161605  -0.422439     0.672338    -0.39127     0.855391    -0.13783     0.135301   -0.297398   -0.724157   -0.0907161  -0.588914    -0.149363    0.0754846  -0.532855    -0.359218   -0.541152    0.0658791    0.0445437   0.674112     0.271386    0.37273      -0.264885 
 -0.571631     0.166446     -0.403035   -0.0136905  -0.11162     -0.137759     0.384322    -0.604375    0.514132     0.6194      0.606646    0.299406   -0.055917    0.0844559   0.616569     0.273607    0.314862   -0.326955    -0.0949088  -0.279115   -0.0685903   -0.0897693   0.0479907    0.257124   -0.341931     -0.455987 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410391
INFO: iteration 2, average log likelihood -1.410380
INFO: iteration 3, average log likelihood -1.410369
INFO: iteration 4, average log likelihood -1.410358
INFO: iteration 5, average log likelihood -1.410348
INFO: iteration 6, average log likelihood -1.410337
INFO: iteration 7, average log likelihood -1.410328
INFO: iteration 8, average log likelihood -1.410318
INFO: iteration 9, average log likelihood -1.410309
INFO: iteration 10, average log likelihood -1.410300
INFO: EM with 100000 data points 10 iterations avll -1.410300
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.723736e+05
      1       7.015804e+05      -2.707932e+05 |       32
      2       6.895543e+05      -1.202614e+04 |       32
      3       6.847406e+05      -4.813716e+03 |       32
      4       6.821507e+05      -2.589853e+03 |       32
      5       6.805297e+05      -1.621034e+03 |       32
      6       6.794283e+05      -1.101356e+03 |       32
      7       6.785687e+05      -8.596236e+02 |       32
      8       6.778521e+05      -7.165947e+02 |       32
      9       6.772709e+05      -5.811768e+02 |       32
     10       6.768182e+05      -4.527262e+02 |       32
     11       6.764294e+05      -3.887932e+02 |       32
     12       6.760991e+05      -3.303474e+02 |       32
     13       6.758217e+05      -2.773365e+02 |       32
     14       6.755746e+05      -2.471212e+02 |       32
     15       6.753363e+05      -2.383022e+02 |       32
     16       6.751119e+05      -2.244298e+02 |       32
     17       6.749096e+05      -2.022214e+02 |       32
     18       6.747234e+05      -1.862370e+02 |       32
     19       6.745436e+05      -1.798094e+02 |       32
     20       6.743929e+05      -1.507151e+02 |       32
     21       6.742778e+05      -1.150468e+02 |       32
     22       6.741728e+05      -1.050177e+02 |       32
     23       6.740800e+05      -9.281957e+01 |       32
     24       6.739828e+05      -9.715442e+01 |       32
     25       6.738893e+05      -9.355578e+01 |       32
     26       6.737973e+05      -9.193427e+01 |       32
     27       6.736968e+05      -1.005137e+02 |       32
     28       6.735916e+05      -1.052859e+02 |       32
     29       6.734771e+05      -1.144326e+02 |       32
     30       6.733647e+05      -1.123957e+02 |       32
     31       6.732561e+05      -1.086593e+02 |       32
     32       6.731533e+05      -1.027625e+02 |       32
     33       6.730637e+05      -8.962764e+01 |       32
     34       6.729816e+05      -8.206717e+01 |       32
     35       6.729043e+05      -7.732246e+01 |       32
     36       6.728405e+05      -6.381993e+01 |       32
     37       6.727886e+05      -5.185247e+01 |       32
     38       6.727438e+05      -4.479807e+01 |       32
     39       6.727040e+05      -3.981173e+01 |       32
     40       6.726672e+05      -3.675592e+01 |       32
     41       6.726273e+05      -3.996201e+01 |       32
     42       6.725873e+05      -3.993880e+01 |       32
     43       6.725540e+05      -3.330555e+01 |       32
     44       6.725235e+05      -3.054391e+01 |       32
     45       6.724922e+05      -3.131656e+01 |       32
     46       6.724569e+05      -3.531465e+01 |       32
     47       6.724254e+05      -3.145452e+01 |       32
     48       6.724000e+05      -2.543266e+01 |       32
     49       6.723762e+05      -2.376216e+01 |       32
     50       6.723545e+05      -2.166894e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 672354.5448807729)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422071
INFO: iteration 2, average log likelihood -1.417071
INFO: iteration 3, average log likelihood -1.415799
INFO: iteration 4, average log likelihood -1.414924
INFO: iteration 5, average log likelihood -1.413994
INFO: iteration 6, average log likelihood -1.413034
INFO: iteration 7, average log likelihood -1.412257
INFO: iteration 8, average log likelihood -1.411762
INFO: iteration 9, average log likelihood -1.411476
INFO: iteration 10, average log likelihood -1.411297
INFO: iteration 11, average log likelihood -1.411170
INFO: iteration 12, average log likelihood -1.411069
INFO: iteration 13, average log likelihood -1.410986
INFO: iteration 14, average log likelihood -1.410914
INFO: iteration 15, average log likelihood -1.410851
INFO: iteration 16, average log likelihood -1.410795
INFO: iteration 17, average log likelihood -1.410746
INFO: iteration 18, average log likelihood -1.410702
INFO: iteration 19, average log likelihood -1.410662
INFO: iteration 20, average log likelihood -1.410626
INFO: iteration 21, average log likelihood -1.410594
INFO: iteration 22, average log likelihood -1.410564
INFO: iteration 23, average log likelihood -1.410537
INFO: iteration 24, average log likelihood -1.410511
INFO: iteration 25, average log likelihood -1.410488
INFO: iteration 26, average log likelihood -1.410466
INFO: iteration 27, average log likelihood -1.410446
INFO: iteration 28, average log likelihood -1.410427
INFO: iteration 29, average log likelihood -1.410408
INFO: iteration 30, average log likelihood -1.410391
INFO: iteration 31, average log likelihood -1.410375
INFO: iteration 32, average log likelihood -1.410359
INFO: iteration 33, average log likelihood -1.410344
INFO: iteration 34, average log likelihood -1.410330
INFO: iteration 35, average log likelihood -1.410317
INFO: iteration 36, average log likelihood -1.410303
INFO: iteration 37, average log likelihood -1.410291
INFO: iteration 38, average log likelihood -1.410278
INFO: iteration 39, average log likelihood -1.410266
INFO: iteration 40, average log likelihood -1.410255
INFO: iteration 41, average log likelihood -1.410243
INFO: iteration 42, average log likelihood -1.410232
INFO: iteration 43, average log likelihood -1.410221
INFO: iteration 44, average log likelihood -1.410211
INFO: iteration 45, average log likelihood -1.410201
INFO: iteration 46, average log likelihood -1.410191
INFO: iteration 47, average log likelihood -1.410181
INFO: iteration 48, average log likelihood -1.410171
INFO: iteration 49, average log likelihood -1.410162
INFO: iteration 50, average log likelihood -1.410153
INFO: EM with 100000 data points 50 iterations avll -1.410153
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.156068     0.0024998   0.188135   -0.23344     0.763194    0.143589     0.755211   -0.737212     0.71916    -0.0109283   -0.268876    -0.247012    0.44802     -0.676308     0.266691     0.120637     -0.624101      0.530598   -0.251809     0.0949504   -0.116057    -0.391249    0.0842967  -0.723108   -0.305937     0.266407  
  0.580176     0.177818   -0.281552    0.28526    -1.15229    -0.28339     -0.791176    0.461984    -0.547204   -0.241361     0.0832214    0.213889   -0.00369027   0.13875      0.010711     0.0299009     0.402057     -0.193568   -0.00398389   0.119682     0.285903     0.25063    -0.342169    0.701356    0.157988    -0.368048  
 -0.0778253   -0.0863211  -0.30257     0.367796   -0.153835   -0.244237     0.0955125  -0.0995283    0.165239    0.1131      -0.157998    -0.0161036   0.305277    -0.182109     0.0173825    0.0233937    -0.000381421  -0.0455936  -0.268527    -0.0383996    0.320222     0.141401    0.23381     0.460602    0.011644    -0.434318  
  0.21091      0.0516235  -0.174479   -0.120147   -0.0575017   0.689289    -0.5165     -0.037012     0.0622654  -0.0319787    0.0337443   -0.977468   -0.333041    -0.299131    -0.207586    -0.526957     -0.562956      0.260007    0.482545     0.098792     0.428224     0.672843   -0.112899   -0.48592    -0.878538     0.0893568 
 -0.155651    -0.183291    0.776388    0.403994    0.236544   -1.28061     -0.0857377   0.224044     0.213249    0.286803    -0.27773      0.388227    0.669129    -0.0314151   -0.00180542   0.000232698   0.24561      -0.279886    0.598626    -0.00999739   0.00765452  -0.029771    0.273307    0.107942   -0.0966473    0.239867  
 -0.235524    -0.250357   -0.0959943   0.0976414   0.0955666  -0.484791     0.186502   -0.323723     0.202264   -0.702776     0.289739    -0.297219    0.0345447   -0.312738    -0.43067      0.115117     -0.63043       0.0994158   0.347276    -0.675086    -0.380094     0.285014    0.522374    0.158814   -0.141437    -0.0517047 
 -1.19683      0.0276727  -0.398298    0.422245    0.101245   -0.0814603    0.598746   -0.387087     0.360743    0.199185     0.728189     0.235508    0.273932    -0.0840892    0.291324     0.0644997     0.516964      0.0396169   0.340601    -0.0232841   -0.59613     -0.44077     0.14184    -0.168798   -0.647259    -0.0759631 
 -0.0584854   -0.0182724   0.259142    0.381582    0.484553    0.00309235  -0.0233612   0.819613    -0.0836434  -0.521067     0.0857262    0.0920157   0.649209    -0.609856     0.153054    -0.0794305     0.379322      0.38452     0.362838     0.00182927   0.330355    -0.605687   -0.40261    -0.174397   -0.273245    -0.0848679 
  0.951923    -0.303321    0.404531   -0.0503412   0.488261    0.114007    -0.326852    0.285402    -0.126088   -0.335909    -0.622159     0.397547    0.0173085   -0.0121113   -0.0697408    0.0861341    -0.00928002   -0.0172221   0.134948     0.323131     0.010943    -0.395235   -0.0684091  -0.299295    0.266369     0.389828  
 -0.675343    -0.22654     0.298833   -0.43457     0.486514   -0.0689236    0.0734923  -0.43946     -0.527608    0.336432    -0.260468     0.728997   -0.0861936   -0.00850013  -0.197005     0.407164     -0.218783      0.463599    0.0350985   -0.542593     0.443955    -0.721317   -0.354127   -0.313574    0.344418     0.206814  
  0.49307      0.776972    0.444405    0.400124   -0.0721627  -0.0553048   -0.708231    0.0367587    0.951051    0.174692    -0.0782044   -0.402827    0.518555    -0.277627     0.386504    -0.568009     -0.69162      -0.683935    0.141654     0.491807    -0.254264     0.679268    0.102173    0.0620606  -0.399268     0.00303524
  0.0879049    0.352676    0.281526    0.0253409   0.0835018  -0.354422     0.599667   -0.0945197    1.11717    -0.00316554   0.63362     -0.0123769   0.162522     0.221123    -0.0943294   -0.0352017     0.288479     -0.35121    -0.314844    -0.332911     0.475107    -0.0130905  -0.286084    0.208452    0.0921171   -0.482066  
 -0.097812     0.421566    0.0751634  -0.49787    -0.0271716  -0.225831    -0.462682    0.35129     -0.261929    0.209521     0.327593     0.158199   -0.301175    -0.110391    -0.158594     0.33376      -0.183721     -0.552844   -0.24413     -0.319742    -0.00352298   0.336265   -0.432474   -0.0744945  -0.0256154    0.872004  
 -0.15456      0.60454    -0.410457   -0.590417   -0.217405   -0.722809    -0.352555    0.469054     0.670145   -0.145438    -0.640784     0.491314    0.296532    -0.440239     0.130556     0.0727091    -0.0757183    -0.554868   -0.610373     0.145521    -0.360809     0.719638   -0.171837    1.0909     -0.626575    -0.44135   
 -0.321351    -0.412149    0.117927    0.547128    0.848629   -0.257716     0.651979    0.0512945    0.400586    0.213723    -0.0223109   -0.0749568  -0.639262    -0.223402    -0.114232     0.50284       0.35047      -0.221731    0.0993145    0.191195    -0.186071    -0.101106    0.71047    -0.181184    0.0209253    0.0715837 
 -0.0781732    0.723041   -0.179303    0.147557   -0.304831    0.0608031   -0.830234   -0.598314     0.408816    0.296533     0.0318779    0.422402    0.366874     0.117541     0.112184    -0.647358     -0.267089      0.179343    0.0304016   -0.308233     0.407902    -0.380495   -0.179897   -0.0752235  -0.044285     0.0568235 
 -0.139124    -0.483      -0.30195    -0.368465   -0.103926   -0.0231554    0.396251    0.432423    -0.957981   -0.344729     0.0591142    0.203362   -0.637778     0.0151243   -0.361458     0.554146      0.567926      0.30267    -0.112884    -0.147158    -0.0884892   -0.591301   -0.129782    0.108267    0.603984    -0.0109915 
 -0.157729    -0.755559   -0.227096    0.214086    0.0310522   0.209013     0.662954    0.681065    -0.481506    0.234696    -0.165967    -1.05965     0.0688484    0.129022     0.21602      0.972293     -0.282837      0.029383   -0.409822    -0.114352    -0.0359169    0.717925   -0.296898    0.673972   -0.563868    -0.642994  
  0.61037      0.154124   -0.156398   -0.464557    0.320763    0.408132    -0.298533    0.123661    -0.0518948   0.542894    -0.230206     0.0653176  -0.124575     0.355745     0.265681     0.0799873    -0.0746282    -0.125011   -0.551822     0.310978     0.207525     0.0169777  -0.111548   -0.0540746   0.0893922    0.518322  
 -0.0727663   -0.0549917  -0.017028   -0.0828295   0.080937    0.0723651    0.0442673  -0.00184372  -0.0245958   0.00172732   0.00600128   0.0114679  -0.0198776   -0.0130268    0.0292592   -0.016256     -0.0270898     0.113259    0.0769781   -0.00896109  -0.0869226   -0.0906724   0.0023465  -0.0922765   0.0215455    0.0826742 
 -0.317522    -0.240515   -0.313614   -0.20798    -0.190338   -0.1685       0.0981165  -0.795366    -0.0593022   0.514856     0.19721      0.273545   -0.532109     0.152131     0.184814     0.302057     -0.160959     -0.263966   -0.139573    -0.369687    -0.235051     0.0457205   0.3609     -0.0501744  -0.0773409    0.0117294 
 -0.0761267   -0.358617    0.143238    0.148678   -0.167245    0.637236     0.240008   -0.54145     -0.101167   -0.387466    -0.061546     0.0235327   0.426147     0.142368     0.430859    -0.171368      0.225868      0.451933    0.327876    -0.216947    -0.124794    -0.51637     0.0341745   0.133926    0.193694    -0.529199  
  0.355568    -0.14099     0.124506   -0.0236137   0.166185   -0.354147    -0.270523    0.472659    -0.0119845  -0.165288    -0.0573578    0.0739414  -0.205917     0.103203    -0.555601     0.23553      -0.0572487    -0.332873   -0.177773     0.140445     0.0102512    0.308933   -0.0188555   0.0926705  -0.0208318    0.261593  
  0.409285    -0.129924   -0.419079    0.121691    0.010674    0.236425     0.86033     0.135403     0.207866   -0.142119    -0.326918    -0.231982    0.223077     0.298368     0.105502     0.116832      0.459577      0.221749   -0.292519     1.00814      0.474453     0.730231    0.145048   -0.280091    0.0415766   -0.427522  
  0.209384    -0.104709    0.11832     0.0811725   0.176111    0.509222    -0.20917    -0.399382     0.07667     0.138593     0.320487    -0.276465   -0.155983     0.135454    -0.180665    -0.656922     -0.37706      -0.0160843  -0.318322    -0.284148     0.194381    -0.104015    0.162249   -0.102785    0.866637     0.012597  
 -0.00353976  -0.106806   -0.307564    0.143843   -0.102573    0.0349798    0.266346   -0.392891     0.40988    -0.0020616    0.0269903   -0.278727   -0.139846     0.00793002  -0.135466    -0.183621     -0.0322864    -0.0034408  -0.160387    -0.151769     0.0869661    0.0946043   0.375625    0.194294    0.12304     -0.314825  
  0.0208567   -0.672196    0.209932    0.272807    0.220054    0.179584    -0.630137    0.142623    -1.0716     -0.148414    -0.292369     0.0365575   0.274036    -0.457187    -0.175271    -0.222732     -0.318412      0.742773   -0.0393951    0.136346    -0.31079      0.459321    0.403887   -0.074783    0.166418     0.26364   
  0.120824     0.424092   -0.551492   -0.352645   -0.276681    0.415158     0.310525   -0.0635206    0.410628   -0.327541     0.0849132    0.0762381  -0.75892      0.180836    -0.0524618    0.0111755     0.27637      -0.259635    0.0656488    0.249946    -0.294491    -0.206589   -0.0790955  -0.0511657   0.203979     0.0445037 
 -1.11074      0.489513   -0.21925     0.126429   -0.804788   -0.232695     0.298888    0.405284    -0.181121   -0.204059     0.124161    -0.432499   -0.375298    -0.340745    -0.419948    -0.425833      0.236986     -0.197028    0.292607    -0.426098    -0.0850254    0.266123   -0.127106    0.429895   -0.253618    -0.154052  
 -0.397693     0.350309   -0.0407853  -1.11628    -0.404471    0.232369    -0.0178839   0.0911509   -0.846165    0.475296     0.016496    -0.110326    0.13268      0.496401     0.185423     0.209699     -0.336885      0.870738    0.1379       0.250754    -0.213352     0.372694    0.0139889   0.0431081   0.0720275    0.295359  
  0.0444652    0.0576524   0.402473    0.0608203   0.0143184  -0.0507822   -0.159622    0.242057    -0.256843   -0.131043     0.164213    -0.0504395   0.0748578   -0.439442     0.394615     0.0735055     0.197232     -0.385452    0.353868     0.0930708    0.128811     0.275292   -0.30431     0.208493   -0.00967002  -0.052605  
  0.215729    -0.114663    0.642746    0.300228    0.0472901   0.493083    -0.050021    0.412232    -0.36223    -0.142736     0.810628    -0.0403017  -0.0392759    0.132525     0.190343    -0.178035      0.393978     -0.842241   -0.227284     0.523812    -0.0187994    0.241968   -0.311578    0.277369    0.423354    -0.922585  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410144
INFO: iteration 2, average log likelihood -1.410135
INFO: iteration 3, average log likelihood -1.410126
INFO: iteration 4, average log likelihood -1.410118
INFO: iteration 5, average log likelihood -1.410109
INFO: iteration 6, average log likelihood -1.410101
INFO: iteration 7, average log likelihood -1.410093
INFO: iteration 8, average log likelihood -1.410085
INFO: iteration 9, average log likelihood -1.410077
INFO: iteration 10, average log likelihood -1.410070
INFO: EM with 100000 data points 10 iterations avll -1.410070
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
