>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.7.0
INFO: Installing JLD v0.6.6
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1262
Commit 9f999b7 (2016-11-16 21:47 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-101-generic #148-Ubuntu SMP Thu Oct 20 22:08:32 UTC 2016 x86_64 x86_64
Memory: 2.939281463623047 GB (678.7421875 MB free)
Uptime: 25552.0 sec
Load Avg:  1.0634765625  1.064453125  1.05029296875
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1507904 s       5654 s     135281 s     611071 s         75 s
#2  3499 MHz     622480 s       4585 s      84226 s    1738220 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.4
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.7.0
 - JLD                           0.6.6
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.1.0
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in #_write#17(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:587
 in #write#14(::Array{Any,1}, ::Function, ::JLD.JldFile, ::String, ::Array{Any,1}, ::JLD.JldWriteSession) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:529
 in #jldopen#9(::Bool, ::Bool, ::Bool, ::Function, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:198
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::Bool, ::Bool, ::Bool, ::Bool, ::Bool) at ./<missing>:0
 in #jldopen#10(::Bool, ::Bool, ::Bool, ::Function, ::String, ::String) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:253
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::String, ::String) at ./<missing>:0
 in #jldopen#11(::Array{Any,1}, ::Function, ::JLD.##34#35{String,Array{Float64,2},Tuple{}}, ::String, ::Vararg{String,N}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:263
 in (::JLD.#kw##jldopen)(::Array{Any,1}, ::JLD.#jldopen, ::Function, ::String, ::String) at ./<missing>:0
 in #save#33(::Bool, ::Bool, ::Function, ::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1217
 in save(::FileIO.File{FileIO.DataFormat{:JLD}}, ::String, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/JLD/src/JLD.jl:1214
 in #save#14(::Array{Any,1}, ::Function, ::String, ::String, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/FileIO/src/loadsave.jl:54
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:8 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##776#778{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.7348131616512132e6,[88629.9,11370.1],
[-95.7921 -62.891 19137.4; -72.3153 -38.122 -19207.2],

Array{Float64,2}[
[89069.0 623.391 276.955; 623.391 89817.7 105.471; 276.955 105.471 65756.3],

[11108.9 -21.0196 54.8866; -21.0196 10329.9 68.525; 54.8866 68.525 34432.1]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.829893e+03
      1       1.113854e+03      -7.160389e+02 |        6
      2       9.701184e+02      -1.437353e+02 |        3
      3       9.294560e+02      -4.066240e+01 |        0
      4       9.294560e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 929.4560236000075)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.072769
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.734488
INFO: iteration 2, lowerbound -3.573691
INFO: iteration 3, lowerbound -3.412629
INFO: iteration 4, lowerbound -3.249178
INFO: dropping number of Gaussions to 7
INFO: iteration 5, lowerbound -3.092430
INFO: iteration 6, lowerbound -2.953606
INFO: dropping number of Gaussions to 6
INFO: iteration 7, lowerbound -2.845267
INFO: iteration 8, lowerbound -2.766508
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.695125
INFO: iteration 10, lowerbound -2.628301
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.558596
INFO: iteration 12, lowerbound -2.486555
INFO: iteration 13, lowerbound -2.426216
INFO: iteration 14, lowerbound -2.379082
INFO: iteration 15, lowerbound -2.344055
INFO: iteration 16, lowerbound -2.319746
INFO: iteration 17, lowerbound -2.308120
INFO: dropping number of Gaussions to 2
INFO: iteration 18, lowerbound -2.303075
INFO: iteration 19, lowerbound -2.299264
INFO: iteration 20, lowerbound -2.299258
INFO: iteration 21, lowerbound -2.299255
INFO: iteration 22, lowerbound -2.299254
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Thu 17 Nov 2016 12:35:36 PM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Thu 17 Nov 2016 12:35:38 PM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Thu 17 Nov 2016 12:35:39 PM UTC: EM with 272 data points 0 iterations avll -2.072769
5.8 data points per parameter
,Thu 17 Nov 2016 12:35:40 PM UTC: GMM converted to Variational GMM
,Thu 17 Nov 2016 12:35:42 PM UTC: iteration 1, lowerbound -3.734488
,Thu 17 Nov 2016 12:35:42 PM UTC: iteration 2, lowerbound -3.573691
,Thu 17 Nov 2016 12:35:42 PM UTC: iteration 3, lowerbound -3.412629
,Thu 17 Nov 2016 12:35:42 PM UTC: iteration 4, lowerbound -3.249178
,Thu 17 Nov 2016 12:35:42 PM UTC: dropping number of Gaussions to 7
,Thu 17 Nov 2016 12:35:42 PM UTC: iteration 5, lowerbound -3.092430
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 6, lowerbound -2.953606
,Thu 17 Nov 2016 12:35:43 PM UTC: dropping number of Gaussions to 6
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 7, lowerbound -2.845267
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 8, lowerbound -2.766508
,Thu 17 Nov 2016 12:35:43 PM UTC: dropping number of Gaussions to 4
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 9, lowerbound -2.695125
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 10, lowerbound -2.628301
,Thu 17 Nov 2016 12:35:43 PM UTC: dropping number of Gaussions to 3
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 11, lowerbound -2.558596
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 12, lowerbound -2.486555
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 13, lowerbound -2.426216
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 14, lowerbound -2.379082
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 15, lowerbound -2.344055
,Thu 17 Nov 2016 12:35:43 PM UTC: iteration 16, lowerbound -2.319746
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 17, lowerbound -2.308120
,Thu 17 Nov 2016 12:35:44 PM UTC: dropping number of Gaussions to 2
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 18, lowerbound -2.303075
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 19, lowerbound -2.299264
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 20, lowerbound -2.299258
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 21, lowerbound -2.299255
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 22, lowerbound -2.299254
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 23, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 24, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 25, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 26, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 27, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 28, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 29, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 30, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 31, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:44 PM UTC: iteration 32, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 33, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 34, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 35, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 36, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 37, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 38, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 39, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 40, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 41, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 42, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 43, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 44, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 45, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 46, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:45 PM UTC: iteration 47, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:46 PM UTC: iteration 48, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:46 PM UTC: iteration 49, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:46 PM UTC: iteration 50, lowerbound -2.299253
,Thu 17 Nov 2016 12:35:46 PM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9691351036180882
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9691351036180885
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9691351036180885
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9939420320474346
avll from llpg:  -0.9939420320474346
avll direct:     -0.9939420320474345
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.00974853  -0.0635197    0.164727     0.0601351   -0.0422879    0.0541052   0.023203    -0.0895241    0.0698334    0.0911819     0.0897574    -0.00224585  -0.106293     0.113062    -0.0272645   -0.00543904   0.154616    -0.0028863    0.0373518     0.10164      0.113008     0.0683322    0.106308     0.0435811    0.048587    -0.0178983 
  0.0450323    0.0399666   -0.0930872    0.0916926   -0.0192687   -0.0285715   0.0201749   -0.0137602    0.0868106    0.0333919     0.050891      0.0115061    0.0767931    0.119501     0.0189933   -0.0392826    0.167238     0.125768    -0.0694322     0.170582     0.0451668    0.12863     -0.0340484   -0.00758298  -0.112215    -0.0246212 
  0.119288     0.0428579    0.015881    -0.0738605   -0.0747453    0.116155    0.105479    -0.064272    -0.290192    -0.114615     -0.0639653    -0.111601     0.278978     0.0936691   -0.181       -0.204521     0.0322541   -0.171707     0.0605406    -0.0273815    0.0374606   -0.0285784   -0.0569763   -0.148402    -0.0889259   -0.142708  
  0.0192894    0.0240698    0.0774615    0.092233     0.122845     0.0571772  -0.0457634    0.0534525   -0.0239966    0.16172       0.0320658    -0.0379856   -0.138888     0.0902233    0.0508581    0.00696165  -0.088291    -0.0702013   -0.1864       -0.0396551    0.00887439  -0.0308541   -0.0119421    0.107476     0.0853519    0.155902  
 -0.10855      0.122351    -0.117344     0.0198593   -0.132165     0.0437648  -0.0268513   -0.0272713   -0.005887    -0.0263037     0.0583713     0.0198284   -0.0623717    0.100399     0.155027    -0.136168     0.238717     0.207012     0.000981182  -0.05638     -0.104635     0.0472752   -0.129887     0.140587    -0.135988    -0.0693938 
 -0.0525374    0.0706255   -0.133451    -0.106894     0.00594075  -0.0207894   0.11193     -0.0544061    0.0257281    0.183874      0.0854334     0.0896014    0.057413     0.0427913    0.0253386    0.0910151   -0.104005    -0.115664     0.0325755    -0.0670086   -0.121327    -0.0865408    0.0444691    0.015933     0.0144879    0.169435  
 -0.140102     0.108369     0.0787224   -0.053121     0.0468615   -0.0567028   0.193504     0.0112344    0.0803382    0.130666      0.0240438    -0.0202903    0.0360073    0.0444715   -0.197785    -0.107109     0.20995      0.0547131   -0.00484455   -0.0733048    0.3122      -0.0325539   -0.0465038   -0.0573204    0.00207577   0.0255607 
 -0.188293     0.00103713   0.0472017    0.0425594    0.037814    -0.0718612  -0.0345859   -0.12358      0.0529664   -0.000736837  -0.0376158     0.120975     0.00819635  -0.0686513    0.149435     0.0283814    0.00802519   0.105263     0.258931      0.0356726   -0.0449056    0.137288    -0.186546     0.0255958    0.080623    -0.0475492 
 -0.0487123    0.0116766   -0.011671    -0.177677     0.0858458    0.251515   -0.0437494    0.0939015    0.0283239    0.0884641    -0.127175     -0.103845    -0.00120597  -0.135528    -0.109401     0.131452     0.155922    -0.0865481    0.040805     -0.0147942    0.163779    -0.115366    -0.148576     0.0235492    0.0208277    0.178592  
 -0.0659883    0.152282    -0.125814     0.146973    -0.153642    -0.0905875   0.126483    -0.0676891   -0.00662411  -0.111664     -0.0706418     0.0504102    0.104477     0.0100348   -0.0478807    0.0321621    0.0290373    0.0376034   -0.0854626    -0.0659175   -0.0572786   -0.0130558   -0.0231232    0.011658    -0.0986628    0.0791187 
 -0.0823987   -0.0492075   -0.00542963   0.035504    -0.0118386   -0.101525   -0.115956     0.00830216   0.029376    -0.0271015    -0.0487037    -0.0328807   -0.0207429    0.103774    -0.0924381   -0.131932    -0.163936    -0.0580115   -0.0498998    -0.008792     0.0645391    0.166615    -0.183779    -0.0157398    0.031031    -0.157965  
  0.102408    -0.0493359    0.120625     0.119226    -0.014797    -0.0914117   0.0493944   -0.0817287   -0.0354941   -0.0387847    -0.00150903   -0.163933     0.0397646    0.205408     0.00554521  -0.0716259    0.118794     0.0716381   -0.0295914     0.0205614    0.0209232   -0.0780083   -0.111305     0.12669      0.05797     -0.014735  
 -0.126617     0.0616971    0.00584491  -0.184659     0.0314205   -0.0304011  -0.0862834   -0.0384643    0.0302946   -0.132022      0.129003     -0.133786    -0.0848706    0.0465314   -0.0212127    0.0759484   -0.206544    -0.0256809    0.123846     -0.0534129   -0.034693     0.0633386    0.00762823   0.153463     0.027195     0.0118775 
 -0.124626    -0.0702608   -0.0875834   -0.0942776    0.105924     0.109709    0.00863286  -0.0409452   -0.061116     0.00899926   -0.118882      0.167518     0.076108    -0.126535    -0.0497734    0.201758    -0.0567945   -0.112199     0.0812166     0.0778297    0.0743558    0.147897    -0.124973    -0.268477    -0.0672399   -0.0933467 
  0.0391272   -0.0331706    0.166849    -0.0224999   -0.0873655    0.083303   -0.135818     0.142153     0.02969     -0.0331112    -0.0405053    -0.0607925   -0.109386     0.026853    -0.0298997   -0.021768     0.138661    -0.138084    -0.0679409     0.0525205   -0.196979     0.0944315    0.126132     0.20376     -0.180896    -0.0466289 
  0.113644     0.232542    -0.160926    -0.0322405   -0.0727537   -0.0674398  -0.13754      0.0903467   -0.0979959   -0.150244     -0.0629256    -0.00172607   0.213465    -0.0650706    0.0252366    0.00497428  -0.0305826    0.00883001  -0.00192286    0.00128187   0.0233776    0.12895      0.0123377   -0.119994     0.187466     0.0771487 
  0.126355     0.0779869    0.0904929    0.0116448   -0.0683605   -0.035157   -0.00279564  -0.0376946   -0.148697     0.0669033    -0.046201     -0.020455     0.0452226   -0.0353263    0.0301307   -0.09818      0.151658    -0.0291566    0.12038      -0.00186554  -0.117835     0.0699825    0.0325829   -0.0651487   -0.092351    -0.0844991 
  0.0414209    0.0624458    0.273201     0.0354375    0.0338487    0.105451   -0.0983652    0.0186819   -0.0200435   -0.072707      0.022645      0.0705887   -0.0769079   -0.0443924    0.10183     -0.0351317    0.146466    -0.0711169    0.0479307     0.0882      -0.015172     0.0179263   -0.0912735   -0.0211162   -0.0230232    0.0217508 
  0.00953012   0.0286451    0.0225564   -0.030883     0.205663     0.0583842   0.072773     0.048996     0.0842179   -0.0823575     0.0805923     0.0779208   -0.044072    -0.110709    -0.120879     0.0553441    0.0441788    0.109878    -0.103091      0.00851215  -0.264793     0.0434918   -0.00576146   0.00542619  -0.139382     0.132599  
 -0.0461511   -0.162749     0.146099    -0.0870992    0.0557134    0.0283799  -0.15875      0.186057    -0.154155    -0.019834      0.0134648    -0.0599695    0.102119    -0.111111    -0.0198275   -0.20735     -0.0747648   -0.0459197    0.0658373    -0.0995582    0.169613     0.0784866    0.0472976    0.184076     0.164083    -0.0912648 
 -0.0240583    0.082424     0.00345727  -0.152511     0.112269    -0.0530375  -0.152605     0.140828     0.0260958    0.0764959     0.18725       0.166801     0.151113     0.02942      0.0751296    0.121605     0.159628    -0.0699385    0.0190885    -0.00032036  -0.0206231    0.0849315   -0.0389127   -0.0186899    0.13462      0.179752  
  0.182443     0.0453531    0.0791909   -0.0807458   -0.153296    -0.125831    0.0750895    0.0093301    0.136656    -0.0204144     0.110139     -0.0837379   -0.0960827    0.00133361   0.129542     0.0847648   -0.115399     0.00525967   0.0364719     0.0842665   -0.029158     0.00251279  -0.091759    -0.0403313    0.00646938  -0.0561152 
 -0.0214691    0.114626     0.0592344    0.213875     0.0116312    0.0188708   0.0813907   -0.0470286    0.0700107   -0.106789     -0.168163     -0.0233816    0.0190068    0.0443301   -0.0554618    0.0228143   -0.0243438   -0.182314     0.0375492     0.0684696   -0.0698568   -0.197908     0.0638792   -0.00322138  -0.127015     0.0413646 
 -0.0671309   -0.0693285   -0.037306     0.02107     -0.0344466    0.054275   -0.191648    -0.0256315    0.122263     0.00137637   -0.0784152     0.0032375   -0.0824119   -0.0593505    0.00418279   0.0357095   -0.140318     0.00542559   0.0153825     0.0499741    0.283829     0.103345    -0.0398741    0.020606     0.0116826   -0.0250924 
 -0.0216986    0.0751207    0.204545    -0.0315559    0.0896442   -0.130092   -0.0093704   -0.117518    -0.0881167    0.0189994     0.0655951     0.0980437    0.125639    -0.145007     0.075998     0.10593     -0.0334561   -0.124002    -0.0842274    -0.0786735    0.0269235   -0.0322136   -0.0611963   -0.199036    -0.0101979    0.0799027 
  0.0225475    0.0298076    0.0999018    0.0506824   -0.0128777   -0.0303862  -0.0808091   -0.0813376   -0.0177953    0.0330573     0.0721402    -0.0515422   -9.01189e-5  -0.0107768    0.200837    -0.0404645   -0.0572292   -0.21581     -0.197216     -0.0333964    0.0135828   -0.00487492   0.049451    -0.00582974   0.250222     0.0204939 
 -0.18152      0.0665459   -0.0175497   -0.17838      0.00111576  -0.0814312   0.0205662   -0.0424292   -0.112946     0.143175      0.000464958   0.0277414   -0.0138807   -0.0254234   -0.0980391   -0.0369097    0.0153768    0.142872    -0.0346552    -0.18756     -0.148103    -0.132294    -0.0306212    0.140127     0.0509952    0.123342  
 -0.133234     0.0404252    0.0839496   -0.0860968   -0.103989    -0.0748522   0.0330906   -0.0151446    0.0142462    0.0870194    -0.0735623     0.0794522   -0.113563     0.0882816    0.0742535   -0.00398788  -0.0773412    0.0720442   -0.0337418     0.0762228   -0.0160424   -0.142493     0.0277665   -0.0397867    0.0565071   -0.146714  
  0.00246209   0.196551     0.0369505    0.00515874   0.0373442   -0.0559992  -0.142361    -0.0140241    0.140795    -0.0293015     0.106383      0.0270555   -0.03668     -0.202406    -0.00471715   0.066224     0.0505177    0.0305922    0.142846     -0.0389794    0.0765756   -0.166052     0.00920064   0.00537587   0.0840965   -0.160128  
 -0.00880953   0.0907063    0.10639     -0.224292     0.0664899    0.184669   -0.0203643   -0.128524     0.122183    -0.191147      0.0455514    -0.0144749   -0.049358    -0.0822771    0.0465153    0.0569814    0.0182332   -0.097028    -0.016794      0.0378892   -0.102264     0.0421612   -0.0589507   -0.163276     0.0532211   -0.118469  
  0.0430093   -0.0792443   -0.0143551   -0.0561496    0.0572944    0.0823321   0.0239844   -0.036251     0.0831721    0.0528883     0.0611629     0.0683323   -0.0816924   -0.161967    -0.08735     -0.203015    -0.143896     0.207083     0.0854595     0.281609     0.0472889    0.194834    -0.0319276    0.0961289   -0.244372    -0.00576909
  0.112803    -0.0957028   -0.12519     -0.0746698   -0.0771551    0.101482    0.115952     0.145803    -0.0724751   -0.234724      0.255469     -0.111489     0.0727066    0.125821    -0.0575746   -0.0625682    0.079535    -0.0901454    0.0889299    -0.101262     0.105224     0.0969933    0.121026     0.198526    -0.0525345   -0.0334969 kind diag, method split
0: avll = -1.431801308930964
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.431855
INFO: iteration 2, average log likelihood -1.431778
INFO: iteration 3, average log likelihood -1.431218
INFO: iteration 4, average log likelihood -1.425911
INFO: iteration 5, average log likelihood -1.411812
INFO: iteration 6, average log likelihood -1.404375
INFO: iteration 7, average log likelihood -1.402564
INFO: iteration 8, average log likelihood -1.401631
INFO: iteration 9, average log likelihood -1.400854
INFO: iteration 10, average log likelihood -1.400246
INFO: iteration 11, average log likelihood -1.399856
INFO: iteration 12, average log likelihood -1.399604
INFO: iteration 13, average log likelihood -1.399416
INFO: iteration 14, average log likelihood -1.399260
INFO: iteration 15, average log likelihood -1.399127
INFO: iteration 16, average log likelihood -1.399017
INFO: iteration 17, average log likelihood -1.398927
INFO: iteration 18, average log likelihood -1.398854
INFO: iteration 19, average log likelihood -1.398793
INFO: iteration 20, average log likelihood -1.398743
INFO: iteration 21, average log likelihood -1.398701
INFO: iteration 22, average log likelihood -1.398667
INFO: iteration 23, average log likelihood -1.398640
INFO: iteration 24, average log likelihood -1.398618
INFO: iteration 25, average log likelihood -1.398600
INFO: iteration 26, average log likelihood -1.398585
INFO: iteration 27, average log likelihood -1.398572
INFO: iteration 28, average log likelihood -1.398562
INFO: iteration 29, average log likelihood -1.398552
INFO: iteration 30, average log likelihood -1.398545
INFO: iteration 31, average log likelihood -1.398538
INFO: iteration 32, average log likelihood -1.398532
INFO: iteration 33, average log likelihood -1.398527
INFO: iteration 34, average log likelihood -1.398523
INFO: iteration 35, average log likelihood -1.398519
INFO: iteration 36, average log likelihood -1.398515
INFO: iteration 37, average log likelihood -1.398512
INFO: iteration 38, average log likelihood -1.398509
INFO: iteration 39, average log likelihood -1.398506
INFO: iteration 40, average log likelihood -1.398504
INFO: iteration 41, average log likelihood -1.398501
INFO: iteration 42, average log likelihood -1.398499
INFO: iteration 43, average log likelihood -1.398496
INFO: iteration 44, average log likelihood -1.398494
INFO: iteration 45, average log likelihood -1.398492
INFO: iteration 46, average log likelihood -1.398490
INFO: iteration 47, average log likelihood -1.398488
INFO: iteration 48, average log likelihood -1.398486
INFO: iteration 49, average log likelihood -1.398485
INFO: iteration 50, average log likelihood -1.398483
INFO: EM with 100000 data points 50 iterations avll -1.398483
952.4 data points per parameter
1: avll = [-1.43185,-1.43178,-1.43122,-1.42591,-1.41181,-1.40438,-1.40256,-1.40163,-1.40085,-1.40025,-1.39986,-1.3996,-1.39942,-1.39926,-1.39913,-1.39902,-1.39893,-1.39885,-1.39879,-1.39874,-1.3987,-1.39867,-1.39864,-1.39862,-1.3986,-1.39859,-1.39857,-1.39856,-1.39855,-1.39854,-1.39854,-1.39853,-1.39853,-1.39852,-1.39852,-1.39852,-1.39851,-1.39851,-1.39851,-1.3985,-1.3985,-1.3985,-1.3985,-1.39849,-1.39849,-1.39849,-1.39849,-1.39849,-1.39848,-1.39848]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.398584
INFO: iteration 2, average log likelihood -1.398458
INFO: iteration 3, average log likelihood -1.397656
INFO: iteration 4, average log likelihood -1.389999
INFO: iteration 5, average log likelihood -1.371590
INFO: iteration 6, average log likelihood -1.362606
INFO: iteration 7, average log likelihood -1.359844
INFO: iteration 8, average log likelihood -1.358492
INFO: iteration 9, average log likelihood -1.357582
INFO: iteration 10, average log likelihood -1.356888
INFO: iteration 11, average log likelihood -1.356316
INFO: iteration 12, average log likelihood -1.355818
INFO: iteration 13, average log likelihood -1.355349
INFO: iteration 14, average log likelihood -1.354848
INFO: iteration 15, average log likelihood -1.354256
INFO: iteration 16, average log likelihood -1.353571
INFO: iteration 17, average log likelihood -1.352788
INFO: iteration 18, average log likelihood -1.351865
INFO: iteration 19, average log likelihood -1.350822
INFO: iteration 20, average log likelihood -1.349756
INFO: iteration 21, average log likelihood -1.348796
INFO: iteration 22, average log likelihood -1.348013
INFO: iteration 23, average log likelihood -1.347459
INFO: iteration 24, average log likelihood -1.347115
INFO: iteration 25, average log likelihood -1.346900
INFO: iteration 26, average log likelihood -1.346756
INFO: iteration 27, average log likelihood -1.346656
INFO: iteration 28, average log likelihood -1.346585
INFO: iteration 29, average log likelihood -1.346533
INFO: iteration 30, average log likelihood -1.346492
INFO: iteration 31, average log likelihood -1.346458
INFO: iteration 32, average log likelihood -1.346428
INFO: iteration 33, average log likelihood -1.346400
INFO: iteration 34, average log likelihood -1.346373
INFO: iteration 35, average log likelihood -1.346345
INFO: iteration 36, average log likelihood -1.346316
INFO: iteration 37, average log likelihood -1.346286
INFO: iteration 38, average log likelihood -1.346254
INFO: iteration 39, average log likelihood -1.346217
INFO: iteration 40, average log likelihood -1.346176
INFO: iteration 41, average log likelihood -1.346125
INFO: iteration 42, average log likelihood -1.346060
INFO: iteration 43, average log likelihood -1.345977
INFO: iteration 44, average log likelihood -1.345875
INFO: iteration 45, average log likelihood -1.345753
INFO: iteration 46, average log likelihood -1.345600
INFO: iteration 47, average log likelihood -1.345405
INFO: iteration 48, average log likelihood -1.345171
INFO: iteration 49, average log likelihood -1.344906
INFO: iteration 50, average log likelihood -1.344639
INFO: EM with 100000 data points 50 iterations avll -1.344639
473.9 data points per parameter
2: avll = [-1.39858,-1.39846,-1.39766,-1.39,-1.37159,-1.36261,-1.35984,-1.35849,-1.35758,-1.35689,-1.35632,-1.35582,-1.35535,-1.35485,-1.35426,-1.35357,-1.35279,-1.35186,-1.35082,-1.34976,-1.3488,-1.34801,-1.34746,-1.34711,-1.3469,-1.34676,-1.34666,-1.34659,-1.34653,-1.34649,-1.34646,-1.34643,-1.3464,-1.34637,-1.34635,-1.34632,-1.34629,-1.34625,-1.34622,-1.34618,-1.34612,-1.34606,-1.34598,-1.34587,-1.34575,-1.3456,-1.34541,-1.34517,-1.34491,-1.34464]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.344556
INFO: iteration 2, average log likelihood -1.344184
INFO: iteration 3, average log likelihood -1.343281
INFO: iteration 4, average log likelihood -1.336269
INFO: iteration 5, average log likelihood -1.318285
INFO: iteration 6, average log likelihood -1.306508
INFO: iteration 7, average log likelihood -1.302032
INFO: iteration 8, average log likelihood -1.300022
INFO: iteration 9, average log likelihood -1.298806
INFO: iteration 10, average log likelihood -1.297883
INFO: iteration 11, average log likelihood -1.297014
INFO: iteration 12, average log likelihood -1.296141
INFO: iteration 13, average log likelihood -1.295395
INFO: iteration 14, average log likelihood -1.294829
INFO: iteration 15, average log likelihood -1.294360
INFO: iteration 16, average log likelihood -1.293887
INFO: iteration 17, average log likelihood -1.293335
INFO: iteration 18, average log likelihood -1.292714
INFO: iteration 19, average log likelihood -1.292136
INFO: iteration 20, average log likelihood -1.291702
INFO: iteration 21, average log likelihood -1.291419
INFO: iteration 22, average log likelihood -1.291236
INFO: iteration 23, average log likelihood -1.291104
INFO: iteration 24, average log likelihood -1.290993
INFO: iteration 25, average log likelihood -1.290895
INFO: iteration 26, average log likelihood -1.290812
INFO: iteration 27, average log likelihood -1.290747
INFO: iteration 28, average log likelihood -1.290702
INFO: iteration 29, average log likelihood -1.290669
INFO: iteration 30, average log likelihood -1.290644
INFO: iteration 31, average log likelihood -1.290621
INFO: iteration 32, average log likelihood -1.290597
INFO: iteration 33, average log likelihood -1.290565
INFO: iteration 34, average log likelihood -1.290521
INFO: iteration 35, average log likelihood -1.290453
INFO: iteration 36, average log likelihood -1.290346
INFO: iteration 37, average log likelihood -1.290156
INFO: iteration 38, average log likelihood -1.289787
INFO: iteration 39, average log likelihood -1.289048
INFO: iteration 40, average log likelihood -1.287785
INFO: iteration 41, average log likelihood -1.286360
INFO: iteration 42, average log likelihood -1.285505
INFO: iteration 43, average log likelihood -1.285137
INFO: iteration 44, average log likelihood -1.284906
INFO: iteration 45, average log likelihood -1.284733
INFO: iteration 46, average log likelihood -1.284606
INFO: iteration 47, average log likelihood -1.284522
INFO: iteration 48, average log likelihood -1.284463
INFO: iteration 49, average log likelihood -1.284412
INFO: iteration 50, average log likelihood -1.284355
INFO: EM with 100000 data points 50 iterations avll -1.284355
236.4 data points per parameter
3: avll = [-1.34456,-1.34418,-1.34328,-1.33627,-1.31828,-1.30651,-1.30203,-1.30002,-1.29881,-1.29788,-1.29701,-1.29614,-1.29539,-1.29483,-1.29436,-1.29389,-1.29334,-1.29271,-1.29214,-1.2917,-1.29142,-1.29124,-1.2911,-1.29099,-1.2909,-1.29081,-1.29075,-1.2907,-1.29067,-1.29064,-1.29062,-1.2906,-1.29057,-1.29052,-1.29045,-1.29035,-1.29016,-1.28979,-1.28905,-1.28779,-1.28636,-1.28551,-1.28514,-1.28491,-1.28473,-1.28461,-1.28452,-1.28446,-1.28441,-1.28436]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.284504
INFO: iteration 2, average log likelihood -1.284178
INFO: iteration 3, average log likelihood -1.283039
INFO: iteration 4, average log likelihood -1.272016
WARNING: Variances had to be floored 3
INFO: iteration 5, average log likelihood -1.235829
INFO: iteration 6, average log likelihood -1.215942
WARNING: Variances had to be floored 3 4 8
INFO: iteration 7, average log likelihood -1.200450
INFO: iteration 8, average log likelihood -1.216721
WARNING: Variances had to be floored 3 4
INFO: iteration 9, average log likelihood -1.204389
INFO: iteration 10, average log likelihood -1.205345
WARNING: Variances had to be floored 3 4 5 8
INFO: iteration 11, average log likelihood -1.194048
INFO: iteration 12, average log likelihood -1.215088
WARNING: Variances had to be floored 3 4
INFO: iteration 13, average log likelihood -1.201608
INFO: iteration 14, average log likelihood -1.204345
WARNING: Variances had to be floored 3 4 8
INFO: iteration 15, average log likelihood -1.193811
WARNING: Variances had to be floored 5
INFO: iteration 16, average log likelihood -1.203301
WARNING: Variances had to be floored 3 4
INFO: iteration 17, average log likelihood -1.201806
INFO: iteration 18, average log likelihood -1.202543
WARNING: Variances had to be floored 3 4 8
INFO: iteration 19, average log likelihood -1.192419
INFO: iteration 20, average log likelihood -1.203011
WARNING: Variances had to be floored 3 4 5
INFO: iteration 21, average log likelihood -1.191127
WARNING: Variances had to be floored 8
INFO: iteration 22, average log likelihood -1.205480
WARNING: Variances had to be floored 3 4
INFO: iteration 23, average log likelihood -1.199843
INFO: iteration 24, average log likelihood -1.201749
WARNING: Variances had to be floored 3 4 8
INFO: iteration 25, average log likelihood -1.191220
WARNING: Variances had to be floored 5
INFO: iteration 26, average log likelihood -1.200842
WARNING: Variances had to be floored 3 4
INFO: iteration 27, average log likelihood -1.200177
WARNING: Variances had to be floored 8
INFO: iteration 28, average log likelihood -1.201798
WARNING: Variances had to be floored 3 4
INFO: iteration 29, average log likelihood -1.197984
INFO: iteration 30, average log likelihood -1.199668
WARNING: Variances had to be floored 3 4 5 8
INFO: iteration 31, average log likelihood -1.189113
INFO: iteration 32, average log likelihood -1.210080
WARNING: Variances had to be floored 3 4
INFO: iteration 33, average log likelihood -1.196680
WARNING: Variances had to be floored 8
INFO: iteration 34, average log likelihood -1.199878
WARNING: Variances had to be floored 3 4
INFO: iteration 35, average log likelihood -1.195871
WARNING: Variances had to be floored 5
INFO: iteration 36, average log likelihood -1.197594
WARNING: Variances had to be floored 3 4 8
INFO: iteration 37, average log likelihood -1.198367
INFO: iteration 38, average log likelihood -1.206533
WARNING: Variances had to be floored 3 4
INFO: iteration 39, average log likelihood -1.194739
WARNING: Variances had to be floored 8
INFO: iteration 40, average log likelihood -1.197766
WARNING: Variances had to be floored 3 4 5
INFO: iteration 41, average log likelihood -1.193914
INFO: iteration 42, average log likelihood -1.206925
WARNING: Variances had to be floored 3 4 8
INFO: iteration 43, average log likelihood -1.194893
INFO: iteration 44, average log likelihood -1.204777
WARNING: Variances had to be floored 3 4
INFO: iteration 45, average log likelihood -1.192821
WARNING: Variances had to be floored 5 8
INFO: iteration 46, average log likelihood -1.195891
WARNING: Variances had to be floored 3 4
INFO: iteration 47, average log likelihood -1.203258
INFO: iteration 48, average log likelihood -1.203491
WARNING: Variances had to be floored 3 4 8
INFO: iteration 49, average log likelihood -1.193049
INFO: iteration 50, average log likelihood -1.202683
INFO: EM with 100000 data points 50 iterations avll -1.202683
118.1 data points per parameter
4: avll = [-1.2845,-1.28418,-1.28304,-1.27202,-1.23583,-1.21594,-1.20045,-1.21672,-1.20439,-1.20534,-1.19405,-1.21509,-1.20161,-1.20435,-1.19381,-1.2033,-1.20181,-1.20254,-1.19242,-1.20301,-1.19113,-1.20548,-1.19984,-1.20175,-1.19122,-1.20084,-1.20018,-1.2018,-1.19798,-1.19967,-1.18911,-1.21008,-1.19668,-1.19988,-1.19587,-1.19759,-1.19837,-1.20653,-1.19474,-1.19777,-1.19391,-1.20692,-1.19489,-1.20478,-1.19282,-1.19589,-1.20326,-1.20349,-1.19305,-1.20268]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 6 7 8 9 10
INFO: iteration 1, average log likelihood -1.191028
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16
INFO: iteration 2, average log likelihood -1.188834
WARNING: Variances had to be floored 5 6 7 8 9 10
INFO: iteration 3, average log likelihood -1.189586
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16
INFO: iteration 4, average log likelihood -1.173092
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 5, average log likelihood -1.125883
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 6, average log likelihood -1.110277
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 7, average log likelihood -1.099411
WARNING: Variances had to be floored 3 5 6 7 8 9 10 15 16 18
INFO: iteration 8, average log likelihood -1.099436
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 9, average log likelihood -1.097214
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 10, average log likelihood -1.108143
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28
INFO: iteration 11, average log likelihood -1.096413
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 12, average log likelihood -1.103457
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 13, average log likelihood -1.093113
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 14, average log likelihood -1.107089
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28
INFO: iteration 15, average log likelihood -1.095870
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 16, average log likelihood -1.103441
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 17, average log likelihood -1.092972
WARNING: Variances had to be floored 3 5 6 7 8 9 10 15 16 18
INFO: iteration 18, average log likelihood -1.106965
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 19, average log likelihood -1.101015
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 20, average log likelihood -1.100184
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 21, average log likelihood -1.091872
WARNING: Variances had to be floored 3 5 6 7 8 9 10 15 16 18
INFO: iteration 22, average log likelihood -1.106342
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 23, average log likelihood -1.101077
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 24, average log likelihood -1.100082
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28 31
INFO: iteration 25, average log likelihood -1.091704
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 26, average log likelihood -1.111464
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 27, average log likelihood -1.097848
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 28, average log likelihood -1.099025
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28 31
INFO: iteration 29, average log likelihood -1.091084
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 30, average log likelihood -1.111522
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 31, average log likelihood -1.097748
WARNING: Variances had to be floored 3 5 6 7 8 9 10 15 16 18
INFO: iteration 32, average log likelihood -1.098856
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 33, average log likelihood -1.096208
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 34, average log likelihood -1.108264
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 35, average log likelihood -1.096642
WARNING: Variances had to be floored 3 5 6 7 8 9 10 15 16 18
INFO: iteration 36, average log likelihood -1.098220
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 37, average log likelihood -1.096267
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 38, average log likelihood -1.108158
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28
INFO: iteration 39, average log likelihood -1.096471
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 40, average log likelihood -1.103343
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 41, average log likelihood -1.093031
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 42, average log likelihood -1.107105
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28
INFO: iteration 43, average log likelihood -1.095852
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 44, average log likelihood -1.103399
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 45, average log likelihood -1.092928
WARNING: Variances had to be floored 3 5 6 7 8 9 10 15 16 18
INFO: iteration 46, average log likelihood -1.106935
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 47, average log likelihood -1.100968
WARNING: Variances had to be floored 5 6 7 8 9 10 15 16 18
INFO: iteration 48, average log likelihood -1.100141
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28 31
INFO: iteration 49, average log likelihood -1.091818
WARNING: Variances had to be floored 3 5 6 7 8 9 10 15 16 18
INFO: iteration 50, average log likelihood -1.106294
INFO: EM with 100000 data points 50 iterations avll -1.106294
59.0 data points per parameter
5: avll = [-1.19103,-1.18883,-1.18959,-1.17309,-1.12588,-1.11028,-1.09941,-1.09944,-1.09721,-1.10814,-1.09641,-1.10346,-1.09311,-1.10709,-1.09587,-1.10344,-1.09297,-1.10697,-1.10101,-1.10018,-1.09187,-1.10634,-1.10108,-1.10008,-1.0917,-1.11146,-1.09785,-1.09902,-1.09108,-1.11152,-1.09775,-1.09886,-1.09621,-1.10826,-1.09664,-1.09822,-1.09627,-1.10816,-1.09647,-1.10334,-1.09303,-1.10711,-1.09585,-1.1034,-1.09293,-1.10693,-1.10097,-1.10014,-1.09182,-1.10629]
[-1.4318,-1.43185,-1.43178,-1.43122,-1.42591,-1.41181,-1.40438,-1.40256,-1.40163,-1.40085,-1.40025,-1.39986,-1.3996,-1.39942,-1.39926,-1.39913,-1.39902,-1.39893,-1.39885,-1.39879,-1.39874,-1.3987,-1.39867,-1.39864,-1.39862,-1.3986,-1.39859,-1.39857,-1.39856,-1.39855,-1.39854,-1.39854,-1.39853,-1.39853,-1.39852,-1.39852,-1.39852,-1.39851,-1.39851,-1.39851,-1.3985,-1.3985,-1.3985,-1.3985,-1.39849,-1.39849,-1.39849,-1.39849,-1.39849,-1.39848,-1.39848,-1.39858,-1.39846,-1.39766,-1.39,-1.37159,-1.36261,-1.35984,-1.35849,-1.35758,-1.35689,-1.35632,-1.35582,-1.35535,-1.35485,-1.35426,-1.35357,-1.35279,-1.35186,-1.35082,-1.34976,-1.3488,-1.34801,-1.34746,-1.34711,-1.3469,-1.34676,-1.34666,-1.34659,-1.34653,-1.34649,-1.34646,-1.34643,-1.3464,-1.34637,-1.34635,-1.34632,-1.34629,-1.34625,-1.34622,-1.34618,-1.34612,-1.34606,-1.34598,-1.34587,-1.34575,-1.3456,-1.34541,-1.34517,-1.34491,-1.34464,-1.34456,-1.34418,-1.34328,-1.33627,-1.31828,-1.30651,-1.30203,-1.30002,-1.29881,-1.29788,-1.29701,-1.29614,-1.29539,-1.29483,-1.29436,-1.29389,-1.29334,-1.29271,-1.29214,-1.2917,-1.29142,-1.29124,-1.2911,-1.29099,-1.2909,-1.29081,-1.29075,-1.2907,-1.29067,-1.29064,-1.29062,-1.2906,-1.29057,-1.29052,-1.29045,-1.29035,-1.29016,-1.28979,-1.28905,-1.28779,-1.28636,-1.28551,-1.28514,-1.28491,-1.28473,-1.28461,-1.28452,-1.28446,-1.28441,-1.28436,-1.2845,-1.28418,-1.28304,-1.27202,-1.23583,-1.21594,-1.20045,-1.21672,-1.20439,-1.20534,-1.19405,-1.21509,-1.20161,-1.20435,-1.19381,-1.2033,-1.20181,-1.20254,-1.19242,-1.20301,-1.19113,-1.20548,-1.19984,-1.20175,-1.19122,-1.20084,-1.20018,-1.2018,-1.19798,-1.19967,-1.18911,-1.21008,-1.19668,-1.19988,-1.19587,-1.19759,-1.19837,-1.20653,-1.19474,-1.19777,-1.19391,-1.20692,-1.19489,-1.20478,-1.19282,-1.19589,-1.20326,-1.20349,-1.19305,-1.20268,-1.19103,-1.18883,-1.18959,-1.17309,-1.12588,-1.11028,-1.09941,-1.09944,-1.09721,-1.10814,-1.09641,-1.10346,-1.09311,-1.10709,-1.09587,-1.10344,-1.09297,-1.10697,-1.10101,-1.10018,-1.09187,-1.10634,-1.10108,-1.10008,-1.0917,-1.11146,-1.09785,-1.09902,-1.09108,-1.11152,-1.09775,-1.09886,-1.09621,-1.10826,-1.09664,-1.09822,-1.09627,-1.10816,-1.09647,-1.10334,-1.09303,-1.10711,-1.09585,-1.1034,-1.09293,-1.10693,-1.10097,-1.10014,-1.09182,-1.10629]
32×26 Array{Float64,2}:
  0.109564    -0.0492963    0.128163     0.128809     -0.0116257   -0.0739128   0.0492507   -0.135386    -0.0362398   -0.0503644   -0.00466395  -0.159712     0.0381237    0.204053     0.0031008   -0.038095     0.119567     0.0728368    -0.0313756   0.00639573    0.0486956  -0.0932789   -0.119202     0.12338       0.0502019   -0.0170859 
 -0.196952     0.0232015    0.0387552    0.0417146     0.0449883   -0.0525343  -0.0278206   -0.125601     0.0507878    0.035717    -0.0310119    0.0983591    0.00837118  -0.0675443    0.146721     0.0516097   -0.0181662    0.11013       0.233127    0.0464638    -0.0534119   0.130267    -0.185565     0.0261038     0.0842594   -0.0388731 
 -0.0365376    0.0991171    0.206844    -0.0272596     0.0896659   -0.118441   -0.0232302   -0.152619    -0.0900584    0.0343567    0.0684431    0.0974949    0.107107    -0.123764     0.0723525    0.0726922   -0.0211404   -0.130516     -0.0944095  -0.0805804     0.0266517  -0.0527044   -0.0768986   -0.194958     -0.00840395   0.0895747 
  0.039984     0.0892918    0.0642814    0.149108      0.0775377    0.035571    0.0208255    0.00948783   0.00688625   0.0327907   -0.0599814   -0.0204861   -0.0596426    0.087558     2.67815e-5   0.00675433  -0.0625802   -0.135374     -0.0737564   0.0102415    -0.0240685  -0.0769591    0.0109584    0.0725655    -0.00841957   0.10113   
  0.125922     0.00619398   0.171548     0.0119959    -0.24828     -0.0383354  -0.00201491  -0.052653    -0.188313     0.138718    -0.0464391   -0.0646194    0.0473597    0.0818722    0.0368822   -0.0307612    0.131014    -0.04432       0.356963   -0.58697      -0.0199421   0.0683401    0.0311213   -0.0638485    -0.0929206   -0.103925  
  0.123666     0.125281    -0.0177696    0.0108425     0.0944323   -0.0379072  -0.00129634  -0.0324597   -0.141637    -0.0356503   -0.0491781    0.0296784    0.0190961   -0.141078     0.0397493   -0.109516     0.205083     0.010556     -0.145382    0.712929     -0.186377    0.145305     0.0328549   -0.0677986    -0.0927173   -0.089182  
 -0.321386    -0.0684613   -0.0712487   -0.093778      0.0889509    0.0704007  -0.1466       0.0209415   -0.0483708    0.00448726  -0.520905     0.179003     0.103118    -0.0974547   -0.0593311    0.173023    -0.0755038   -0.104         0.0818236   0.0703161     0.108146    0.121675    -0.380994    -0.25647      -0.0617128   -0.169874  
  0.0772263   -0.0686297   -0.120121    -0.0937887     0.107614     0.14313     0.17777     -0.123678    -0.0748188    0.014258     0.342277     0.173157     0.0489178   -0.113186    -0.0331943    0.233892    -0.0780553   -0.0875811     0.0814423   0.0785074     0.0479669   0.188816     0.107706    -0.290993     -0.0704388   -0.0120044 
 -0.0408436    0.164927    -0.126543     0.187943     -0.336778    -0.168018    0.16963     -0.117792    -0.0063527   -0.166284    -0.0849773    0.0497576   -0.274743    -0.0112419   -0.153418     0.143956     0.0225066    0.0358032    -0.109263   -0.0172939    -0.0270031  -0.162608    -0.0387502    0.0129195    -0.01243      0.0842445 
 -0.0760748    0.119128    -0.126389     0.111499     -0.0179025    0.0354039   0.104469    -0.0276156   -0.00726742  -0.0453243   -0.088839     0.0496652    0.559345     0.0309979    0.0162627   -0.0920058    0.0637408    0.0410272    -0.0250866  -0.0490983    -0.0651329   0.104804    -0.0110034   -0.00605085   -0.19934      0.0807184 
 -0.143261     0.0461242    0.0905364   -0.0700426    -0.0929064   -0.0866344   0.0177428   -0.0231941    0.0164533    0.0865923   -0.0761761    0.0433336   -0.105383     0.0907629    0.0866037    0.0220688   -0.0737021    0.0732235    -0.0453504   0.0772029    -0.0256462  -0.147281     0.0282016   -0.0333052     0.0336417   -0.141978  
 -0.195638     0.0678424   -0.00196431  -0.155376     -0.0238978   -0.0817183   0.00996318  -0.0798946   -0.114299     0.140666    -0.00704059   0.00276323  -0.0182643   -0.0215659   -0.080886    -0.0364112    0.0132437    0.136439     -0.0233426  -0.18573      -0.158449   -0.11118     -0.0292721    0.139673      0.0954983    0.124801  
 -0.0491518   -0.0591299    0.0275131    0.0307025    -0.0350024    0.0279425  -0.192832    -0.156035     0.135139    -0.0171312   -0.017084     0.0852965   -0.0978776   -0.0612765    0.0190102    0.0643335   -0.055351     0.00334018    0.0188499   0.0261449     0.190908    0.104858     0.0184165   -0.102707      0.111814    -0.0524943 
 -0.0788008   -0.0807581   -0.0347807   -0.0070796    -0.0101616    0.0772132  -0.193538     0.0779896    0.147957     0.0427508   -0.0916794   -0.0202301   -0.0839895   -0.0780298   -0.0140555    0.0833346   -0.178971     0.00459188    0.0246647   0.0788717     0.342561    0.110188    -0.0790027    0.259265     -0.0312444   -0.0177595 
  0.119783    -0.06358     -0.112668    -0.0699367    -1.40441      0.244732    0.113665     0.230281    -0.022546    -0.222617     0.152868    -0.132041    -0.184772     0.12537     -0.0833601    0.0289911    0.102193    -0.0284231     0.0419731   0.0978402     0.105228    0.0613785    0.203566     0.127659      0.0116067   -0.0316407 
  0.119518    -0.114643    -0.133303    -0.0765725     0.750662     0.0414971   0.117676     0.094537    -0.0828858   -0.247447     0.291367    -0.0731123    0.186926     0.128362    -0.0770712   -0.124733     0.0691672   -0.111856      0.0553336  -0.148462      0.105437    0.111906     0.116024     0.255014     -0.100667    -0.0340317 
  0.105501     0.0452784   -0.0208575    0.00705018   -0.0870087   -0.0682034   0.0509954   -0.00999884   0.137819     0.0174264    0.0692515   -0.0390757    0.0124169    0.0615592    0.0910194    0.0547293    0.0185653    0.0738241    -0.0141569   0.141473      0.0074241   0.0774111   -0.0711348   -0.0555101    -0.0400847   -0.0224468 
  0.0407622   -0.0318594    0.165828    -0.0336471    -0.122761     0.0509775  -0.157325     0.141873     0.0219691   -0.0350665   -0.0742405   -0.0852881   -0.0954114    0.041985    -0.056978    -0.017386     0.110118    -0.166247     -0.0686768   0.0573099    -0.1946      0.0821024    0.121518     0.194        -0.180279    -0.0375364 
 -0.00672766  -0.0940957    0.0472381   -0.0432147     0.0828121    0.0556413  -0.00911087  -0.0373897    0.0726285    0.0697628    0.0605511    0.0937091   -0.0923876   -0.193016    -0.112493    -0.197507    -0.142545     0.197157      0.0899575   0.299137      0.0489908   0.197247    -0.0236045    0.107581     -0.270916    -0.0278536 
 -0.0864599    0.0502325   -0.147749    -0.104159     -0.0308255   -0.0233727   0.114148    -0.0500892    0.0670854    0.16078      0.0838994    0.10949      0.042256     0.0208464    0.0207398    0.055402    -0.10867     -0.111801      0.0331948  -0.0682975    -0.118115   -0.0586224    0.042947     0.0109505     0.0265212    0.178019  
 -0.143108    -0.0464896   -0.00276407   0.0197126    -0.00227307  -0.0314264  -0.128956    -0.0698421    0.0300251    0.00773045  -0.0832668   -0.0537086    0.0552452    0.26944     -0.113114    -0.132238    -0.150981    -0.0428636    -0.596393   -0.0269157     0.0266245   0.176814    -0.230058     0.00863325   -0.0355716   -0.167623  
 -0.0342452   -0.0513392   -0.0239666    0.0475323    -0.0227081   -0.168013   -0.113297     0.0197105    0.0258211   -0.0738006    0.00106114  -0.0506041   -0.104974    -0.109469    -0.0366978   -0.123873    -0.186866    -0.117921      0.562008   -0.0407373     0.141759    0.217467    -0.194691    -0.0320542     0.0911196   -0.140334  
  0.0480771    0.247097    -1.1077      -0.00888378    0.0423734   -0.0234917  -0.143419    -0.0703922    0.162296    -0.0278205    0.0951442   -0.0254531   -0.0433026   -0.204227     0.206396     0.0879865    0.0458311   -0.0891098     0.123899   -0.0274262     0.101715   -0.0892745    0.0107525    0.00369426    0.184854    -0.178538  
 -0.0525109    0.20945      1.14294      0.0262263     0.0311783   -0.0696541  -0.137503    -0.102265     0.129019    -0.0300975    0.128201     0.0971401   -0.0294123   -0.201163    -0.0855626    0.0705832    0.0492295    0.0580907     0.149426   -0.0473351     0.0971073  -0.166745    -0.00930758   0.00759216   -0.0192391   -0.12282   
 -0.120416     0.118528     0.102112    -0.110265      0.0438854   -0.0588819   0.188546     0.00828064   0.10103      0.146755     0.0294219   -0.0187381    0.0287631    0.0322711   -0.184764    -0.0981273    0.192136     0.0541664    -0.003175   -0.0903344     0.321079   -0.0278955   -0.0376022   -0.0631163     0.00348772   0.0253821 
  0.00436141   0.0564021    0.0690502   -0.130707      0.134316     0.109938    0.0186215   -0.0403638    0.107669    -0.131895     0.0641075    0.0329597   -0.0377024   -0.0962381   -0.0421035    0.0546971    0.0117142    0.00535215   -0.0595615  -0.000370483  -0.185334    0.049901    -0.0104052   -0.0818596    -0.0097571    0.00512874
  0.112545     0.243376    -0.155694     0.000274376  -0.0734242   -0.0655369  -0.145877     0.097514    -0.081873    -0.161107    -0.0630754    0.00981731   0.228008    -0.0639041    0.0310131    0.00649525  -0.0205821    0.000450135  -0.0053376   0.00776941    0.0283243   0.127866     0.036046    -0.21433       0.218064     0.071588  
  0.0396068    0.0614722    0.257771     0.0509488     0.0349093    0.0868171  -0.142523     0.00942643  -0.0164043   -0.0731174   -0.0162921    0.0356925   -0.0824128   -0.0447272    0.127099    -0.0295338    0.0884903   -0.0928509     0.0459111   0.0886022    -0.0307062   0.0400828   -0.114699    -0.0218536    -0.0106578    0.015531  
 -0.0237284    0.0300117    0.0637359   -0.0740124    -0.0397332    0.0545105   0.042748    -0.0526241   -0.0704664   -0.0654284    0.0519749   -0.0854526    0.045916     0.0986542   -0.0812484   -0.0360429    0.00600835  -0.0674759     0.0759023   0.010651      0.0534502   0.0290098    0.00899692   0.0175704     0.00335066  -0.047691  
 -0.100591    -0.00725218   0.0136501   -0.0392944    -0.0348338    0.0699029  -0.082315     0.0807653   -0.0821796   -0.0187091    0.0454687   -0.0325481    0.021059    -0.01903      0.0632686   -0.157795     0.0768698    0.0522047     0.0174267  -0.0791211     0.017225    0.0781019   -0.0334541    0.130724      0.0122299   -0.0785477 
  0.0125036    0.0268746    0.0897467    0.0554798    -0.00202026   0.0336534  -0.0685255   -0.0746451   -0.0131138   -0.0466581    0.0702923   -0.0268426   -0.00517077   0.00571176   0.183694    -0.0425289   -0.0428374   -0.196973     -0.175162   -0.0567839    -0.0187859  -0.00240914   0.0560628   -0.0218843     0.268267    -0.0111594 
 -0.0318663    0.0517689   -0.0174822   -0.182283      0.103671     0.108263   -0.0987525    0.127398     0.038414     0.0900162    0.0187632    0.05043      0.0744578   -0.0705289   -0.0171054    0.120023     0.16308     -0.0749115     0.0343194  -0.012948      0.0705553  -0.00744344  -0.0957199    0.000822341   0.0913375    0.209278  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 1, average log likelihood -1.101011
WARNING: Variances had to be floored 5 6 7 8 9 10 11 15 16 18 28
INFO: iteration 2, average log likelihood -1.087544
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28 31
INFO: iteration 3, average log likelihood -1.091679
WARNING: Variances had to be floored 5 6 7 8 9 10 11 15 16 18 28
INFO: iteration 4, average log likelihood -1.093479
WARNING: Variances had to be floored 5 6 7 8 9 10 11 28
INFO: iteration 5, average log likelihood -1.094677
WARNING: Variances had to be floored 5 6 7 8 9 10 11 15 16 18 28 31
INFO: iteration 6, average log likelihood -1.083807
WARNING: Variances had to be floored 3 5 6 7 8 9 10 11 28
INFO: iteration 7, average log likelihood -1.093866
WARNING: Variances had to be floored 5 6 7 8 9 10 11 15 16 18 28
INFO: iteration 8, average log likelihood -1.083969
WARNING: Variances had to be floored 5 6 7 8 9 10 11 13 28 31
INFO: iteration 9, average log likelihood -1.083175
WARNING: Variances had to be floored 5 6 7 8 9 10 11 15 16 18 28
INFO: iteration 10, average log likelihood -1.093261
INFO: EM with 100000 data points 10 iterations avll -1.093261
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.880308e+05
      1       7.083479e+05      -1.796830e+05 |       32
      2       6.772498e+05      -3.109812e+04 |       32
      3       6.643232e+05      -1.292657e+04 |       32
      4       6.567293e+05      -7.593912e+03 |       32
      5       6.501984e+05      -6.530821e+03 |       32
      6       6.447330e+05      -5.465406e+03 |       32
      7       6.412822e+05      -3.450853e+03 |       32
      8       6.387748e+05      -2.507357e+03 |       32
      9       6.368450e+05      -1.929863e+03 |       32
     10       6.357234e+05      -1.121561e+03 |       32
     11       6.349867e+05      -7.366791e+02 |       32
     12       6.341987e+05      -7.880576e+02 |       32
     13       6.329872e+05      -1.211501e+03 |       32
     14       6.319118e+05      -1.075399e+03 |       32
     15       6.313595e+05      -5.522311e+02 |       32
     16       6.311109e+05      -2.486200e+02 |       32
     17       6.309745e+05      -1.364343e+02 |       32
     18       6.308864e+05      -8.810797e+01 |       32
     19       6.308313e+05      -5.503971e+01 |       32
     20       6.307917e+05      -3.967038e+01 |       31
     21       6.307655e+05      -2.614419e+01 |       32
     22       6.307504e+05      -1.510347e+01 |       29
     23       6.307413e+05      -9.134642e+00 |       31
     24       6.307365e+05      -4.742937e+00 |       26
     25       6.307335e+05      -3.014830e+00 |       20
     26       6.307321e+05      -1.415579e+00 |       16
     27       6.307315e+05      -5.726391e-01 |        9
     28       6.307312e+05      -3.286789e-01 |       12
     29       6.307308e+05      -3.984877e-01 |        7
     30       6.307307e+05      -1.395069e-01 |        0
     31       6.307307e+05       0.000000e+00 |        0
K-means converged with 31 iterations (objv = 630730.6731389796)
INFO: K-means with 32000 data points using 31 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.351050
INFO: iteration 2, average log likelihood -1.320058
INFO: iteration 3, average log likelihood -1.288803
INFO: iteration 4, average log likelihood -1.252375
INFO: iteration 5, average log likelihood -1.198990
WARNING: Variances had to be floored 23
INFO: iteration 6, average log likelihood -1.142141
WARNING: Variances had to be floored 1 3 18 19 25 32
INFO: iteration 7, average log likelihood -1.101601
WARNING: Variances had to be floored 10 21 22
INFO: iteration 8, average log likelihood -1.131769
WARNING: Variances had to be floored 8 12
INFO: iteration 9, average log likelihood -1.122739
WARNING: Variances had to be floored 9 23 25 32
INFO: iteration 10, average log likelihood -1.092650
WARNING: Variances had to be floored 3 18 19 21
INFO: iteration 11, average log likelihood -1.115548
WARNING: Variances had to be floored 1
INFO: iteration 12, average log likelihood -1.121456
WARNING: Variances had to be floored 12 23
INFO: iteration 13, average log likelihood -1.075321
WARNING: Variances had to be floored 3 8 10 19 25 32
INFO: iteration 14, average log likelihood -1.071617
WARNING: Variances had to be floored 21
INFO: iteration 15, average log likelihood -1.125253
WARNING: Variances had to be floored 1 12 18 23
INFO: iteration 16, average log likelihood -1.091980
WARNING: Variances had to be floored 32
INFO: iteration 17, average log likelihood -1.102426
WARNING: Variances had to be floored 3 8 10 19 25
INFO: iteration 18, average log likelihood -1.071548
WARNING: Variances had to be floored 1 12 21 23
INFO: iteration 19, average log likelihood -1.101946
WARNING: Variances had to be floored 18 32
INFO: iteration 20, average log likelihood -1.113315
INFO: iteration 21, average log likelihood -1.098923
WARNING: Variances had to be floored 1 3 8 10 12 22 23 25
INFO: iteration 22, average log likelihood -1.048423
WARNING: Variances had to be floored 19 21 32
INFO: iteration 23, average log likelihood -1.115605
WARNING: Variances had to be floored 18
INFO: iteration 24, average log likelihood -1.123931
WARNING: Variances had to be floored 1 12 23
INFO: iteration 25, average log likelihood -1.084459
WARNING: Variances had to be floored 3 8 10 19 25 32
INFO: iteration 26, average log likelihood -1.072426
WARNING: Variances had to be floored 21
INFO: iteration 27, average log likelihood -1.124779
WARNING: Variances had to be floored 1 12 18 23
INFO: iteration 28, average log likelihood -1.090279
WARNING: Variances had to be floored 22 32
INFO: iteration 29, average log likelihood -1.098753
WARNING: Variances had to be floored 3 8 10 25
INFO: iteration 30, average log likelihood -1.081183
WARNING: Variances had to be floored 1 12 19 21 23
INFO: iteration 31, average log likelihood -1.091714
WARNING: Variances had to be floored 18 32
INFO: iteration 32, average log likelihood -1.120557
INFO: iteration 33, average log likelihood -1.099810
WARNING: Variances had to be floored 3 8 10 12 19 23 25
INFO: iteration 34, average log likelihood -1.049422
WARNING: Variances had to be floored 1 21 22 32
INFO: iteration 35, average log likelihood -1.103653
WARNING: Variances had to be floored 18
INFO: iteration 36, average log likelihood -1.127423
WARNING: Variances had to be floored 23
INFO: iteration 37, average log likelihood -1.088468
WARNING: Variances had to be floored 1 3 8 10 22 25 32
INFO: iteration 38, average log likelihood -1.056176
WARNING: Variances had to be floored 21
INFO: iteration 39, average log likelihood -1.126480
WARNING: Variances had to be floored 18 19 23
INFO: iteration 40, average log likelihood -1.094832
WARNING: Variances had to be floored 1 32
INFO: iteration 41, average log likelihood -1.095295
WARNING: Variances had to be floored 3 8 10 25
INFO: iteration 42, average log likelihood -1.079460
WARNING: Variances had to be floored 21 22 23
INFO: iteration 43, average log likelihood -1.097917
WARNING: Variances had to be floored 1 12 19 32
INFO: iteration 44, average log likelihood -1.097123
WARNING: Variances had to be floored 18
INFO: iteration 45, average log likelihood -1.101417
WARNING: Variances had to be floored 3 8 10 23 25
INFO: iteration 46, average log likelihood -1.061105
WARNING: Variances had to be floored 1 19 21 22 32
INFO: iteration 47, average log likelihood -1.103613
WARNING: Variances had to be floored 12
INFO: iteration 48, average log likelihood -1.127444
WARNING: Variances had to be floored 18 23
INFO: iteration 49, average log likelihood -1.077986
WARNING: Variances had to be floored 1 3 8 10 19 25 32
INFO: iteration 50, average log likelihood -1.060226
INFO: EM with 100000 data points 50 iterations avll -1.060226
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.126834     0.0638357    0.0756011     0.00906005  -0.0697268   -0.0364777  -0.00199001  -0.0391474   -0.166425     0.05335     -0.0489442   -0.0140162    0.0326421   -0.0312676    0.0374958   -0.0666077    0.165534   -0.0171465     0.103746      0.0704846   -0.0990445   0.104682     0.0301175    -0.069145    -0.0923799    -0.0981119 
 -0.0484734    0.0168295   -0.0342352    -0.192431     0.0888343    0.256963   -0.0559026    0.0998918    0.028167     0.0779894   -0.122596    -0.100903    -0.0203526   -0.133881    -0.0942096    0.108702     0.156294   -0.0961941     0.0421016    -0.00104314   0.159306   -0.109646    -0.139589     -0.00579249   0.0772048     0.212174  
 -0.0588762    0.14203     -0.126627      0.151627    -0.180827    -0.0679883   0.135852    -0.074006    -0.00650595  -0.108209    -0.0860758    0.0501529    0.143793     0.0104105   -0.0685269    0.0276189    0.0435953   0.0385144    -0.068777     -0.0335797   -0.0472517  -0.0310499   -0.0251692     0.00329382  -0.106558      0.0840799 
  0.0662117    0.0593359    0.0141028    -0.0840803   -0.0748751    0.11919     0.116852    -0.0224907   -0.287       -0.113449    -0.0427687   -0.110033     0.278697     0.111311    -0.175116    -0.192823     0.0206984  -0.184693      0.077089     -0.0216338    0.0524354  -0.0466634   -0.0677256    -0.122788    -0.0580329    -0.150622  
 -0.0864596    0.0524541   -0.153556     -0.103788    -0.029105    -0.0242233   0.11874     -0.0499886    0.066334     0.160042     0.0849803    0.108513     0.0435906    0.0189217    0.0226814    0.0619599   -0.112388   -0.112476      0.0333364    -0.0679738   -0.11931    -0.0582608    0.0419452     0.00849226   0.0295972     0.176219  
 -0.19723      0.0229822    0.0388343     0.041672     0.0453764   -0.0522935  -0.0278797   -0.124085     0.0508355    0.0349197   -0.0310722    0.0982287    0.00836794  -0.0674108    0.146616     0.050747    -0.0177173   0.109573      0.233217      0.0472801   -0.0535673   0.130507    -0.185457      0.0261131    0.0842492    -0.0393599 
  0.109762    -0.0550487    0.126494      0.126053    -0.0130336   -0.075516    0.0510477   -0.140151    -0.036697    -0.051009    -0.00435647  -0.152202     0.0378298    0.204749     0.00223572  -0.0435984    0.122046    0.0757908    -0.0397204     0.00780471   0.0477289  -0.097054    -0.132249      0.120565     0.0507356    -0.0157931 
 -0.0113194   -0.070401     0.190233      0.049334    -0.061651     0.0668356   0.0228325   -0.119552     0.0601873    0.0714654    0.075205     0.0113959   -0.0965943    0.126628    -0.0313918    0.00543916   0.17357     0.00689978    0.0209941     0.101661     0.116695    0.0674241    0.0761636     0.039829     0.0656685    -0.0248716 
  0.0309783    0.171375     0.0598311     0.206363     0.0305121    0.0160941   0.0787114   -0.0611936    0.047284    -0.0781117   -0.147043    -0.012063     0.0258517    0.0558439   -0.0515835    0.0275985   -0.0178714  -0.190544      0.0627326     0.0654604   -0.0635939  -0.183638     0.0628586     0.0185812   -0.112657      0.0398563 
 -0.150269     0.0443113    0.0890176    -0.0793403   -0.092778    -0.0836407   0.0189832   -0.0249831    0.0139595    0.0869403   -0.0758241    0.0443284   -0.102296     0.0918687    0.0939357    0.0168631   -0.0758073   0.0742993    -0.0424189     0.0765449   -0.0237619  -0.149823     0.0278006    -0.0350998    0.0373431    -0.14616   
  0.00501066   0.0291495    0.0322872    -0.0250137    0.206197     0.0580881   0.0610148    0.06078      0.09016     -0.0747826    0.0815383    0.0777505   -0.0735001   -0.111571    -0.136122     0.0564386    0.0465876   0.111811     -0.115203     -0.0436944   -0.266303    0.0514231    0.000145877   0.00807194  -0.117579      0.144303  
  0.0292071   -0.0361534   -0.144583     -0.0592704    0.0534796    0.0578338   0.0369439   -0.0959207   -0.136379    -0.00451815  -0.0612454    0.129518     0.0848497   -0.075176    -0.0143      -0.0651198    0.222835   -0.0449829     0.0292073     0.0916318    0.0218134   0.12133      0.0425977    -0.186899    -0.0557188    -0.166756  
  0.174699     0.0434381    0.0577551    -0.0806781   -0.130314    -0.093457    0.0863696    0.00664226   0.173988    -0.0157463    0.0803548   -0.082742    -0.0399167    0.0010417    0.105402     0.106472    -0.104652   -0.0147952     0.0276606     0.0871122   -0.028815    0.0224761   -0.0951944    -0.0441278    0.0304977    -0.049032  
  0.111696     0.240984    -0.154875      0.00104874  -0.0734791   -0.0659344  -0.145863     0.0980972   -0.0808937   -0.16236     -0.0637566    0.00865392   0.227933    -0.0638716    0.0293249    0.00596043  -0.0196806   0.000804595  -0.00462779    0.00882059   0.0273686   0.127908     0.0372874    -0.211615     0.217526      0.0712626 
 -0.0688587   -0.0738599   -0.0262166     0.00585834  -0.024577     0.0572392  -0.193313    -0.00868603   0.142554     0.013385    -0.0776406    0.0182014   -0.0910011   -0.068323    -0.00573852   0.0763838   -0.150964    0.00345034    0.0254594     0.0575221    0.291611    0.109355    -0.0505564     0.119002     0.020211     -0.027327  
  0.00543537   0.0878322    0.119775     -0.228223     0.0651733    0.166969   -0.01816     -0.133197     0.129669    -0.184075     0.0474827   -0.00897171  -0.00450715  -0.080983     0.0501486    0.0567782   -0.0134046  -0.0929041    -0.00681456    0.0475962   -0.109102    0.0482926   -0.0245863    -0.16789      0.092739     -0.120083  
 -0.0879289   -0.0489678   -0.0137531     0.0327853   -0.0123777   -0.100349   -0.121099    -0.0246003    0.0278248   -0.0307369   -0.042387    -0.0531115   -0.0270979    0.0803454   -0.0756155   -0.127905    -0.172147   -0.0808797    -0.0182964    -0.0335546    0.0877455   0.196028    -0.207954     -0.0119755    0.0283943    -0.153306  
 -0.134017    -0.0702278   -0.0382605    -0.0901747    0.0961459    0.095448   -0.00748337  -0.0337314   -0.038592    -0.00378971  -0.107423     0.104995     0.0911686   -0.114213    -0.0494375    0.355987    -0.123679   -0.106689      0.0904734     0.0232845    0.10491     0.141656    -0.253599     -0.223636    -0.000619721  -0.0931937 
  0.0493314    0.0575116    0.272122      0.0780413    0.0485485    0.106953   -0.14059      0.00113426  -0.0210572   -0.0646864   -0.0366703    0.0580111   -0.0818056   -0.0442583    0.146312    -0.0311745    0.0353724  -0.0980509     0.0411624     0.0925028   -0.0222189   0.0463754   -0.115696     -0.0298434   -0.010042      0.00156087
 -0.00587601   0.0510313    0.135085      0.0250032    0.107457    -0.0279261  -0.0301036   -0.0391633   -0.0585763    0.0844668    0.0503255    0.0394595   -0.0113068   -0.0103538    0.0617526    0.0434105   -0.065969   -0.101311     -0.142087     -0.0588291    0.0202513  -0.00250664  -0.0708931    -0.0527341    0.0332606     0.120013  
 -0.102005     0.119825    -0.114749      0.0189066   -0.131784     0.078582   -0.00910567  -0.0157401   -0.0162844   -0.0147396    0.0727394   -0.00906448  -0.0593195    0.084997     0.149638    -0.128516     0.237549    0.176168     -0.000491475  -0.0608374   -0.156918    0.062515    -0.135453      0.0923363   -0.131216     -0.0681899 
  0.0744368    0.0602395    0.270105     -0.125092    -0.0150433    0.0986178  -0.117101     0.123633    -0.0190302   -0.0902767    0.0536414   -0.027247    -0.0641306   -0.0431614    0.0514225   -0.0481258    0.543995   -0.0775111     0.122624      0.0854101    0.0176184  -0.0509765   -0.0675409     0.0297731   -0.0356302     0.103148  
 -0.0983079   -0.145991     0.123593     -0.102165     0.0749107    0.0509732  -0.163163     0.177069    -0.170215    -0.0222465    0.0238526   -0.0329193    0.0993496   -0.155016    -0.0254013   -0.32001     -0.0955251  -0.0703984     0.021977     -0.0804566    0.220955    0.0836156    0.122841      0.163546     0.137354     -0.104161  
 -0.120619     0.11973      0.101748     -0.105969     0.0426767   -0.0560157   0.193955     0.00786141   0.0975225    0.145857     0.027446    -0.0184676    0.0318801    0.0331126   -0.186336    -0.0991874    0.193273    0.0544577    -0.00282531   -0.0899516    0.325317   -0.027896    -0.0366933    -0.0637389    0.00233494    0.0252411 
  0.12022     -0.0982617   -0.127006     -0.0734655    0.0462712    0.107775    0.117186     0.140839    -0.0632107   -0.240491     0.2484      -0.0931894    0.0651919    0.127597    -0.0786157   -0.0787739    0.0802255  -0.0845335     0.052295     -0.0703201    0.105457    0.0954832    0.146419      0.217362    -0.0645505    -0.0329865 
 -0.0050407   -0.0954253    0.0460948    -0.0426214    0.0814736    0.0635308  -0.0104719   -0.0373161    0.067198     0.0726159    0.0593253    0.087959    -0.0923899   -0.19216     -0.115808    -0.198798    -0.144782    0.199507      0.090105      0.297903     0.0482251   0.196698    -0.0238398     0.106534    -0.27127      -0.0293972 
 -0.0221346    0.0769185    0.0181514    -0.146885     0.112101    -0.0761844  -0.157214     0.143141     0.0507667    0.0895957    0.183057     0.194321     0.156105     0.0114129    0.0818021    0.125363     0.161326   -0.0585156     0.0159555    -0.0212458   -0.0220289   0.101289    -0.0305288    -0.00387422   0.125243      0.184514  
 -0.195661     0.0679929   -0.00255431   -0.156527    -0.0252275   -0.0830357   0.0102327   -0.0829743   -0.114542     0.14005     -0.00777958   0.00551906  -0.0194906   -0.020725    -0.0803123   -0.0362157    0.0112751   0.136822     -0.0224552    -0.184088    -0.157536   -0.109667    -0.0289352     0.138893     0.0964773     0.1225    
 -0.138139     0.0940854    0.000476999  -0.176401     0.0180413   -0.0449401  -0.0428647   -0.035823     0.0285401   -0.142876     0.130805    -0.143202    -0.0811649    0.0345027   -0.0229763    0.0789324   -0.204266   -0.0210738     0.123256     -0.0666871   -0.013434    0.0911916    0.0234648     0.148236     0.0157693     0.0318234 
 -0.00321386   0.226713     0.062276      0.00879568   0.036742    -0.0475058  -0.14018     -0.0875373    0.144917    -0.0290048    0.112454     0.0387116   -0.0360381   -0.202677     0.0541686    0.0802953    0.0476612  -0.0115661     0.136956     -0.0378477    0.100035   -0.130217     0.00082021    0.0058524    0.0799281    -0.149615  
  0.0180618   -0.00608735   0.0241797     0.0206653   -0.0725849    0.0217777  -0.0724231    0.0638712    0.0471245    0.00469033  -0.0261494   -0.0191647   -0.0151266    0.0728443   -0.00764629   0.0164725    0.11681    -0.027949     -0.0523113     0.122596    -0.0842373   0.11412      0.0529845     0.0529951   -0.15374      -0.0243913 
  0.0225773    0.026192     0.0965704     0.0638204   -0.00478381   0.0447783  -0.0665055   -0.0865815   -0.0178723   -0.0474275    0.0703403   -0.0342496   -0.00114337   0.00181566   0.197225    -0.0504452   -0.0577231  -0.211932     -0.191122     -0.0530706   -0.0175062   0.0116236    0.0649738    -0.0250843    0.275993     -0.0156052 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 21 22
INFO: iteration 1, average log likelihood -1.128531
WARNING: Variances had to be floored 21 22 23
INFO: iteration 2, average log likelihood -1.084845
WARNING: Variances had to be floored 1 12 21 22 32
INFO: iteration 3, average log likelihood -1.052762
WARNING: Variances had to be floored 3 8 10 18 21 22 23 25
INFO: iteration 4, average log likelihood -1.061195
WARNING: Variances had to be floored 19 21 22
INFO: iteration 5, average log likelihood -1.099929
WARNING: Variances had to be floored 1 21 22 23 32
INFO: iteration 6, average log likelihood -1.067673
WARNING: Variances had to be floored 12 18 21 22 25
INFO: iteration 7, average log likelihood -1.075947
WARNING: Variances had to be floored 3 8 10 21 22 23
INFO: iteration 8, average log likelihood -1.059868
WARNING: Variances had to be floored 1 21 22 32
INFO: iteration 9, average log likelihood -1.071156
WARNING: Variances had to be floored 12 19 21 22 23 25
INFO: iteration 10, average log likelihood -1.074899
INFO: EM with 100000 data points 10 iterations avll -1.074899
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0113366     0.0559331    0.115408    -0.0952234    0.178931    -0.0238839  -0.188494     0.0268173    0.00324575  -0.0392924   -0.0709552  -0.116001     0.227679     0.031223    -0.12179       0.0487196   -0.0814318    0.0925954    0.0461569   -0.0158237   -0.245785    -0.0400564    0.102712    -0.145954      0.116722     0.0457907 
 -0.000309224   0.0328839    0.212574     0.0983861   -0.0460285    0.106669   -0.140626    -0.12567     -0.0772353   -0.0624705    0.0232647   0.140198     0.0972498    0.0562075   -0.0988649     0.274864    -0.133625    -0.0438432   -0.0888897   -0.0128886    0.127792    -0.138825    -0.0240135   -0.00276632    0.0157755    0.0420597 
  0.015985     -0.169284    -0.0944769   -0.17441     -0.109257    -0.0583348  -0.0333951    0.145553    -0.0336443    0.043613    -0.0213891  -0.0425035   -0.0602679    0.0501306   -0.0498468     0.0107441   -0.159468     0.0991142    0.187294    -0.325311    -0.280622     0.0604944   -0.0258959   -0.0300156    -0.222781     0.0481551 
 -0.102422      0.0689976    0.154743     0.118113    -0.121866     0.0551899  -0.129817     0.0569038   -0.00154537   0.0887992   -0.0639312   0.185334     0.0535334    0.10303      0.0543676     0.0406449   -0.0298953   -0.0532787   -0.111788    -0.00351336  -0.170283    -0.164721    -0.0416987   -0.0537858    -0.00426082   0.147502  
  0.0618423    -0.0572556    0.0239031   -0.0491876   -0.0429288    0.149029    0.0808153   -0.147002     0.128186    -0.043309    -0.0741246   0.0193331    0.0372132   -0.126169    -0.188378     -0.202369    -0.015948     0.0199637    0.100033     0.106037    -0.0646518   -0.0902743    0.00818004  -0.195181     -0.0100001   -0.110351  
  0.0994427     0.0284107   -0.0746423    0.183387    -0.210494     0.212863   -0.130683    -0.00283363  -0.0709988    0.230972    -0.119251    0.0830293   -0.100202     0.22753      0.129988     -0.126654    -0.176393    -0.0169056    0.116762    -0.0404387    0.103694    -0.13115     -0.11316     -0.22968       0.155377    -0.0153155 
 -0.0552772     0.0037377   -0.00399069  -0.0190836   -0.229945     0.0209609   0.0013614   -0.0304515   -0.0905339   -0.140286    -0.129956    0.0200403   -0.095509    -0.0912861   -0.116959      0.0324271    0.0723543   -0.0411772   -0.0268078    0.0793552   -0.0401225    0.0240129    0.139847    -0.0993982    -0.0569295   -0.14232   
  0.0323943     0.00607788  -0.0670828    0.177594    -0.0156794    0.0762561   0.121748    -0.00507269  -0.0111331    0.0566424   -0.0754873  -0.0199671    0.0615693    0.026856    -0.0177161    -0.0248      -0.0860079    0.0580451   -0.0564958   -0.113649    -0.0510479    0.156057    -0.0138966   -0.0509857    -0.115454    -0.0668896 
 -0.120942     -0.0547694    0.0391717   -0.0207568    0.0387748   -0.100299   -0.0406159   -0.0146481    0.0133566   -0.0349143   -0.0302416  -0.011584     0.119844    -0.0386159   -0.0218313    -0.0116848    0.0502085    0.111882    -0.0859366    0.0530345   -0.0518888    0.142309    -0.128048    -0.00259058    0.108706     0.0331208 
 -0.164294     -0.0709012   -0.0220639    0.0258958   -0.0448278   -0.122335    0.14953      0.0813656    0.191423     0.00238377   0.121063    0.0133314   -0.00503002  -0.0999706   -0.0337168    -0.00496963   0.0330728   -0.00298366   0.0518692   -0.0622231    0.00487139   0.0416059   -0.040472     0.0207309     0.0754948   -0.0315264 
 -0.0473296    -0.15568      0.0497009    0.114636     0.108867     0.07967    -0.059917     0.160629    -0.147253    -0.0560413   -0.0856491  -0.131211     0.182988    -0.157874    -0.0943089     0.0364539   -0.149234    -0.0572347   -0.0348625    0.0836899   -0.0548775    0.133277     0.0451321    0.0394433    -0.04891     -0.0248694 
  0.153163     -0.00758102  -0.156484     0.026522    -0.0162982    0.0678682   0.0910197   -0.0627715    0.043015     0.0502014    0.0475478   0.0249066   -0.0820574   -0.102916     0.129687     -0.110125    -0.0161959    0.0120251   -0.066448     0.040161     0.218964     0.115745    -0.0976604   -0.0320002    -0.00121787   0.059871  
 -0.165492     -0.0642341    0.101361    -0.0118752   -0.0764504   -0.164766   -0.0051895    0.0802959   -0.114917    -0.070365    -0.107827    0.111712    -0.0281827   -0.0181164   -0.0736611     0.00708962   0.0367282    0.036624     0.0318848   -0.0758143   -0.00701529   0.0704712   -0.0523095    0.163254     -0.0216332   -0.00193024
  0.0344001     0.0901731   -0.0835708    0.175625    -0.0606596    0.16121    -0.00480281   0.0310954    0.0582142   -0.0108662   -0.0398348   0.065554    -0.205568    -0.0251698    0.00450887    0.201817    -0.00835629  -0.0329203    0.00397512  -0.0371309   -0.154996     0.096003     0.0732862    0.130496      0.182533    -0.0187374 
 -0.0344894    -0.00861205   0.00728305  -0.0327814    0.0903683   -0.0621244   0.0903294    0.0825589   -0.105561     0.148075    -0.121536   -0.0736341    0.185492     0.0328646   -0.0944825    -0.261986     0.212095     0.137373    -0.0287734    0.156247    -0.0647244   -0.020862     0.0710041   -0.117247      0.113615    -0.00212396
 -0.138405     -0.0934798    0.068055    -0.0243455   -0.00555769  -0.0548134   0.0552159    0.0281227    0.0138641   -0.00698852   0.0540046   5.77081e-5  -0.0748409   -0.15502     -0.026806     -0.0976006    0.155316     0.00243691  -0.119379    -0.0915656   -0.149917     0.00671062  -0.0571219   -0.00108603   -0.0281119    0.0700383 
  0.0605087    -0.11158     -0.0202869    0.151712     0.111705     0.0616088  -0.00756809   0.0225646   -0.123675     0.0340311   -0.0847061  -0.0966977    0.0841382   -0.13913      0.0120211     0.0169127   -0.00296226   0.0187452    0.0575655   -0.148475    -0.105859     0.133778     0.112295    -0.0100453     0.1045       0.069326  
 -0.111805      0.107847     0.0762768   -0.079912    -0.00340585  -0.0420945   0.0912466    0.091953     0.0729251   -0.0572775    0.212349    0.00730391   0.0541672    0.0724166    0.0779104    -0.119962    -0.00787322   0.050612    -0.128738     0.00812687   0.0402793    0.209879    -0.0022975   -0.0437359     0.109033     0.137357  
  0.0708386     0.107701     0.0443052    0.0314174    0.148286     0.138324   -0.270676     0.0563118   -0.0914806    0.0106482    0.0670625  -0.174857     0.125054     0.169242     0.0421452     0.0824019    0.00553943  -0.067427    -0.123616     0.204994     0.0351428    0.13781      0.0462449    0.102778      0.0179675   -0.0814334 
 -0.0496914     0.283608    -0.166774     0.0266494   -0.00702397   0.0334127   0.165155    -0.102349     0.10501      0.0675889   -0.126138    0.088078     0.0351382    0.0545243    0.126809     -0.0712129   -0.0710441   -0.0626943    0.0793347   -0.111929     0.215775    -0.0114037    0.087489    -0.200983     -0.0247088    0.086619  
  0.0083067     0.0667717   -0.171785    -0.031481    -0.0679355    0.0921595  -0.117618    -0.0896469    0.0687102   -0.175044     0.176182    0.0478804   -0.174367    -0.0841637    0.102322     -0.0467508   -0.102731    -0.108153     0.202647    -0.017071    -0.0475024    0.0498982    0.00653445   0.0446074     0.0974242   -0.106519  
  0.143236      0.0291518    0.013881     0.112478     0.0648102    0.141971   -0.0756697    0.0534367   -0.0172788    0.15396      0.0732394   0.153127     0.0995139    0.251166    -0.000907242   0.204445     0.0529872   -0.176633     0.00892869  -0.0818983    0.107859    -0.0191367   -0.12087      0.0815435     0.193257    -0.0523417 
 -0.0225792    -0.0973271    0.123258     0.147073     0.0748169    0.0287032   0.0755977    0.0239873   -0.126134     0.0214904    0.120233    0.319335     0.036639    -0.0866555    0.085913      0.0264917    0.066154     0.0365326   -0.0631619   -0.0387859    0.231474     0.0382802   -0.150663    -0.0955252     0.0109878    0.216499  
 -0.0917942    -0.104249     0.121826    -0.0927819   -0.184062     0.201669   -0.186819    -0.0721417   -0.0194313   -0.171271     0.0370952   0.0337034    0.0488612   -0.125593     0.134291     -0.0387305    0.103336     0.0385736   -0.0669371   -0.0843216   -0.0672294    0.0166062    0.0263286    0.0145199    -0.0215954   -0.0306331 
 -0.134496     -0.131008    -0.0408414    0.00762107  -0.0747104   -0.0221405  -0.0685007    0.0780077    0.0406393   -0.12948      0.154224   -0.0215886   -0.142852    -0.00537037   0.0100837     0.0665728   -0.0525729    0.00467848  -0.0893838   -0.0231014   -0.0510095    0.039945     0.129157     0.00945257   -0.0938861   -0.0644763 
 -0.108331     -0.204556    -0.0664956   -0.0818738    0.10525      0.055255    0.0146082    0.0565942    0.0512619    0.0178624   -0.0536581   0.206399    -0.158473     0.0235161   -0.100472      0.096503    -0.0346007   -0.02918     -0.123924    -0.041803    -0.210541     0.0365607    0.16113      0.136478      0.00790788  -0.038839  
 -0.109143     -0.0147148   -0.0546394    0.0188485    0.0304274   -0.157212    0.151014     0.142914     0.128914     0.137626    -0.193175   -0.0135008    0.231041    -0.0944397   -0.0472331    -0.0763524    0.0410928    0.02968      0.00556833  -0.0401945    0.0568168    0.18419      0.12276      0.000188304   0.060986     0.0970489 
  0.0786914     0.207227     0.0275611   -0.16646     -0.137007    -0.158241   -0.0882703    0.0554182    0.194479    -0.160383     0.0474304  -0.058654    -0.0401837   -0.0475859    0.0944227     0.0737378    0.160025    -0.141527    -0.051737    -0.0628296    0.018379    -0.0734893    0.0877594    0.00044033    0.0723969    0.0595002 
 -0.0451424    -0.0402284   -0.128173     0.0563751    0.454109    -0.0786614   0.0678618    0.0933375    0.0517998    0.013497    -0.224401   -0.10247     -0.175901     0.209849     0.0445131    -0.157738    -0.110616     0.116648     0.0539113    0.0444928   -0.0689712    0.177689     0.117778     0.190305      0.156465     0.060583  
 -0.0914708     0.324935    -0.00227861  -0.0695674    0.0113561   -0.101132   -0.0659222   -0.0866871   -0.136746    -0.0337528   -0.0235513  -0.0384439   -0.0840557   -0.0773308   -0.0609957    -0.166707     0.105842    -0.148522    -0.105664    -0.0449656   -0.0354196    0.123218     0.0286782    0.162519     -0.0619464   -0.059227  
  0.0324662     0.0305742    0.118363     0.0759747    0.0419029   -0.276165   -0.180513     0.0377829   -0.117968    -0.00616381   0.143337    0.0311401   -0.0560234    0.0447968   -0.0859677    -0.033743     0.043635    -0.184553    -0.0753635   -0.120228     0.0528028    0.015842     0.0812925   -0.0366655     0.0252431   -0.0899585 
  0.0696223     0.00188722  -0.135009     0.0510604    0.050596    -0.148543    0.0122045   -0.107521    -0.0746238    0.0331602   -0.0828264  -0.0216556   -0.178108    -0.0191941    0.0976998     0.00835985  -0.0305324   -0.00685447  -0.00544866   0.0526419    0.110768     0.109685    -0.170066     0.0856576     0.159518    -0.135656  kind full, method split
0: avll = -1.4281210117694139
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.428140
INFO: iteration 2, average log likelihood -1.428079
INFO: iteration 3, average log likelihood -1.428034
INFO: iteration 4, average log likelihood -1.427985
INFO: iteration 5, average log likelihood -1.427926
INFO: iteration 6, average log likelihood -1.427853
INFO: iteration 7, average log likelihood -1.427754
INFO: iteration 8, average log likelihood -1.427599
INFO: iteration 9, average log likelihood -1.427318
INFO: iteration 10, average log likelihood -1.426785
INFO: iteration 11, average log likelihood -1.425899
INFO: iteration 12, average log likelihood -1.424786
INFO: iteration 13, average log likelihood -1.423834
INFO: iteration 14, average log likelihood -1.423277
INFO: iteration 15, average log likelihood -1.423025
INFO: iteration 16, average log likelihood -1.422921
INFO: iteration 17, average log likelihood -1.422879
INFO: iteration 18, average log likelihood -1.422861
INFO: iteration 19, average log likelihood -1.422853
INFO: iteration 20, average log likelihood -1.422849
INFO: iteration 21, average log likelihood -1.422848
INFO: iteration 22, average log likelihood -1.422847
INFO: iteration 23, average log likelihood -1.422846
INFO: iteration 24, average log likelihood -1.422846
INFO: iteration 25, average log likelihood -1.422845
INFO: iteration 26, average log likelihood -1.422845
INFO: iteration 27, average log likelihood -1.422845
INFO: iteration 28, average log likelihood -1.422844
INFO: iteration 29, average log likelihood -1.422844
INFO: iteration 30, average log likelihood -1.422844
INFO: iteration 31, average log likelihood -1.422844
INFO: iteration 32, average log likelihood -1.422844
INFO: iteration 33, average log likelihood -1.422843
INFO: iteration 34, average log likelihood -1.422843
INFO: iteration 35, average log likelihood -1.422843
INFO: iteration 36, average log likelihood -1.422843
INFO: iteration 37, average log likelihood -1.422843
INFO: iteration 38, average log likelihood -1.422843
INFO: iteration 39, average log likelihood -1.422843
INFO: iteration 40, average log likelihood -1.422843
INFO: iteration 41, average log likelihood -1.422843
INFO: iteration 42, average log likelihood -1.422843
INFO: iteration 43, average log likelihood -1.422843
INFO: iteration 44, average log likelihood -1.422843
INFO: iteration 45, average log likelihood -1.422843
INFO: iteration 46, average log likelihood -1.422843
INFO: iteration 47, average log likelihood -1.422842
INFO: iteration 48, average log likelihood -1.422842
INFO: iteration 49, average log likelihood -1.422842
INFO: iteration 50, average log likelihood -1.422842
INFO: EM with 100000 data points 50 iterations avll -1.422842
952.4 data points per parameter
1: avll = [-1.42814,-1.42808,-1.42803,-1.42798,-1.42793,-1.42785,-1.42775,-1.4276,-1.42732,-1.42679,-1.4259,-1.42479,-1.42383,-1.42328,-1.42303,-1.42292,-1.42288,-1.42286,-1.42285,-1.42285,-1.42285,-1.42285,-1.42285,-1.42285,-1.42285,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422858
INFO: iteration 2, average log likelihood -1.422787
INFO: iteration 3, average log likelihood -1.422730
INFO: iteration 4, average log likelihood -1.422667
INFO: iteration 5, average log likelihood -1.422593
INFO: iteration 6, average log likelihood -1.422509
INFO: iteration 7, average log likelihood -1.422419
INFO: iteration 8, average log likelihood -1.422327
INFO: iteration 9, average log likelihood -1.422238
INFO: iteration 10, average log likelihood -1.422153
INFO: iteration 11, average log likelihood -1.422075
INFO: iteration 12, average log likelihood -1.422005
INFO: iteration 13, average log likelihood -1.421944
INFO: iteration 14, average log likelihood -1.421891
INFO: iteration 15, average log likelihood -1.421847
INFO: iteration 16, average log likelihood -1.421810
INFO: iteration 17, average log likelihood -1.421778
INFO: iteration 18, average log likelihood -1.421750
INFO: iteration 19, average log likelihood -1.421725
INFO: iteration 20, average log likelihood -1.421702
INFO: iteration 21, average log likelihood -1.421683
INFO: iteration 22, average log likelihood -1.421665
INFO: iteration 23, average log likelihood -1.421650
INFO: iteration 24, average log likelihood -1.421637
INFO: iteration 25, average log likelihood -1.421626
INFO: iteration 26, average log likelihood -1.421617
INFO: iteration 27, average log likelihood -1.421610
INFO: iteration 28, average log likelihood -1.421603
INFO: iteration 29, average log likelihood -1.421598
INFO: iteration 30, average log likelihood -1.421594
INFO: iteration 31, average log likelihood -1.421590
INFO: iteration 32, average log likelihood -1.421587
INFO: iteration 33, average log likelihood -1.421585
INFO: iteration 34, average log likelihood -1.421583
INFO: iteration 35, average log likelihood -1.421581
INFO: iteration 36, average log likelihood -1.421580
INFO: iteration 37, average log likelihood -1.421578
INFO: iteration 38, average log likelihood -1.421577
INFO: iteration 39, average log likelihood -1.421576
INFO: iteration 40, average log likelihood -1.421575
INFO: iteration 41, average log likelihood -1.421575
INFO: iteration 42, average log likelihood -1.421574
INFO: iteration 43, average log likelihood -1.421573
INFO: iteration 44, average log likelihood -1.421573
INFO: iteration 45, average log likelihood -1.421572
INFO: iteration 46, average log likelihood -1.421571
INFO: iteration 47, average log likelihood -1.421571
INFO: iteration 48, average log likelihood -1.421570
INFO: iteration 49, average log likelihood -1.421570
INFO: iteration 50, average log likelihood -1.421570
INFO: EM with 100000 data points 50 iterations avll -1.421570
473.9 data points per parameter
2: avll = [-1.42286,-1.42279,-1.42273,-1.42267,-1.42259,-1.42251,-1.42242,-1.42233,-1.42224,-1.42215,-1.42208,-1.42201,-1.42194,-1.42189,-1.42185,-1.42181,-1.42178,-1.42175,-1.42172,-1.4217,-1.42168,-1.42167,-1.42165,-1.42164,-1.42163,-1.42162,-1.42161,-1.4216,-1.4216,-1.42159,-1.42159,-1.42159,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421585
INFO: iteration 2, average log likelihood -1.421524
INFO: iteration 3, average log likelihood -1.421477
INFO: iteration 4, average log likelihood -1.421425
INFO: iteration 5, average log likelihood -1.421366
INFO: iteration 6, average log likelihood -1.421299
INFO: iteration 7, average log likelihood -1.421230
INFO: iteration 8, average log likelihood -1.421160
INFO: iteration 9, average log likelihood -1.421094
INFO: iteration 10, average log likelihood -1.421034
INFO: iteration 11, average log likelihood -1.420980
INFO: iteration 12, average log likelihood -1.420931
INFO: iteration 13, average log likelihood -1.420886
INFO: iteration 14, average log likelihood -1.420844
INFO: iteration 15, average log likelihood -1.420803
INFO: iteration 16, average log likelihood -1.420764
INFO: iteration 17, average log likelihood -1.420725
INFO: iteration 18, average log likelihood -1.420687
INFO: iteration 19, average log likelihood -1.420649
INFO: iteration 20, average log likelihood -1.420613
INFO: iteration 21, average log likelihood -1.420578
INFO: iteration 22, average log likelihood -1.420545
INFO: iteration 23, average log likelihood -1.420515
INFO: iteration 24, average log likelihood -1.420487
INFO: iteration 25, average log likelihood -1.420462
INFO: iteration 26, average log likelihood -1.420440
INFO: iteration 27, average log likelihood -1.420420
INFO: iteration 28, average log likelihood -1.420402
INFO: iteration 29, average log likelihood -1.420386
INFO: iteration 30, average log likelihood -1.420371
INFO: iteration 31, average log likelihood -1.420357
INFO: iteration 32, average log likelihood -1.420344
INFO: iteration 33, average log likelihood -1.420332
INFO: iteration 34, average log likelihood -1.420320
INFO: iteration 35, average log likelihood -1.420309
INFO: iteration 36, average log likelihood -1.420298
INFO: iteration 37, average log likelihood -1.420288
INFO: iteration 38, average log likelihood -1.420278
INFO: iteration 39, average log likelihood -1.420269
INFO: iteration 40, average log likelihood -1.420259
INFO: iteration 41, average log likelihood -1.420250
INFO: iteration 42, average log likelihood -1.420241
INFO: iteration 43, average log likelihood -1.420232
INFO: iteration 44, average log likelihood -1.420223
INFO: iteration 45, average log likelihood -1.420215
INFO: iteration 46, average log likelihood -1.420207
INFO: iteration 47, average log likelihood -1.420198
INFO: iteration 48, average log likelihood -1.420190
INFO: iteration 49, average log likelihood -1.420182
INFO: iteration 50, average log likelihood -1.420174
INFO: EM with 100000 data points 50 iterations avll -1.420174
236.4 data points per parameter
3: avll = [-1.42158,-1.42152,-1.42148,-1.42143,-1.42137,-1.4213,-1.42123,-1.42116,-1.42109,-1.42103,-1.42098,-1.42093,-1.42089,-1.42084,-1.4208,-1.42076,-1.42073,-1.42069,-1.42065,-1.42061,-1.42058,-1.42054,-1.42051,-1.42049,-1.42046,-1.42044,-1.42042,-1.4204,-1.42039,-1.42037,-1.42036,-1.42034,-1.42033,-1.42032,-1.42031,-1.4203,-1.42029,-1.42028,-1.42027,-1.42026,-1.42025,-1.42024,-1.42023,-1.42022,-1.42021,-1.42021,-1.4202,-1.42019,-1.42018,-1.42017]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420175
INFO: iteration 2, average log likelihood -1.420112
INFO: iteration 3, average log likelihood -1.420052
INFO: iteration 4, average log likelihood -1.419983
INFO: iteration 5, average log likelihood -1.419899
INFO: iteration 6, average log likelihood -1.419799
INFO: iteration 7, average log likelihood -1.419684
INFO: iteration 8, average log likelihood -1.419559
INFO: iteration 9, average log likelihood -1.419433
INFO: iteration 10, average log likelihood -1.419310
INFO: iteration 11, average log likelihood -1.419197
INFO: iteration 12, average log likelihood -1.419095
INFO: iteration 13, average log likelihood -1.419003
INFO: iteration 14, average log likelihood -1.418921
INFO: iteration 15, average log likelihood -1.418850
INFO: iteration 16, average log likelihood -1.418787
INFO: iteration 17, average log likelihood -1.418732
INFO: iteration 18, average log likelihood -1.418685
INFO: iteration 19, average log likelihood -1.418645
INFO: iteration 20, average log likelihood -1.418610
INFO: iteration 21, average log likelihood -1.418580
INFO: iteration 22, average log likelihood -1.418553
INFO: iteration 23, average log likelihood -1.418529
INFO: iteration 24, average log likelihood -1.418507
INFO: iteration 25, average log likelihood -1.418488
INFO: iteration 26, average log likelihood -1.418469
INFO: iteration 27, average log likelihood -1.418452
INFO: iteration 28, average log likelihood -1.418437
INFO: iteration 29, average log likelihood -1.418422
INFO: iteration 30, average log likelihood -1.418407
INFO: iteration 31, average log likelihood -1.418394
INFO: iteration 32, average log likelihood -1.418381
INFO: iteration 33, average log likelihood -1.418369
INFO: iteration 34, average log likelihood -1.418357
INFO: iteration 35, average log likelihood -1.418346
INFO: iteration 36, average log likelihood -1.418335
INFO: iteration 37, average log likelihood -1.418325
INFO: iteration 38, average log likelihood -1.418315
INFO: iteration 39, average log likelihood -1.418305
INFO: iteration 40, average log likelihood -1.418295
INFO: iteration 41, average log likelihood -1.418286
INFO: iteration 42, average log likelihood -1.418277
INFO: iteration 43, average log likelihood -1.418268
INFO: iteration 44, average log likelihood -1.418260
INFO: iteration 45, average log likelihood -1.418251
INFO: iteration 46, average log likelihood -1.418243
INFO: iteration 47, average log likelihood -1.418236
INFO: iteration 48, average log likelihood -1.418228
INFO: iteration 49, average log likelihood -1.418220
INFO: iteration 50, average log likelihood -1.418213
INFO: EM with 100000 data points 50 iterations avll -1.418213
118.1 data points per parameter
4: avll = [-1.42018,-1.42011,-1.42005,-1.41998,-1.4199,-1.4198,-1.41968,-1.41956,-1.41943,-1.41931,-1.4192,-1.41909,-1.419,-1.41892,-1.41885,-1.41879,-1.41873,-1.41869,-1.41864,-1.41861,-1.41858,-1.41855,-1.41853,-1.41851,-1.41849,-1.41847,-1.41845,-1.41844,-1.41842,-1.41841,-1.41839,-1.41838,-1.41837,-1.41836,-1.41835,-1.41834,-1.41832,-1.41831,-1.4183,-1.4183,-1.41829,-1.41828,-1.41827,-1.41826,-1.41825,-1.41824,-1.41824,-1.41823,-1.41822,-1.41821]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418215
INFO: iteration 2, average log likelihood -1.418154
INFO: iteration 3, average log likelihood -1.418095
INFO: iteration 4, average log likelihood -1.418027
INFO: iteration 5, average log likelihood -1.417942
INFO: iteration 6, average log likelihood -1.417834
INFO: iteration 7, average log likelihood -1.417701
INFO: iteration 8, average log likelihood -1.417546
INFO: iteration 9, average log likelihood -1.417377
INFO: iteration 10, average log likelihood -1.417204
INFO: iteration 11, average log likelihood -1.417036
INFO: iteration 12, average log likelihood -1.416881
INFO: iteration 13, average log likelihood -1.416741
INFO: iteration 14, average log likelihood -1.416618
INFO: iteration 15, average log likelihood -1.416510
INFO: iteration 16, average log likelihood -1.416416
INFO: iteration 17, average log likelihood -1.416334
INFO: iteration 18, average log likelihood -1.416262
INFO: iteration 19, average log likelihood -1.416199
INFO: iteration 20, average log likelihood -1.416143
INFO: iteration 21, average log likelihood -1.416093
INFO: iteration 22, average log likelihood -1.416048
INFO: iteration 23, average log likelihood -1.416008
INFO: iteration 24, average log likelihood -1.415972
INFO: iteration 25, average log likelihood -1.415939
INFO: iteration 26, average log likelihood -1.415910
INFO: iteration 27, average log likelihood -1.415882
INFO: iteration 28, average log likelihood -1.415857
INFO: iteration 29, average log likelihood -1.415835
INFO: iteration 30, average log likelihood -1.415813
INFO: iteration 31, average log likelihood -1.415794
INFO: iteration 32, average log likelihood -1.415775
INFO: iteration 33, average log likelihood -1.415758
INFO: iteration 34, average log likelihood -1.415742
INFO: iteration 35, average log likelihood -1.415727
INFO: iteration 36, average log likelihood -1.415713
INFO: iteration 37, average log likelihood -1.415699
INFO: iteration 38, average log likelihood -1.415686
INFO: iteration 39, average log likelihood -1.415674
INFO: iteration 40, average log likelihood -1.415662
INFO: iteration 41, average log likelihood -1.415650
INFO: iteration 42, average log likelihood -1.415639
INFO: iteration 43, average log likelihood -1.415629
INFO: iteration 44, average log likelihood -1.415619
INFO: iteration 45, average log likelihood -1.415609
INFO: iteration 46, average log likelihood -1.415599
INFO: iteration 47, average log likelihood -1.415590
INFO: iteration 48, average log likelihood -1.415581
INFO: iteration 49, average log likelihood -1.415572
INFO: iteration 50, average log likelihood -1.415564
INFO: EM with 100000 data points 50 iterations avll -1.415564
59.0 data points per parameter
5: avll = [-1.41821,-1.41815,-1.4181,-1.41803,-1.41794,-1.41783,-1.4177,-1.41755,-1.41738,-1.4172,-1.41704,-1.41688,-1.41674,-1.41662,-1.41651,-1.41642,-1.41633,-1.41626,-1.4162,-1.41614,-1.41609,-1.41605,-1.41601,-1.41597,-1.41594,-1.41591,-1.41588,-1.41586,-1.41583,-1.41581,-1.41579,-1.41578,-1.41576,-1.41574,-1.41573,-1.41571,-1.4157,-1.41569,-1.41567,-1.41566,-1.41565,-1.41564,-1.41563,-1.41562,-1.41561,-1.4156,-1.41559,-1.41558,-1.41557,-1.41556]
[-1.42812,-1.42814,-1.42808,-1.42803,-1.42798,-1.42793,-1.42785,-1.42775,-1.4276,-1.42732,-1.42679,-1.4259,-1.42479,-1.42383,-1.42328,-1.42303,-1.42292,-1.42288,-1.42286,-1.42285,-1.42285,-1.42285,-1.42285,-1.42285,-1.42285,-1.42285,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42284,-1.42286,-1.42279,-1.42273,-1.42267,-1.42259,-1.42251,-1.42242,-1.42233,-1.42224,-1.42215,-1.42208,-1.42201,-1.42194,-1.42189,-1.42185,-1.42181,-1.42178,-1.42175,-1.42172,-1.4217,-1.42168,-1.42167,-1.42165,-1.42164,-1.42163,-1.42162,-1.42161,-1.4216,-1.4216,-1.42159,-1.42159,-1.42159,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42158,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42157,-1.42158,-1.42152,-1.42148,-1.42143,-1.42137,-1.4213,-1.42123,-1.42116,-1.42109,-1.42103,-1.42098,-1.42093,-1.42089,-1.42084,-1.4208,-1.42076,-1.42073,-1.42069,-1.42065,-1.42061,-1.42058,-1.42054,-1.42051,-1.42049,-1.42046,-1.42044,-1.42042,-1.4204,-1.42039,-1.42037,-1.42036,-1.42034,-1.42033,-1.42032,-1.42031,-1.4203,-1.42029,-1.42028,-1.42027,-1.42026,-1.42025,-1.42024,-1.42023,-1.42022,-1.42021,-1.42021,-1.4202,-1.42019,-1.42018,-1.42017,-1.42018,-1.42011,-1.42005,-1.41998,-1.4199,-1.4198,-1.41968,-1.41956,-1.41943,-1.41931,-1.4192,-1.41909,-1.419,-1.41892,-1.41885,-1.41879,-1.41873,-1.41869,-1.41864,-1.41861,-1.41858,-1.41855,-1.41853,-1.41851,-1.41849,-1.41847,-1.41845,-1.41844,-1.41842,-1.41841,-1.41839,-1.41838,-1.41837,-1.41836,-1.41835,-1.41834,-1.41832,-1.41831,-1.4183,-1.4183,-1.41829,-1.41828,-1.41827,-1.41826,-1.41825,-1.41824,-1.41824,-1.41823,-1.41822,-1.41821,-1.41821,-1.41815,-1.4181,-1.41803,-1.41794,-1.41783,-1.4177,-1.41755,-1.41738,-1.4172,-1.41704,-1.41688,-1.41674,-1.41662,-1.41651,-1.41642,-1.41633,-1.41626,-1.4162,-1.41614,-1.41609,-1.41605,-1.41601,-1.41597,-1.41594,-1.41591,-1.41588,-1.41586,-1.41583,-1.41581,-1.41579,-1.41578,-1.41576,-1.41574,-1.41573,-1.41571,-1.4157,-1.41569,-1.41567,-1.41566,-1.41565,-1.41564,-1.41563,-1.41562,-1.41561,-1.4156,-1.41559,-1.41558,-1.41557,-1.41556]
32×26 Array{Float64,2}:
 -0.377231   -0.348027     0.788526     0.177376    0.596835   -0.052658   -0.0912217  -0.462932   -0.655358    -0.119922   -0.136536    -0.686079   -1.21564     0.617056   -0.342883   -0.39971      0.00732535  -0.316726     -0.784972     0.143605    -0.434404    0.0544917   -0.186906    0.21096     0.745912      0.242325  
 -0.401013    0.403613     0.0584184    0.0899053   0.799097    0.150507    0.142255   -0.655991   -0.0694295   -0.0861091  -0.408644    -0.496001   -0.368486    0.231121    0.0509604   0.513557    -0.154027    -0.254773     -0.0919535   -0.242888    -0.587731   -0.709209    -0.0405214   0.173751   -0.0255324     0.086039  
 -0.123673   -0.610454    -0.130252    -0.563264    0.399364   -0.0286689   0.112167    0.539375    0.103481    -0.305426    0.643718     0.187908    0.365272   -0.0499622  -0.34583     0.13062      0.0995021   -0.325718     -0.834563    -0.124686    -0.800624    0.101324     0.238644    0.219107    0.184959      0.26852   
  0.149339    0.356952     0.305185    -0.0895046   0.594503    0.344688   -0.0888891   0.0317646  -0.432841     0.215689   -0.727585    -0.296616    0.785727   -0.565093   -0.0553958  -0.0181844   -0.362382    -0.840752     -0.199055    -0.18305     -0.867781   -0.12195     -0.0104809  -0.0596543   0.144629      0.275591  
  0.072089   -0.0989059   -0.239014    -0.0449522   0.0451258   0.23811     0.150733    0.0956649   0.121699    -0.452938    0.0872609   -0.0693967  -0.217431    0.215175   -0.432848   -0.207545    -0.103939    -0.187199     -0.264737    -0.0121445   -0.235162   -0.259919    -0.401327    0.120415    0.272996     -0.00973404
 -0.507103    0.531258    -0.199334    -0.0132596  -0.607728    0.103444    0.0891697  -0.035752    0.140139     0.0840882  -0.293611    -0.0938437   0.0565591  -0.477477   -0.112528    0.557211    -0.341063     0.52584       0.0681008   -0.324954     0.0245034  -0.213498     0.0376153   0.0594857  -0.356173     -0.271745  
  0.768767    0.274962     0.00367796  -0.57749    -0.428345    0.197199   -0.0871631   0.144613   -0.406457    -0.399773    0.109842    -0.146248   -0.956505    0.637658    0.168727   -0.34532      0.0243117    0.14022       0.502968    -0.305246     0.427107    0.089831     0.160606    0.354909   -0.0851297    -0.497811  
  0.693009    0.400035     0.273156    -0.67498    -0.119179    0.0661639   0.559827    0.281299    0.711548    -0.665984   -0.136625    -0.270643   -0.0706668   0.102051    0.152895   -0.248135     0.263123     0.354122     -0.356554     0.405307     0.491032   -0.323242     0.261887   -0.277613    0.288829     -0.783098  
 -0.0952949   0.209518    -0.320061     0.366635    0.0512618   0.098735    0.538757    0.148068   -0.00651385  -0.348022    0.504703     0.113285   -0.34778     0.577805   -0.0753384  -0.170661     0.0979254    0.520418     -0.198267    -0.210588    -0.333733    0.287793    -0.494987    0.28322    -0.356034      0.242496  
 -0.208042   -0.158943     0.252563     0.361975   -0.306845    0.100683    0.317837   -0.340629    0.156587    -0.330715    0.181138     0.544047   -0.356689    0.0415179   0.118337    0.36428      0.689945     0.176815     -0.0958395   -0.143352     0.25493     0.131948     0.624746    0.316214    0.0870931     0.185735  
  0.502521    0.151413    -0.0377057    0.254078   -0.116675   -0.373004    0.0180987  -0.383602    0.145231    -0.15304    -0.121569     0.441931   -0.512171   -0.143      -0.0715986  -0.19331      0.240288     0.0947123    -0.256063     0.961599     0.19209    -0.109932    -0.141452    0.102495   -0.0304135     1.02506   
 -0.199723   -0.0551992   -0.568598     0.731113   -0.276266    0.182168    0.266758   -0.162877    0.391357     0.563632    0.667341     0.817355    0.197456   -0.315123   -0.464967    0.328655     0.740087     0.464731     -0.803615     1.07805      0.273661    0.180875     0.583852    0.400125    0.173008      0.397415  
  0.121506    0.453669    -1.06183      0.097513    0.110471   -0.382576   -0.492092   -0.423813    0.0528776   -0.341494    0.0306972    0.0780766  -0.587827    0.167908    0.0594852  -0.268329    -0.251697     0.554638     -0.149545    -0.00324992   0.382148   -0.0656724   -0.264528   -0.693115   -0.285039      0.18068   
 -0.227737    0.331358     0.028045     0.67749     0.0690956   0.253848   -0.754728   -0.0708933  -0.333907     0.322915    0.303814    -0.152595    0.679939    0.10183     0.133269   -0.115756    -0.411937    -0.0621237     0.519665     0.594452     0.184196   -0.309639    -0.979108   -0.479494   -0.307607      0.56197   
 -0.0666555  -0.523224     0.7076       0.186252   -0.487199   -0.0305462   0.0338377   0.47134     0.139274     0.574545    0.0163937    0.279819    0.201128    0.0249704   0.272375   -0.150229     0.254161    -0.0227398     0.336297    -0.132208     0.129254    0.732681     0.35913    -0.101933    0.269563     -0.0675106 
  0.0262199   0.0374382    0.204743     0.177496    0.923071   -0.207222   -0.385387   -0.468785   -0.45853      0.0908642   0.0174814   -0.0275624   0.0715348   0.0930402   0.324012   -0.0122629    0.448269    -0.201142      0.0890351    0.28373      0.129229    0.265048     0.376639   -0.101374   -0.0886493     0.336707  
  0.20093    -0.134197     0.251644     0.203964   -0.308459    0.0658801   0.271499    0.764103    0.352064     0.601561   -0.0816225   -0.158948    0.417295    0.0503752  -0.309885   -0.316083    -0.111759     0.244124      0.232574    -0.0962759   -0.613395   -0.0247257   -0.23079     0.30725    -0.290955     -0.356359  
  0.190771   -0.00723893  -0.611785     0.069281   -0.699301   -0.0802819   0.0445963   0.688158    0.392767    -0.0483987   0.148623     0.595552    0.454149    0.0443745  -0.225383   -0.117729    -0.211876     0.236438      0.2821       0.23748      0.468856    0.233231    -0.131357   -0.155618    0.0115198    -0.21574   
 -0.357222    0.0315576   -0.0215156   -0.212069    0.11809     0.0762695  -0.108826    0.326096   -0.160768     0.0578533  -0.357955    -0.246361    0.0551652   0.206077    0.502235    0.0493999   -0.793031    -0.386683      0.149944    -1.05942     -0.125584    0.304695    -0.211738   -0.47        0.166129     -0.721556  
  0.0760592  -0.087044     0.160122    -0.713657    0.0366534  -0.191954    0.0039387   0.383142    0.312505     0.0310792  -0.537854     0.392704   -0.334362   -0.563383    0.64133     0.150209     0.350072     0.400084     -0.0677028   -0.662204    -0.14753     0.179571     0.355259    0.121604    0.109721     -0.38295   
  0.15992    -0.466113     0.174469    -0.579388   -0.195444   -0.321928   -0.689069   -0.0667051   0.0141321   -0.0924852  -0.209123    -0.358198    0.109513   -0.497474   -0.361407    0.0266012   -0.323242    -0.0132249    -0.435469     0.169545    -0.0858944  -0.66029      0.0029881  -0.0330444   0.152817     -0.258397  
  0.0668617  -0.210431     0.0346578   -0.689896   -0.0666669  -0.242675    0.109481    0.534286   -0.213103    -0.192408   -0.0494389   -0.105606    0.27284    -0.317806   -0.260487   -0.0738464   -0.26128      0.178226     -0.560244     0.234319     0.404878    0.977609     0.524326   -0.247267   -0.0691761     0.0224203 
  0.19381    -0.205245     0.249998    -0.0626731   0.434657   -0.0751529   0.102565    0.114669   -0.275539     0.503848   -0.0563948    0.0311992   0.273709    0.119753    0.179035   -0.31978      0.293882    -0.26501       0.157214     0.570366     0.0371181   0.338289     0.408879   -0.375051    0.157521      0.505077  
  0.608762   -0.0950526    0.0715144   -0.0621209   0.41079    -0.287274   -0.257714    0.43792     0.794695     0.152238   -0.309527     0.382071    0.0149918   0.224832    0.416326   -0.16873      0.284459    -0.273819      0.148144     0.230543    -0.300874   -0.189465     0.193985    0.0179914   0.190538     -0.019067  
  0.0593062  -0.377173    -0.00833299  -0.0106949  -0.0264704  -0.160346   -0.148962    0.0989165  -0.100212     0.118141    0.00511652  -0.0477846  -0.122737    0.0788849   0.273692   -0.121294    -0.164303    -0.000166856  -0.104088    -0.453563     0.0699279   0.395876    -0.0176206  -0.245142   -0.101374     -0.119817  
  0.0815572   0.0334941   -0.101385     0.144124    0.212288   -0.109052   -0.156006   -0.0890863  -0.0353197   -0.0974452  -0.0357922    0.0911029  -0.0798444   0.0760016  -0.0578097  -0.13998      0.109583    -0.0259158     0.00383874   0.295684     0.0427905  -0.00152805   0.020596   -0.0679304   0.0773495     0.208005  
 -0.0825989   0.381052    -0.368667    -0.318343   -0.139408    0.256964    0.176958    0.120026    0.248588    -0.283793   -0.185963    -0.0212779  -0.111599   -0.135341   -0.0468317   0.1228      -0.260949     0.214119     -0.0796226   -0.132405    -0.234417   -0.156642     0.151001    0.0893515  -0.000600401  -0.141526  
 -0.33851     0.0461099    0.367613     0.0854087  -0.213609    0.270844    0.155365    0.0141097  -0.123389     0.0901537   0.229393     0.0359825   0.232577   -0.284304   -0.157241    0.256243     0.0701811   -0.292889      0.0311063   -0.159484    -0.136932   -0.0229331   -0.0240876   0.371794    0.0600744    -0.102891  
 -0.787086   -0.360056    -0.374011     0.686296    0.0841709   0.268801   -0.201094   -0.578538    0.103022    -0.0912642   0.0598231    0.144987    0.114422   -0.249153   -0.224365   -0.00549771  -0.0824127   -0.120231      0.0455075   -0.328155    -0.336622   -0.413762    -0.796226   -0.0463318   0.181451      0.351218  
  0.0264917   0.288598     0.485199     0.415456   -0.199403   -0.31609    -0.374243   -0.37141     0.0404811    0.364519   -0.341452    -0.143338    0.214734   -0.0726037   0.168795    0.0296687    0.0617523   -0.315264      0.795667     0.171704     0.365933   -0.691028    -0.0369026  -0.254795    0.410549     -0.38207   
 -0.698604    0.132773     0.151091     0.482573   -0.18213     0.135899   -0.0841472  -0.724666   -0.381805     0.112144    0.150442    -0.225806    0.0890431  -0.189822   -0.0546551  -0.268962     0.146953     0.0992427     0.127276    -0.371572     0.530949    0.533488    -0.0968692   0.0618889  -0.114151      0.0492299 
 -0.338154    0.0475804   -0.114838     0.422659   -0.284468    0.120152    0.0413585  -0.23155    -0.890917     0.202334    0.326935    -0.125386   -0.0386696   0.124146   -0.28884     0.61833     -0.272495    -0.558285      0.321439    -0.158597     0.293814    0.375248    -0.213718    0.211026    0.0309723     0.00751361INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415555
INFO: iteration 2, average log likelihood -1.415547
INFO: iteration 3, average log likelihood -1.415539
INFO: iteration 4, average log likelihood -1.415532
INFO: iteration 5, average log likelihood -1.415524
INFO: iteration 6, average log likelihood -1.415517
INFO: iteration 7, average log likelihood -1.415510
INFO: iteration 8, average log likelihood -1.415503
INFO: iteration 9, average log likelihood -1.415496
INFO: iteration 10, average log likelihood -1.415489
INFO: EM with 100000 data points 10 iterations avll -1.415489
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.965520e+05
      1       7.114205e+05      -2.851315e+05 |       32
      2       7.005421e+05      -1.087837e+04 |       32
      3       6.957234e+05      -4.818710e+03 |       32
      4       6.930540e+05      -2.669409e+03 |       32
      5       6.913465e+05      -1.707478e+03 |       32
      6       6.901546e+05      -1.191959e+03 |       32
      7       6.892423e+05      -9.122244e+02 |       32
      8       6.885245e+05      -7.178598e+02 |       32
      9       6.879563e+05      -5.681755e+02 |       32
     10       6.875233e+05      -4.330118e+02 |       32
     11       6.871742e+05      -3.490525e+02 |       32
     12       6.868843e+05      -2.899406e+02 |       32
     13       6.866197e+05      -2.645741e+02 |       32
     14       6.863975e+05      -2.222300e+02 |       32
     15       6.862158e+05      -1.816633e+02 |       32
     16       6.860366e+05      -1.792529e+02 |       32
     17       6.858527e+05      -1.838770e+02 |       32
     18       6.856853e+05      -1.673983e+02 |       32
     19       6.855269e+05      -1.584062e+02 |       32
     20       6.853805e+05      -1.464263e+02 |       32
     21       6.852548e+05      -1.256335e+02 |       32
     22       6.851404e+05      -1.144804e+02 |       32
     23       6.850461e+05      -9.421864e+01 |       32
     24       6.849575e+05      -8.864086e+01 |       32
     25       6.848714e+05      -8.612431e+01 |       32
     26       6.847834e+05      -8.797817e+01 |       32
     27       6.846974e+05      -8.604488e+01 |       32
     28       6.846026e+05      -9.480077e+01 |       32
     29       6.845057e+05      -9.681298e+01 |       32
     30       6.844165e+05      -8.920484e+01 |       32
     31       6.843313e+05      -8.525651e+01 |       32
     32       6.842474e+05      -8.384156e+01 |       32
     33       6.841643e+05      -8.312691e+01 |       32
     34       6.840854e+05      -7.889769e+01 |       32
     35       6.840177e+05      -6.774985e+01 |       32
     36       6.839543e+05      -6.335697e+01 |       32
     37       6.838995e+05      -5.476065e+01 |       32
     38       6.838514e+05      -4.818814e+01 |       32
     39       6.838021e+05      -4.923658e+01 |       32
     40       6.837468e+05      -5.528032e+01 |       32
     41       6.836886e+05      -5.823077e+01 |       32
     42       6.836282e+05      -6.038365e+01 |       32
     43       6.835694e+05      -5.885792e+01 |       32
     44       6.835237e+05      -4.567523e+01 |       32
     45       6.834855e+05      -3.821597e+01 |       32
     46       6.834464e+05      -3.912279e+01 |       32
     47       6.834082e+05      -3.817499e+01 |       32
     48       6.833702e+05      -3.795833e+01 |       32
     49       6.833336e+05      -3.660286e+01 |       32
     50       6.833005e+05      -3.316940e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 683300.4502164847)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.427831
INFO: iteration 2, average log likelihood -1.422705
INFO: iteration 3, average log likelihood -1.421330
INFO: iteration 4, average log likelihood -1.420325
INFO: iteration 5, average log likelihood -1.419243
INFO: iteration 6, average log likelihood -1.418190
INFO: iteration 7, average log likelihood -1.417428
INFO: iteration 8, average log likelihood -1.416997
INFO: iteration 9, average log likelihood -1.416759
INFO: iteration 10, average log likelihood -1.416607
INFO: iteration 11, average log likelihood -1.416494
INFO: iteration 12, average log likelihood -1.416404
INFO: iteration 13, average log likelihood -1.416328
INFO: iteration 14, average log likelihood -1.416262
INFO: iteration 15, average log likelihood -1.416203
INFO: iteration 16, average log likelihood -1.416151
INFO: iteration 17, average log likelihood -1.416104
INFO: iteration 18, average log likelihood -1.416061
INFO: iteration 19, average log likelihood -1.416022
INFO: iteration 20, average log likelihood -1.415985
INFO: iteration 21, average log likelihood -1.415952
INFO: iteration 22, average log likelihood -1.415920
INFO: iteration 23, average log likelihood -1.415891
INFO: iteration 24, average log likelihood -1.415864
INFO: iteration 25, average log likelihood -1.415838
INFO: iteration 26, average log likelihood -1.415814
INFO: iteration 27, average log likelihood -1.415792
INFO: iteration 28, average log likelihood -1.415771
INFO: iteration 29, average log likelihood -1.415751
INFO: iteration 30, average log likelihood -1.415732
INFO: iteration 31, average log likelihood -1.415714
INFO: iteration 32, average log likelihood -1.415697
INFO: iteration 33, average log likelihood -1.415682
INFO: iteration 34, average log likelihood -1.415667
INFO: iteration 35, average log likelihood -1.415652
INFO: iteration 36, average log likelihood -1.415639
INFO: iteration 37, average log likelihood -1.415626
INFO: iteration 38, average log likelihood -1.415613
INFO: iteration 39, average log likelihood -1.415601
INFO: iteration 40, average log likelihood -1.415590
INFO: iteration 41, average log likelihood -1.415579
INFO: iteration 42, average log likelihood -1.415569
INFO: iteration 43, average log likelihood -1.415558
INFO: iteration 44, average log likelihood -1.415548
INFO: iteration 45, average log likelihood -1.415539
INFO: iteration 46, average log likelihood -1.415529
INFO: iteration 47, average log likelihood -1.415520
INFO: iteration 48, average log likelihood -1.415511
INFO: iteration 49, average log likelihood -1.415502
INFO: iteration 50, average log likelihood -1.415493
INFO: EM with 100000 data points 50 iterations avll -1.415493
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.651609    0.12841    -0.0642305   0.888995     0.173257    0.0497424  -0.23838     -0.430385    -0.416651     0.442719     0.334576     0.206856     0.453226    -0.25736      -0.082428    0.295464   -0.00129582  -0.253257    0.098102    0.0754832   -0.286038     0.0516638   -0.201508    -0.0200845  -0.0795683   0.777136 
 -0.0347151   0.173295   -0.333635    0.623797    -0.474948   -0.347046    0.193778    -0.0765992    0.276571    -0.0604847    0.540122     0.584612    -0.47677      0.170071     -0.308839   -0.247936    0.601886     0.683862   -0.132726    0.476763     0.423812     0.248388     0.00887334   0.341493   -0.18273     0.529955 
 -0.215107    0.215303   -0.0128668   0.404788    -0.0738682   0.119344    0.0546246   -0.465857    -0.134713    -0.125373     0.0464842    0.235788    -0.152965     0.00280557    0.0185887   0.039211    0.188212     0.0243813   0.202468   -0.0578248    0.173944     0.0605659    0.0478968    0.0787775   0.153958    0.210062 
  0.366419   -0.0102601   0.358125   -0.386624     0.314196   -0.200213    0.0494963    0.205757     0.534416    -0.174825    -0.196525     0.183796    -0.258939    -0.0871194     0.566278    0.125257    0.491862     0.0302109  -0.143997   -0.108857    -0.0638237   -0.126676     0.539759    -0.125494    0.387994   -0.33105  
  0.118202    0.329417   -0.156063   -0.436943    -0.191135    0.27308     0.44318      0.777018     0.613072    -0.339119    -0.117198    -0.0438211    0.211971    -0.0806108     0.0788902  -0.140211   -0.11683      0.218249    0.080749   -0.17506     -0.162171     0.038734     0.138896     0.0635532   0.105927   -0.484751 
 -0.192341   -0.301228    0.0624003  -0.0223916   -0.316949    0.400493    0.607384    -0.0193047    0.415163    -0.242959     0.373558     0.35815      0.0846091   -0.430402     -0.203143    0.343698    0.849822     0.0713415  -0.636416    0.0563475    0.144868    -0.0694069    0.695995     0.251461    0.502047    0.031696 
  0.498447    0.563657   -0.443285    0.18013     -0.379618    0.129569    0.310743     0.0039252    0.00238275   0.259488    -0.557797     0.141199     0.0364772    0.115682     -0.43311     0.0753211  -0.957629    -0.0560908  -0.319832    0.438327    -0.47548     -0.560139    -0.378088     0.130137   -0.035562    0.331904 
  0.536372   -0.25823    -0.168399   -0.0915122    0.158195   -0.551024   -0.17681      0.294499     0.551448    -0.200453    -0.0997401    0.295079    -0.221241     0.128935     -0.0656968  -0.0520683   0.197165     0.0851728  -0.293246    0.587527    -0.0406827   -0.252255     0.112018    -0.16172     0.250019    0.084381 
 -0.202119    0.274369    0.659279    0.730119    -0.692567    0.0383931  -0.290108    -0.598476    -0.024983     0.419032    -0.38435     -0.459658     0.0896977   -0.221776      0.305929   -0.0265433  -0.0145746    0.107753    0.735228    0.0971753    0.640429    -0.184593    -0.170584    -0.144873    0.0790645  -0.176577 
 -0.16754     0.0862376  -0.14886    -0.232587    -0.0775534   0.155536    0.350795    -0.149306    -0.0843661   -0.43158      0.121113     0.0930213   -0.740709    -0.0134754     0.335138    0.0186474  -0.168433     0.504218   -0.58019    -0.366308    -0.345491     0.418371     0.10047      0.14931    -0.336609    0.363771 
  0.142391   -0.0505225  -0.486126    0.0887735   -0.469308    0.0651361  -0.356478     0.514534     0.294479     0.274744     0.0923513    0.820669     0.696239    -0.0207432     0.282693   -0.299563   -0.157317     0.0333593   0.863751    0.421166     0.447179     0.401669    -0.047083    -0.542126   -0.113001    0.257076 
 -0.0873887  -0.400488    0.102612   -0.620512     0.256747    0.0134621  -0.642581    -0.24662     -0.230103     0.121728    -0.426088    -0.13188      0.271977    -0.793902      0.0650841  -0.0141114  -0.51617     -0.519454   -0.142542    0.019904    -0.220667    -0.401662     0.146006    -0.665377    0.487115    0.200923 
  0.203269   -0.3494      0.645762    0.287108    -0.172071   -0.0601121   0.23634      0.810684     0.199777     0.750886    -0.0201347   -0.180607     0.584164    -0.166251     -0.167408   -0.414427    0.124645     0.308664    0.0901811   0.13495     -0.647122     0.074134    -0.183704     0.039158   -0.346291   -0.170395 
 -0.285921   -0.429799   -0.469468    0.00456119  -0.14146     0.181011   -0.5152       0.0588893    0.357732    -0.571468     0.0493737   -0.116615     0.477736    -0.148028     -0.858673   -0.188292   -0.631914     0.157316   -0.333285   -0.475384    -0.152072    -0.0557522   -0.598683    -0.102921   -0.1244     -0.134281 
 -0.3348     -0.169334    0.529528   -0.0354071    0.58779     0.0561636   0.0616106   -0.219884    -0.132261    -0.188732    -0.0504483   -0.737902    -0.585663     0.354355     -0.537459   -0.146122    0.215727    -0.220134   -0.782951    0.0430503   -0.719695    -0.670924    -0.275069     0.573839    0.245104   -0.0831571
 -0.125425    0.657057   -0.846575    0.12423      0.32951    -0.436183   -0.422982    -0.470161    -0.0824894   -0.238538     0.138222    -0.188059    -0.284529     0.0917924     0.231395    0.067644   -0.380983     0.610338   -0.190985    0.040031     0.374952    -0.120787    -0.367676    -0.624139   -0.754296    0.023382 
 -0.23587     0.0533946  -0.189407   -0.253963    -0.551271   -0.239546   -0.177193     0.591001     0.489912     0.376705    -0.651855     0.440634     0.127524    -0.23227       0.283559    0.455173   -0.244047     0.4033      0.32369    -0.693595    -0.199067     0.167534     0.286692     0.275827   -0.192745   -0.674378 
  0.0517303  -0.369599   -0.232689   -0.591905     0.539548   -0.150336    0.0920155    0.787528    -0.0295729   -0.13843      0.596452     0.23307      0.528265     0.144047     -0.251252    0.165708   -0.105296    -0.274768   -0.878414   -0.00107894  -0.615671     0.277895     0.174697     0.232669   -0.0972509   0.201672 
 -0.399438    0.0260066  -0.349183    0.709366     0.284807    0.380447   -0.0360434   -0.465186     0.3433      -0.194666     0.00265362   0.00517662  -0.304251     0.222397      0.0679465  -0.0717974   0.173261    -0.219205    0.349897   -0.277456    -0.528579    -0.613407    -0.636075     0.199643    0.133131    0.146012 
 -0.020169   -0.185776   -0.0329481   0.0367496    0.102299    0.0285015  -0.0287679    0.18093     -0.0521277    0.154512     0.045161     0.0174313    0.094783     0.0474925     0.04111    -0.122955   -0.0664923   -0.0533713  -0.03752    -0.0815891   -0.188402     0.197927     0.0160189   -0.0739851  -0.0766473   0.109654 
 -0.450502   -0.0279552  -0.0506687   0.257217    -0.46219     0.203733    0.138718    -0.157279    -0.602252     0.215056     0.375748    -0.265702     0.102692     0.0354861    -0.413592    0.296184   -0.315095    -0.225999    0.164229   -0.204705     0.456284     0.327822    -0.295303     0.254701    0.0201267  -0.31308  
  0.422361    0.420189    0.182222   -0.687437     0.880571    0.20134    -0.110005    -0.0299539   -0.208419    -0.00522268  -0.548144    -0.143319     0.10356     -0.132906      0.358049   -0.252371    0.627792     0.0886817   0.197444    0.386619    -0.394138     0.218172     0.332718     0.447073   -0.265369    0.178351 
 -0.238436   -0.116533    0.667499    0.751334     0.255547    0.15763     0.143646    -0.00632344  -0.331598    -0.220273     0.298586     0.323847     0.0362889    0.571358      0.185776    0.673729    0.723962    -0.187409    0.565731   -0.072042     0.666006     0.891087     0.369804     0.325481   -0.526571   -0.17086  
  0.807277    0.205152   -0.150325   -0.574572    -0.552133    0.120962    0.146131     0.170371     0.128237    -0.580407     0.00309008  -0.129458    -0.597383     0.465448      0.0426655  -0.406974    0.0997348    0.42093     0.126288    0.0129263    0.632042    -0.167823     0.101003    -0.110747    0.113814   -0.649383 
  0.0275361  -0.178917    0.178205    0.311388     0.689887   -0.295174   -0.389464    -0.484562    -0.405053     0.171374    -0.0415973   -0.147059    -0.391258     0.315003      0.170818   -0.490053    0.380109    -0.368326   -0.168083    0.230185     0.288526     0.296607     0.14211     -0.306778    0.171116    0.582773 
  0.207864    0.363531    0.324873    0.206115     0.347637   -0.323622   -0.25314      0.101155     0.0699782    0.493087    -0.169854    -0.131295     0.587004     0.189005      0.264786   -0.0813986   0.10733     -0.530511    0.649213    0.238022     0.174511    -0.559618    -0.0518571   -0.297416    0.294304   -0.282799 
 -0.34422     0.438677    0.273969   -0.160049     0.720009    0.404026    0.246228    -0.582263    -0.333614     0.0652946   -0.687613    -0.574942     0.149787     0.000409402   0.35481     0.768148   -0.721402    -0.406007   -0.0370954  -0.713544    -0.728488    -0.256026     0.0753042   -0.186519    0.034884   -0.221179 
 -0.235565   -0.108123    0.0688909  -0.0380157   -0.0783919  -0.0919621  -0.081492     0.466748    -0.281661    -0.00286449   0.0620015   -0.0765072   -0.38316      0.364699      0.526022   -0.266043   -0.385464    -0.195402    0.236989   -1.18525      0.00603512   0.513714    -0.299828    -0.336245    0.182975   -0.510524 
 -0.152358   -0.0192206  -0.2361      0.00876875  -0.117572    0.185498    0.101455    -0.0274161   -0.216502    -0.171835     0.107363    -0.0564181   -0.00266281  -0.0403883    -0.447439    0.0310597  -0.291046    -0.0923643  -0.11301    -0.00447938  -0.0176968    0.00613286  -0.281363     0.0274156   0.0995625   0.0790625
  0.263839   -0.257748    0.180381   -0.550645    -0.0912795  -0.30239     0.00204804   0.373059    -0.203716     0.0614245   -0.198393     0.023509     0.248458    -0.293289      0.0505185  -0.231951   -0.0704721    0.0345273  -0.329494    0.307041     0.566495     0.887338     0.558071    -0.502557    0.0291083   0.0561453
 -0.0105926  -0.711739    0.719374   -0.163087    -0.0621358   0.056717    0.242786     0.395368    -0.244275     0.22477     -0.260463     0.199401     0.132819     0.149788     -0.229553   -0.325227    0.308302    -0.790931    0.256355    0.0389637   -0.45064      0.758591     0.362337     0.473268    0.949656    0.381473 
 -0.0150283   0.0410399   0.256595   -0.297822    -0.192663   -0.10059    -0.250139    -0.194091    -0.129094    -0.113932    -0.122525    -0.222948    -0.159788    -0.323139     -0.100867    0.263405   -0.0286462   -0.0593341  -0.0420165  -0.059505     0.0463641   -0.342217    -0.00432893   0.23371    -0.0674336  -0.364535 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415484
INFO: iteration 2, average log likelihood -1.415475
INFO: iteration 3, average log likelihood -1.415466
INFO: iteration 4, average log likelihood -1.415457
INFO: iteration 5, average log likelihood -1.415448
INFO: iteration 6, average log likelihood -1.415439
INFO: iteration 7, average log likelihood -1.415430
INFO: iteration 8, average log likelihood -1.415421
INFO: iteration 9, average log likelihood -1.415412
INFO: iteration 10, average log likelihood -1.415403
INFO: EM with 100000 data points 10 iterations avll -1.415403
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
