>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.1
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1185
Commit ca2046e (2016-11-04 08:41 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (657.31640625 MB free)
Uptime: 23531.0 sec
Load Avg:  1.01806640625  1.0302734375  1.04541015625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1496244 s         87 s     127910 s     481314 s         58 s
#2  3499 MHz     626751 s       5444 s      73478 s    1577437 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.1
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.2580205205426177e6,[99702.2,297.845],
[359.755 119.223 115.331; -855.366 131.884 -220.227],

Array{Float64,2}[
[96893.6 417.302 -584.046; 417.302 1.00133e5 253.842; -584.046 253.842 98777.4],

[2510.5 -315.085 559.21; -315.085 291.887 -195.942; 559.21 -195.942 399.093]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.263258e+03
      1       1.292143e+03      -9.711153e+02 |        8
      2       1.185260e+03      -1.068830e+02 |        4
      3       1.172863e+03      -1.239708e+01 |        2
      4       1.147968e+03      -2.489436e+01 |        2
      5       1.140242e+03      -7.726513e+00 |        0
      6       1.140242e+03       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 1140.241867319101)
INFO: K-means with 272 data points using 6 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.066129
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.754897
INFO: iteration 2, lowerbound -3.638828
INFO: iteration 3, lowerbound -3.502712
INFO: iteration 4, lowerbound -3.333014
INFO: iteration 5, lowerbound -3.147136
INFO: dropping number of Gaussions to 7
INFO: iteration 6, lowerbound -2.965321
INFO: iteration 7, lowerbound -2.816224
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.702984
INFO: iteration 9, lowerbound -2.603626
INFO: iteration 10, lowerbound -2.521786
INFO: iteration 11, lowerbound -2.454261
INFO: dropping number of Gaussions to 4
INFO: iteration 12, lowerbound -2.397179
INFO: dropping number of Gaussions to 3
INFO: iteration 13, lowerbound -2.350062
INFO: iteration 14, lowerbound -2.319782
INFO: iteration 15, lowerbound -2.308001
INFO: dropping number of Gaussions to 2
INFO: iteration 16, lowerbound -2.303030
INFO: iteration 17, lowerbound -2.299263
INFO: iteration 18, lowerbound -2.299257
INFO: iteration 19, lowerbound -2.299255
INFO: iteration 20, lowerbound -2.299254
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sun 06 Nov 2016 11:01:56 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 06 Nov 2016 11:01:57 AM UTC: K-means with 272 data points using 6 iterations
11.3 data points per parameter
,Sun 06 Nov 2016 11:01:59 AM UTC: EM with 272 data points 0 iterations avll -2.066129
5.8 data points per parameter
,Sun 06 Nov 2016 11:01:59 AM UTC: GMM converted to Variational GMM
,Sun 06 Nov 2016 11:02:01 AM UTC: iteration 1, lowerbound -3.754897
,Sun 06 Nov 2016 11:02:01 AM UTC: iteration 2, lowerbound -3.638828
,Sun 06 Nov 2016 11:02:01 AM UTC: iteration 3, lowerbound -3.502712
,Sun 06 Nov 2016 11:02:01 AM UTC: iteration 4, lowerbound -3.333014
,Sun 06 Nov 2016 11:02:01 AM UTC: iteration 5, lowerbound -3.147136
,Sun 06 Nov 2016 11:02:02 AM UTC: dropping number of Gaussions to 7
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 6, lowerbound -2.965321
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 7, lowerbound -2.816224
,Sun 06 Nov 2016 11:02:02 AM UTC: dropping number of Gaussions to 5
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 8, lowerbound -2.702984
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 9, lowerbound -2.603626
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 10, lowerbound -2.521786
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 11, lowerbound -2.454261
,Sun 06 Nov 2016 11:02:02 AM UTC: dropping number of Gaussions to 4
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 12, lowerbound -2.397179
,Sun 06 Nov 2016 11:02:02 AM UTC: dropping number of Gaussions to 3
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 13, lowerbound -2.350062
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 14, lowerbound -2.319782
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 15, lowerbound -2.308001
,Sun 06 Nov 2016 11:02:02 AM UTC: dropping number of Gaussions to 2
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 16, lowerbound -2.303030
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 17, lowerbound -2.299263
,Sun 06 Nov 2016 11:02:02 AM UTC: iteration 18, lowerbound -2.299257
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 19, lowerbound -2.299255
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 20, lowerbound -2.299254
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 21, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 22, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 23, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 24, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 25, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 26, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 27, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 28, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 29, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 30, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 31, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 32, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 33, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 34, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:03 AM UTC: iteration 35, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 36, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 37, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 38, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 39, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 40, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 41, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 42, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 43, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 44, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 45, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 46, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 47, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 48, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 49, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: iteration 50, lowerbound -2.299253
,Sun 06 Nov 2016 11:02:04 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -1.0215236615141334
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -1.0215236615141334
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -1.0215236615141334
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.00000000003
avll from stats: -0.9985733125933808
avll from llpg:  -0.9985733125933808
avll direct:     -0.9985733125933808
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.00298884   -0.0428652   -0.0525526   -0.10012      -0.090207    -0.0726893    -0.0648781    0.0415693    -0.0290662    0.0409079    0.00207287   0.014875    -0.100415      0.0217796    -0.0501387   -0.187205    -0.0213161    0.0750631   -0.0879132   -0.210124      0.117639    -0.0653002    0.0688576    -0.201376    -0.01214    -0.0280251 
 -0.0826178    -0.0183176    0.0501604   -0.0797153     0.0482926    0.252425      0.0554879    0.104558      0.0581813   -0.0572891   -0.219838     0.0273422   -0.0620913     0.0760445     0.102667     0.173696    -0.0501055   -0.0347363   -0.115939    -0.0396085    -0.077819    -0.114882     0.0517659    -0.0197424   -0.0522638   0.00574051
 -0.270002      0.0512986    0.0133039    0.0234266     0.0533246    0.125744     -0.103216    -0.00290152    0.141506    -0.03048     -0.107197     0.0557292    0.0711751     0.0457493     0.0316602    0.00470365   0.0368237    0.0724279   -0.125226    -0.00592901   -0.142121     0.104498    -0.000326589   0.0258694   -0.0610701   0.120241  
  0.100824      0.0848286   -0.0589204    0.0265328    -0.293774     0.061114      0.220965    -0.0341434     0.071996    -0.0242998   -0.125689     0.0506498   -0.133847      0.00976612   -0.0968237    0.00832864   0.009189    -0.0273269    0.0159271   -0.0838851    -0.0614863    0.124444     0.0111108    -0.0773232   -0.148778    0.0120778 
  0.0132119     0.03975      0.208794     0.0597674     0.0355918    0.0390889    -0.063577    -0.0424028    -0.0628887   -0.0261882   -0.0640349   -0.0973689    0.136864      0.169485     -0.137657    -0.0869472   -0.141712     0.00585261  -0.127234     0.0441168    -0.256316     0.0988163   -0.0077529     0.0415263   -0.170099    0.0475705 
 -0.0808104     0.0233254    0.0501304    0.0160559     0.110583    -0.151787     -0.0272222    0.154631      0.0145297    0.140824    -0.0655126    0.196563    -0.128627     -0.182787      0.113172    -0.0876124    0.0189069   -0.0309342   -0.134717    -0.0941686    -0.0527685   -0.0604995    0.0588728    -0.106069     0.0691725   0.0281193 
 -0.0738539     0.193165     0.127458     0.0563109     0.136623    -0.181178     -0.015263     0.202317      0.139516     0.0061038    0.0766217   -0.0453145    0.0750501    -0.161578     -0.0225051    0.144641    -0.00843183   0.00381649   0.112456     0.0649468    -0.0915313   -0.00942594   0.00497117   -0.0302781    0.0371329  -0.00351772
  0.205886     -0.0729255    0.0218308   -0.173586     -0.04894      0.0658326     0.0402825   -0.149809     -0.0230148   -0.0561027    0.0580312    0.148686    -0.124802      0.133382      0.138648     0.00726757  -0.0199227    0.020303    -0.0645457    0.0246552    -0.0476126    0.129637    -0.0332603    -0.168432    -0.0293251  -0.0428243 
 -0.107505      0.0437842   -0.0725003    0.0393814    -0.0866786   -0.061936     -0.0314239   -0.0783855    -0.16637      0.0390101    0.0729657    0.0538316   -0.0106659    -0.1249       -0.109426     0.0805143   -0.0872044    0.0626016    0.07803      0.0465349    -0.0885652    0.0758827   -0.15218       0.0314248    0.103957    0.013788  
 -0.0790191     0.0057567   -0.00524432   0.215542      0.00517282   0.0236987    -0.0577233    0.000818155   0.0931913    0.205622    -0.0351722   -0.0288559   -0.103355      0.0455454    -0.0144582   -0.0205392    0.0758858   -0.0843962    0.0113938    0.111379      0.0955337    0.0217466    0.0327825    -0.117311     0.0749929  -0.0794745 
  0.114192      0.0126701    0.150461     0.0213158    -0.0344787    0.0918015    -0.0170834    0.0874866    -0.0274286   -0.157566     0.144669    -0.174429     0.215576     -0.118828      0.159522     0.104114    -0.101972     0.0826688   -0.0174694   -0.120279      0.0471071   -0.0506874    0.0796242    -0.0214959   -0.0950397  -0.0646313 
  0.0640152    -0.160646     0.0105883   -0.0538791     0.0514874    0.0311411    -0.0267625   -0.234866     -0.0385849    0.0754169    0.0525622    0.0259625    0.106411     -0.101824      0.185692     0.107301     0.0290218   -0.226909     0.0303897   -0.199302     -0.0485758    0.145555    -0.0610461     0.0199865   -0.0969723   0.071975  
  0.0157145     0.176885    -0.199697     0.0845379    -0.104932    -0.0401789     0.134665    -0.00311356   -0.144811    -0.1609       0.0842221    0.0351692    0.112013     -0.187386     -0.0275313   -0.115973    -0.259668     0.0819993    0.143765    -0.104182      0.00432771   0.0523245   -0.140982      0.154187    -0.133479   -0.0615778 
 -0.163372     -0.0848361   -0.190095     0.027476      0.132039     0.0180493    -0.017322     0.0184385     0.0951478   -0.0480063    0.188778    -0.00575066  -0.0769849     0.028081     -0.171106    -0.104026     0.107842     0.0458391   -0.0308297   -0.0423801    -0.00798948   0.175997    -0.0108689    -0.0827451    0.151527   -0.0268186 
  0.223002     -0.0285048   -0.0495023   -0.0632279    -0.0526594    0.0342562    -0.101486    -0.0647731    -0.396457    -0.147423    -0.211526     0.121001     0.255813      0.084612      0.0801245    0.0504288   -0.0168116    0.0205545    0.123805     0.0845708     0.209389     0.0440727   -0.0554552    -0.0546879   -0.0739161   0.0285566 
 -0.121087     -0.252406    -0.0878153   -0.0495756     0.0136634   -0.0369146     0.0416677    0.0155201     0.0580499    0.0902905   -0.224788     0.0341065   -0.0550905    -0.0715601     0.159266     0.0974306    0.0659174    0.042235     0.00530063   0.0820042    -0.131544    -0.0468725    0.0377896     0.130504    -0.0842834  -0.12832   
  0.000640709   0.116424    -0.0794403   -0.000710984   0.0792597    0.0520403     0.166594     0.172659     -0.0270029   -0.117447     0.0844907   -0.0621453    0.0802528     0.0872034     0.00133032   0.081072    -0.089523    -0.10345      0.00730616   0.0884911    -0.0871233    0.191036     0.00866443    0.0748709    0.0659857   0.0585874 
  0.00225138   -0.0745345   -0.0255385   -0.102961     -0.0704392   -0.142715      0.0474801    0.0240428    -0.0183588   -0.0860104   -0.136089     0.113357     0.0563083    -0.108193     -0.0767293   -0.114718     0.0387255   -0.0394901   -0.136601    -0.0770282    -0.0982498    0.0822067   -0.045813      0.017006    -0.0356439  -0.159196  
 -0.108761      0.0776506   -0.162143     0.0204068    -0.134502     0.0679887     0.0561995   -0.0795389    -0.0105244    0.146358     0.0442829   -0.154653     0.084405     -0.0334445     0.202953    -0.0408666   -0.0192532   -0.0113627    0.0501229    0.0969718    -0.0424704    0.00837085   0.0286839    -0.0032701    0.0449518  -0.184043  
 -0.216493     -0.0570072    0.00263154  -0.141657      0.0401049   -0.000498999  -0.0770859   -0.0558032    -0.0541328    0.0620484   -0.0385831   -0.0100039   -0.0866665     0.00974377   -0.0555109   -0.177754    -0.153656     0.0193146    0.129392    -0.0133731    -0.0184471    0.0740216    0.158084      0.0712922   -0.0268465   0.0370499 
  0.145509     -0.0704476   -0.142008     0.018784      0.108072     0.120599     -0.151591     0.0353305    -0.0660087    0.0616719    0.0513893   -0.0183542    0.0677383     0.0247934    -0.125278    -0.128073    -0.127395    -0.185817     0.0264454    0.000155182  -0.00302389   0.0249137   -0.065206      0.132372    -0.0413312  -0.0196422 
 -0.0519324     0.0943448   -0.0666207    0.0846907    -0.103597    -0.12445      -0.0299676    0.053224     -0.00269266  -0.131013    -0.113291     0.0238076    0.0259205     0.0027033    -0.177597     0.0805249    0.0118944   -0.104131    -0.0272122   -0.123164      0.250178    -0.0346253    0.164236     -0.00334059   0.108518   -0.0200593 
  0.00475736   -0.00804168   0.0575325   -0.258808     -0.0297523   -0.164036     -0.222877    -0.00422871   -0.223397     0.190169     0.192134     0.0679417    0.0413694     0.115865      0.00260597  -0.00323455   0.0114668   -0.0618545    0.128135     0.151398     -0.0873306   -0.0035571   -0.0236393     0.0744966   -0.129029   -0.0678213 
 -0.106236      0.0528751   -0.158832    -0.0282219    -0.126677    -0.0597464    -0.0615077   -0.291903     -0.016758     0.116763    -0.0735869    0.00212563  -0.225004      0.0140953    -0.0699232   -0.118187    -0.0691107   -0.00914853   0.00188627  -0.0196445    -0.163107    -0.177371    -0.036822     -0.323551     0.0268338   0.0991217 
  0.119796      0.159374    -0.0268939    0.144387      0.0970544    0.138855      0.131543     0.0215832     0.138008    -0.0695147   -0.184399    -0.122055    -0.00377718    0.244728     -0.0121176   -0.130084    -0.0946294   -0.0581558   -0.0933482   -0.00161311    0.180836     0.0742834   -0.0279318     0.0551038    0.0499679  -0.0145323 
  0.128617      0.0440014    0.0443512   -0.115373     -0.107156     0.0503799     0.00785829   0.136004     -0.0488839   -0.0688408    0.106437    -0.0139568   -0.0397141    -0.0887803     0.0685949    0.211714     0.0718879    0.0305806    0.0242947    0.0297407     0.032109    -0.0600768    0.0463166    -0.0884175   -0.0737848   0.155627  
 -0.0642496     0.217777     0.0452786    0.105589     -0.0163991   -0.0335971     0.0365486   -0.0422572     0.18596     -0.12069      0.0616635   -0.0800378   -0.00435046    0.0497137     0.0694198    0.0959529    0.0613063    0.10581     -0.0916033    0.153425      0.0568607    0.157763     0.0123051     0.0723404    0.101824    0.0675761 
  0.130342     -0.122417    -0.0608723   -0.0362355    -0.243048     0.0526437     0.0660121   -0.0830289     0.00170034   0.0190529    0.118933     0.00875063   0.0143822    -0.0828349    -0.055495    -0.0834259    0.0701871    0.0715533    0.0837997   -0.119258      0.0767687   -0.146192    -0.017234      0.152747    -0.0407845  -0.0246148 
 -0.116046      0.00970031   0.00286368   0.0397248    -0.0963195   -0.118999      0.101501    -0.193633      0.0288537    0.137558    -0.0499595   -0.0392207   -0.000307014   0.0577876    -0.179615    -0.018657    -0.0420501   -0.204635    -0.0892163   -0.0935082    -0.0279845   -0.0978569    0.167234      0.0950406   -0.117344    0.0537423 
 -0.114288     -0.0461396   -0.104022    -0.123156      0.102757    -0.104347      0.11505      0.131145     -0.04789     -0.0224568    0.0447992    0.0171626    0.0544697    -0.0555708     0.0484417   -0.0319533   -0.107909     0.0455542    0.0880808    0.129503     -0.076533     0.0822091   -0.0453147    -0.0580741    0.0361945  -0.0223219 
 -0.0358014    -0.171606     0.0994833   -0.0556491    -0.143618     0.11095      -0.0529182   -0.0843935    -0.16216     -0.0766728   -0.0461412    0.111009    -0.171329     -0.142544      0.00595567   0.00524738  -0.143479    -0.0215979    0.0556212    0.140161     -0.0507346   -0.0734579    0.00244795   -0.161643     0.0914797   0.132256  
  0.0144706     0.0189971    0.0134879   -0.0301792     0.0791381    0.122543      0.0471379   -0.0661884    -0.0221654   -0.00694359  -0.0613617   -0.0153022   -0.187018     -0.000418184  -0.0423632   -0.127384    -0.0109713    0.0430643    0.135853    -0.0665786    -0.11088      0.143887    -0.141755     -0.0339732    0.11197    -0.127266  kind diag, method split
0: avll = -1.3997239789119456
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.399815
INFO: iteration 2, average log likelihood -1.399725
INFO: iteration 3, average log likelihood -1.399201
INFO: iteration 4, average log likelihood -1.393817
INFO: iteration 5, average log likelihood -1.378976
INFO: iteration 6, average log likelihood -1.370973
INFO: iteration 7, average log likelihood -1.368910
INFO: iteration 8, average log likelihood -1.367757
INFO: iteration 9, average log likelihood -1.367168
INFO: iteration 10, average log likelihood -1.366894
INFO: iteration 11, average log likelihood -1.366741
INFO: iteration 12, average log likelihood -1.366647
INFO: iteration 13, average log likelihood -1.366586
INFO: iteration 14, average log likelihood -1.366545
INFO: iteration 15, average log likelihood -1.366514
INFO: iteration 16, average log likelihood -1.366491
INFO: iteration 17, average log likelihood -1.366473
INFO: iteration 18, average log likelihood -1.366457
INFO: iteration 19, average log likelihood -1.366442
INFO: iteration 20, average log likelihood -1.366428
INFO: iteration 21, average log likelihood -1.366415
INFO: iteration 22, average log likelihood -1.366402
INFO: iteration 23, average log likelihood -1.366389
INFO: iteration 24, average log likelihood -1.366375
INFO: iteration 25, average log likelihood -1.366363
INFO: iteration 26, average log likelihood -1.366350
INFO: iteration 27, average log likelihood -1.366337
INFO: iteration 28, average log likelihood -1.366325
INFO: iteration 29, average log likelihood -1.366313
INFO: iteration 30, average log likelihood -1.366302
INFO: iteration 31, average log likelihood -1.366290
INFO: iteration 32, average log likelihood -1.366279
INFO: iteration 33, average log likelihood -1.366269
INFO: iteration 34, average log likelihood -1.366259
INFO: iteration 35, average log likelihood -1.366249
INFO: iteration 36, average log likelihood -1.366240
INFO: iteration 37, average log likelihood -1.366231
INFO: iteration 38, average log likelihood -1.366223
INFO: iteration 39, average log likelihood -1.366216
INFO: iteration 40, average log likelihood -1.366209
INFO: iteration 41, average log likelihood -1.366203
INFO: iteration 42, average log likelihood -1.366197
INFO: iteration 43, average log likelihood -1.366192
INFO: iteration 44, average log likelihood -1.366187
INFO: iteration 45, average log likelihood -1.366183
INFO: iteration 46, average log likelihood -1.366178
INFO: iteration 47, average log likelihood -1.366175
INFO: iteration 48, average log likelihood -1.366171
INFO: iteration 49, average log likelihood -1.366168
INFO: iteration 50, average log likelihood -1.366165
INFO: EM with 100000 data points 50 iterations avll -1.366165
952.4 data points per parameter
1: avll = [-1.39982,-1.39972,-1.3992,-1.39382,-1.37898,-1.37097,-1.36891,-1.36776,-1.36717,-1.36689,-1.36674,-1.36665,-1.36659,-1.36654,-1.36651,-1.36649,-1.36647,-1.36646,-1.36644,-1.36643,-1.36641,-1.3664,-1.36639,-1.36638,-1.36636,-1.36635,-1.36634,-1.36633,-1.36631,-1.3663,-1.36629,-1.36628,-1.36627,-1.36626,-1.36625,-1.36624,-1.36623,-1.36622,-1.36622,-1.36621,-1.3662,-1.3662,-1.36619,-1.36619,-1.36618,-1.36618,-1.36617,-1.36617,-1.36617,-1.36617]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.366277
INFO: iteration 2, average log likelihood -1.366165
INFO: iteration 3, average log likelihood -1.365742
INFO: iteration 4, average log likelihood -1.361898
INFO: iteration 5, average log likelihood -1.349568
INFO: iteration 6, average log likelihood -1.338911
INFO: iteration 7, average log likelihood -1.335183
INFO: iteration 8, average log likelihood -1.333551
INFO: iteration 9, average log likelihood -1.332457
INFO: iteration 10, average log likelihood -1.331603
INFO: iteration 11, average log likelihood -1.330933
INFO: iteration 12, average log likelihood -1.330384
INFO: iteration 13, average log likelihood -1.329880
INFO: iteration 14, average log likelihood -1.329372
INFO: iteration 15, average log likelihood -1.328833
INFO: iteration 16, average log likelihood -1.328242
INFO: iteration 17, average log likelihood -1.327600
INFO: iteration 18, average log likelihood -1.326933
INFO: iteration 19, average log likelihood -1.326270
INFO: iteration 20, average log likelihood -1.325642
INFO: iteration 21, average log likelihood -1.325072
INFO: iteration 22, average log likelihood -1.324563
INFO: iteration 23, average log likelihood -1.324078
INFO: iteration 24, average log likelihood -1.323589
INFO: iteration 25, average log likelihood -1.323100
INFO: iteration 26, average log likelihood -1.322627
INFO: iteration 27, average log likelihood -1.322180
INFO: iteration 28, average log likelihood -1.321754
INFO: iteration 29, average log likelihood -1.321347
INFO: iteration 30, average log likelihood -1.320963
INFO: iteration 31, average log likelihood -1.320625
INFO: iteration 32, average log likelihood -1.320344
INFO: iteration 33, average log likelihood -1.320123
INFO: iteration 34, average log likelihood -1.319951
INFO: iteration 35, average log likelihood -1.319819
INFO: iteration 36, average log likelihood -1.319718
INFO: iteration 37, average log likelihood -1.319638
INFO: iteration 38, average log likelihood -1.319572
INFO: iteration 39, average log likelihood -1.319516
INFO: iteration 40, average log likelihood -1.319464
INFO: iteration 41, average log likelihood -1.319416
INFO: iteration 42, average log likelihood -1.319368
INFO: iteration 43, average log likelihood -1.319322
INFO: iteration 44, average log likelihood -1.319275
INFO: iteration 45, average log likelihood -1.319230
INFO: iteration 46, average log likelihood -1.319187
INFO: iteration 47, average log likelihood -1.319149
INFO: iteration 48, average log likelihood -1.319117
INFO: iteration 49, average log likelihood -1.319089
INFO: iteration 50, average log likelihood -1.319066
INFO: EM with 100000 data points 50 iterations avll -1.319066
473.9 data points per parameter
2: avll = [-1.36628,-1.36616,-1.36574,-1.3619,-1.34957,-1.33891,-1.33518,-1.33355,-1.33246,-1.3316,-1.33093,-1.33038,-1.32988,-1.32937,-1.32883,-1.32824,-1.3276,-1.32693,-1.32627,-1.32564,-1.32507,-1.32456,-1.32408,-1.32359,-1.3231,-1.32263,-1.32218,-1.32175,-1.32135,-1.32096,-1.32063,-1.32034,-1.32012,-1.31995,-1.31982,-1.31972,-1.31964,-1.31957,-1.31952,-1.31946,-1.31942,-1.31937,-1.31932,-1.31928,-1.31923,-1.31919,-1.31915,-1.31912,-1.31909,-1.31907]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.319206
INFO: iteration 2, average log likelihood -1.319018
INFO: iteration 3, average log likelihood -1.318268
INFO: iteration 4, average log likelihood -1.311967
INFO: iteration 5, average log likelihood -1.294927
INFO: iteration 6, average log likelihood -1.281578
INFO: iteration 7, average log likelihood -1.275537
INFO: iteration 8, average log likelihood -1.272582
INFO: iteration 9, average log likelihood -1.270792
INFO: iteration 10, average log likelihood -1.269336
INFO: iteration 11, average log likelihood -1.267904
INFO: iteration 12, average log likelihood -1.266459
INFO: iteration 13, average log likelihood -1.265224
INFO: iteration 14, average log likelihood -1.264379
INFO: iteration 15, average log likelihood -1.263797
INFO: iteration 16, average log likelihood -1.263350
INFO: iteration 17, average log likelihood -1.262977
INFO: iteration 18, average log likelihood -1.262652
INFO: iteration 19, average log likelihood -1.262355
INFO: iteration 20, average log likelihood -1.262075
INFO: iteration 21, average log likelihood -1.261804
INFO: iteration 22, average log likelihood -1.261546
INFO: iteration 23, average log likelihood -1.261310
INFO: iteration 24, average log likelihood -1.261104
INFO: iteration 25, average log likelihood -1.260938
INFO: iteration 26, average log likelihood -1.260814
INFO: iteration 27, average log likelihood -1.260727
INFO: iteration 28, average log likelihood -1.260666
INFO: iteration 29, average log likelihood -1.260622
INFO: iteration 30, average log likelihood -1.260590
INFO: iteration 31, average log likelihood -1.260565
INFO: iteration 32, average log likelihood -1.260547
INFO: iteration 33, average log likelihood -1.260532
INFO: iteration 34, average log likelihood -1.260521
INFO: iteration 35, average log likelihood -1.260512
INFO: iteration 36, average log likelihood -1.260504
INFO: iteration 37, average log likelihood -1.260498
INFO: iteration 38, average log likelihood -1.260493
INFO: iteration 39, average log likelihood -1.260489
INFO: iteration 40, average log likelihood -1.260486
INFO: iteration 41, average log likelihood -1.260483
INFO: iteration 42, average log likelihood -1.260481
INFO: iteration 43, average log likelihood -1.260479
INFO: iteration 44, average log likelihood -1.260477
INFO: iteration 45, average log likelihood -1.260476
INFO: iteration 46, average log likelihood -1.260475
INFO: iteration 47, average log likelihood -1.260474
INFO: iteration 48, average log likelihood -1.260473
INFO: iteration 49, average log likelihood -1.260472
INFO: iteration 50, average log likelihood -1.260471
INFO: EM with 100000 data points 50 iterations avll -1.260471
236.4 data points per parameter
3: avll = [-1.31921,-1.31902,-1.31827,-1.31197,-1.29493,-1.28158,-1.27554,-1.27258,-1.27079,-1.26934,-1.2679,-1.26646,-1.26522,-1.26438,-1.2638,-1.26335,-1.26298,-1.26265,-1.26236,-1.26207,-1.2618,-1.26155,-1.26131,-1.2611,-1.26094,-1.26081,-1.26073,-1.26067,-1.26062,-1.26059,-1.26057,-1.26055,-1.26053,-1.26052,-1.26051,-1.2605,-1.2605,-1.26049,-1.26049,-1.26049,-1.26048,-1.26048,-1.26048,-1.26048,-1.26048,-1.26047,-1.26047,-1.26047,-1.26047,-1.26047]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.260662
INFO: iteration 2, average log likelihood -1.260441
INFO: iteration 3, average log likelihood -1.259426
INFO: iteration 4, average log likelihood -1.248616
INFO: iteration 5, average log likelihood -1.218161
INFO: iteration 6, average log likelihood -1.186026
INFO: iteration 7, average log likelihood -1.173880
INFO: iteration 8, average log likelihood -1.169153
INFO: iteration 9, average log likelihood -1.166410
INFO: iteration 10, average log likelihood -1.164735
INFO: iteration 11, average log likelihood -1.163897
INFO: iteration 12, average log likelihood -1.163416
INFO: iteration 13, average log likelihood -1.162926
INFO: iteration 14, average log likelihood -1.161991
INFO: iteration 15, average log likelihood -1.160553
INFO: iteration 16, average log likelihood -1.159668
INFO: iteration 17, average log likelihood -1.159239
INFO: iteration 18, average log likelihood -1.158946
INFO: iteration 19, average log likelihood -1.158751
INFO: iteration 20, average log likelihood -1.158554
INFO: iteration 21, average log likelihood -1.158263
INFO: iteration 22, average log likelihood -1.157735
INFO: iteration 23, average log likelihood -1.156831
INFO: iteration 24, average log likelihood -1.155801
INFO: iteration 25, average log likelihood -1.155145
INFO: iteration 26, average log likelihood -1.154860
INFO: iteration 27, average log likelihood -1.154687
INFO: iteration 28, average log likelihood -1.154558
INFO: iteration 29, average log likelihood -1.154463
INFO: iteration 30, average log likelihood -1.154401
INFO: iteration 31, average log likelihood -1.154363
INFO: iteration 32, average log likelihood -1.154338
INFO: iteration 33, average log likelihood -1.154319
INFO: iteration 34, average log likelihood -1.154300
INFO: iteration 35, average log likelihood -1.154278
INFO: iteration 36, average log likelihood -1.154249
INFO: iteration 37, average log likelihood -1.154213
INFO: iteration 38, average log likelihood -1.154174
INFO: iteration 39, average log likelihood -1.154138
INFO: iteration 40, average log likelihood -1.154109
INFO: iteration 41, average log likelihood -1.154085
INFO: iteration 42, average log likelihood -1.154064
INFO: iteration 43, average log likelihood -1.154044
INFO: iteration 44, average log likelihood -1.154023
INFO: iteration 45, average log likelihood -1.153998
INFO: iteration 46, average log likelihood -1.153965
INFO: iteration 47, average log likelihood -1.153922
INFO: iteration 48, average log likelihood -1.153867
INFO: iteration 49, average log likelihood -1.153800
INFO: iteration 50, average log likelihood -1.153719
INFO: EM with 100000 data points 50 iterations avll -1.153719
118.1 data points per parameter
4: avll = [-1.26066,-1.26044,-1.25943,-1.24862,-1.21816,-1.18603,-1.17388,-1.16915,-1.16641,-1.16473,-1.1639,-1.16342,-1.16293,-1.16199,-1.16055,-1.15967,-1.15924,-1.15895,-1.15875,-1.15855,-1.15826,-1.15773,-1.15683,-1.1558,-1.15514,-1.15486,-1.15469,-1.15456,-1.15446,-1.1544,-1.15436,-1.15434,-1.15432,-1.1543,-1.15428,-1.15425,-1.15421,-1.15417,-1.15414,-1.15411,-1.15408,-1.15406,-1.15404,-1.15402,-1.154,-1.15397,-1.15392,-1.15387,-1.1538,-1.15372]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.153902
INFO: iteration 2, average log likelihood -1.153463
INFO: iteration 3, average log likelihood -1.152407
WARNING: Variances had to be floored 20
INFO: iteration 4, average log likelihood -1.141268
WARNING: Variances had to be floored 1 2 20 21
INFO: iteration 5, average log likelihood -1.103736
WARNING: Variances had to be floored 20 21
INFO: iteration 6, average log likelihood -1.093666
WARNING: Variances had to be floored 1 2 20 21 22 24
INFO: iteration 7, average log likelihood -1.066574
WARNING: Variances had to be floored 20 32
INFO: iteration 8, average log likelihood -1.087659
WARNING: Variances had to be floored 1 2 18 20 21 29
INFO: iteration 9, average log likelihood -1.073577
WARNING: Variances had to be floored 20
INFO: iteration 10, average log likelihood -1.086699
WARNING: Variances had to be floored 1 2 20 21 22 24
INFO: iteration 11, average log likelihood -1.053672
WARNING: Variances had to be floored 8 20 32
INFO: iteration 12, average log likelihood -1.081764
WARNING: Variances had to be floored 1 2 18 20 21
INFO: iteration 13, average log likelihood -1.068784
WARNING: Variances had to be floored 20 21 29
INFO: iteration 14, average log likelihood -1.076817
WARNING: Variances had to be floored 1 2 20 22 24
INFO: iteration 15, average log likelihood -1.064160
WARNING: Variances had to be floored 20 21 32
INFO: iteration 16, average log likelihood -1.078324
WARNING: Variances had to be floored 1 2 20 21
INFO: iteration 17, average log likelihood -1.068548
WARNING: Variances had to be floored 8 18 20 22
INFO: iteration 18, average log likelihood -1.067163
WARNING: Variances had to be floored 1 2 20 21 24 29
INFO: iteration 19, average log likelihood -1.066014
WARNING: Variances had to be floored 20 32
INFO: iteration 20, average log likelihood -1.083552
WARNING: Variances had to be floored 1 2 20 21 22
INFO: iteration 21, average log likelihood -1.061648
WARNING: Variances had to be floored 18 20
INFO: iteration 22, average log likelihood -1.076599
WARNING: Variances had to be floored 1 2 8 20 21 24
INFO: iteration 23, average log likelihood -1.067693
WARNING: Variances had to be floored 20 29 32
INFO: iteration 24, average log likelihood -1.074596
WARNING: Variances had to be floored 1 2 20 21 22
INFO: iteration 25, average log likelihood -1.065587
WARNING: Variances had to be floored 18 20
INFO: iteration 26, average log likelihood -1.079055
WARNING: Variances had to be floored 1 2 20 21 24
INFO: iteration 27, average log likelihood -1.069196
WARNING: Variances had to be floored 20 32
INFO: iteration 28, average log likelihood -1.074841
WARNING: Variances had to be floored 1 2 8 20 21 22 29
INFO: iteration 29, average log likelihood -1.057561
WARNING: Variances had to be floored 18 20
INFO: iteration 30, average log likelihood -1.084015
WARNING: Variances had to be floored 1 2 20 21
INFO: iteration 31, average log likelihood -1.071964
WARNING: Variances had to be floored 20 21 24 32
INFO: iteration 32, average log likelihood -1.066066
WARNING: Variances had to be floored 1 2 20 22
INFO: iteration 33, average log likelihood -1.069990
WARNING: Variances had to be floored 18 20 21 29
INFO: iteration 34, average log likelihood -1.071691
WARNING: Variances had to be floored 1 2 8 20
INFO: iteration 35, average log likelihood -1.075953
WARNING: Variances had to be floored 20 21 22 32
INFO: iteration 36, average log likelihood -1.062505
WARNING: Variances had to be floored 1 2 20 24
INFO: iteration 37, average log likelihood -1.069836
WARNING: Variances had to be floored 18 20 21
INFO: iteration 38, average log likelihood -1.078924
WARNING: Variances had to be floored 1 2 20 29
INFO: iteration 39, average log likelihood -1.068777
WARNING: Variances had to be floored 20 21 22 32
INFO: iteration 40, average log likelihood -1.066904
WARNING: Variances had to be floored 1 2 8 20
INFO: iteration 41, average log likelihood -1.073704
WARNING: Variances had to be floored 18 20 21 24
INFO: iteration 42, average log likelihood -1.069452
WARNING: Variances had to be floored 1 2 20
INFO: iteration 43, average log likelihood -1.074341
WARNING: Variances had to be floored 20 21 22 29 32
INFO: iteration 44, average log likelihood -1.060194
WARNING: Variances had to be floored 1 2 20
INFO: iteration 45, average log likelihood -1.079590
WARNING: Variances had to be floored 18 20 21 24
INFO: iteration 46, average log likelihood -1.073551
WARNING: Variances had to be floored 1 2 20
INFO: iteration 47, average log likelihood -1.075517
WARNING: Variances had to be floored 8 20 21 22 32
INFO: iteration 48, average log likelihood -1.060447
WARNING: Variances had to be floored 1 2 20 29
INFO: iteration 49, average log likelihood -1.071933
WARNING: Variances had to be floored 18 20 21
INFO: iteration 50, average log likelihood -1.078947
INFO: EM with 100000 data points 50 iterations avll -1.078947
59.0 data points per parameter
5: avll = [-1.1539,-1.15346,-1.15241,-1.14127,-1.10374,-1.09367,-1.06657,-1.08766,-1.07358,-1.0867,-1.05367,-1.08176,-1.06878,-1.07682,-1.06416,-1.07832,-1.06855,-1.06716,-1.06601,-1.08355,-1.06165,-1.0766,-1.06769,-1.0746,-1.06559,-1.07905,-1.0692,-1.07484,-1.05756,-1.08402,-1.07196,-1.06607,-1.06999,-1.07169,-1.07595,-1.0625,-1.06984,-1.07892,-1.06878,-1.0669,-1.0737,-1.06945,-1.07434,-1.06019,-1.07959,-1.07355,-1.07552,-1.06045,-1.07193,-1.07895]
[-1.39972,-1.39982,-1.39972,-1.3992,-1.39382,-1.37898,-1.37097,-1.36891,-1.36776,-1.36717,-1.36689,-1.36674,-1.36665,-1.36659,-1.36654,-1.36651,-1.36649,-1.36647,-1.36646,-1.36644,-1.36643,-1.36641,-1.3664,-1.36639,-1.36638,-1.36636,-1.36635,-1.36634,-1.36633,-1.36631,-1.3663,-1.36629,-1.36628,-1.36627,-1.36626,-1.36625,-1.36624,-1.36623,-1.36622,-1.36622,-1.36621,-1.3662,-1.3662,-1.36619,-1.36619,-1.36618,-1.36618,-1.36617,-1.36617,-1.36617,-1.36617,-1.36628,-1.36616,-1.36574,-1.3619,-1.34957,-1.33891,-1.33518,-1.33355,-1.33246,-1.3316,-1.33093,-1.33038,-1.32988,-1.32937,-1.32883,-1.32824,-1.3276,-1.32693,-1.32627,-1.32564,-1.32507,-1.32456,-1.32408,-1.32359,-1.3231,-1.32263,-1.32218,-1.32175,-1.32135,-1.32096,-1.32063,-1.32034,-1.32012,-1.31995,-1.31982,-1.31972,-1.31964,-1.31957,-1.31952,-1.31946,-1.31942,-1.31937,-1.31932,-1.31928,-1.31923,-1.31919,-1.31915,-1.31912,-1.31909,-1.31907,-1.31921,-1.31902,-1.31827,-1.31197,-1.29493,-1.28158,-1.27554,-1.27258,-1.27079,-1.26934,-1.2679,-1.26646,-1.26522,-1.26438,-1.2638,-1.26335,-1.26298,-1.26265,-1.26236,-1.26207,-1.2618,-1.26155,-1.26131,-1.2611,-1.26094,-1.26081,-1.26073,-1.26067,-1.26062,-1.26059,-1.26057,-1.26055,-1.26053,-1.26052,-1.26051,-1.2605,-1.2605,-1.26049,-1.26049,-1.26049,-1.26048,-1.26048,-1.26048,-1.26048,-1.26048,-1.26047,-1.26047,-1.26047,-1.26047,-1.26047,-1.26066,-1.26044,-1.25943,-1.24862,-1.21816,-1.18603,-1.17388,-1.16915,-1.16641,-1.16473,-1.1639,-1.16342,-1.16293,-1.16199,-1.16055,-1.15967,-1.15924,-1.15895,-1.15875,-1.15855,-1.15826,-1.15773,-1.15683,-1.1558,-1.15514,-1.15486,-1.15469,-1.15456,-1.15446,-1.1544,-1.15436,-1.15434,-1.15432,-1.1543,-1.15428,-1.15425,-1.15421,-1.15417,-1.15414,-1.15411,-1.15408,-1.15406,-1.15404,-1.15402,-1.154,-1.15397,-1.15392,-1.15387,-1.1538,-1.15372,-1.1539,-1.15346,-1.15241,-1.14127,-1.10374,-1.09367,-1.06657,-1.08766,-1.07358,-1.0867,-1.05367,-1.08176,-1.06878,-1.07682,-1.06416,-1.07832,-1.06855,-1.06716,-1.06601,-1.08355,-1.06165,-1.0766,-1.06769,-1.0746,-1.06559,-1.07905,-1.0692,-1.07484,-1.05756,-1.08402,-1.07196,-1.06607,-1.06999,-1.07169,-1.07595,-1.0625,-1.06984,-1.07892,-1.06878,-1.0669,-1.0737,-1.06945,-1.07434,-1.06019,-1.07959,-1.07355,-1.07552,-1.06045,-1.07193,-1.07895]
32×26 Array{Float64,2}:
 -0.19216      0.0504426   -0.152149     -0.0276261   -0.134044    -0.0853709   -0.0644062  -0.276746    -0.0171655    0.121582    -0.0671624  -0.0166699   -0.214388     0.0560024   -0.0746692   -0.0994545   -0.0743695   -0.00797983    0.000979128  -0.00422005  -0.15819     -0.171505    -0.0559566    -0.333577     0.0387593    0.0993102 
 -0.112069     0.00969409  -0.000294319   0.0401078   -0.0939925   -0.126087     0.115316   -0.157504     0.0307705    0.150812    -0.0537609  -0.035549    -0.00596112   0.056644    -0.149932    -0.0782131   -0.0367402   -0.176449     -0.087854     -0.0695907   -0.0329683   -0.0934977    0.16136       0.0859842   -0.113157     0.0537569 
 -0.0272075    0.0535048   -0.0777725     0.0635675   -0.167009    -0.0712981   -0.145854   -0.0983104   -0.176076     0.0249937    0.0659666   0.0707941   -0.231435     0.021777    -0.0491636    0.0785865   -0.092457    -0.422133      0.0784087     0.0997282   -0.165879     0.0805228   -0.105409      0.040088     0.112301     0.0193561 
 -0.208506     0.045246    -0.0858388     0.024452    -0.0278804   -0.0497095    0.0527184  -0.0765148   -0.138289     0.0431632    0.0781133   0.0349068    0.209367    -0.209296    -0.171261     0.0642859   -0.0808657    0.619078      0.0694838    -0.0180717   -0.0413438    0.0753109   -0.197747      0.0938621    0.0950942    0.00812572
  0.116004    -0.0570611   -0.121381      0.0207918    0.0400345    0.0582722   -0.0592253  -0.298815    -0.0507239    0.0694083    0.0827384  -0.0615851    0.112465     0.0518846   -0.159467    -0.103624    -0.116337    -0.290147     -0.01518      -0.030902     0.100219    -0.0296278   -0.0022301     0.196058    -0.0395121    0.00815104
  0.128093    -0.0731738   -0.146054      0.0156473    0.1592       0.129628    -0.179827    0.122925    -0.0655737    0.0652296    0.0145015   7.5249e-5    0.0394473   -0.0246786   -0.0966264   -0.141071    -0.148191    -0.17023       0.0377068     0.0516892   -0.0736663    0.0329241   -0.0956397     0.159222    -0.0529738   -0.0164914 
 -0.161498     0.0799646   -0.191601      0.021        0.101087     0.00130181  -0.0125244  -0.154792     0.0627667   -0.0364615    0.235592    0.00846932  -0.0159947    0.0320002   -0.865491    -0.0886779    0.0463407    0.0440725    -0.0594092    -0.420635     0.0165571    0.153764     0.00319379   -0.00197928   0.147041    -0.0403255 
 -0.163667    -0.177547    -0.187953      0.0274222    0.187751     0.0242262   -0.0142821   0.136761     0.146663    -0.0421689    0.161399   -0.0145904   -0.154172     0.0252301    0.481037    -0.115691     0.15239      0.0478139    -0.0771782     0.129808    -0.0123723    0.187255    -0.00936748   -0.139475     0.16285      0.00787114
  0.00474821  -0.319105    -0.10351      -0.0409806   -0.214703    -0.0529494    0.0546624  -0.0726939    0.029158     0.170952     0.14169     0.0973075    0.017041    -0.1026      -0.0803087   -0.508289     0.0792642    0.000172456  -0.120749     -0.201643     0.0589805   -0.14728     -0.0701315     0.11631     -0.0426383   -0.0760123 
  0.188958     0.261999    -0.0315925    -0.0447051   -0.271744     0.149214     0.050459   -0.135425    -0.0214558   -0.0904898    0.0875656  -0.111261     0.0138461   -0.0698804   -0.0412809    0.216673     0.0591514    0.116354      0.247813     -0.158696     0.100164    -0.141846     0.0219645     0.208559    -0.181041    -0.00508408
 -0.0158622   -0.0251005    0.00556554   -0.0298292   -0.00766935  -0.0752434   -0.130913    0.0143952   -0.0453977    0.204624     0.0429165   0.0210323   -0.0354212    0.106862     0.00548912  -0.00171181   0.0206083   -0.0913192     0.0675292     0.129015    -0.0193079   -0.00267993   0.0269308    -0.00671717  -0.0261607   -0.0868171 
  0.104075     0.111417    -0.0374828     0.106777    -0.109377     0.0712104    0.182815   -0.00336759   0.0979566   -0.0472764   -0.151426   -0.0398435   -0.0719259    0.106569    -0.0808333   -0.0456397   -0.0319321   -0.048438     -0.0399041    -0.0353747    0.079861     0.0961712   -0.00567947    0.00771343  -0.0495035   -0.00745446
 -0.209935     0.0194209    0.0311727     0.0302441   -0.128714     0.114318    -0.0573346   0.0165283    0.00365466  -0.066651    -0.130348    0.0175382    0.015294     0.100534    -0.0267716    0.136541     0.0407819    0.134859     -0.103611      0.00783055  -0.118871     0.0479173   -0.0282401     0.156077    -0.0607764    0.180557  
 -0.324055     0.0654304   -0.0059752    -0.00840811   0.178508     0.12136     -0.11396     0.0264219    0.402603    -0.002914    -0.0843823   0.0878466    0.106802    -0.0250427    0.0779786   -0.097905     0.067424     0.042842     -0.134741     -0.0174967   -0.152192     0.125602    -0.0546427    -0.108835    -0.0764633    0.124136  
 -0.00439355   0.175579    -0.197385      0.0899866   -0.079437    -0.0415177    0.13355    -0.00206211  -0.137819    -0.161936     0.0838137   0.0286123    0.117981    -0.155942     0.0259056   -0.0439271   -0.300568     0.0692726     0.155691     -0.105044    -0.00326521   0.0727105   -0.141326      0.164757    -0.130295    -0.0723391 
  0.0576062   -0.0238056    0.00715651   -0.131274    -0.0714601   -0.066425     0.037415    0.079344    -0.0296654   -0.0669978   -0.0246666   0.0719824    0.0135599   -0.1044      -0.0131341    0.0292502    0.0561396   -0.0047534    -0.0635777    -0.0216579   -0.0301915    0.0193706   -0.000634196  -0.016029    -0.0567212   -0.0236033 
 -0.0382636   -0.0257523    0.0438773    -0.0404856   -0.0169291   -0.0305299   -0.0504633   0.0171149   -0.0589124    0.044381    -0.0469665   0.0924871   -0.139336    -0.121901     0.0337436   -0.0809742   -0.0544417   -0.00937147   -0.0504417    -0.0494947   -0.00162787  -0.0700545    0.0401754    -0.154944     0.0584469    0.0560493 
  0.0132174   -0.00344037   0.169635      0.0482917    0.0493387    0.0309979   -0.070727   -0.0236615   -0.0545444   -0.0738716   -0.0777616  -0.0932285    0.15542      0.13798     -0.103213    -0.0829493   -0.140962     0.0223646    -0.124214      0.00558854  -0.23773      0.0834734    0.0316714     0.0335437   -0.153676     0.0344842 
 -0.0878286    0.205434     0.128534      0.0567286    0.142214    -0.190901    -0.0365898   0.193585     0.146385     0.0175711    0.0959386  -0.0383271    0.0505511   -0.140251    -0.0213467    0.145025    -0.00158163  -0.000940371   0.125075      0.0664578   -0.120439    -0.00423451   0.00169502   -0.0292426    0.0365905    0.0193826 
  0.0218726    0.00320501   0.0145276    -0.0353256    0.0775251    0.145784     0.0353843  -0.0727784   -0.0149688   -0.00820408  -0.0591066   0.00104743  -0.20377      0.00409931  -0.0454071   -0.120618    -0.0111315    0.0372341     0.130618     -0.0749024   -0.0623323    0.145228    -0.140151     -0.033925     0.0992185   -0.126819  
  0.0103273    0.104422    -0.0875358    -0.00447929   0.0929808    0.0248256    0.151523    0.151546    -0.0314947   -0.142434     0.0767682  -0.0698013    0.0609083    0.0927573   -0.00127553   0.0811972   -0.130335    -0.0972111     0.00960474    0.0793699   -0.0846808    0.193666     0.0265541     0.078827     0.0505429    0.0361385 
  0.10769      0.0298495    0.158571      0.010279    -0.0540486    0.114147    -0.0223985   0.0857062   -0.020277    -0.164109     0.119473   -0.169266     0.196048    -0.118863     0.155762     0.103614    -0.0916156    0.0942017    -0.0229964    -0.112231     0.0277934   -0.0487731    0.0676575    -0.0243592   -0.0910235   -0.016479  
 -0.196154    -0.118666    -0.0296084    -0.120917     0.0208139   -0.0139598   -0.0572315  -0.0381072   -0.0205342    0.0401786   -0.104801    0.00335329  -0.0775876   -0.0261263    0.0299923   -0.0838732   -0.0591218    0.014413      0.0722705     0.0408272   -0.0437623    0.0432044    0.102998      0.115206    -0.0396452   -0.0263257 
  0.0599497   -0.168382     0.00627992   -0.0133982    0.0664053    0.0193797   -0.0106676  -0.194332    -0.0288231    0.0722953    0.0412972   0.0265801    0.0987745   -0.0758332    0.167718     0.103173     0.0359964   -0.202893      0.0119057    -0.144569    -0.089348     0.162466    -0.0529        0.0187186   -0.0894291    0.135302  
 -0.0727247   -0.0456695   -0.0820647    -0.119547     0.0892059   -0.0504267    0.101427    0.124117    -0.485099    -0.0820304    0.060493   -0.00550367   0.0428187   -0.043671     0.00888176  -0.0228683   -0.100064     0.0465756     0.0994838     0.0764851    0.0313651    0.0588118   -0.0623546    -0.0996989    0.0676844   -0.0137577 
 -0.153554    -0.0498075   -0.127763     -0.12483      0.100337    -0.156355     0.121399    0.13026      0.299951     0.00746213   0.0380218   0.0432368    0.0433602   -0.074033     0.0369399    0.0258986   -0.107683     0.0173659     0.0824657     0.170001    -0.185942     0.0895852   -0.0179866    -0.0273614   -0.00327683  -0.0282787 
 -0.0116466    0.0783515   -0.438458      0.0938209   -0.065729    -0.248851    -0.0567937   0.12858     -0.0514535    0.0449943   -0.0397152  -0.0793296    0.00107095   0.0184734   -0.110242     0.124733     0.16118     -0.117369     -0.0375832    -0.162732     0.25154     -0.0393565    0.157996     -0.0195354    0.126259    -0.0262711 
 -0.130217     0.0922727    0.256037      0.0826732   -0.171578     0.0218774   -0.0621902  -0.0159769    0.0597099   -0.240753    -0.179256    0.153578     0.0440504   -0.00546171  -0.223383     0.0506248   -0.130091    -0.0794946    -0.0280367    -0.094946     0.249487    -0.00488188   0.174231      0.00998874   0.0905708   -0.0119332 
 -0.0652787    0.196844     0.0401578     0.0739631   -0.0154395   -0.0509199    0.0331593  -0.034081     0.160464    -0.123604     0.0253004  -0.0737146   -0.0214291    0.0189035    0.0617155    0.0800851    0.0595711    0.133314     -0.0626362     0.146531     0.0594887    0.152077     0.00191602    0.0599375    0.0901297    0.0594709 
 -0.0755151   -0.029789     0.0514083    -0.0817597    0.0524399    0.257554     0.0577157   0.0995024    0.0597242   -0.0829076   -0.211032    0.0387643   -0.0631243    0.0681284    0.103852     0.166323    -0.0601504   -0.0281553    -0.0844928    -0.0372807   -0.0671294   -0.123579     0.00458044   -0.0266347   -0.050857    -0.00668743
  0.0799392    0.032298    -0.115435     -0.0380266   -0.0980415    0.0958166   -0.0143164  -0.0704747   -0.19483      0.0156676   -0.0895667  -0.0159882    0.172601     0.0219828    0.143081    -0.00311614  -0.0209774    0.0158876     0.0879786     0.0867761    0.0718509    0.0208322    0.000330961  -0.0212392   -0.0160251   -0.0859651 
  0.168252    -0.0677302    0.0319886    -0.149033    -0.0529921    0.0675206    0.0465944  -0.0889606   -0.0326651   -0.059402     0.0469033   0.148261    -0.0906398    0.100588     0.136599     0.0381006   -0.0201808    0.0309832    -0.0386457     0.0403635   -0.0257195    0.141806    -0.0340077    -0.160241    -0.0654345   -0.0602715 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 20 21 24
INFO: iteration 1, average log likelihood -1.068022
WARNING: Variances had to be floored 1 2 20 21 22 24 32
INFO: iteration 2, average log likelihood -1.055256
WARNING: Variances had to be floored 1 2 20 21 24 29
INFO: iteration 3, average log likelihood -1.057184
WARNING: Variances had to be floored 1 2 18 20 21 22 24 32
INFO: iteration 4, average log likelihood -1.055665
WARNING: Variances had to be floored 1 2 20 21 24
INFO: iteration 5, average log likelihood -1.065367
WARNING: Variances had to be floored 1 2 20 21 22 24 29 32
INFO: iteration 6, average log likelihood -1.053555
WARNING: Variances had to be floored 1 2 8 20 21 24
INFO: iteration 7, average log likelihood -1.060558
WARNING: Variances had to be floored 1 2 18 20 21 22 24 32
INFO: iteration 8, average log likelihood -1.053458
WARNING: Variances had to be floored 1 2 20 21 24 29
INFO: iteration 9, average log likelihood -1.064151
WARNING: Variances had to be floored 1 2 20 21 22 24 32
INFO: iteration 10, average log likelihood -1.056982
INFO: EM with 100000 data points 10 iterations avll -1.056982
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.917859e+05
      1       6.712366e+05      -2.205493e+05 |       32
      2       6.438908e+05      -2.734581e+04 |       32
      3       6.278240e+05      -1.606671e+04 |       32
      4       6.177766e+05      -1.004741e+04 |       32
      5       6.117854e+05      -5.991261e+03 |       32
      6       6.075070e+05      -4.278405e+03 |       32
      7       6.040734e+05      -3.433613e+03 |       32
      8       6.018006e+05      -2.272770e+03 |       32
      9       6.000086e+05      -1.792004e+03 |       32
     10       5.984449e+05      -1.563650e+03 |       32
     11       5.971011e+05      -1.343786e+03 |       32
     12       5.964343e+05      -6.668461e+02 |       32
     13       5.961950e+05      -2.392694e+02 |       32
     14       5.960458e+05      -1.492276e+02 |       32
     15       5.959053e+05      -1.405294e+02 |       32
     16       5.957638e+05      -1.415203e+02 |       32
     17       5.955773e+05      -1.864068e+02 |       32
     18       5.953138e+05      -2.635401e+02 |       32
     19       5.949403e+05      -3.735510e+02 |       31
     20       5.943779e+05      -5.623953e+02 |       31
     21       5.935222e+05      -8.556511e+02 |       32
     22       5.930165e+05      -5.057474e+02 |       32
     23       5.928477e+05      -1.687779e+02 |       31
     24       5.927598e+05      -8.783840e+01 |       31
     25       5.927228e+05      -3.707975e+01 |       31
     26       5.927062e+05      -1.656795e+01 |       31
     27       5.926965e+05      -9.671709e+00 |       28
     28       5.926906e+05      -5.974035e+00 |       26
     29       5.926871e+05      -3.403827e+00 |       21
     30       5.926848e+05      -2.372192e+00 |       17
     31       5.926832e+05      -1.537283e+00 |       18
     32       5.926815e+05      -1.706197e+00 |       22
     33       5.926790e+05      -2.579812e+00 |       26
     34       5.926764e+05      -2.576664e+00 |       23
     35       5.926743e+05      -2.030515e+00 |       15
     36       5.926733e+05      -1.060405e+00 |       18
     37       5.926722e+05      -1.119866e+00 |       18
     38       5.926711e+05      -1.070612e+00 |       16
     39       5.926700e+05      -1.123345e+00 |       16
     40       5.926689e+05      -1.052619e+00 |       15
     41       5.926675e+05      -1.384930e+00 |       15
     42       5.926664e+05      -1.105718e+00 |       16
     43       5.926654e+05      -9.990266e-01 |       14
     44       5.926644e+05      -1.052776e+00 |       12
     45       5.926637e+05      -7.220337e-01 |       15
     46       5.926631e+05      -5.744963e-01 |       15
     47       5.926626e+05      -5.252162e-01 |       13
     48       5.926617e+05      -8.517026e-01 |       13
     49       5.926608e+05      -8.882537e-01 |       19
     50       5.926599e+05      -9.536623e-01 |        7
K-means terminated without convergence after 50 iterations (objv = 592659.860231518)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.315210
INFO: iteration 2, average log likelihood -1.281785
INFO: iteration 3, average log likelihood -1.248038
INFO: iteration 4, average log likelihood -1.214495
INFO: iteration 5, average log likelihood -1.175796
INFO: iteration 6, average log likelihood -1.132192
WARNING: Variances had to be floored 1 20 23 31
INFO: iteration 7, average log likelihood -1.087029
WARNING: Variances had to be floored 18
INFO: iteration 8, average log likelihood -1.114481
WARNING: Variances had to be floored 6 11 17
INFO: iteration 9, average log likelihood -1.080110
WARNING: Variances had to be floored 24
INFO: iteration 10, average log likelihood -1.097260
WARNING: Variances had to be floored 20 23 32
INFO: iteration 11, average log likelihood -1.061248
WARNING: Variances had to be floored 4 12
INFO: iteration 12, average log likelihood -1.049847
WARNING: Variances had to be floored 1 11 17 18 24
INFO: iteration 13, average log likelihood -1.049473
WARNING: Variances had to be floored 6 20 23
INFO: iteration 14, average log likelihood -1.085717
WARNING: Variances had to be floored 31 32
INFO: iteration 15, average log likelihood -1.090162
WARNING: Variances had to be floored 12 17 30
INFO: iteration 16, average log likelihood -1.065042
WARNING: Variances had to be floored 11 18 20
INFO: iteration 17, average log likelihood -1.077960
WARNING: Variances had to be floored 1 6 24 32
INFO: iteration 18, average log likelihood -1.081769
INFO: iteration 19, average log likelihood -1.094293
WARNING: Variances had to be floored 4 17 31
INFO: iteration 20, average log likelihood -1.053541
WARNING: Variances had to be floored 11 18 20 23 30
INFO: iteration 21, average log likelihood -1.055382
WARNING: Variances had to be floored 1 6 12 24
INFO: iteration 22, average log likelihood -1.079135
WARNING: Variances had to be floored 17 32
INFO: iteration 23, average log likelihood -1.097018
WARNING: Variances had to be floored 31
INFO: iteration 24, average log likelihood -1.080429
WARNING: Variances had to be floored 11 12 20
INFO: iteration 25, average log likelihood -1.050434
WARNING: Variances had to be floored 1 6 18 24 32
INFO: iteration 26, average log likelihood -1.061001
WARNING: Variances had to be floored 4 17 30 31
INFO: iteration 27, average log likelihood -1.081850
WARNING: Variances had to be floored 23
INFO: iteration 28, average log likelihood -1.084265
WARNING: Variances had to be floored 11
INFO: iteration 29, average log likelihood -1.059940
WARNING: Variances had to be floored 1 12 17 18 20 24
INFO: iteration 30, average log likelihood -1.032885
WARNING: Variances had to be floored 6 31 32
INFO: iteration 31, average log likelihood -1.066305
WARNING: Variances had to be floored 4 11
INFO: iteration 32, average log likelihood -1.070614
WARNING: Variances had to be floored 20 23 30
INFO: iteration 33, average log likelihood -1.053183
WARNING: Variances had to be floored 1 12 17 18 24 31
INFO: iteration 34, average log likelihood -1.055605
INFO: iteration 35, average log likelihood -1.092300
WARNING: Variances had to be floored 4 6 32
INFO: iteration 36, average log likelihood -1.045805
WARNING: Variances had to be floored 11 20 23 31
INFO: iteration 37, average log likelihood -1.041509
WARNING: Variances had to be floored 1 12 18
INFO: iteration 38, average log likelihood -1.068814
WARNING: Variances had to be floored 24 30
INFO: iteration 39, average log likelihood -1.074132
WARNING: Variances had to be floored 4 6 17
INFO: iteration 40, average log likelihood -1.050407
WARNING: Variances had to be floored 11 20 23 31
INFO: iteration 41, average log likelihood -1.046543
WARNING: Variances had to be floored 1 12 18 32
INFO: iteration 42, average log likelihood -1.057914
WARNING: Variances had to be floored 24
INFO: iteration 43, average log likelihood -1.077834
WARNING: Variances had to be floored 4 6 20 23 30 31
INFO: iteration 44, average log likelihood -1.037464
INFO: iteration 45, average log likelihood -1.096262
WARNING: Variances had to be floored 1 18 24 32
INFO: iteration 46, average log likelihood -1.050489
WARNING: Variances had to be floored 12 17 31
INFO: iteration 47, average log likelihood -1.066920
WARNING: Variances had to be floored 20 23
INFO: iteration 48, average log likelihood -1.074067
WARNING: Variances had to be floored 6
INFO: iteration 49, average log likelihood -1.063227
WARNING: Variances had to be floored 1 11 24 30
INFO: iteration 50, average log likelihood -1.044952
INFO: EM with 100000 data points 50 iterations avll -1.044952
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.123387    -0.256449     -0.0963978   -0.0310316     0.00428338  -0.0354631     0.00878532   0.00621503   0.0569712    0.0675666  -0.227627     0.0252699   -0.0543121   -0.0770506    0.16825      0.0972712    0.0782707    0.0394167    -0.00797041   0.0814774   -0.124575    -0.0189103    0.0685795     0.124092    -0.0764709  -0.127795  
  0.0893494    0.0196223     0.0287532   -0.148617     -0.084534     0.0084769     0.024205     0.115415    -0.0406112   -0.0745917   0.0532208   -0.00274964  -0.0214      -0.0915047    0.023028     0.154545     0.0791215    0.0159019    -0.00548444   0.00954129   0.024927    -0.0379771    0.0338285    -0.0458236   -0.0716134   0.0727163 
  0.122869     0.159916     -0.0181954    0.184583      0.095815     0.141638      0.143931     0.0253635    0.127601    -0.0693944  -0.172031    -0.138525    -2.41614e-5   0.222637    -0.0845421   -0.103407    -0.0866756   -0.0588192    -0.0905812   -0.00119142   0.184799     0.0713938   -0.0202716     0.0725632    0.047104   -0.0249683 
  0.00498814   0.131461      0.213933     0.0491957    -0.0791601    0.081613     -0.061987    -0.0515121   -0.103559    -0.0927821  -0.111092    -0.0646536    0.224001     0.191292    -0.149757    -0.0727287   -0.132231    -0.00131107   -0.098103    -0.00593831  -0.166543     0.128131     0.0106508     0.0061897   -0.10579     0.0527609 
 -0.102237     0.0769593    -0.0765791    0.085334     -0.128471    -0.0132281    -0.0294548    0.0208272    0.0239089   -0.178505   -0.169786     0.14128      0.0257815    0.00742027  -0.168661     0.0724733   -0.0432815   -0.0813795    -0.0361315   -0.116163     0.251809     0.00789023   0.186854      0.0011493    0.106154   -0.00818439
 -0.0244432   -0.219004      0.0960997   -0.0460831    -0.126901     0.102575     -0.0670206   -0.104608    -0.235929    -0.0922435  -0.0336534    0.127369    -0.182136    -0.184991     0.0414574    0.0125552   -0.141478    -0.0329433     0.0446209    0.124872    -0.112426    -0.0833416    0.00429458   -0.12055      0.0723083   0.127868  
 -0.273453     0.0472793     0.00936048   0.0120982     0.053836     0.12427      -0.0967868    0.0193048    0.235208    -0.0334034  -0.0968102    0.0595891    0.0729598    0.0297687    0.0338503   -0.00151706   0.0559303    0.0822422    -0.125398    -0.00830119  -0.13893      0.0940513   -0.0474527    -0.00201704  -0.0727062   0.155878  
  0.0631417    0.0742426     0.0400638   -0.000444691   0.0147407    0.0787736     0.0712194    0.125622    -0.0295589   -0.164469    0.108658    -0.126771     0.140432    -0.0123994    0.0782492    0.0928689   -0.123186    -0.00675814   -0.00757457  -0.0219599   -0.0264516    0.0695297    0.0489738     0.025991    -0.0220708   0.015816  
  0.0117488   -0.0292456    -0.0690128    0.0261902     0.011706     0.000604526  -0.0194485   -0.090261    -0.0210863    0.103103   -0.00310359  -0.0329594    0.0323205    0.0290097   -0.135861    -0.105057    -0.0888668   -0.196231     -0.0329895   -0.0268505   -0.0181517   -0.0416952    0.0444908     0.127025    -0.0713548   0.0236556 
 -0.117991     0.0507969    -0.081228     0.0443565    -0.0876593   -0.0582713    -0.0489098   -0.0890961   -0.158087     0.0332398   0.0715173    0.0582934   -0.0158222   -0.0909112   -0.106783     0.0734426   -0.0880847    0.0942898     0.0743944    0.0441116   -0.10733      0.0787393   -0.14949       0.0633435    0.107033    0.015724  
  0.0243166   -0.000241039   0.0125721   -0.0371353     0.0753938    0.154805      0.0422783   -0.080506    -0.0181591   -0.0129213  -0.063359    -0.00278094  -0.205337     0.00217305  -0.046065    -0.125139    -0.0129383    0.0379213     0.126001    -0.0719609   -0.0650639    0.149012    -0.140745     -0.0352811    0.102624   -0.124923  
  0.155118    -0.10483       0.0485674   -0.148806     -0.0310741    0.0687541     0.0438233   -0.108135    -0.0238469   -0.0514948   0.0631452    0.139183    -0.112722     0.0892107    0.143907     0.031244    -0.0330765    0.0394964    -0.0499955    0.0387866   -0.0536812    0.132701    -0.0312907    -0.158926    -0.0690595  -0.0516774 
 -0.0853092    0.201219      0.127548     0.0542359     0.140444    -0.188485     -0.0403555    0.195455     0.144216     0.0146255   0.0961826   -0.035964     0.0443294   -0.13473     -0.0213159    0.14327     -0.00153661   0.00326573    0.122405     0.0604668   -0.121436    -0.00586528   0.000304398  -0.0296165    0.0369513   0.0154691 
 -0.044792     0.0866004    -0.142656     0.0183751    -0.140043     0.134006      0.0732491   -0.0797976   -5.48865e-5   0.145527    0.0343722   -0.142503     0.0917938   -0.0139821    0.202514    -0.0441338   -0.0305046   -0.0143444     0.0699661    0.098846    -0.04912      0.00995947   0.0461552     4.03556e-5   0.0454232  -0.160724  
  0.0128492    0.0305821     0.0326237   -0.255674     -0.0281129   -0.156364     -0.211406    -0.00637212  -0.213076     0.184711    0.160447     0.0641368    0.0328101    0.174418    -0.00193861   0.00546863  -0.0433044   -0.0880875     0.1273       0.15794     -0.0983651   -0.0201264   -0.0113828     0.0681651   -0.119133   -0.0794303 
 -0.12081     -0.0475629    -0.104402    -0.120077      0.0950783   -0.102172      0.112094     0.130469    -0.0666027   -0.0311892   0.0471421    0.0196937    0.0442343   -0.0600837    0.0261971    0.00588782  -0.104432     0.0290407     0.0903101    0.132031    -0.0887003    0.0776782   -0.0433416    -0.0618334    0.0356847  -0.015667  
  0.0138347   -0.125339      0.232987     0.0502117     0.124762     0.0274373    -0.056839    -0.0566741   -0.00401687  -0.0158666  -0.0533924   -0.105309     0.146811     0.0380235   -0.123558    -0.0729119   -0.183445    -0.0528176    -0.0940961    0.0199245   -0.246916     0.0186821    0.0507837    -0.00793801  -0.206571    0.0742809 
  0.22174     -0.0251097    -0.0692173   -0.0880769    -0.0479703    0.0654957    -0.0981969   -0.0633561   -0.404853    -0.157524   -0.210826     0.126186     0.253312     0.0727079    0.0796853    0.0447776   -0.0171105    0.0401277     0.102543     0.0731471    0.206197     0.0455358   -0.0585031    -0.0543401   -0.0778914   0.00140493
 -0.00405983   0.175641     -0.197431     0.0898805    -0.0793404   -0.0414416     0.133435    -0.00212311  -0.137615    -0.161901    0.0838357    0.0282954    0.117995    -0.155211     0.0252936   -0.0438613   -0.300586     0.0693334     0.155517    -0.105062    -0.00320389   0.0727839   -0.14131       0.164676    -0.130423   -0.0720763 
  0.0306671   -0.0711274    -0.0278242   -0.108684     -0.0725634   -0.172887      0.046182     0.0221142   -0.0206406   -0.0572455  -0.131651     0.183881     0.0493433   -0.121966    -0.0896818   -0.165087     0.0142633   -0.0388476    -0.144683    -0.0680727   -0.110239     0.0998706   -0.0527156     0.00438042  -0.0198151  -0.142823  
  0.0980245   -0.0230018    -0.0662521   -0.0460233    -0.244121     0.0491374     0.0539255   -0.105206     0.00376148   0.0383993   0.114732    -0.00854309   0.0157072   -0.0866299   -0.0611002   -0.142131     0.0701879    0.0633372     0.0618806   -0.179764     0.0800645   -0.145389    -0.0224048     0.163641    -0.11232    -0.0341388 
 -0.0363755    0.0153523     0.0136635   -0.0473672     0.0424942   -0.100953     -0.0517954    0.0914546   -0.00355374   0.0849473  -0.0401153    0.0746275   -0.0953734   -0.0826545    0.0295598   -0.13466     -0.0117485    0.0251572    -0.103584    -0.164857     0.0343839   -0.0702841    0.0544624    -0.15399      0.0287155   0.0167011 
 -0.0263929    0.0973408     0.0440101   -0.0168384     0.0331322    0.117871     -0.0230907   -0.0581449   -0.0308539   -0.0101204  -0.056602     0.0157232   -0.19138      0.0400664    0.0057006   -0.0661321   -0.0582619    0.011071      0.0707927    0.0158207   -0.065788     0.0992986   -0.0990898    -0.0844748    0.101274    0.0283724 
  0.0589013   -0.157673      0.00899317  -0.0172817     0.0755443    0.0195694    -0.0109223   -0.203923    -0.0310268    0.0724276   0.0526316    0.0277983    0.104231    -0.0812536    0.168824     0.104033     0.0225323   -0.218837      0.0179818   -0.157384    -0.0946771    0.176118    -0.0587569     0.0233647   -0.0850095   0.143221  
  0.0570159    0.122148     -0.0958422    0.096506     -0.0717257   -0.484821     -0.174743     0.191726    -0.0746802    0.207862    0.123715    -0.378647     0.0204521    0.00581892  -0.172532     0.150643     0.22837     -0.173021     -0.0180362   -0.173511     0.245689    -0.13316      0.0799487    -0.0283682    0.121937   -0.062075  
  0.0989698    0.0804496    -0.0447591    0.0182193    -0.315419    -0.00144079    0.22775     -0.0335372    0.0690077   -0.0267961  -0.12922      0.0520123   -0.143716    -0.00734821  -0.0860438    0.0164725    0.014706    -0.0310441     0.00969672  -0.0574121   -0.0218581    0.119923     0.0120112    -0.0483298   -0.155072    0.00814696
 -0.162708    -0.0589899    -0.189801     0.0248443     0.148618     0.013771     -0.0130496    0.00262981   0.107473    -0.038242    0.19561     -0.00361515  -0.0900399    0.0282996   -0.139788    -0.103096     0.104319     0.0458587    -0.0685758   -0.120644     0.00105289   0.172266    -0.00355894   -0.0758694    0.155948   -0.0139004 
 -0.218974    -0.0365692     0.00401325  -0.142071      0.0294084   -0.0046487    -0.0764885   -0.0691833   -0.0568278    0.0229853  -0.0333943   -0.0199881   -0.0872724    0.00286058  -0.0350404   -0.180692    -0.149357     0.000248765   0.113005     0.0181259   -0.0110958    0.0968649    0.12565       0.109872    -0.0306475   0.0509926 
 -0.0744208   -0.03059       0.0515664   -0.0783832     0.0504589    0.259378      0.0579554    0.0987305    0.0623864   -0.0855697  -0.215004     0.041283    -0.0636596    0.0687706    0.104517     0.171048    -0.0603815   -0.0282101    -0.0852205   -0.0381464   -0.0658782   -0.122299     0.00470184   -0.0236817   -0.0478792  -0.00257703
 -0.0677016    0.217541      0.0441037    0.0706179    -0.0161371   -0.0521135     0.0342468   -0.0381344    0.17777     -0.127008    0.03154     -0.0811738   -0.0142984    0.0177215    0.0784116    0.0842389    0.061019     0.130146     -0.061803     0.156485     0.0581871    0.169184    -0.000965173   0.0701726    0.10191     0.0625323 
 -0.184001     0.0697761    -0.138276    -0.0306331    -0.133907    -0.0732137    -0.069901    -0.28369     -0.0208186    0.108027   -0.0698123   -0.0214812   -0.215053     0.0472006   -0.0798043   -0.0847798   -0.0922412   -0.00865063   -0.00277247   0.0365165   -0.162502    -0.153319    -0.0510605    -0.297271     0.0296115   0.101739  
 -0.0377809   -0.0402314     0.00256932   0.200927      0.0127229    0.0103792    -0.0528619    0.0351009    0.0846237    0.1705     -0.0386959   -0.0183826   -0.091442     0.0458751   -0.0280571   -0.0237796    0.0583463   -0.0931555     0.00550218   0.119582     0.0450148    0.0249475    0.0533176    -0.0987238    0.0646669  -0.0826958 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 18 20 32
INFO: iteration 1, average log likelihood -1.058934
WARNING: Variances had to be floored 4 12 18 20 23 31 32
INFO: iteration 2, average log likelihood -1.021674
WARNING: Variances had to be floored 4 6 12 18 20 31 32
INFO: iteration 3, average log likelihood -1.017502
WARNING: Variances had to be floored 1 4 11 12 17 18 20 24 31 32
INFO: iteration 4, average log likelihood -1.011880
WARNING: Variances had to be floored 4 6 18 20 23 30 31 32
INFO: iteration 5, average log likelihood -1.021037
WARNING: Variances had to be floored 12 18 20 31 32
INFO: iteration 6, average log likelihood -1.042138
WARNING: Variances had to be floored 4 11 12 18 20 24 31 32
INFO: iteration 7, average log likelihood -1.013259
WARNING: Variances had to be floored 4 6 18 20 23 31 32
INFO: iteration 8, average log likelihood -1.018762
WARNING: Variances had to be floored 1 4 12 17 18 20 31 32
INFO: iteration 9, average log likelihood -1.023380
WARNING: Variances had to be floored 4 6 11 18 20 24 30 31 32
INFO: iteration 10, average log likelihood -1.011858
INFO: EM with 100000 data points 10 iterations avll -1.011858
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.204433     -0.124931     0.01992     -0.00105377  -0.173248    -0.0875991   -0.141423     0.00552178   0.133127    -0.0335401   -0.111935     0.082796   -0.157162     0.108186      0.207491    -0.162307     0.111534    -0.167614     0.0409909    0.0452594     0.182997      0.0424284   -0.00116675  -0.0103676    -0.0771032  -0.207814  
  0.0981156     0.00614512   0.0720755   -0.0581842   -0.203184     0.132024    -0.0840419    0.128192    -0.075666     0.0898928   -0.0503322   -0.0957593  -0.0946204    0.122308      0.186395     0.079667    -0.0548882    0.0209058    0.104169     0.0166819     0.0449819     0.149252     0.121934    -0.0756751    -0.0673772  -0.0472005 
  0.119324     -0.128729     0.0113749    0.196483     0.07814     -0.103982     0.0735811   -0.118418     0.0172051   -0.0899197   -0.0976276    0.125482   -0.0357643   -0.0782698    -0.108386    -0.225442     0.253159    -0.0147176    0.0593903   -0.0133673    -0.0462461     0.0974099   -0.0194597    0.0779002    -0.113435   -0.204592  
 -0.101659      0.0703771   -0.154988     0.0561223    0.114165    -0.147057     0.0601584    0.22638     -0.155798    -0.060003    -0.0285531    0.130779   -0.168881    -0.0538338    -0.0726499    0.0302812    0.0434774    0.129846     0.0931411   -0.0415894     0.0990296     0.229921     0.109774     0.00513826    0.115626   -0.00857801
  0.022122     -0.14906     -0.126858     0.0437819   -0.0124304   -0.0443199    0.244601    -0.0527693   -0.0257511   -0.220773    -0.0804106   -0.0628523   0.0333518   -0.155489     -0.0138059    0.0366526    0.0807354    0.03702      0.0054445   -0.0883767     0.0845629    -0.127154     0.0798422   -0.0105318    -0.248035   -0.0671951 
 -0.0400569    -0.0625626    0.0767054    0.0436032   -0.00120712   0.0637687   -0.0632935   -0.235109    -0.0660899   -0.0139536    0.119694    -0.17316    -0.0259047    0.103534      0.0231496   -0.0706799    0.0632007    0.0676687   -0.0324866    0.0237129    -0.00534149   -0.0494769   -0.0663332   -0.0518515     0.0342332  -0.0898851 
  0.0948174     0.0898565   -0.00119124   0.0204438    0.0902717    0.00673337  -0.119612    -0.0571369    0.168454     0.0712238    0.14877     -0.0703186  -0.0720103   -0.0223789     0.00211564   0.159495    -0.1424      -0.138168    -0.0913842   -0.158584     -0.188336      0.0230599    0.172922    -0.0884694    -0.0372918   0.035316  
  0.122069      0.191706     0.0147815   -0.0352811    0.115057    -0.0672076   -0.0436415   -0.120132     0.0940199    0.0468162   -0.214431    -0.137886   -0.0145827    0.0613847    -0.0668057    0.00963432   0.15318     -0.0339976   -0.171106    -0.0290172     0.136893      0.13607      0.0484105    0.000798932   0.125713    0.133953  
 -0.0416133     0.128199    -0.151855     0.00173775  -0.0140228   -0.12818     -0.0107346   -0.0738923   -0.152323     0.0265838    0.232177     0.146715   -0.0580227   -0.103323     -0.0919507    0.0323176    0.185838     0.060631    -0.0896791   -0.0924613    -0.0881867    -0.102541    -0.139016     0.04414      -0.0636643  -0.0888362 
 -0.0457089    -0.00680141   0.0412544   -0.139621    -0.144478    -0.0175245    0.0906839   -0.0218796    0.0554888   -0.011167    -0.0436162    0.0235988   0.11101      0.136758     -0.102358     0.0165843   -0.0845715   -0.0764756    0.0362182    0.094151     -0.0885789     0.0123298    0.00653585  -0.150839      0.0669728  -0.0617208 
 -0.204655     -0.0918769    0.00876531   0.0537176    0.0121088    0.00139261  -0.100345    -0.0773908    0.0122617   -0.0383816    0.016228     0.0318539  -0.00789102   0.0379107    -0.136009    -0.0318806    0.115793     0.164399     0.0446832    0.0667315     0.0266269    -0.0267227    0.0580733   -0.0457172    -0.095385   -0.0633712 
 -0.0514773    -0.146486    -0.0701647   -0.100759    -0.0785548    0.0393036    0.0566039    0.086781     0.0198685    0.0324486    0.00941584  -0.149293    0.0784504   -0.123207      0.0337812   -0.0562553   -0.0228794    0.02928     -0.0126342    0.00113954    0.000759283  -0.058759     0.013717    -0.12655       0.108743    0.0464682 
  0.0105511    -0.131097     0.113182    -0.00429644  -0.0776083    0.00373825  -0.25075      0.0505092   -0.164341    -0.0608745    0.0268506   -0.0414142  -0.163649     0.0429327    -0.134428     0.217986     0.0988445    0.0357159   -0.0365279    0.0472051    -0.005783     -0.148731    -0.175305     0.0518419     0.0562314   0.00710327
 -0.0373602    -0.101318     0.0788121   -0.022196     0.168598     0.186778    -0.0243529   -0.0797402    0.0815686   -0.0218326   -0.134883     0.0473435   0.00384016   0.11795      -0.0356603    0.173232    -0.00854008   0.0223183    0.121344     0.00265364   -0.129789      0.101552     0.070316    -0.0911142     0.105654   -0.010387  
  0.0914634    -0.133721     0.0854654    0.102987    -0.174384     0.0344051    0.0185223    0.0073018    0.0823402   -0.12222      0.0805093    0.0273546  -0.11358     -0.0187883     0.10929      0.197852    -0.195394    -0.0744718    0.022676     0.000754228   0.0778759    -0.0646513   -0.0230558    0.112367     -0.116044   -0.120523  
  0.166762      0.0508      -0.0103721   -0.169087    -0.156852     0.0235139   -0.133863    -0.209834    -0.0258562   -0.00216863   0.0396387   -0.180982    0.0281216   -0.107211     -0.0254678   -0.154713    -0.0553234    0.0751582   -0.0444318   -0.137616     -0.0330252     0.0191311    0.0291182    0.0206711    -0.0540503  -0.0878782 
  0.0524312    -0.0248902   -0.227817     0.0949296   -0.0474578   -0.0104628   -0.00124657  -0.137963     0.00404498  -0.0579649   -0.0935015    0.133402   -0.0615674    0.005922      0.0699732    0.107093    -0.0399568    0.166696     0.130434     0.039296     -0.187365      0.158887    -0.0372267    0.0243512    -0.11431     0.0656704 
  0.00622002   -0.110703     0.0316462    0.100484     0.0731038   -0.170563    -0.0436371   -0.0255091    0.0414551   -0.0748019    0.118568    -0.128215   -0.00981823  -0.0587175     0.0789121    0.0411789    0.178265    -0.0213644    0.00508089   0.0132427     0.107797      0.0118744    0.0213871    0.062696      0.135735    0.14559   
  0.0726399     0.081601    -0.014882     0.152851     0.178104    -0.00843047  -0.123884     0.0231513   -0.11369      0.0556201   -0.0571594   -0.137751    0.243989    -0.047786     -0.157331     0.0289743    0.0947033   -0.130648    -0.354911    -0.181577     -0.139694     -0.12658      0.0628933    0.011536      0.160066    0.0268066 
  0.254289      0.02982     -0.114368     0.0260735   -0.061773    -0.139402     0.115162     0.156625    -0.0730031    0.00142581  -0.0308684    0.0741718  -0.133034     0.120533      0.0823054   -0.0575308   -0.116148     0.0742118    0.00144497  -0.116366      0.0715557    -0.087883    -0.0542231   -0.124986     -0.128176    0.0101521 
 -0.120075     -0.0437796    0.215021    -0.106893     0.0259496    0.0829831   -0.0258469    0.102358    -0.105436     0.00725473  -0.0600271    0.176687   -0.0509324   -0.141182      0.18342     -0.0260902    0.0672763   -0.0156091    0.155106    -0.169188      0.0346657    -0.0464185   -0.0479236   -0.0443308     0.0681197  -0.0235576 
 -0.229214      0.0651597   -0.00745765  -0.172428     0.107559    -0.0626884    0.278606    -0.155168    -0.0478955   -0.103543     0.0779389   -0.0520491   0.043419     0.113061      0.0908492    0.0491771    0.112802     0.0648375    0.00699058   0.0448645     0.012331      0.0637057    0.0360108    0.0435303    -0.0179875  -0.0716319 
 -0.104784     -0.247999     0.0222027   -0.0628066    0.102179     0.0776873   -0.00399071   0.065438    -0.131488     0.0863152   -0.0138951    0.0927298  -0.184527     0.0491752    -0.0989782   -0.0668117    0.0986727   -0.0082876   -0.0828832   -0.0970984     0.0425332     0.129041     0.0960347    0.0126589    -0.0743119  -0.0167108 
  0.000285294   0.0942426   -0.0846457    0.102151     0.0801392   -0.0741338   -0.0838442   -0.0181286    0.0235839   -0.167429     0.0483851   -0.0942548   0.139299     0.0280226    -0.106988    -0.0616176    0.0396394   -0.0550454   -0.0193094    0.0211689     0.0104264    -0.0388864    0.142046     0.124597      0.110994    0.00794426
 -0.0264036    -0.03888     -0.152362    -0.0798143    0.0161181    0.11917     -0.169757    -0.0782672    0.0721247   -0.0831114    0.0749394   -0.0980073  -0.0899821    0.165683      0.206025    -0.085579    -0.0737052   -0.0186042   -0.12826     -0.226109     -0.131646      0.00528037   0.124276    -0.010103     -0.0122938   0.0839865 
  0.0351866    -0.0489109   -0.00875375   0.137432    -0.0219929   -0.0483139   -0.00575295  -0.0113569   -0.0467193   -0.0370212   -0.0737586   -0.0502186  -0.00586616  -0.159741      0.0725738    0.00809722  -0.0999408    0.0215755    0.0155068    0.113804      0.138174     -0.006774     0.142101     0.150922     -0.0508772  -0.0683453 
  0.122361      0.057415     0.107393     0.240496    -0.271637    -0.0298522   -0.0435935    0.0377538   -0.111578    -0.0290639    0.041601     0.100202   -0.0652901   -0.0729076     0.0192208    0.151344     0.0822571   -0.0761005    0.0656088    0.0256475     0.0293381    -0.13995     -0.0341572   -0.063922      0.0484242   0.113413  
  0.000707099   0.0345464    0.125144    -0.157155     0.057649    -0.0404263    0.11118      0.0770131    0.0164127    0.0861894    0.128317     0.0187508  -0.0427101    0.0356221    -0.018136    -0.0216362   -0.0645788    0.00848263  -0.259753    -0.0063288    -0.0505026     0.00257901   0.041749    -0.0308384     0.147045   -0.30562   
  0.0655963     0.00715822  -0.0731305   -0.081746    -0.0149602   -0.256445     0.0670783    0.0721262    0.00772561   0.037228     0.0590811   -0.0239512  -0.0939748   -0.0419571    -0.128737    -0.070697    -0.0566141   -0.038936     0.00804823   0.136967      0.110399     -0.124156     0.0135062   -0.0609448    -0.0061723   0.0469486 
  0.0769556    -0.00685884  -0.0679325    0.0940874   -0.131582     0.0659925    0.0890703    0.130752    -0.0828094   -0.00785045  -0.153958    -0.130592   -0.0365322   -0.000323157  -0.103648    -0.00907378   0.0488283   -0.173762     0.0340021    0.0141121    -0.041952      0.21661     -0.119408     0.0704981     0.0592156  -0.0863868 
 -0.188777     -0.135548     0.0534101    0.0738378   -0.0346182    0.022138    -0.187404    -0.0247723    0.0254489    0.052898     0.036406     0.180123    0.0593822    0.22291      -0.163119    -0.107363     0.0354755    0.0539266   -0.0535716   -0.00900434   -0.123479      0.0867798    0.0457441    0.0393701    -0.0484891   0.171859  
  0.00167286   -0.0336314   -0.0815106   -0.1411       0.0725873    0.00309867  -0.0298206   -0.0591624   -0.134273     0.00908837  -0.00730375  -0.0251416   0.0253488   -0.0255601     0.130929    -0.00231697  -0.0888453    0.181745    -0.0058711   -0.0270234     0.0978612     0.0820298   -0.0820758    0.164535     -0.0409707  -0.092634  kind full, method split
0: avll = -1.4234998176035214
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.423519
INFO: iteration 2, average log likelihood -1.423441
INFO: iteration 3, average log likelihood -1.423380
INFO: iteration 4, average log likelihood -1.423308
INFO: iteration 5, average log likelihood -1.423222
INFO: iteration 6, average log likelihood -1.423124
INFO: iteration 7, average log likelihood -1.423022
INFO: iteration 8, average log likelihood -1.422924
INFO: iteration 9, average log likelihood -1.422836
INFO: iteration 10, average log likelihood -1.422754
INFO: iteration 11, average log likelihood -1.422660
INFO: iteration 12, average log likelihood -1.422515
INFO: iteration 13, average log likelihood -1.422257
INFO: iteration 14, average log likelihood -1.421792
INFO: iteration 15, average log likelihood -1.421046
INFO: iteration 16, average log likelihood -1.420073
INFO: iteration 17, average log likelihood -1.419137
INFO: iteration 18, average log likelihood -1.418492
INFO: iteration 19, average log likelihood -1.418152
INFO: iteration 20, average log likelihood -1.417997
INFO: iteration 21, average log likelihood -1.417929
INFO: iteration 22, average log likelihood -1.417900
INFO: iteration 23, average log likelihood -1.417887
INFO: iteration 24, average log likelihood -1.417881
INFO: iteration 25, average log likelihood -1.417879
INFO: iteration 26, average log likelihood -1.417877
INFO: iteration 27, average log likelihood -1.417877
INFO: iteration 28, average log likelihood -1.417876
INFO: iteration 29, average log likelihood -1.417876
INFO: iteration 30, average log likelihood -1.417876
INFO: iteration 31, average log likelihood -1.417876
INFO: iteration 32, average log likelihood -1.417875
INFO: iteration 33, average log likelihood -1.417875
INFO: iteration 34, average log likelihood -1.417875
INFO: iteration 35, average log likelihood -1.417875
INFO: iteration 36, average log likelihood -1.417875
INFO: iteration 37, average log likelihood -1.417875
INFO: iteration 38, average log likelihood -1.417875
INFO: iteration 39, average log likelihood -1.417875
INFO: iteration 40, average log likelihood -1.417875
INFO: iteration 41, average log likelihood -1.417875
INFO: iteration 42, average log likelihood -1.417875
INFO: iteration 43, average log likelihood -1.417875
INFO: iteration 44, average log likelihood -1.417874
INFO: iteration 45, average log likelihood -1.417874
INFO: iteration 46, average log likelihood -1.417874
INFO: iteration 47, average log likelihood -1.417874
INFO: iteration 48, average log likelihood -1.417874
INFO: iteration 49, average log likelihood -1.417874
INFO: iteration 50, average log likelihood -1.417874
INFO: EM with 100000 data points 50 iterations avll -1.417874
952.4 data points per parameter
1: avll = [-1.42352,-1.42344,-1.42338,-1.42331,-1.42322,-1.42312,-1.42302,-1.42292,-1.42284,-1.42275,-1.42266,-1.42252,-1.42226,-1.42179,-1.42105,-1.42007,-1.41914,-1.41849,-1.41815,-1.418,-1.41793,-1.4179,-1.41789,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417890
INFO: iteration 2, average log likelihood -1.417818
INFO: iteration 3, average log likelihood -1.417758
INFO: iteration 4, average log likelihood -1.417688
INFO: iteration 5, average log likelihood -1.417602
INFO: iteration 6, average log likelihood -1.417500
INFO: iteration 7, average log likelihood -1.417386
INFO: iteration 8, average log likelihood -1.417269
INFO: iteration 9, average log likelihood -1.417158
INFO: iteration 10, average log likelihood -1.417064
INFO: iteration 11, average log likelihood -1.416989
INFO: iteration 12, average log likelihood -1.416934
INFO: iteration 13, average log likelihood -1.416893
INFO: iteration 14, average log likelihood -1.416863
INFO: iteration 15, average log likelihood -1.416839
INFO: iteration 16, average log likelihood -1.416819
INFO: iteration 17, average log likelihood -1.416802
INFO: iteration 18, average log likelihood -1.416786
INFO: iteration 19, average log likelihood -1.416770
INFO: iteration 20, average log likelihood -1.416755
INFO: iteration 21, average log likelihood -1.416740
INFO: iteration 22, average log likelihood -1.416725
INFO: iteration 23, average log likelihood -1.416710
INFO: iteration 24, average log likelihood -1.416696
INFO: iteration 25, average log likelihood -1.416683
INFO: iteration 26, average log likelihood -1.416669
INFO: iteration 27, average log likelihood -1.416657
INFO: iteration 28, average log likelihood -1.416646
INFO: iteration 29, average log likelihood -1.416635
INFO: iteration 30, average log likelihood -1.416625
INFO: iteration 31, average log likelihood -1.416616
INFO: iteration 32, average log likelihood -1.416608
INFO: iteration 33, average log likelihood -1.416601
INFO: iteration 34, average log likelihood -1.416595
INFO: iteration 35, average log likelihood -1.416589
INFO: iteration 36, average log likelihood -1.416584
INFO: iteration 37, average log likelihood -1.416580
INFO: iteration 38, average log likelihood -1.416576
INFO: iteration 39, average log likelihood -1.416572
INFO: iteration 40, average log likelihood -1.416569
INFO: iteration 41, average log likelihood -1.416567
INFO: iteration 42, average log likelihood -1.416564
INFO: iteration 43, average log likelihood -1.416562
INFO: iteration 44, average log likelihood -1.416560
INFO: iteration 45, average log likelihood -1.416559
INFO: iteration 46, average log likelihood -1.416557
INFO: iteration 47, average log likelihood -1.416556
INFO: iteration 48, average log likelihood -1.416555
INFO: iteration 49, average log likelihood -1.416553
INFO: iteration 50, average log likelihood -1.416552
INFO: EM with 100000 data points 50 iterations avll -1.416552
473.9 data points per parameter
2: avll = [-1.41789,-1.41782,-1.41776,-1.41769,-1.4176,-1.4175,-1.41739,-1.41727,-1.41716,-1.41706,-1.41699,-1.41693,-1.41689,-1.41686,-1.41684,-1.41682,-1.4168,-1.41679,-1.41677,-1.41675,-1.41674,-1.41673,-1.41671,-1.4167,-1.41668,-1.41667,-1.41666,-1.41665,-1.41663,-1.41663,-1.41662,-1.41661,-1.4166,-1.41659,-1.41659,-1.41658,-1.41658,-1.41658,-1.41657,-1.41657,-1.41657,-1.41656,-1.41656,-1.41656,-1.41656,-1.41656,-1.41656,-1.41655,-1.41655,-1.41655]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416562
INFO: iteration 2, average log likelihood -1.416503
INFO: iteration 3, average log likelihood -1.416450
INFO: iteration 4, average log likelihood -1.416389
INFO: iteration 5, average log likelihood -1.416314
INFO: iteration 6, average log likelihood -1.416225
INFO: iteration 7, average log likelihood -1.416123
INFO: iteration 8, average log likelihood -1.416014
INFO: iteration 9, average log likelihood -1.415904
INFO: iteration 10, average log likelihood -1.415800
INFO: iteration 11, average log likelihood -1.415704
INFO: iteration 12, average log likelihood -1.415617
INFO: iteration 13, average log likelihood -1.415540
INFO: iteration 14, average log likelihood -1.415473
INFO: iteration 15, average log likelihood -1.415414
INFO: iteration 16, average log likelihood -1.415364
INFO: iteration 17, average log likelihood -1.415322
INFO: iteration 18, average log likelihood -1.415286
INFO: iteration 19, average log likelihood -1.415256
INFO: iteration 20, average log likelihood -1.415229
INFO: iteration 21, average log likelihood -1.415206
INFO: iteration 22, average log likelihood -1.415186
INFO: iteration 23, average log likelihood -1.415167
INFO: iteration 24, average log likelihood -1.415150
INFO: iteration 25, average log likelihood -1.415134
INFO: iteration 26, average log likelihood -1.415120
INFO: iteration 27, average log likelihood -1.415106
INFO: iteration 28, average log likelihood -1.415093
INFO: iteration 29, average log likelihood -1.415080
INFO: iteration 30, average log likelihood -1.415069
INFO: iteration 31, average log likelihood -1.415057
INFO: iteration 32, average log likelihood -1.415046
INFO: iteration 33, average log likelihood -1.415036
INFO: iteration 34, average log likelihood -1.415026
INFO: iteration 35, average log likelihood -1.415016
INFO: iteration 36, average log likelihood -1.415006
INFO: iteration 37, average log likelihood -1.414996
INFO: iteration 38, average log likelihood -1.414987
INFO: iteration 39, average log likelihood -1.414978
INFO: iteration 40, average log likelihood -1.414969
INFO: iteration 41, average log likelihood -1.414960
INFO: iteration 42, average log likelihood -1.414951
INFO: iteration 43, average log likelihood -1.414942
INFO: iteration 44, average log likelihood -1.414934
INFO: iteration 45, average log likelihood -1.414925
INFO: iteration 46, average log likelihood -1.414917
INFO: iteration 47, average log likelihood -1.414909
INFO: iteration 48, average log likelihood -1.414901
INFO: iteration 49, average log likelihood -1.414894
INFO: iteration 50, average log likelihood -1.414886
INFO: EM with 100000 data points 50 iterations avll -1.414886
236.4 data points per parameter
3: avll = [-1.41656,-1.4165,-1.41645,-1.41639,-1.41631,-1.41622,-1.41612,-1.41601,-1.4159,-1.4158,-1.4157,-1.41562,-1.41554,-1.41547,-1.41541,-1.41536,-1.41532,-1.41529,-1.41526,-1.41523,-1.41521,-1.41519,-1.41517,-1.41515,-1.41513,-1.41512,-1.41511,-1.41509,-1.41508,-1.41507,-1.41506,-1.41505,-1.41504,-1.41503,-1.41502,-1.41501,-1.415,-1.41499,-1.41498,-1.41497,-1.41496,-1.41495,-1.41494,-1.41493,-1.41493,-1.41492,-1.41491,-1.4149,-1.41489,-1.41489]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414888
INFO: iteration 2, average log likelihood -1.414828
INFO: iteration 3, average log likelihood -1.414771
INFO: iteration 4, average log likelihood -1.414706
INFO: iteration 5, average log likelihood -1.414625
INFO: iteration 6, average log likelihood -1.414526
INFO: iteration 7, average log likelihood -1.414409
INFO: iteration 8, average log likelihood -1.414277
INFO: iteration 9, average log likelihood -1.414135
INFO: iteration 10, average log likelihood -1.413993
INFO: iteration 11, average log likelihood -1.413857
INFO: iteration 12, average log likelihood -1.413733
INFO: iteration 13, average log likelihood -1.413623
INFO: iteration 14, average log likelihood -1.413528
INFO: iteration 15, average log likelihood -1.413448
INFO: iteration 16, average log likelihood -1.413380
INFO: iteration 17, average log likelihood -1.413324
INFO: iteration 18, average log likelihood -1.413276
INFO: iteration 19, average log likelihood -1.413237
INFO: iteration 20, average log likelihood -1.413203
INFO: iteration 21, average log likelihood -1.413173
INFO: iteration 22, average log likelihood -1.413147
INFO: iteration 23, average log likelihood -1.413124
INFO: iteration 24, average log likelihood -1.413104
INFO: iteration 25, average log likelihood -1.413085
INFO: iteration 26, average log likelihood -1.413068
INFO: iteration 27, average log likelihood -1.413052
INFO: iteration 28, average log likelihood -1.413037
INFO: iteration 29, average log likelihood -1.413023
INFO: iteration 30, average log likelihood -1.413010
INFO: iteration 31, average log likelihood -1.412998
INFO: iteration 32, average log likelihood -1.412986
INFO: iteration 33, average log likelihood -1.412975
INFO: iteration 34, average log likelihood -1.412965
INFO: iteration 35, average log likelihood -1.412955
INFO: iteration 36, average log likelihood -1.412946
INFO: iteration 37, average log likelihood -1.412937
INFO: iteration 38, average log likelihood -1.412929
INFO: iteration 39, average log likelihood -1.412921
INFO: iteration 40, average log likelihood -1.412913
INFO: iteration 41, average log likelihood -1.412905
INFO: iteration 42, average log likelihood -1.412898
INFO: iteration 43, average log likelihood -1.412892
INFO: iteration 44, average log likelihood -1.412885
INFO: iteration 45, average log likelihood -1.412878
INFO: iteration 46, average log likelihood -1.412872
INFO: iteration 47, average log likelihood -1.412866
INFO: iteration 48, average log likelihood -1.412861
INFO: iteration 49, average log likelihood -1.412855
INFO: iteration 50, average log likelihood -1.412849
INFO: EM with 100000 data points 50 iterations avll -1.412849
118.1 data points per parameter
4: avll = [-1.41489,-1.41483,-1.41477,-1.41471,-1.41463,-1.41453,-1.41441,-1.41428,-1.41414,-1.41399,-1.41386,-1.41373,-1.41362,-1.41353,-1.41345,-1.41338,-1.41332,-1.41328,-1.41324,-1.4132,-1.41317,-1.41315,-1.41312,-1.4131,-1.41308,-1.41307,-1.41305,-1.41304,-1.41302,-1.41301,-1.413,-1.41299,-1.41298,-1.41297,-1.41296,-1.41295,-1.41294,-1.41293,-1.41292,-1.41291,-1.41291,-1.4129,-1.41289,-1.41288,-1.41288,-1.41287,-1.41287,-1.41286,-1.41285,-1.41285]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412852
INFO: iteration 2, average log likelihood -1.412793
INFO: iteration 3, average log likelihood -1.412737
INFO: iteration 4, average log likelihood -1.412672
INFO: iteration 5, average log likelihood -1.412593
INFO: iteration 6, average log likelihood -1.412492
INFO: iteration 7, average log likelihood -1.412369
INFO: iteration 8, average log likelihood -1.412224
INFO: iteration 9, average log likelihood -1.412064
INFO: iteration 10, average log likelihood -1.411897
INFO: iteration 11, average log likelihood -1.411734
INFO: iteration 12, average log likelihood -1.411583
INFO: iteration 13, average log likelihood -1.411448
INFO: iteration 14, average log likelihood -1.411330
INFO: iteration 15, average log likelihood -1.411228
INFO: iteration 16, average log likelihood -1.411140
INFO: iteration 17, average log likelihood -1.411063
INFO: iteration 18, average log likelihood -1.410994
INFO: iteration 19, average log likelihood -1.410933
INFO: iteration 20, average log likelihood -1.410877
INFO: iteration 21, average log likelihood -1.410826
INFO: iteration 22, average log likelihood -1.410779
INFO: iteration 23, average log likelihood -1.410734
INFO: iteration 24, average log likelihood -1.410693
INFO: iteration 25, average log likelihood -1.410654
INFO: iteration 26, average log likelihood -1.410616
INFO: iteration 27, average log likelihood -1.410581
INFO: iteration 28, average log likelihood -1.410547
INFO: iteration 29, average log likelihood -1.410514
INFO: iteration 30, average log likelihood -1.410483
INFO: iteration 31, average log likelihood -1.410452
INFO: iteration 32, average log likelihood -1.410423
INFO: iteration 33, average log likelihood -1.410395
INFO: iteration 34, average log likelihood -1.410369
INFO: iteration 35, average log likelihood -1.410343
INFO: iteration 36, average log likelihood -1.410318
INFO: iteration 37, average log likelihood -1.410294
INFO: iteration 38, average log likelihood -1.410271
INFO: iteration 39, average log likelihood -1.410249
INFO: iteration 40, average log likelihood -1.410229
INFO: iteration 41, average log likelihood -1.410208
INFO: iteration 42, average log likelihood -1.410189
INFO: iteration 43, average log likelihood -1.410171
INFO: iteration 44, average log likelihood -1.410153
INFO: iteration 45, average log likelihood -1.410137
INFO: iteration 46, average log likelihood -1.410121
INFO: iteration 47, average log likelihood -1.410105
INFO: iteration 48, average log likelihood -1.410091
INFO: iteration 49, average log likelihood -1.410077
INFO: iteration 50, average log likelihood -1.410064
INFO: EM with 100000 data points 50 iterations avll -1.410064
59.0 data points per parameter
5: avll = [-1.41285,-1.41279,-1.41274,-1.41267,-1.41259,-1.41249,-1.41237,-1.41222,-1.41206,-1.4119,-1.41173,-1.41158,-1.41145,-1.41133,-1.41123,-1.41114,-1.41106,-1.41099,-1.41093,-1.41088,-1.41083,-1.41078,-1.41073,-1.41069,-1.41065,-1.41062,-1.41058,-1.41055,-1.41051,-1.41048,-1.41045,-1.41042,-1.4104,-1.41037,-1.41034,-1.41032,-1.41029,-1.41027,-1.41025,-1.41023,-1.41021,-1.41019,-1.41017,-1.41015,-1.41014,-1.41012,-1.41011,-1.41009,-1.41008,-1.41006]
[-1.4235,-1.42352,-1.42344,-1.42338,-1.42331,-1.42322,-1.42312,-1.42302,-1.42292,-1.42284,-1.42275,-1.42266,-1.42252,-1.42226,-1.42179,-1.42105,-1.42007,-1.41914,-1.41849,-1.41815,-1.418,-1.41793,-1.4179,-1.41789,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41788,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41787,-1.41789,-1.41782,-1.41776,-1.41769,-1.4176,-1.4175,-1.41739,-1.41727,-1.41716,-1.41706,-1.41699,-1.41693,-1.41689,-1.41686,-1.41684,-1.41682,-1.4168,-1.41679,-1.41677,-1.41675,-1.41674,-1.41673,-1.41671,-1.4167,-1.41668,-1.41667,-1.41666,-1.41665,-1.41663,-1.41663,-1.41662,-1.41661,-1.4166,-1.41659,-1.41659,-1.41658,-1.41658,-1.41658,-1.41657,-1.41657,-1.41657,-1.41656,-1.41656,-1.41656,-1.41656,-1.41656,-1.41656,-1.41655,-1.41655,-1.41655,-1.41656,-1.4165,-1.41645,-1.41639,-1.41631,-1.41622,-1.41612,-1.41601,-1.4159,-1.4158,-1.4157,-1.41562,-1.41554,-1.41547,-1.41541,-1.41536,-1.41532,-1.41529,-1.41526,-1.41523,-1.41521,-1.41519,-1.41517,-1.41515,-1.41513,-1.41512,-1.41511,-1.41509,-1.41508,-1.41507,-1.41506,-1.41505,-1.41504,-1.41503,-1.41502,-1.41501,-1.415,-1.41499,-1.41498,-1.41497,-1.41496,-1.41495,-1.41494,-1.41493,-1.41493,-1.41492,-1.41491,-1.4149,-1.41489,-1.41489,-1.41489,-1.41483,-1.41477,-1.41471,-1.41463,-1.41453,-1.41441,-1.41428,-1.41414,-1.41399,-1.41386,-1.41373,-1.41362,-1.41353,-1.41345,-1.41338,-1.41332,-1.41328,-1.41324,-1.4132,-1.41317,-1.41315,-1.41312,-1.4131,-1.41308,-1.41307,-1.41305,-1.41304,-1.41302,-1.41301,-1.413,-1.41299,-1.41298,-1.41297,-1.41296,-1.41295,-1.41294,-1.41293,-1.41292,-1.41291,-1.41291,-1.4129,-1.41289,-1.41288,-1.41288,-1.41287,-1.41287,-1.41286,-1.41285,-1.41285,-1.41285,-1.41279,-1.41274,-1.41267,-1.41259,-1.41249,-1.41237,-1.41222,-1.41206,-1.4119,-1.41173,-1.41158,-1.41145,-1.41133,-1.41123,-1.41114,-1.41106,-1.41099,-1.41093,-1.41088,-1.41083,-1.41078,-1.41073,-1.41069,-1.41065,-1.41062,-1.41058,-1.41055,-1.41051,-1.41048,-1.41045,-1.41042,-1.4104,-1.41037,-1.41034,-1.41032,-1.41029,-1.41027,-1.41025,-1.41023,-1.41021,-1.41019,-1.41017,-1.41015,-1.41014,-1.41012,-1.41011,-1.41009,-1.41008,-1.41006]
32×26 Array{Float64,2}:
  0.364479     0.00937164  -0.137093    -0.473339    0.111924    0.218743   -0.397259    -0.449472   -1.08386    -0.718929    0.257725    -0.389471    0.430481   -0.247229    0.459983   -0.225742    -0.284225   -0.208073     0.310633    -0.098883     0.254007   -0.402717     0.461912   -0.249085     0.230089     0.513044 
 -0.42949     -0.216414     0.436124    -0.302711    0.141911    0.379399   -0.268759     0.123973   -0.0589886   0.0239241   0.271814    -0.145631    0.240212   -0.27508     0.163769   -0.133595    -0.703605   -0.0765507    0.32482      0.269786     0.633294   -0.260693     0.145748   -0.0545929   -0.230668     0.57498  
  0.00512508  -0.302427    -0.0715721   -0.0498568  -0.400118   -0.127616   -0.740439    -0.489157   -0.139653   -0.190172    0.351035    -0.357255    0.878455    0.26791    -0.327431    0.0180346   -0.233593   -0.18814     -0.0681634   -0.193738    -0.372653   -0.059884     0.118961    0.117207     0.357282     0.296706 
  0.340087    -0.412926     0.534211    -0.0351474   0.779917   -0.229077    0.202932    -0.079181    0.357118    0.326399    0.104106    -0.194532    0.398213    0.279489   -0.259446   -0.587633    -0.487296   -0.166681    -0.240548    -0.597055    -0.167538    0.229016     0.429417   -0.0599699    0.393008    -0.0756724
 -0.0495082    0.0361276    0.0391107    0.0541427   0.0602845   0.0820219   0.115371    -0.136721   -0.0581109  -0.403623   -0.0429871    0.0454652  -0.0724536  -0.0338666  -0.0573027  -0.333554     0.164105   -0.0525555    0.137935    -0.172775     0.041843    0.0603058    0.235495   -0.0263893   -0.0801665   -0.03953  
 -0.234447     0.0697987   -0.203776    -0.0989028  -0.304191    0.109866   -0.250697     0.0255386   0.0541048  -0.376039   -0.165844    -0.0119153  -0.291457   -0.147635    0.0935534   0.379191    -0.0415932   0.086751    -0.00439074   0.150594    -0.095989    0.0673317   -0.154673   -0.186791     0.0772673    0.184212 
  0.267857    -0.0576323   -0.302048     0.117808   -0.0400711  -0.152167    0.125394    -0.0837683  -0.0829929   0.324338   -0.0253567   -0.139938    0.0311539  -0.043621   -0.106201    0.050123     0.375713    0.222476    -0.453384    -0.357356    -0.223845   -0.0279197   -0.0971537   0.00499509   0.0853145   -0.140472 
 -0.00996406  -0.0707652    0.34636      0.072608   -0.0841226  -0.093749    0.00663169   0.0252698   0.219942    0.295305   -0.0087944    0.0936044  -0.20381     0.193528   -0.129862    0.259112    -0.233503   -0.0987826    0.0384739    0.372994     0.0789794  -0.0790577   -0.199987    0.0401181   -0.0905549   -0.202473 
  0.614018    -0.345639    -0.324939     0.113418    0.386193   -0.271601   -0.105528    -0.222699   -0.256756   -0.200782   -0.334536     0.167005    0.504741    0.0751667  -0.111279   -0.240472     0.56616    -0.0972546   -0.0137469    0.354415    -0.252153    0.455478    -0.533075    0.298073    -0.0536891   -0.339032 
  0.160143    -0.0486883   -0.419886    -0.0187623   0.175084    0.0362579   0.0696089    0.0382412  -0.58854    -0.361608   -0.142949    -0.117899    0.318668   -0.401236    0.0784096  -0.675542     0.815976    0.180069     0.150308    -0.676865    -0.114161    0.182257     0.692452    0.0135816    0.546801    -0.0292732
  0.217666     0.588658    -0.326256    -0.76787    -0.277367   -0.0861107  -0.177607     0.170581    0.510672   -0.0473462   0.497913    -0.240248   -0.0862577  -0.0079072  -0.230211    0.419212     0.519656    0.0503386    0.287922    -0.216847    -0.0672011   0.433276     0.0334785   0.10088      0.0720009    0.44662  
  0.14188      0.48578     -0.644729     0.35969     0.453987   -0.0669692   0.206984    -0.193299    0.305675    0.245929    0.390219    -0.283889    0.203328    0.430335   -0.353761    0.00982662   0.411415    0.0171537    0.189688    -0.105652     0.553778   -0.605113    -0.21558    -0.193533     0.0515193    0.385104 
  0.122501    -0.0615143   -0.284772    -0.201655   -0.304121   -0.0473507   0.0431174    0.165169   -0.504155    0.413897    0.685325    -0.0524472   0.0742795  -0.191136    0.802457   -0.0887101    0.0951793  -0.416379     0.153058    -0.109738    -0.120165   -0.0885138   -0.475785    0.492667    -0.326962     0.0901734
  0.39797      0.0389612    0.0490061   -0.330709    0.146886   -0.0808366   0.211364     0.253822   -0.588949    0.683892    0.094052    -0.251274    0.241879   -0.0125097   0.345131    0.168289     0.166764   -0.11581      0.151566    -0.00993324   0.529567    0.0613114    0.686988    0.0475747    0.00561063  -0.365148 
 -0.122869     0.361682    -0.00570881  -0.213824    0.778118    0.479978   -0.232278     1.1819     -0.0665775   0.105984    0.297428     0.0241186  -0.405868    0.0893206   0.357251    0.0386022   -0.197584   -0.548271     0.0607124   -0.0624934   -0.380667    0.486833     0.10023    -0.301795    -0.0337573   -0.303529 
  0.329396     0.283071    -0.0391597    0.0154054   0.549809    0.216906    0.638507     0.43841    -0.0542453   0.396576   -0.248456     0.492786   -0.651339   -0.209814    0.0547204  -0.0284808    0.295407    0.260583    -0.120545     0.0223584    0.274443    0.0758594    0.290697   -0.138681    -0.223955    -0.662498 
 -0.528835     0.138011     0.238473    -0.0413106  -0.249169    0.19956    -0.337519    -0.0702694   0.183642   -0.397944   -0.498393     0.0568528  -0.529137    0.265791   -0.35535     0.687306    -0.131033    0.184636     0.229593     0.41014      0.462909   -0.182223     0.317108   -0.336986     0.193327     0.302066 
 -0.692342     0.389353     0.166181     0.209241   -0.0402775  -0.36418     0.347008     0.347304    0.0405959  -0.551154   -0.311247     0.686286   -0.217205   -0.0966873   0.0837616  -0.0254284    0.0529897   0.20503      0.134269     0.178553     0.1508      0.0397887   -0.130483    0.0427139    0.0136786    0.483726 
 -0.108971    -0.25009      0.530427    -0.160195   -0.992166   -0.232795   -0.360104     0.182616   -0.176731    0.0823176   0.015027     0.08148    -0.16778    -0.0780942   0.822443    0.401059    -0.423235   -0.158409    -0.294653     0.206764    -0.0937519  -0.4998      -0.391204   -0.337304    -0.135284    -0.597052 
 -0.146796    -0.254764     0.529646     0.453641   -0.375604   -0.360811    0.48932      0.402929    0.42042     0.484175    0.00237434  -0.118875   -0.726125    0.234769   -0.221486    0.513837    -0.200801    0.00260537  -0.150248     0.195805    -0.077538   -0.0443424   -0.523196    0.0182475   -0.431121    -0.648864 
 -0.0498192   -0.149952    -0.0429945    0.195575    0.178962    0.0117239  -0.135877    -0.380437    0.592654   -0.130389   -0.0933369    0.329683   -0.215328    0.709397   -0.108352   -0.488026    -0.305664   -0.230339    -0.408602     0.351513    -0.360081    0.630073    -0.33584     0.717289    -0.0443956   -0.24039  
  0.0462381    0.181401     0.217361    -0.197992    0.298701    0.485424   -0.553446    -0.580814    0.704195   -0.0220595   0.471163     0.481982   -0.0828263   0.890236   -0.120498    0.370985    -0.456614   -0.254501     0.38214      0.151049     0.011929   -0.208948    -0.220334    0.451126    -0.368658    -0.0684335
 -0.0850797   -0.0294609   -0.688556    -0.0733711   0.154182   -0.494372    0.215801     0.116298   -0.0925008   0.708724    0.391672     0.433306    0.668695    0.179234    0.146577   -0.128452    -0.736692    0.228369    -0.783873    -0.00735372   0.191329    0.288693    -0.403965    0.345268     0.222264     0.236875 
 -0.462456     0.0609442    0.245502     0.131054   -0.649002   -0.630232   -0.516486    -0.0615692  -0.0523734  -0.20369     0.425049     0.127144    1.27809     0.785082    0.280282   -0.278923    -0.855652    0.202616    -0.156574    -0.123059     0.303003    0.302652    -0.0623612  -0.655945    -0.320929     1.21371  
  0.0999385   -0.797183     0.245578     0.0342676  -0.75324    -0.427551    0.299451    -0.805465   -0.0408643  -0.0287438  -0.473358    -0.196942    0.48239    -0.450887   -0.353354   -0.0562135    0.151063    0.927954    -0.0748672   -0.0307644    0.0741336  -0.119059     0.433567    0.0888785    0.484374     0.0815615
 -0.266086    -0.308977     0.30582      0.529433    0.0537742   0.040392    0.0119249   -0.533177    0.062957   -0.198898   -0.497029     0.0144487  -0.241894   -0.553555   -0.736771   -0.388269    -0.188509    0.3696      -0.180707     0.394426     0.164053   -0.561482    -0.12351    -0.53214      0.161592    -0.111426 
  0.224524    -0.110251     0.310094     0.0889367   0.0462128   0.336079   -0.353488    -0.676397   -0.214728   -0.114614   -0.0207143   -0.616169   -0.891049   -0.265193   -0.447408    0.131413     0.876712   -0.711349     0.170749    -0.603544    -0.673204   -0.74996      0.137422   -0.0887706   -0.206812    -0.530277 
 -0.144904    -0.761304     0.394075     0.708763   -0.132504    0.465546   -0.272306    -0.133678   -0.286394   -0.648111   -0.428025     0.143028   -0.0737401  -0.213793    0.36618    -0.247548    -0.119505    0.259466    -0.041745    -0.286492    -0.511084   -0.056046     0.426827   -0.147495    -0.333977    -0.349653 
 -0.0928603    0.0201086   -0.672406     0.190528   -0.704989   -0.357225    0.530687     0.219131   -0.564759   -0.0352692  -0.182344    -0.93197    -0.550995   -0.387315    0.21369     0.355185     0.353775   -0.0773818   -0.372157    -0.248416    -0.0636274   0.361398     0.171717    0.0643591   -0.23841     -0.0839242
  0.0881342    0.327688    -0.635213     0.330911   -0.406529    0.119481   -0.156146     0.108034    0.491093   -0.30509    -0.154233    -0.126311   -0.272366   -0.164947    0.192731    0.257329     0.634396    0.274168    -0.207605    -0.0434177   -0.813991    0.345539    -0.741881    0.111729     0.299452     0.264062 
  0.500065    -0.140072    -0.0992385   -0.35795     0.291959    0.229425    0.0433003   -0.149008   -0.439893    0.563911    0.236889    -0.400334    0.304823   -0.073514   -0.155294   -0.186805     0.277587   -0.156277    -0.196454    -0.512716     0.24096    -0.00435669   0.281392    0.18568     -0.178045    -0.371783 
 -0.222956     0.0349688    0.164574     0.0321825   0.143624   -0.103341    0.0218374    0.152441    0.0629665  -0.19058    -0.00105701   0.436204   -0.0375711   0.0732644   0.0291449  -0.113253    -0.242453   -0.0487646    0.200522     0.284221     0.112936    0.00477141   0.0269708  -0.0130664   -0.0128726    0.052472 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410051
INFO: iteration 2, average log likelihood -1.410039
INFO: iteration 3, average log likelihood -1.410027
INFO: iteration 4, average log likelihood -1.410016
INFO: iteration 5, average log likelihood -1.410006
INFO: iteration 6, average log likelihood -1.409996
INFO: iteration 7, average log likelihood -1.409986
INFO: iteration 8, average log likelihood -1.409977
INFO: iteration 9, average log likelihood -1.409968
INFO: iteration 10, average log likelihood -1.409960
INFO: EM with 100000 data points 10 iterations avll -1.409960
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.918328e+05
      1       7.078898e+05      -1.839429e+05 |       32
      2       6.921717e+05      -1.571813e+04 |       32
      3       6.859612e+05      -6.210526e+03 |       32
      4       6.830220e+05      -2.939148e+03 |       32
      5       6.812231e+05      -1.798865e+03 |       32
      6       6.799538e+05      -1.269319e+03 |       32
      7       6.790161e+05      -9.377741e+02 |       32
      8       6.782956e+05      -7.204670e+02 |       32
      9       6.777127e+05      -5.828907e+02 |       32
     10       6.772248e+05      -4.878503e+02 |       32
     11       6.768069e+05      -4.178975e+02 |       32
     12       6.764215e+05      -3.854768e+02 |       32
     13       6.760988e+05      -3.226538e+02 |       32
     14       6.758040e+05      -2.948128e+02 |       32
     15       6.755516e+05      -2.524429e+02 |       32
     16       6.753315e+05      -2.200864e+02 |       32
     17       6.751312e+05      -2.003232e+02 |       32
     18       6.749353e+05      -1.958359e+02 |       32
     19       6.747700e+05      -1.652813e+02 |       32
     20       6.746242e+05      -1.458096e+02 |       32
     21       6.744841e+05      -1.401691e+02 |       32
     22       6.743519e+05      -1.321803e+02 |       32
     23       6.742333e+05      -1.185332e+02 |       32
     24       6.741348e+05      -9.850888e+01 |       32
     25       6.740522e+05      -8.259537e+01 |       32
     26       6.739670e+05      -8.525368e+01 |       32
     27       6.738907e+05      -7.625624e+01 |       32
     28       6.738288e+05      -6.194574e+01 |       32
     29       6.737703e+05      -5.845528e+01 |       32
     30       6.737199e+05      -5.044641e+01 |       32
     31       6.736801e+05      -3.982372e+01 |       32
     32       6.736470e+05      -3.306935e+01 |       32
     33       6.736176e+05      -2.942553e+01 |       32
     34       6.735895e+05      -2.806816e+01 |       32
     35       6.735668e+05      -2.273924e+01 |       32
     36       6.735463e+05      -2.046648e+01 |       32
     37       6.735248e+05      -2.146377e+01 |       32
     38       6.735029e+05      -2.189780e+01 |       32
     39       6.734800e+05      -2.291669e+01 |       32
     40       6.734539e+05      -2.607760e+01 |       32
     41       6.734290e+05      -2.492668e+01 |       32
     42       6.734031e+05      -2.592977e+01 |       32
     43       6.733745e+05      -2.861951e+01 |       32
     44       6.733483e+05      -2.615783e+01 |       32
     45       6.733190e+05      -2.929306e+01 |       32
     46       6.732904e+05      -2.860098e+01 |       32
     47       6.732621e+05      -2.827842e+01 |       32
     48       6.732369e+05      -2.518776e+01 |       32
     49       6.732145e+05      -2.244117e+01 |       32
     50       6.731916e+05      -2.286198e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 673191.6350324771)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.422294
INFO: iteration 2, average log likelihood -1.417265
INFO: iteration 3, average log likelihood -1.415850
INFO: iteration 4, average log likelihood -1.414733
INFO: iteration 5, average log likelihood -1.413536
INFO: iteration 6, average log likelihood -1.412480
INFO: iteration 7, average log likelihood -1.411809
INFO: iteration 8, average log likelihood -1.411457
INFO: iteration 9, average log likelihood -1.411265
INFO: iteration 10, average log likelihood -1.411142
INFO: iteration 11, average log likelihood -1.411051
INFO: iteration 12, average log likelihood -1.410976
INFO: iteration 13, average log likelihood -1.410912
INFO: iteration 14, average log likelihood -1.410856
INFO: iteration 15, average log likelihood -1.410804
INFO: iteration 16, average log likelihood -1.410757
INFO: iteration 17, average log likelihood -1.410713
INFO: iteration 18, average log likelihood -1.410671
INFO: iteration 19, average log likelihood -1.410631
INFO: iteration 20, average log likelihood -1.410594
INFO: iteration 21, average log likelihood -1.410557
INFO: iteration 22, average log likelihood -1.410523
INFO: iteration 23, average log likelihood -1.410489
INFO: iteration 24, average log likelihood -1.410457
INFO: iteration 25, average log likelihood -1.410426
INFO: iteration 26, average log likelihood -1.410396
INFO: iteration 27, average log likelihood -1.410368
INFO: iteration 28, average log likelihood -1.410340
INFO: iteration 29, average log likelihood -1.410313
INFO: iteration 30, average log likelihood -1.410287
INFO: iteration 31, average log likelihood -1.410262
INFO: iteration 32, average log likelihood -1.410237
INFO: iteration 33, average log likelihood -1.410214
INFO: iteration 34, average log likelihood -1.410191
INFO: iteration 35, average log likelihood -1.410168
INFO: iteration 36, average log likelihood -1.410147
INFO: iteration 37, average log likelihood -1.410126
INFO: iteration 38, average log likelihood -1.410105
INFO: iteration 39, average log likelihood -1.410086
INFO: iteration 40, average log likelihood -1.410067
INFO: iteration 41, average log likelihood -1.410048
INFO: iteration 42, average log likelihood -1.410031
INFO: iteration 43, average log likelihood -1.410013
INFO: iteration 44, average log likelihood -1.409997
INFO: iteration 45, average log likelihood -1.409981
INFO: iteration 46, average log likelihood -1.409966
INFO: iteration 47, average log likelihood -1.409951
INFO: iteration 48, average log likelihood -1.409936
INFO: iteration 49, average log likelihood -1.409923
INFO: iteration 50, average log likelihood -1.409909
INFO: EM with 100000 data points 50 iterations avll -1.409909
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.255809    0.124969     -0.22175      0.219137   -0.108012    -0.0997504   0.032326   -0.0985626  -0.0937145  -0.0571483     0.205536    0.139478    -0.13973     0.0343091    0.121159    0.0900921   0.266928     0.199524     -0.145413    -0.434726   -0.145302   -0.299222   -0.269356     0.19312     -0.325237    0.262736 
 -0.845646   -0.136273      0.237358    -0.122782   -0.471529     0.194432   -0.485776    0.0215643  -0.0342543  -0.253405     -0.609778    0.00172484  -0.396898   -0.0121395    0.109955    0.902687   -0.227484     0.207874      0.151892     0.464577    0.559317   -0.345269   -0.0472468   -0.323633     0.31735     0.171136 
  0.147791    0.356533      0.688121    -0.328638    0.418259     0.0849554   0.108042   -0.0146417  -0.641534    0.267073     -0.100706    0.400721    -0.0841851   0.489053    -0.446384    0.0136905   0.116288     0.0217992     0.179374     0.142679    0.630686    0.296483    1.27862      0.105639    -0.278697   -0.100371 
  0.030023   -0.115131     -0.534501    -0.400372   -0.804845     0.130156    0.311695    0.141549   -0.20121     0.607927      0.234462   -1.02064     -0.398538   -0.354984    -0.49995     0.401192    0.194838     0.0442534    -0.829638    -0.587483    0.251594   -0.191192    0.4179       0.182909    -0.0888863  -0.171179 
 -0.162245    0.429099     -0.517396     0.0965129   0.496455     0.413794   -0.0755929  -0.220789   -0.122295   -0.36997       0.0520364   0.00303095   0.201684    0.00264583  -0.314011   -0.518987    0.647504    -0.0659932     0.416385    -0.201452    0.41553    -0.0987005   0.391241     0.119579     0.164989    0.389842 
  0.863828    0.327529     -0.410182    -0.85237     0.192817     0.203977   -0.0149794   0.0489476   0.0245782   0.197314      0.598759   -0.215093     0.446612    0.503086     0.571208    0.418489    0.505473    -0.37498       0.22423     -0.621006   -0.195161    0.452954    0.390104     0.500141    -0.0613142   0.195967 
 -0.680745    0.100187      0.163778     0.432628    0.0172962   -0.177672    0.473255   -0.32285     0.0408479  -0.334748     -0.47019     0.369596    -0.362954   -0.346082    -0.556585   -0.267917   -0.368219     0.332658     -0.191183     0.35966     0.283169   -0.555153   -0.163087    -0.182664     0.0248227   0.19133  
  0.330177    0.0220062    -0.382894    -0.16215    -0.112174    -0.189127    0.174976    0.0377809  -0.459544   -0.224741     -0.172208   -0.416878     0.305285   -0.495605     0.0270427  -0.458957    0.870383     0.261838      0.100088    -0.822097   -0.260259    0.110724    0.55612     -0.176937     0.6636     -0.112661 
  0.239597    0.12496      -0.188266    -0.162312   -0.151037    -0.232223    0.355737    0.153687   -0.707448    0.415872      0.267081   -0.707738     0.0136496  -0.132918     0.673401    0.248245    0.197313    -0.484186      0.327302     0.0858186   0.508825   -0.0247464   0.0838968    0.00505785  -0.293738   -0.282446 
 -0.389532    0.0318231    -0.171338    -0.502447   -0.0468443   -0.541496    0.711768    1.03669    -0.15641    -0.270596     -0.0915967   1.28234      0.11061    -0.0156442    0.324393    0.274219    0.0500286    0.414352     -0.00519594   0.0316646   0.334059    0.038741   -0.00799366   0.342209    -0.562083   -0.167523 
  0.101495    0.105864     -0.909182     0.148627   -0.125275    -0.314277    0.0029839  -0.119041   -0.0970497   0.543229      0.389236    0.146491     0.868604    0.266953     0.172815   -0.210798   -0.286735     0.445446     -0.915744    -0.239706   -0.130988    0.539759   -0.659155     0.379846     0.21198     0.352333 
  0.0282006   0.0338003    -0.0352921    0.020679   -0.0258197   -0.0181306   0.0732046   0.0784222   0.0215003  -0.000332688  -0.07261     0.0156941   -0.255372   -0.048957     0.089705   -0.0186712   0.0869746    0.000722257  -0.0680307    0.0283814  -0.0689585   0.140169   -0.0716205    0.0536192   -0.0683264  -0.169125 
 -0.294079    0.189508     -0.387841     0.0273721  -0.908695    -0.102072   -0.234797    0.0141615   0.322768   -0.643298      0.0596586  -0.366687    -0.108427   -0.235169    -0.0159506   0.302755    0.322884     0.0703866     0.0735408   -0.137738   -0.634633    0.403042   -0.231383     0.113113     0.224187    0.494344 
 -0.155721   -0.262512      0.00331543  -0.257878   -0.058632    -0.0367958  -0.301538    0.210492   -0.154744   -0.0186207     0.107111    0.249112    -0.0252425   0.258213     0.312143   -0.255107   -0.270957    -0.535016     -0.0532144    0.448303   -0.0822093   0.347614   -0.504995     0.739366    -0.1404      0.0860747
  0.128022   -0.255376     -0.037893    -0.473565   -0.0201779    0.240824   -0.546256   -0.354078   -0.613536   -0.379415      0.331176   -0.291322     0.539516   -0.303837     0.31793    -0.223777   -0.378157    -0.0243649     0.141902    -0.181425    0.120257   -0.187361    0.372744    -0.117215     0.21445     0.471673 
 -0.0613135   0.0616816     0.197317     0.0196613   0.181955     0.341402   -0.438208   -0.799894    0.827555   -0.169652      0.194291    0.547235    -0.0614849   0.675898    -0.11492     0.0484091  -0.494249    -0.149993      0.173136     0.322231   -0.116066    0.0660476  -0.297889     0.518553    -0.194912   -0.125028 
 -0.449331    0.000187419   0.480906    -0.0664692  -0.276323    -0.279328   -0.510312    0.0584862   0.0228684  -0.153708      0.364796    0.0713526    0.631478    0.543808    -0.0736706  -0.050207   -0.919148     0.0179839     0.0903823    0.124194    0.326629   -0.0616054   0.197848    -0.600469    -0.175377    0.733528 
 -0.167596   -0.187543      0.528089     0.416144   -0.191545    -0.189793    0.344272    0.446236    0.496606    0.403961     -0.137329    0.0275096   -0.748705    0.31228     -0.19425     0.443446   -0.189732    -0.0111549    -0.123168     0.319508   -0.102144    0.0531395  -0.504481     0.0631981   -0.450923   -0.57716  
  0.153642    0.774867     -0.428809    -0.388981    0.150042    -0.328779    0.151003    0.12872     0.541615    0.455379      0.402034   -0.00637942  -0.11829     0.0642635   -0.479229    0.388093    0.163062     0.131477      0.208866     0.295568    0.56119    -0.136369   -0.509119    -0.208154    -0.0771813   0.481727 
  0.243662   -0.619635      0.366156     0.087924    0.702795    -0.376979    0.294139   -0.0380489   0.0293159   0.524643      0.150814   -0.0971414    0.555087    0.2648      -0.17797    -0.72771    -0.457203    -0.384394     -0.275524    -0.539282   -0.0966139   0.239181    0.224604     0.240585     0.34197    -0.278476 
  0.0433071   0.00258741    0.530934    -0.111849    0.127598     0.343033   -0.0171895  -0.334546   -0.0980938  -0.197693      0.0151543  -0.21496     -1.05557    -0.379333    -0.30132     0.037934    0.689139    -0.506653      0.443087    -0.348285   -0.334164   -0.609133    0.348276    -0.174546    -0.500254   -0.732503 
  0.140546    0.0673332    -0.0352227   -0.0613417   0.444425     0.0868654   0.296141    0.630558   -0.136717    0.53589      -0.0845314   0.142867    -0.129316   -0.365773     0.351813   -0.0690282   0.0662429    0.286647     -0.107857    -0.0971487   0.235529    0.102024    0.46776     -0.295952     0.288141   -0.360329 
 -0.176267    0.623847      0.00993839  -0.209746    0.696189     0.591098   -0.208834    1.20147     0.170747    0.0489735     0.352448    0.0509788   -0.581932    0.224488     0.214332    0.186865   -0.277116    -0.607431      0.105683    -0.100217   -0.468687    0.432666    0.0293105   -0.352202    -0.0841062  -0.173782 
  0.434462   -0.0599393    -0.250951     0.0728042   0.0988518   -0.254916    0.246481   -0.0725124  -0.200355    0.222254     -0.104121   -0.00565055   0.0405392   0.0407891   -0.0848813  -0.169226    0.397533    -0.0282608    -0.35619     -0.124855   -0.278733    0.360003   -0.0866355    0.227448    -0.0218508  -0.419031 
 -0.135648    0.0822407     0.0791959   -0.048234   -0.119241     0.197509   -0.0296868   0.0286619   0.0312396  -0.355992     -0.153051    0.0289799   -0.230478   -0.149135     0.0296147   0.0237877   0.00413403  -0.0254432     0.147944     0.115827    0.0341272   0.109355    0.0404801   -0.193428    -0.106331   -0.0103916
 -0.115307   -0.832558      0.399987     0.646484   -0.16758      0.350134   -0.158773   -0.0781956  -0.530953   -0.612908     -0.40832     0.0486345   -0.0515865  -0.221654     0.395026   -0.349034   -0.140695     0.135561     -0.0711466   -0.3262     -0.383332   -0.0427231   0.466441    -0.0688211   -0.386823   -0.360077 
  0.135789   -0.95601       0.417027     0.239213   -0.303768    -0.308502    0.239686   -0.843648   -0.500861    0.148006     -0.497495    0.00581633   0.49464    -0.742083    -0.0890653  -0.450701    0.205558     0.776383      0.085212     0.319558    0.348839   -0.281172    0.30505     -0.12281      0.384587   -0.243715 
  0.213121   -0.282446      0.258452     0.293615    0.00167654  -0.0435603  -0.296039   -0.53745     0.526045   -0.294437     -0.272567   -0.0897042    0.0151388   0.285006    -0.828813   -0.0335592   0.0144311    0.415348     -0.449169    -0.191736   -0.187373   -0.0828476   0.219971    -0.281546     0.170036   -0.0443112
  0.128108   -0.112806     -0.0186232   -0.051428    0.0314618   -0.166634   -0.081684   -0.1964      0.164258    0.27562       0.259644   -0.17244      0.280607    0.232783    -0.188442    0.0764506  -0.122242    -0.100456      0.122469     0.0292856   0.113559   -0.183222    0.08711     -0.0691717    0.340549    0.0564738
 -0.0906659   0.238627      0.113134     0.891604    0.1293      -0.478872    0.088824    0.454922    0.108506   -0.933552     -0.628793    0.436468    -0.153854    0.0365194    0.397487   -0.099866    0.142613     0.0320632     0.312136     0.669721   -0.252592    0.39673    -0.234301    -0.51162      0.603301    0.392065 
  0.863195   -0.248204     -0.645269     0.54336     0.407567     0.29191    -0.136937   -0.528577   -0.203577   -0.130629     -0.426661   -0.379541    -0.0379797  -0.170809    -0.371683    0.181983    0.882358     0.059108     -0.211325    -0.028895   -0.318109   -0.198724   -0.413844     0.164439    -0.0543047  -0.195506 
  0.279391   -0.26561       0.32707     -0.0314853  -0.730223    -0.0793304  -0.393991    0.0925904  -0.204012    0.461607      0.351236   -0.0719963   -0.0723179  -0.0974851    0.80321     0.447582   -0.290508    -0.136834     -0.245343     0.234836   -0.373341   -0.576172   -0.626959    -0.273984    -0.211545   -0.628816 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409897
INFO: iteration 2, average log likelihood -1.409884
INFO: iteration 3, average log likelihood -1.409872
INFO: iteration 4, average log likelihood -1.409861
INFO: iteration 5, average log likelihood -1.409850
INFO: iteration 6, average log likelihood -1.409839
INFO: iteration 7, average log likelihood -1.409829
INFO: iteration 8, average log likelihood -1.409819
INFO: iteration 9, average log likelihood -1.409810
INFO: iteration 10, average log likelihood -1.409800
INFO: EM with 100000 data points 10 iterations avll -1.409800
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
