>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.9
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-98-generic #145-Ubuntu SMP Sat Oct 8 20:13:07 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (690.7734375 MB free)
Uptime: 24465.0 sec
Load Avg:  1.0634765625  1.04931640625  1.04736328125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3503 MHz    1472369 s        698 s     172866 s     544458 s         63 s
#2  3503 MHz     690340 s       7339 s      94240 s    1555391 s          2 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.9
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-3.7740503606238924e6,[77532.5,22467.5],
[-27958.7 -3797.27 3189.94; 28023.9 3580.66 -3477.63],

Array{Float64,2}[
[59143.7 -1972.57 996.592; -1972.57 76762.5 -1102.81; 996.592 -1102.81 83282.6],

[40821.6 1855.5 -1623.99; 1855.5 23183.4 1279.24; -1623.99 1279.24 16090.9]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.081857e+03
      1       8.616035e+02      -2.202539e+02 |        6
      2       8.463168e+02      -1.528666e+01 |        0
      3       8.463168e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 846.3168145875816)
INFO: K-means with 272 data points using 3 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.062655
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.837942
INFO: iteration 2, lowerbound -3.727310
INFO: iteration 3, lowerbound -3.593318
INFO: iteration 4, lowerbound -3.419709
INFO: iteration 5, lowerbound -3.222258
INFO: iteration 6, lowerbound -3.026519
INFO: iteration 7, lowerbound -2.855729
INFO: dropping number of Gaussions to 7
INFO: iteration 8, lowerbound -2.716292
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.585365
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.480162
INFO: iteration 11, lowerbound -2.412384
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.367060
INFO: iteration 13, lowerbound -2.332846
INFO: iteration 14, lowerbound -2.313348
INFO: iteration 15, lowerbound -2.307441
INFO: dropping number of Gaussions to 2
INFO: iteration 16, lowerbound -2.302930
INFO: iteration 17, lowerbound -2.299261
INFO: iteration 18, lowerbound -2.299257
INFO: iteration 19, lowerbound -2.299255
INFO: iteration 20, lowerbound -2.299254
INFO: iteration 21, lowerbound -2.299253
INFO: iteration 22, lowerbound -2.299253
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sun 16 Oct 2016 11:17:29 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sun 16 Oct 2016 11:17:31 AM UTC: K-means with 272 data points using 3 iterations
11.3 data points per parameter
,Sun 16 Oct 2016 11:17:32 AM UTC: EM with 272 data points 0 iterations avll -2.062655
5.8 data points per parameter
,Sun 16 Oct 2016 11:17:33 AM UTC: GMM converted to Variational GMM
,Sun 16 Oct 2016 11:17:35 AM UTC: iteration 1, lowerbound -3.837942
,Sun 16 Oct 2016 11:17:35 AM UTC: iteration 2, lowerbound -3.727310
,Sun 16 Oct 2016 11:17:35 AM UTC: iteration 3, lowerbound -3.593318
,Sun 16 Oct 2016 11:17:35 AM UTC: iteration 4, lowerbound -3.419709
,Sun 16 Oct 2016 11:17:35 AM UTC: iteration 5, lowerbound -3.222258
,Sun 16 Oct 2016 11:17:35 AM UTC: iteration 6, lowerbound -3.026519
,Sun 16 Oct 2016 11:17:35 AM UTC: iteration 7, lowerbound -2.855729
,Sun 16 Oct 2016 11:17:36 AM UTC: dropping number of Gaussions to 7
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 8, lowerbound -2.716292
,Sun 16 Oct 2016 11:17:36 AM UTC: dropping number of Gaussions to 5
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 9, lowerbound -2.585365
,Sun 16 Oct 2016 11:17:36 AM UTC: dropping number of Gaussions to 4
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 10, lowerbound -2.480162
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 11, lowerbound -2.412384
,Sun 16 Oct 2016 11:17:36 AM UTC: dropping number of Gaussions to 3
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 12, lowerbound -2.367060
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 13, lowerbound -2.332846
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 14, lowerbound -2.313348
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 15, lowerbound -2.307441
,Sun 16 Oct 2016 11:17:36 AM UTC: dropping number of Gaussions to 2
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 16, lowerbound -2.302930
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 17, lowerbound -2.299261
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 18, lowerbound -2.299257
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 19, lowerbound -2.299255
,Sun 16 Oct 2016 11:17:36 AM UTC: iteration 20, lowerbound -2.299254
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 21, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 22, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 23, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 24, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 25, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 26, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 27, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 28, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 29, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 30, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 31, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 32, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:37 AM UTC: iteration 33, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 34, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 35, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 36, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 37, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 38, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 39, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 40, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 41, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 42, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 43, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 44, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 45, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 46, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 47, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:38 AM UTC: iteration 48, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:39 AM UTC: iteration 49, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:39 AM UTC: iteration 50, lowerbound -2.299253
,Sun 16 Oct 2016 11:17:39 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [95.9549,178.045]
β = [95.9549,178.045]
m = [2.00023 53.852; 4.2503 79.2869]
ν = [97.9549,180.045]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.375876 -0.00895312; 0.0 0.0127487],

[0.184042 -0.00764405; 0.0 0.00858171]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9768606338689455
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9768606338689458
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9768606338689458
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9775370562540079
avll from llpg:  -0.9775370562540077
avll direct:     -0.9775370562540077
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.180796    -0.0340329    -0.00581045  -0.00444461   0.0189777    0.0739299    -0.0809137    -0.0033932     0.0212232   -0.0346318    0.0496425    0.00329189  -0.0553406   -0.025782     0.0133293   -0.188075    -0.0257725   -0.036055      0.0584167     0.14399       0.075858    -0.0446805    0.115136    -0.169189    -0.0993101     0.00560442
 -0.121451     0.0351381     0.0744453    0.0108132    0.117355    -0.0231555     0.0668336    -1.9418e-5    -0.0622959    0.0478388   -0.0612963    0.0355615    0.111719    -0.0221752   -0.0731518    0.0329125    0.0537345    0.058518      0.0147767    -0.033385      0.0489791    0.124913    -0.0570045    0.273552     0.242793      0.218376  
 -0.101877    -0.0814834    -0.034208    -0.0616026    0.044426    -0.0967803    -0.149981      0.0378162     0.0342784    0.0880102   -0.0705978   -0.0230286   -0.0552436    0.0675249   -0.00967513  -0.0164452    0.0281793    0.175543      0.163927      0.00395961   -0.0363275    0.209771    -0.0653116    0.0428511   -0.0467396     0.0636089 
 -0.174071    -0.0982648    -0.111196    -0.0111852   -0.112427     0.0519715     0.0940139     0.249607     -0.110612    -0.0438606    0.010223     0.208861     0.145264     0.0462147    0.097209     0.059905    -0.182254     0.0311267    -0.0845459     0.108639     -0.211772     0.0409659   -0.0738781    0.107691    -0.127994      0.130172  
 -0.0532709    0.0687894    -0.195163    -0.099732    -0.121613    -0.135311     -0.103822     -0.242534      0.0755819    0.18139      0.0494169   -0.00070435  -0.00313003  -0.130761     0.155977    -0.0534014    0.126185    -0.168047     -0.169011      0.19223      -0.00759477   0.160379     0.0322454   -0.00908306  -0.0271872    -0.149438  
  0.0180225    0.000357059   0.226845     0.157373     0.142639    -0.0130665    -0.151518      0.0932653    -0.0379532   -0.116641    -0.0325176    0.0247608   -0.0982425    0.10358     -0.143631    -0.00361088  -0.0278837    0.120985      0.0802965    -0.0775196    -0.0512967    0.119067    -0.0986373    0.144564    -0.0886139    -0.0615638 
 -0.0346478    0.0110784     0.164081     0.100832     0.0695723   -0.0796844     0.0554305     0.132342     -0.158603     0.018366    -0.0667407    0.011424    -0.217463    -0.0963712    0.0679803   -0.068622     0.0647188    0.00896475   -0.0258612    -0.0912005    -0.114704     0.031696     0.00515844  -0.0536291   -0.0148636    -0.11427   
  0.0977367    0.102175     -0.0690287   -0.0151096   -0.0239476   -0.0361491     0.184712      0.166725      0.0314768   -0.167139     0.0354702    0.0123771   -0.0558927    0.0897836   -0.103689    -0.206203     0.129727     0.398331      0.186306      0.0344047    -0.0213436   -0.0389782    0.0216324    0.105491    -0.144162     -0.143213  
  0.0876743   -0.279973     -0.16309      0.041404    -0.077778     0.00761734   -0.067602      0.103757     -0.149602    -0.0759162    0.152439     0.00531327  -0.0889409    0.0055093   -0.0535246    0.121676    -0.0927989    0.240207     -0.0589246     0.0778629     0.0879601   -0.0249175   -0.0248465    0.0155355   -0.171416     -0.250942  
 -0.0322325   -0.00315968    0.030144     0.139138     0.0497452    0.0949844     0.136167     -0.0301795    -0.0915804   -0.0174454   -0.0466897    0.0658923   -0.0125191    0.0579874    0.167819    -0.120258     0.0681814   -0.000810755  -0.0424063    -0.0651727    -0.103581    -0.0528433   -0.065026    -0.0430175   -0.0637668    -0.0208706 
  0.0207671   -0.033664      0.10967      0.020895     0.0299596    0.00500831   -0.086162      0.156685     -0.12297      0.0556616    0.0636055   -0.17881      0.031042     0.0422249    0.0879314    0.0309513   -0.171989     0.0157728     0.0697873    -0.104565      0.078328     0.12957     -0.053299     0.104973     0.180844     -0.0386956 
 -0.0314346    0.0802793    -0.0830794    0.0393365    0.013932     0.030969      0.0333958    -0.0513939     0.179502    -0.123512     0.0964291    0.0167956    0.0292487   -0.0642948    0.239586    -0.0194602    0.0740389    0.0666093     0.202115      0.213507      0.0628292    0.170383     0.199118     0.0790454   -0.0666504    -0.0494091 
  0.0496227    0.106448      0.0227599   -0.191682     0.131851     0.000786209  -0.00827119    0.117615      0.04498     -0.242813    -0.0905133    0.066415     0.0549711    0.0564063   -0.0238299    0.0154388    0.0305022   -0.0597443     0.148988      0.0223722     0.0291409   -0.096004     0.0708707    0.157307    -0.1196       -0.00708749
 -0.101899     0.0702514     0.00778897  -0.211625     0.0357096   -0.158983      0.054396      0.00821668    0.00695358   0.0643209   -0.00968214  -0.0984857   -0.0268577   -0.107984     0.00164581  -0.147698    -0.203706     0.00366609    0.0501474    -0.088347      0.129111     0.0777929   -0.0784118   -0.142686    -0.0290365     0.0366566 
 -0.0257717   -0.0566673     0.0376947   -0.00854685   0.115084    -0.0440472     0.0727929     0.108603     -0.178088    -0.272526     0.127776    -0.0269436   -0.0470419   -0.00124326   0.243055     0.210434     0.130156    -0.17337       0.0798977     0.0987261    -0.198108    -0.00426686   0.219398    -0.108092     0.117619     -0.075735  
  0.170566     0.0569072    -0.0549668   -0.0484937   -0.0479908   -0.0793041     0.0158225    -0.126936     -0.196405    -0.159528    -0.164121    -0.0550492   -0.166328     0.00381098  -0.053944    -0.137015    -0.0282453    0.00793141    0.0780728     0.0223518     0.0326454    0.0120872    0.0566891   -0.131854     0.206016     -0.0493799 
  0.0700685   -0.0380754    -0.105721    -0.156871     0.00179869  -0.0513739     0.209313     -0.0647048    -0.0942125    0.0700081    0.0869524   -0.07928     -0.0505787   -0.130069     0.0404449    0.06699     -0.0320848    0.070159      0.042545      0.00748109    0.0832062   -0.0200142   -0.120449     0.0545117   -0.0082302    -0.0835129 
  0.0164255   -0.000828868  -0.0103204   -0.139985     0.00608297  -0.0431433    -0.0750461     0.0687181     0.181235     0.032235     0.0107661    0.0542816   -0.0177084   -0.0358152   -0.0839877    0.00886155   0.100468     0.0793939     0.0819741    -0.0966921     0.0523117   -0.040279    -0.0100063   -0.0924998   -0.0520369     0.0411933 
  0.0158654    0.126679      0.0690328   -0.0554685   -0.0175325   -0.0340874     0.165236      0.029736     -0.210013    -0.0501351   -0.105179     0.00996041  -0.0733607    0.0386962    0.0402193    0.101826     0.0175559   -0.115282      0.00575305    0.191145      0.0414288    0.166816    -0.0513185    0.039776     0.00748216    0.00559608
 -0.182065     0.0110566    -0.0736541    0.031373     0.216287    -0.0230749    -0.21         -0.106021      0.125703    -0.0508286   -0.0654901   -0.0576242    0.187756     0.00734509  -0.187966     0.136317     0.124479    -0.00832623   -0.206438      0.0978935     0.0376171    0.0774171   -0.322306    -0.0217443   -0.0400987     0.0881965 
 -0.0913741    0.0287029     0.0043341    0.0225331    0.093068     0.071223     -0.0641571     0.0268675    -0.0624281    0.0456097   -0.201857     0.0222669   -0.110118     0.0921613    0.0934196   -0.0989178    0.0928381   -0.206094      0.0349572     0.037105      0.0764751    0.16883      0.0119976   -0.0602027   -0.0958615    -0.010631  
 -0.0086733   -0.0487609     0.101977     0.208055    -0.120091    -0.154581     -0.08491      -0.160546      0.166796     0.00965802   0.0292578   -0.212552     0.0470231    0.228282     0.0729685    0.033508     0.0515156   -0.191619      0.080289     -0.110926      0.0768108    0.0341706   -0.106002    -0.13143      0.0498539    -0.125327  
  0.102485    -0.0469382    -0.108258    -0.145833     0.0847286    0.0593571    -0.0262556     0.0493673    -0.0479188   -0.0311047   -0.0360827   -0.173729    -0.0474567   -0.0761052    0.0774874    0.0794487   -0.0797691    0.144246      0.0581632     0.0948084    -0.187603     0.106122     0.121734    -0.131482    -0.0810112    -0.0998124 
 -0.00544062  -0.177812      0.0497686    0.00467083   0.124023     0.0590506    -0.0827516    -0.0503528     0.0256843   -0.0335778    0.0513133   -0.145644     0.0999775    0.176376    -0.0299723   -0.0942155   -0.00744815   0.00879177    0.0406783    -0.0129093    -0.0730617    0.0400297   -0.00588059   0.0994724   -0.0175386     0.0446789 
  0.174371     0.0870253     0.118361    -0.0542147    0.149365    -0.0489357    -0.0816468    -0.000392926   0.0856004    0.0954213   -0.184965     0.054498    -0.0979933    0.0818278    0.0681676    0.0134912    0.0506061    0.0914835    -0.0818037     0.0858184     0.0990829   -0.091597    -0.0372008    0.111598     0.101388      0.0277521 
 -0.020044     0.0329967     0.0908828    0.0583646   -0.11107     -0.0163052    -0.0250866    -0.00255066   -0.133822     0.0720085   -0.0656997   -0.0119787   -0.278305     0.110273    -0.0891585   -0.0587008   -0.0626757    0.0308806    -0.0292572    -0.000268598   0.165102    -0.114699     0.0607197   -0.0312907   -0.0851512     0.0245856 
  0.0369609   -0.238696      0.0491406    0.0883357    0.149388    -0.130984      0.00242604   -0.11294       0.0750077    0.0767999    0.131763    -0.230332     0.156666    -0.0329198   -0.177509     0.036402     0.166381     0.0630364    -0.000957012  -0.102146     -0.10848     -0.0844183   -0.0506321   -0.178563    -0.05288       0.199828  
 -0.187448     0.00247947    0.0491977    0.196453     0.112026     0.00679413   -0.000875861  -0.0625406    -0.160765     0.052668    -0.0747376    0.0777378    0.141489     0.148165    -0.106147     0.114276    -0.106586     0.0314072    -0.0789444    -0.00161779   -0.0363619   -0.0819891   -0.0563407    0.00131591   0.000672819   0.0984583 
  0.103006     0.0946458    -0.0529474    0.11824     -0.133498     0.0302904    -0.167558     -0.0292413    -0.107256     0.265815    -0.145031     0.0766485    0.125312    -0.119195    -0.0274289   -0.210059    -0.12712      0.0832215     0.0866873     0.121974     -0.0504151    0.0393549   -0.0734709    0.0306386    0.0284016    -0.199459  
  0.116646    -0.0782138     0.2625      -0.0667607    0.0110616   -0.205117     -0.0673248     0.0689813     0.14219      0.12566     -0.0460191   -0.0563746    0.0263499   -0.043554    -0.12545      0.122129    -0.0715751    0.0797099    -0.128542      0.0544502    -0.024373     0.0157348   -0.0821373   -0.0716136   -0.150538     -0.076328  
 -0.0026235   -0.0192496     0.0407015   -0.119075    -0.0478094   -0.182442     -0.182859     -0.119783      0.023315     0.0371774    0.0620941   -0.0732027   -0.0601111   -0.0403822    0.0669602    0.0570714   -0.0370362    0.0704249     0.00814573    0.0179186    -0.0339006    0.0236819   -0.0337138    0.00324267  -0.0764316     0.0320612 
  0.0331276    0.0776241     0.0324294   -0.045321     0.086501    -0.0572566    -0.158125     -0.0548816    -0.22435     -0.0403002   -0.0470794   -0.115144    -0.035719     0.171268    -0.0689374    0.0770995   -0.0561802    0.0564887    -0.00165283   -0.0447149    -0.139773     0.0508955    0.121351    -0.194987    -0.0708785     0.144553  kind diag, method split
0: avll = -1.438630817936067
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.438712
INFO: iteration 2, average log likelihood -1.438648
INFO: iteration 3, average log likelihood -1.438373
INFO: iteration 4, average log likelihood -1.434760
INFO: iteration 5, average log likelihood -1.417506
INFO: iteration 6, average log likelihood -1.402004
INFO: iteration 7, average log likelihood -1.398545
INFO: iteration 8, average log likelihood -1.397735
INFO: iteration 9, average log likelihood -1.397390
INFO: iteration 10, average log likelihood -1.397171
INFO: iteration 11, average log likelihood -1.397014
INFO: iteration 12, average log likelihood -1.396897
INFO: iteration 13, average log likelihood -1.396805
INFO: iteration 14, average log likelihood -1.396735
INFO: iteration 15, average log likelihood -1.396683
INFO: iteration 16, average log likelihood -1.396643
INFO: iteration 17, average log likelihood -1.396612
INFO: iteration 18, average log likelihood -1.396588
INFO: iteration 19, average log likelihood -1.396569
INFO: iteration 20, average log likelihood -1.396554
INFO: iteration 21, average log likelihood -1.396543
INFO: iteration 22, average log likelihood -1.396534
INFO: iteration 23, average log likelihood -1.396526
INFO: iteration 24, average log likelihood -1.396521
INFO: iteration 25, average log likelihood -1.396516
INFO: iteration 26, average log likelihood -1.396513
INFO: iteration 27, average log likelihood -1.396510
INFO: iteration 28, average log likelihood -1.396508
INFO: iteration 29, average log likelihood -1.396506
INFO: iteration 30, average log likelihood -1.396505
INFO: iteration 31, average log likelihood -1.396504
INFO: iteration 32, average log likelihood -1.396503
INFO: iteration 33, average log likelihood -1.396502
INFO: iteration 34, average log likelihood -1.396502
INFO: iteration 35, average log likelihood -1.396501
INFO: iteration 36, average log likelihood -1.396501
INFO: iteration 37, average log likelihood -1.396501
INFO: iteration 38, average log likelihood -1.396500
INFO: iteration 39, average log likelihood -1.396500
INFO: iteration 40, average log likelihood -1.396500
INFO: iteration 41, average log likelihood -1.396500
INFO: iteration 42, average log likelihood -1.396500
INFO: iteration 43, average log likelihood -1.396500
INFO: iteration 44, average log likelihood -1.396500
INFO: iteration 45, average log likelihood -1.396500
INFO: iteration 46, average log likelihood -1.396500
INFO: iteration 47, average log likelihood -1.396500
INFO: iteration 48, average log likelihood -1.396500
INFO: iteration 49, average log likelihood -1.396500
INFO: iteration 50, average log likelihood -1.396500
INFO: EM with 100000 data points 50 iterations avll -1.396500
952.4 data points per parameter
1: avll = [-1.43871,-1.43865,-1.43837,-1.43476,-1.41751,-1.402,-1.39854,-1.39773,-1.39739,-1.39717,-1.39701,-1.3969,-1.39681,-1.39674,-1.39668,-1.39664,-1.39661,-1.39659,-1.39657,-1.39655,-1.39654,-1.39653,-1.39653,-1.39652,-1.39652,-1.39651,-1.39651,-1.39651,-1.39651,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.396625
INFO: iteration 2, average log likelihood -1.396504
INFO: iteration 3, average log likelihood -1.396127
INFO: iteration 4, average log likelihood -1.392888
INFO: iteration 5, average log likelihood -1.380959
INFO: iteration 6, average log likelihood -1.367972
INFO: iteration 7, average log likelihood -1.361033
INFO: iteration 8, average log likelihood -1.357453
INFO: iteration 9, average log likelihood -1.355659
INFO: iteration 10, average log likelihood -1.354724
INFO: iteration 11, average log likelihood -1.354185
INFO: iteration 12, average log likelihood -1.353840
INFO: iteration 13, average log likelihood -1.353598
INFO: iteration 14, average log likelihood -1.353420
INFO: iteration 15, average log likelihood -1.353282
INFO: iteration 16, average log likelihood -1.353174
INFO: iteration 17, average log likelihood -1.353089
INFO: iteration 18, average log likelihood -1.353021
INFO: iteration 19, average log likelihood -1.352967
INFO: iteration 20, average log likelihood -1.352923
INFO: iteration 21, average log likelihood -1.352887
INFO: iteration 22, average log likelihood -1.352860
INFO: iteration 23, average log likelihood -1.352839
INFO: iteration 24, average log likelihood -1.352823
INFO: iteration 25, average log likelihood -1.352811
INFO: iteration 26, average log likelihood -1.352801
INFO: iteration 27, average log likelihood -1.352793
INFO: iteration 28, average log likelihood -1.352786
INFO: iteration 29, average log likelihood -1.352781
INFO: iteration 30, average log likelihood -1.352776
INFO: iteration 31, average log likelihood -1.352772
INFO: iteration 32, average log likelihood -1.352768
INFO: iteration 33, average log likelihood -1.352764
INFO: iteration 34, average log likelihood -1.352761
INFO: iteration 35, average log likelihood -1.352758
INFO: iteration 36, average log likelihood -1.352755
INFO: iteration 37, average log likelihood -1.352752
INFO: iteration 38, average log likelihood -1.352748
INFO: iteration 39, average log likelihood -1.352745
INFO: iteration 40, average log likelihood -1.352742
INFO: iteration 41, average log likelihood -1.352739
INFO: iteration 42, average log likelihood -1.352736
INFO: iteration 43, average log likelihood -1.352733
INFO: iteration 44, average log likelihood -1.352730
INFO: iteration 45, average log likelihood -1.352728
INFO: iteration 46, average log likelihood -1.352725
INFO: iteration 47, average log likelihood -1.352724
INFO: iteration 48, average log likelihood -1.352722
INFO: iteration 49, average log likelihood -1.352721
INFO: iteration 50, average log likelihood -1.352719
INFO: EM with 100000 data points 50 iterations avll -1.352719
473.9 data points per parameter
2: avll = [-1.39662,-1.3965,-1.39613,-1.39289,-1.38096,-1.36797,-1.36103,-1.35745,-1.35566,-1.35472,-1.35418,-1.35384,-1.3536,-1.35342,-1.35328,-1.35317,-1.35309,-1.35302,-1.35297,-1.35292,-1.35289,-1.35286,-1.35284,-1.35282,-1.35281,-1.3528,-1.35279,-1.35279,-1.35278,-1.35278,-1.35277,-1.35277,-1.35276,-1.35276,-1.35276,-1.35275,-1.35275,-1.35275,-1.35275,-1.35274,-1.35274,-1.35274,-1.35273,-1.35273,-1.35273,-1.35273,-1.35272,-1.35272,-1.35272,-1.35272]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.352892
INFO: iteration 2, average log likelihood -1.352717
INFO: iteration 3, average log likelihood -1.352110
INFO: iteration 4, average log likelihood -1.346948
INFO: iteration 5, average log likelihood -1.331325
INFO: iteration 6, average log likelihood -1.316512
INFO: iteration 7, average log likelihood -1.309516
INFO: iteration 8, average log likelihood -1.306484
INFO: iteration 9, average log likelihood -1.304504
INFO: iteration 10, average log likelihood -1.302404
INFO: iteration 11, average log likelihood -1.299999
INFO: iteration 12, average log likelihood -1.297962
INFO: iteration 13, average log likelihood -1.296798
INFO: iteration 14, average log likelihood -1.296271
INFO: iteration 15, average log likelihood -1.295963
INFO: iteration 16, average log likelihood -1.295661
INFO: iteration 17, average log likelihood -1.295287
INFO: iteration 18, average log likelihood -1.294778
INFO: iteration 19, average log likelihood -1.294049
INFO: iteration 20, average log likelihood -1.293064
INFO: iteration 21, average log likelihood -1.291982
INFO: iteration 22, average log likelihood -1.290923
INFO: iteration 23, average log likelihood -1.289492
INFO: iteration 24, average log likelihood -1.287614
INFO: iteration 25, average log likelihood -1.286247
INFO: iteration 26, average log likelihood -1.285657
INFO: iteration 27, average log likelihood -1.285341
INFO: iteration 28, average log likelihood -1.285126
INFO: iteration 29, average log likelihood -1.284934
INFO: iteration 30, average log likelihood -1.284698
INFO: iteration 31, average log likelihood -1.284336
INFO: iteration 32, average log likelihood -1.283810
INFO: iteration 33, average log likelihood -1.283130
INFO: iteration 34, average log likelihood -1.282409
INFO: iteration 35, average log likelihood -1.281776
INFO: iteration 36, average log likelihood -1.281224
INFO: iteration 37, average log likelihood -1.280823
WARNING: Variances had to be floored 1
INFO: iteration 38, average log likelihood -1.280651
INFO: iteration 39, average log likelihood -1.294302
INFO: iteration 40, average log likelihood -1.289163
INFO: iteration 41, average log likelihood -1.287592
INFO: iteration 42, average log likelihood -1.287019
INFO: iteration 43, average log likelihood -1.286608
INFO: iteration 44, average log likelihood -1.286231
INFO: iteration 45, average log likelihood -1.285847
INFO: iteration 46, average log likelihood -1.285438
INFO: iteration 47, average log likelihood -1.285016
INFO: iteration 48, average log likelihood -1.284606
INFO: iteration 49, average log likelihood -1.284226
INFO: iteration 50, average log likelihood -1.283868
INFO: EM with 100000 data points 50 iterations avll -1.283868
236.4 data points per parameter
3: avll = [-1.35289,-1.35272,-1.35211,-1.34695,-1.33132,-1.31651,-1.30952,-1.30648,-1.3045,-1.3024,-1.3,-1.29796,-1.2968,-1.29627,-1.29596,-1.29566,-1.29529,-1.29478,-1.29405,-1.29306,-1.29198,-1.29092,-1.28949,-1.28761,-1.28625,-1.28566,-1.28534,-1.28513,-1.28493,-1.2847,-1.28434,-1.28381,-1.28313,-1.28241,-1.28178,-1.28122,-1.28082,-1.28065,-1.2943,-1.28916,-1.28759,-1.28702,-1.28661,-1.28623,-1.28585,-1.28544,-1.28502,-1.28461,-1.28423,-1.28387]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.283708
INFO: iteration 2, average log likelihood -1.283073
INFO: iteration 3, average log likelihood -1.281578
INFO: iteration 4, average log likelihood -1.268985
WARNING: Variances had to be floored 1
INFO: iteration 5, average log likelihood -1.234825
WARNING: Variances had to be floored 2
INFO: iteration 6, average log likelihood -1.223405
INFO: iteration 7, average log likelihood -1.215942
WARNING: Variances had to be floored 1
INFO: iteration 8, average log likelihood -1.197374
WARNING: Variances had to be floored 10
INFO: iteration 9, average log likelihood -1.197140
INFO: iteration 10, average log likelihood -1.199747
WARNING: Variances had to be floored 2
INFO: iteration 11, average log likelihood -1.189603
WARNING: Variances had to be floored 1 15
INFO: iteration 12, average log likelihood -1.192842
INFO: iteration 13, average log likelihood -1.202471
WARNING: Variances had to be floored 10
INFO: iteration 14, average log likelihood -1.189185
INFO: iteration 15, average log likelihood -1.194769
WARNING: Variances had to be floored 1 2
INFO: iteration 16, average log likelihood -1.183669
INFO: iteration 17, average log likelihood -1.201622
INFO: iteration 18, average log likelihood -1.193070
WARNING: Variances had to be floored 1
INFO: iteration 19, average log likelihood -1.183262
WARNING: Variances had to be floored 2 10
INFO: iteration 20, average log likelihood -1.186350
INFO: iteration 21, average log likelihood -1.204250
INFO: iteration 22, average log likelihood -1.189764
WARNING: Variances had to be floored 1
INFO: iteration 23, average log likelihood -1.181242
INFO: iteration 24, average log likelihood -1.189586
WARNING: Variances had to be floored 2 10
INFO: iteration 25, average log likelihood -1.182438
INFO: iteration 26, average log likelihood -1.201222
WARNING: Variances had to be floored 1
INFO: iteration 27, average log likelihood -1.186403
INFO: iteration 28, average log likelihood -1.193178
INFO: iteration 29, average log likelihood -1.185110
WARNING: Variances had to be floored 2 10
INFO: iteration 30, average log likelihood -1.180023
WARNING: Variances had to be floored 1
INFO: iteration 31, average log likelihood -1.197550
INFO: iteration 32, average log likelihood -1.197580
INFO: iteration 33, average log likelihood -1.188005
INFO: iteration 34, average log likelihood -1.181807
WARNING: Variances had to be floored 1 2 10
INFO: iteration 35, average log likelihood -1.175708
INFO: iteration 36, average log likelihood -1.207851
INFO: iteration 37, average log likelihood -1.195678
INFO: iteration 38, average log likelihood -1.185820
WARNING: Variances had to be floored 1
INFO: iteration 39, average log likelihood -1.178328
WARNING: Variances had to be floored 2 10
INFO: iteration 40, average log likelihood -1.187012
INFO: iteration 41, average log likelihood -1.203792
WARNING: Variances had to be floored 1
INFO: iteration 42, average log likelihood -1.189562
INFO: iteration 43, average log likelihood -1.192867
INFO: iteration 44, average log likelihood -1.184969
WARNING: Variances had to be floored 2 10
INFO: iteration 45, average log likelihood -1.180226
WARNING: Variances had to be floored 1
INFO: iteration 46, average log likelihood -1.197736
INFO: iteration 47, average log likelihood -1.197582
INFO: iteration 48, average log likelihood -1.187999
INFO: iteration 49, average log likelihood -1.181821
WARNING: Variances had to be floored 1 2 10
INFO: iteration 50, average log likelihood -1.175755
INFO: EM with 100000 data points 50 iterations avll -1.175755
118.1 data points per parameter
4: avll = [-1.28371,-1.28307,-1.28158,-1.26899,-1.23482,-1.22341,-1.21594,-1.19737,-1.19714,-1.19975,-1.1896,-1.19284,-1.20247,-1.18918,-1.19477,-1.18367,-1.20162,-1.19307,-1.18326,-1.18635,-1.20425,-1.18976,-1.18124,-1.18959,-1.18244,-1.20122,-1.1864,-1.19318,-1.18511,-1.18002,-1.19755,-1.19758,-1.188,-1.18181,-1.17571,-1.20785,-1.19568,-1.18582,-1.17833,-1.18701,-1.20379,-1.18956,-1.19287,-1.18497,-1.18023,-1.19774,-1.19758,-1.188,-1.18182,-1.17575]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.208081
INFO: iteration 2, average log likelihood -1.195642
WARNING: Variances had to be floored 1 29
INFO: iteration 3, average log likelihood -1.184808
WARNING: Variances had to be floored 2 4 29 30
INFO: iteration 4, average log likelihood -1.166711
WARNING: Variances had to be floored 3 5 6 20 29 30
INFO: iteration 5, average log likelihood -1.141109
WARNING: Variances had to be floored 2 11 29 30
INFO: iteration 6, average log likelihood -1.148063
WARNING: Variances had to be floored 5 12 19 29 30
INFO: iteration 7, average log likelihood -1.132490
WARNING: Variances had to be floored 2 3 6 29 30
INFO: iteration 8, average log likelihood -1.125662
WARNING: Variances had to be floored 5 11 20 29 30
INFO: iteration 9, average log likelihood -1.126881
WARNING: Variances had to be floored 1 2 6 29 30
INFO: iteration 10, average log likelihood -1.123951
WARNING: Variances had to be floored 3 5 12 29 30
INFO: iteration 11, average log likelihood -1.122998
WARNING: Variances had to be floored 6 11 20 29 30
INFO: iteration 12, average log likelihood -1.122341
WARNING: Variances had to be floored 2 5 29 30
INFO: iteration 13, average log likelihood -1.115918
WARNING: Variances had to be floored 3 6 29 30
INFO: iteration 14, average log likelihood -1.094312
WARNING: Variances had to be floored 1 2 5 11 12 20 29 30
INFO: iteration 15, average log likelihood -1.085509
WARNING: Variances had to be floored 6 29 30
INFO: iteration 16, average log likelihood -1.119498
WARNING: Variances had to be floored 3 5 29 30
INFO: iteration 17, average log likelihood -1.101479
WARNING: Variances had to be floored 2 6 11 29 30
INFO: iteration 18, average log likelihood -1.088879
WARNING: Variances had to be floored 5 12 20 29 30
INFO: iteration 19, average log likelihood -1.089803
WARNING: Variances had to be floored 1 3 6 29 30
INFO: iteration 20, average log likelihood -1.091838
WARNING: Variances had to be floored 2 5 11 29 30
INFO: iteration 21, average log likelihood -1.097477
WARNING: Variances had to be floored 6 12 20 29 30
INFO: iteration 22, average log likelihood -1.098169
WARNING: Variances had to be floored 2 3 5 29 30
INFO: iteration 23, average log likelihood -1.101222
WARNING: Variances had to be floored 6 11 29 30
INFO: iteration 24, average log likelihood -1.091890
WARNING: Variances had to be floored 1 5 12 20 29 30
INFO: iteration 25, average log likelihood -1.084281
WARNING: Variances had to be floored 2 3 6 29 30
INFO: iteration 26, average log likelihood -1.098942
WARNING: Variances had to be floored 5 11 29 30
INFO: iteration 27, average log likelihood -1.105396
WARNING: Variances had to be floored 6 29 30
INFO: iteration 28, average log likelihood -1.088060
WARNING: Variances had to be floored 2 3 5 12 20 29 30
INFO: iteration 29, average log likelihood -1.065337
WARNING: Variances had to be floored 1 6 11 29 30
INFO: iteration 30, average log likelihood -1.100267
WARNING: Variances had to be floored 2 5 29 30
INFO: iteration 31, average log likelihood -1.102665
WARNING: Variances had to be floored 3 6 12 29 30
INFO: iteration 32, average log likelihood -1.085547
WARNING: Variances had to be floored 2 5 11 20 29 30
INFO: iteration 33, average log likelihood -1.091845
WARNING: Variances had to be floored 6 29 30
INFO: iteration 34, average log likelihood -1.097253
WARNING: Variances had to be floored 1 3 5 29 30
INFO: iteration 35, average log likelihood -1.076028
WARNING: Variances had to be floored 2 6 11 12 29 30
INFO: iteration 36, average log likelihood -1.084163
WARNING: Variances had to be floored 5 29 30
INFO: iteration 37, average log likelihood -1.105672
WARNING: Variances had to be floored 3 6 20 29 30
INFO: iteration 38, average log likelihood -1.079802
WARNING: Variances had to be floored 2 5 11 29 30
INFO: iteration 39, average log likelihood -1.089368
WARNING: Variances had to be floored 1 6 12 29 30
INFO: iteration 40, average log likelihood -1.085611
WARNING: Variances had to be floored 3 5 29 30
INFO: iteration 41, average log likelihood -1.095165
WARNING: Variances had to be floored 2 6 11 29 30
INFO: iteration 42, average log likelihood -1.084492
WARNING: Variances had to be floored 5 20 29 30
INFO: iteration 43, average log likelihood -1.089289
WARNING: Variances had to be floored 3 6 12 29 30
INFO: iteration 44, average log likelihood -1.080748
WARNING: Variances had to be floored 2 4 5 11 29 30
INFO: iteration 45, average log likelihood -1.089368
WARNING: Variances had to be floored 6 29 30
INFO: iteration 46, average log likelihood -1.102741
WARNING: Variances had to be floored 5 20 29 30
INFO: iteration 47, average log likelihood -1.088197
WARNING: Variances had to be floored 1 2 3 6 29 30
INFO: iteration 48, average log likelihood -1.080680
WARNING: Variances had to be floored 5 12 29 30
INFO: iteration 49, average log likelihood -1.103899
WARNING: Variances had to be floored 6 11 29 30
INFO: iteration 50, average log likelihood -1.096883
INFO: EM with 100000 data points 50 iterations avll -1.096883
59.0 data points per parameter
5: avll = [-1.20808,-1.19564,-1.18481,-1.16671,-1.14111,-1.14806,-1.13249,-1.12566,-1.12688,-1.12395,-1.123,-1.12234,-1.11592,-1.09431,-1.08551,-1.1195,-1.10148,-1.08888,-1.0898,-1.09184,-1.09748,-1.09817,-1.10122,-1.09189,-1.08428,-1.09894,-1.1054,-1.08806,-1.06534,-1.10027,-1.10267,-1.08555,-1.09185,-1.09725,-1.07603,-1.08416,-1.10567,-1.0798,-1.08937,-1.08561,-1.09517,-1.08449,-1.08929,-1.08075,-1.08937,-1.10274,-1.0882,-1.08068,-1.1039,-1.09688]
[-1.43863,-1.43871,-1.43865,-1.43837,-1.43476,-1.41751,-1.402,-1.39854,-1.39773,-1.39739,-1.39717,-1.39701,-1.3969,-1.39681,-1.39674,-1.39668,-1.39664,-1.39661,-1.39659,-1.39657,-1.39655,-1.39654,-1.39653,-1.39653,-1.39652,-1.39652,-1.39651,-1.39651,-1.39651,-1.39651,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.3965,-1.39662,-1.3965,-1.39613,-1.39289,-1.38096,-1.36797,-1.36103,-1.35745,-1.35566,-1.35472,-1.35418,-1.35384,-1.3536,-1.35342,-1.35328,-1.35317,-1.35309,-1.35302,-1.35297,-1.35292,-1.35289,-1.35286,-1.35284,-1.35282,-1.35281,-1.3528,-1.35279,-1.35279,-1.35278,-1.35278,-1.35277,-1.35277,-1.35276,-1.35276,-1.35276,-1.35275,-1.35275,-1.35275,-1.35275,-1.35274,-1.35274,-1.35274,-1.35273,-1.35273,-1.35273,-1.35273,-1.35272,-1.35272,-1.35272,-1.35272,-1.35289,-1.35272,-1.35211,-1.34695,-1.33132,-1.31651,-1.30952,-1.30648,-1.3045,-1.3024,-1.3,-1.29796,-1.2968,-1.29627,-1.29596,-1.29566,-1.29529,-1.29478,-1.29405,-1.29306,-1.29198,-1.29092,-1.28949,-1.28761,-1.28625,-1.28566,-1.28534,-1.28513,-1.28493,-1.2847,-1.28434,-1.28381,-1.28313,-1.28241,-1.28178,-1.28122,-1.28082,-1.28065,-1.2943,-1.28916,-1.28759,-1.28702,-1.28661,-1.28623,-1.28585,-1.28544,-1.28502,-1.28461,-1.28423,-1.28387,-1.28371,-1.28307,-1.28158,-1.26899,-1.23482,-1.22341,-1.21594,-1.19737,-1.19714,-1.19975,-1.1896,-1.19284,-1.20247,-1.18918,-1.19477,-1.18367,-1.20162,-1.19307,-1.18326,-1.18635,-1.20425,-1.18976,-1.18124,-1.18959,-1.18244,-1.20122,-1.1864,-1.19318,-1.18511,-1.18002,-1.19755,-1.19758,-1.188,-1.18181,-1.17571,-1.20785,-1.19568,-1.18582,-1.17833,-1.18701,-1.20379,-1.18956,-1.19287,-1.18497,-1.18023,-1.19774,-1.19758,-1.188,-1.18182,-1.17575,-1.20808,-1.19564,-1.18481,-1.16671,-1.14111,-1.14806,-1.13249,-1.12566,-1.12688,-1.12395,-1.123,-1.12234,-1.11592,-1.09431,-1.08551,-1.1195,-1.10148,-1.08888,-1.0898,-1.09184,-1.09748,-1.09817,-1.10122,-1.09189,-1.08428,-1.09894,-1.1054,-1.08806,-1.06534,-1.10027,-1.10267,-1.08555,-1.09185,-1.09725,-1.07603,-1.08416,-1.10567,-1.0798,-1.08937,-1.08561,-1.09517,-1.08449,-1.08929,-1.08075,-1.08937,-1.10274,-1.0882,-1.08068,-1.1039,-1.09688]
32×26 Array{Float64,2}:
  0.107586    -0.096415      0.214939    -0.0178585    0.00567204  -0.159833    -0.0610199    0.0757785    0.171096    0.0795376   -0.0437007   -0.0712662    0.0200101   -0.0456532   -0.102283      0.13084     -0.0691805    0.0853046    -0.152271     0.0186279   -0.00405805  -0.0292966    -0.101483    -0.0839283   -0.121485    -0.0886958 
  0.0123454   -0.0410264     0.100788     0.0209175   -0.0203671   -0.0149111   -0.0835545    0.181104    -0.150913   -0.015687     0.0628594   -0.198179     0.0286567    0.0493835    0.0885922     0.00203792  -0.142796    -0.000533607   0.0529138   -0.102361     0.0924156    0.112589     -0.0535816    0.108299     0.191113    -0.0383935 
  0.0960221    0.102588     -0.0619567   -0.0128862    0.00872728  -0.0169527    0.190359     0.157819     0.0878606  -0.151633     0.0157003    0.0339185   -0.0634137    0.0773515   -0.0725883    -0.162896     0.122895     0.364779      0.147949     0.0326031   -0.0263949   -0.0396678     0.00422421   0.083459    -0.142754    -0.154662  
  0.111991    -0.184629     -0.123343    -0.05731     -0.00215273   0.0190225   -0.0573684    0.073554    -0.0984541  -0.0534714    0.0693899   -0.0852762   -0.079784    -0.0335463   -0.00233393    0.0828607   -0.0599105    0.184094     -0.00177593   0.0776536   -0.0432293    0.0358632     0.038337    -0.0931398   -0.135406    -0.180854  
 -0.0143246   -0.040557      0.0979143    0.220127    -0.118459    -0.136253    -0.0835373   -0.164264     0.170779   -0.00862989   0.0256331   -0.210748     0.0542858    0.215561     0.0753716     0.00277495   0.0527991   -0.178792      0.0950141   -0.109711     0.0722424    0.0296875    -0.103205    -0.133803     0.0425943   -0.103307  
 -0.182495    -0.0340147    -0.0120018   -0.00862039   0.0231728    0.060204    -0.0895694    0.00149532   0.0209485  -0.0310397    0.0451523    0.00630833  -0.0629796   -0.0554674    0.0293405    -0.187305    -0.016335    -0.0925834     0.0668417    0.136285     0.0746393   -0.0429935     0.130199    -0.168858    -0.0999818    0.00479193
 -0.0893265    0.0383169     0.00149262   0.0107368    0.0934678    0.0717037   -0.0616433    0.0372879   -0.0682765   0.0452842   -0.19998      0.0058079   -0.160366     0.101145     0.0766374    -0.0974602    0.0872334   -0.205392      0.0516604    0.0487962    0.093995     0.168876      0.0252763   -0.0801799   -0.129188     0.00453393
  0.0798544    0.104003     -0.0537863    0.122148    -0.129369     0.0274647   -0.166851    -0.0226752   -0.0736712   0.28366     -0.1534       0.0937657    0.11349     -0.126418    -0.00802958   -0.220538    -0.132104     0.0714376     0.0869935    0.0895561   -0.0505713    0.0427307    -0.0881471    0.0291567    0.0191287   -0.200189  
 -0.00296778   0.043364      0.103397     0.0229338    0.0165097   -0.0522795   -0.0397184    0.0216084   -0.176687    0.00536953  -0.0831767   -0.0414039   -0.170521     0.0284473   -0.0507946    -0.0114513   -0.0224932    0.0233003    -0.00681875  -0.0567481   -0.0247258    0.000624088   0.0701369   -0.0789822   -0.0539453    0.0323294 
 -0.00409154  -0.0361996     0.0369953   -0.118671    -0.0468085   -0.178652    -0.178349    -0.139506     0.0208238   0.00842696   0.0649431   -0.0712486   -0.0568864   -0.0185682    0.0660627     0.0603027   -0.0274345    0.087147     -0.00232968  -0.0371342   -0.0375071    0.000580849   0.0081909    0.00430074  -0.0395416    0.0330319 
 -0.180534    -0.00253253   -0.0464531    0.0365478    0.192864    -0.0398701   -0.187122    -0.102016     0.118148   -0.0540504   -0.071803    -0.0556309    0.180596     0.00764566  -0.199182      0.146826     0.108774    -0.0101531    -0.206558     0.131548     0.0279884    0.105142     -0.300012    -0.01888     -0.00596101   0.0832832 
 -0.0182224    0.0512853    -0.0493459    0.0290215    0.030086     0.0222875   -0.0535472   -0.0500439    0.184742   -0.107005     0.0881431   -0.0202159    0.0453789   -0.0343008    0.252989     -0.0324365    0.0516606    0.0642801     0.189965     0.174726     0.0805521    0.153036      0.16219      0.0708758   -0.0774342   -0.0367766 
 -0.10936      0.0277475     0.145016    -0.0377137    0.0830494   -0.0327892    0.0510593   -0.00396889  -0.0374843   0.145853    -0.0444491    0.02846      0.108636    -0.0255258   -0.212794      0.0340732    0.00792458   0.0525875     0.0236422   -0.0425822    0.0957757    0.1277       -0.0524333    0.416701    -0.0359775    0.210178  
 -0.137395     0.0677651     0.0225816   -0.0320005    0.160927    -0.0125853    0.0842412   -0.00131164  -0.0789398   0.00366819  -0.0826923    0.0427005    0.114572    -0.0250369    0.0710942     0.030988     0.0871776    0.0559119     0.00650129  -0.0300088   -0.00271698   0.124761     -0.0676974    0.128757     0.584153     0.225667  
  0.00764331   0.0770323     0.0987702   -0.0996296   -0.0544358   -0.637818     0.177943    -0.0578343   -0.214064   -0.0556966   -0.118591     0.00518057  -0.148908     0.02815     -0.00866844    0.126413     0.0109534   -0.115377      0.105412     0.15546     -0.00530991   0.147398     -0.0491588    0.0555428    0.0288478   -0.0329496 
  0.0138216    0.115457      0.0903357   -0.0548156    0.0292295    0.481878     0.160887     0.11264     -0.185021   -0.0450298   -0.0991301    0.0250359   -0.00622119   0.0451417    0.036936      0.0831654    0.01824     -0.162278     -0.0252959    0.211612     0.0681222    0.176617     -0.0517598    0.017542    -0.048722     0.0365095 
 -0.060011    -0.00193872   -0.0685128   -0.162999    -0.00329049  -0.0763452   -0.0422696   -0.0852876    0.0522283   0.103722     0.00244978  -0.0786863    0.00979574  -0.0409555    0.0398748    -0.0897865   -0.0473438   -0.0799995    -0.0210206    0.0280948    0.0206396    0.0947231    -0.030668    -0.0465534   -0.0258311   -0.0245063 
  0.166656     0.0929981     0.0833049   -0.0517118    0.151257    -0.0478183   -0.0221936    0.0152261    0.0107079   0.10262     -0.196015     0.0700069   -0.0988217    0.084716     0.0702344     0.0099545    0.0837057    0.0975267    -0.0680529    0.084061     0.131342    -0.0368017    -0.054212     0.107773     0.091349     0.0458205 
 -0.18578      0.0049109     0.0331197    0.190671     0.0942015    0.00684918   0.0203844   -0.0561266   -0.193525    0.0354956   -0.0820688    0.0603109    0.114511     0.146229    -0.103864      0.123158    -0.0958189    0.0292803    -0.0733273    0.00249314  -0.0365477   -0.0954538    -0.053495     0.0347015    0.00325228   0.0900177 
  0.0432755    0.0854041     0.0617702   -0.180868     0.117636    -0.0287239    0.0276965    0.0801527    0.078456   -0.184273    -0.0335474    0.034862     0.0547667    0.0547439   -0.0249029     0.0329801    0.0276551   -0.0458758    -0.0189243    0.026538     0.0485634   -0.0871579     0.0526063    0.0754439   -0.11901      0.0114812 
  0.0429704   -0.246844      0.0165686    0.0777842    0.105208    -0.132528     0.00146242  -0.087715     0.0830173   0.0932827    0.127035    -0.22396      0.161208    -0.0490039   -0.136081      0.0254502    0.154408     0.0613106    -0.00709337  -0.0340036   -0.10733     -0.0757344    -0.0137441   -0.18901     -0.0622371    0.196123  
  0.00872294  -0.00682089    0.145644     0.138142     0.0967727    0.00420908  -0.0377927    0.0767098   -0.0217176  -0.0269806   -0.0501792    0.0326069   -0.0229519    0.0517207   -0.000729578  -0.0452491    0.0233978    0.0592113     0.00251201  -0.0449316   -0.0667662    0.0479764    -0.0837467    0.0341572   -0.0817788   -0.0463856 
  0.165886     0.2066       -0.0512262   -0.0333426    0.0634222   -0.066034     0.0462302   -0.173388    -0.140912   -0.125773    -0.169284    -0.0282958   -0.146057    -0.0110066   -0.0701645    -0.134232    -0.12273      0.00651835    0.0187842    0.0915332    0.0226413    0.0103159    -0.82428     -0.143272     0.209454    -0.115365  
  0.166435    -0.0630518    -0.0628934   -0.00356695  -0.196617    -0.100915     0.00569799  -0.0861611   -0.246568   -0.199882    -0.175897    -0.0701961   -0.165404     0.0228616    0.0312266    -0.131042     0.137272     0.0102816     0.13346     -0.0428951    0.0402459    0.0124729     1.07724     -0.134985     0.202821     0.0588956 
 -0.184269    -0.313424      0.255189    -0.169415     0.0466935   -0.0777496   -0.153734    -0.7504       0.0840038   0.0143344   -0.0692235   -0.0740227    0.0564604    0.0260262   -0.00837453   -0.0295421    0.0946344    0.142282      0.243091    -0.0361732   -0.0343946    0.186443     -0.0681572    0.0408426   -0.0184155    0.037778  
 -0.0622183   -0.0280146    -0.250732    -0.0157457    0.0387116   -0.12964     -0.145592     0.54652      0.0209299   0.139748    -0.0673361    0.05516     -0.158103     0.105333    -0.0125628    -0.00877273  -0.0330033    0.211865      0.144975     0.0672741   -0.0201902    0.232448     -0.0674053    0.0443953   -0.0787081    0.0706658 
 -0.00435414  -0.060946      0.0621426   -0.0378442    0.111383     0.00878214   0.0882197    0.140028    -0.193715   -0.229511     0.139067    -0.0466695   -0.745052     0.0646766    0.229834      0.208915     0.150358    -0.184244      0.108514     0.162665    -0.202644     0.0857995     0.271694    -0.108387     0.0657205   -0.0719892 
 -0.0355598   -0.0116223    -0.00100488  -0.00134015   0.117861    -0.0787231    0.0693238    0.0050923   -0.198844   -0.29125      0.120377    -0.0760215    0.797136    -0.0558813    0.260341      0.21375      0.113473    -0.145748      0.143614    -0.00734789  -0.190389    -0.0543013     0.186798    -0.108312     0.115152    -0.0760674 
 -0.123818    -0.0912043    -0.108832    -0.0327395   -0.0737165    0.0719728    0.152762     0.150308    -0.0995694  -0.0020233    0.023692     0.203324     0.0649794   -0.0259764    0.0735641     0.0595459   -0.120393     0.035108     -0.0797621    0.0353795   -0.210089     0.0387088    -0.0942317    0.087702    -0.0746515    0.0938745 
  0.161711     0.000408525  -0.10246     -0.379633     0.0751387   -0.37492      0.192539    -0.0682148   -0.123887    0.067126     0.158266    -0.612629    -0.080446    -0.163505     0.0398702     0.104372    -0.0367794    0.0853342     0.43446      0.00752016   0.586092    -0.175806     -0.133055     0.0512688   -0.0157147   -0.223621  
  0.0282154   -0.0545664    -0.00548979  -0.111144    -0.0158076   -0.0606673   -0.105336     0.080062     0.168934    0.0664926    0.00638813   0.0430458   -0.329245    -0.0972973   -0.082072     -0.00452253   0.0843301    0.0796889     0.157502     0.0270854    0.050271    -0.168878     -0.00489633  -0.0819807   -0.0632912    0.267103  
  0.0175526    0.113358     -0.0440548   -0.161561     0.0269843   -0.023284    -0.0354039    0.0504232    0.206576   -0.0175367   -0.0471422    0.0554009    0.509961     0.00116905  -0.0868046     0.0273365    0.103823     0.075379     -0.0647444   -0.201743     0.0409949    0.0306085    -0.0162556   -0.0971673   -0.0347431   -0.113243  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 3 5 20 29 30
INFO: iteration 1, average log likelihood -1.084948
WARNING: Variances had to be floored 2 3 5 6 20 29 30
INFO: iteration 2, average log likelihood -1.062057
WARNING: Variances had to be floored 1 2 3 5 11 12 20 29 30
INFO: iteration 3, average log likelihood -1.063656
WARNING: Variances had to be floored 2 3 5 6 20 29 30
INFO: iteration 4, average log likelihood -1.076044
WARNING: Variances had to be floored 2 3 5 20 29 30
INFO: iteration 5, average log likelihood -1.070481
WARNING: Variances had to be floored 1 2 3 5 6 11 12 20 29 30
INFO: iteration 6, average log likelihood -1.055097
WARNING: Variances had to be floored 2 3 5 20 29 30
INFO: iteration 7, average log likelihood -1.084590
WARNING: Variances had to be floored 2 3 5 6 20 29 30
INFO: iteration 8, average log likelihood -1.061876
WARNING: Variances had to be floored 1 2 3 5 11 12 20 29 30
INFO: iteration 9, average log likelihood -1.063591
WARNING: Variances had to be floored 2 3 5 6 20 29 30
INFO: iteration 10, average log likelihood -1.076048
INFO: EM with 100000 data points 10 iterations avll -1.076048
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.929510e+05
      1       7.148471e+05      -1.781039e+05 |       32
      2       6.833636e+05      -3.148346e+04 |       32
      3       6.694455e+05      -1.391816e+04 |       32
      4       6.603295e+05      -9.115948e+03 |       32
      5       6.548557e+05      -5.473797e+03 |       32
      6       6.518368e+05      -3.018951e+03 |       32
      7       6.499066e+05      -1.930175e+03 |       32
      8       6.485054e+05      -1.401255e+03 |       32
      9       6.474662e+05      -1.039138e+03 |       32
     10       6.466286e+05      -8.376196e+02 |       32
     11       6.458724e+05      -7.562044e+02 |       32
     12       6.450440e+05      -8.283985e+02 |       32
     13       6.442876e+05      -7.563830e+02 |       32
     14       6.436774e+05      -6.101893e+02 |       32
     15       6.432475e+05      -4.298755e+02 |       32
     16       6.428169e+05      -4.306361e+02 |       32
     17       6.422871e+05      -5.298264e+02 |       32
     18       6.415193e+05      -7.677808e+02 |       32
     19       6.401575e+05      -1.361830e+03 |       32
     20       6.388366e+05      -1.320845e+03 |       32
     21       6.379835e+05      -8.530859e+02 |       32
     22       6.374080e+05      -5.755263e+02 |       32
     23       6.368928e+05      -5.151723e+02 |       32
     24       6.363958e+05      -4.970759e+02 |       32
     25       6.360108e+05      -3.849818e+02 |       32
     26       6.357979e+05      -2.129367e+02 |       32
     27       6.356407e+05      -1.571142e+02 |       32
     28       6.354633e+05      -1.774633e+02 |       32
     29       6.352496e+05      -2.136874e+02 |       32
     30       6.350244e+05      -2.251492e+02 |       32
     31       6.348278e+05      -1.966520e+02 |       32
     32       6.347054e+05      -1.223374e+02 |       31
     33       6.346235e+05      -8.197942e+01 |       32
     34       6.345561e+05      -6.737780e+01 |       32
     35       6.345036e+05      -5.250521e+01 |       32
     36       6.344648e+05      -3.874905e+01 |       32
     37       6.344270e+05      -3.784604e+01 |       32
     38       6.343993e+05      -2.767141e+01 |       32
     39       6.343788e+05      -2.054194e+01 |       32
     40       6.343584e+05      -2.037115e+01 |       31
     41       6.343397e+05      -1.867220e+01 |       28
     42       6.343245e+05      -1.526450e+01 |       32
     43       6.343076e+05      -1.686691e+01 |       29
     44       6.342936e+05      -1.403445e+01 |       31
     45       6.342857e+05      -7.818721e+00 |       27
     46       6.342805e+05      -5.213731e+00 |       25
     47       6.342758e+05      -4.710578e+00 |       27
     48       6.342710e+05      -4.807586e+00 |       25
     49       6.342676e+05      -3.460901e+00 |       26
     50       6.342637e+05      -3.821928e+00 |       26
K-means terminated without convergence after 50 iterations (objv = 634263.7338000643)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.344603
INFO: iteration 2, average log likelihood -1.315769
INFO: iteration 3, average log likelihood -1.285702
INFO: iteration 4, average log likelihood -1.250891
INFO: iteration 5, average log likelihood -1.211773
WARNING: Variances had to be floored 31
INFO: iteration 6, average log likelihood -1.153837
WARNING: Variances had to be floored 12 16 20 28
INFO: iteration 7, average log likelihood -1.111879
INFO: iteration 8, average log likelihood -1.125667
WARNING: Variances had to be floored 15 26
INFO: iteration 9, average log likelihood -1.072884
WARNING: Variances had to be floored 1 3 14 31
INFO: iteration 10, average log likelihood -1.063486
WARNING: Variances had to be floored 12 16 20
INFO: iteration 11, average log likelihood -1.094011
WARNING: Variances had to be floored 10 28
INFO: iteration 12, average log likelihood -1.100115
WARNING: Variances had to be floored 15 26 31
INFO: iteration 13, average log likelihood -1.062122
WARNING: Variances had to be floored 3 14 16 25
INFO: iteration 14, average log likelihood -1.065953
WARNING: Variances had to be floored 1 12 20
INFO: iteration 15, average log likelihood -1.082411
INFO: iteration 16, average log likelihood -1.085814
WARNING: Variances had to be floored 10 15 16 20 21 26 28 31
INFO: iteration 17, average log likelihood -1.020524
WARNING: Variances had to be floored 1 3 12 25
INFO: iteration 18, average log likelihood -1.092909
WARNING: Variances had to be floored 14
INFO: iteration 19, average log likelihood -1.115540
INFO: iteration 20, average log likelihood -1.080683
WARNING: Variances had to be floored 12 15 31
INFO: iteration 21, average log likelihood -1.038360
WARNING: Variances had to be floored 16 20 21 25 28
INFO: iteration 22, average log likelihood -1.048948
WARNING: Variances had to be floored 3 10 12 14 26
INFO: iteration 23, average log likelihood -1.068151
WARNING: Variances had to be floored 1 31
INFO: iteration 24, average log likelihood -1.085941
WARNING: Variances had to be floored 15
INFO: iteration 25, average log likelihood -1.078928
WARNING: Variances had to be floored 12 16 20 28
INFO: iteration 26, average log likelihood -1.044954
WARNING: Variances had to be floored 3 14 21 25 26 31
INFO: iteration 27, average log likelihood -1.054976
WARNING: Variances had to be floored 1 10 15
INFO: iteration 28, average log likelihood -1.091633
WARNING: Variances had to be floored 12
INFO: iteration 29, average log likelihood -1.092314
WARNING: Variances had to be floored 16 20 28
INFO: iteration 30, average log likelihood -1.057462
WARNING: Variances had to be floored 3 14 15 26 31
INFO: iteration 31, average log likelihood -1.047490
WARNING: Variances had to be floored 1 12 21 25
INFO: iteration 32, average log likelihood -1.077162
WARNING: Variances had to be floored 10
INFO: iteration 33, average log likelihood -1.096009
WARNING: Variances had to be floored 16 20 28
INFO: iteration 34, average log likelihood -1.042375
WARNING: Variances had to be floored 3 12 14 15 26 31
INFO: iteration 35, average log likelihood -1.037729
WARNING: Variances had to be floored 1 21 25
INFO: iteration 36, average log likelihood -1.094369
WARNING: Variances had to be floored 10
INFO: iteration 37, average log likelihood -1.084981
WARNING: Variances had to be floored 12 16 20 28
INFO: iteration 38, average log likelihood -1.036171
WARNING: Variances had to be floored 3 14 15 26 31
INFO: iteration 39, average log likelihood -1.052202
WARNING: Variances had to be floored 1 21 25
INFO: iteration 40, average log likelihood -1.082682
WARNING: Variances had to be floored 10 12
INFO: iteration 41, average log likelihood -1.080464
WARNING: Variances had to be floored 16 20 28
INFO: iteration 42, average log likelihood -1.051118
WARNING: Variances had to be floored 3 14 15 26 31
INFO: iteration 43, average log likelihood -1.041632
WARNING: Variances had to be floored 1 12 21 25
INFO: iteration 44, average log likelihood -1.078459
INFO: iteration 45, average log likelihood -1.096672
WARNING: Variances had to be floored 10 12 15 16 20 28
INFO: iteration 46, average log likelihood -1.033572
WARNING: Variances had to be floored 3 14 26 31
INFO: iteration 47, average log likelihood -1.069122
WARNING: Variances had to be floored 1 21 25
INFO: iteration 48, average log likelihood -1.086050
WARNING: Variances had to be floored 12
INFO: iteration 49, average log likelihood -1.093250
WARNING: Variances had to be floored 15 16 28
INFO: iteration 50, average log likelihood -1.060620
INFO: EM with 100000 data points 50 iterations avll -1.060620
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.0925956    0.102125    -0.0651905    -0.019537     0.00776767  -0.0122797    0.189587     0.13707      0.0935647   -0.157619     0.0112387   0.0431523    -0.0616646   0.0583265    -0.0719123   -0.174803     0.120119     0.417298      0.154327     0.0523423   -0.0197308   -0.0276873    -0.000415339   0.0871148   -0.125786    -0.150758  
  0.0270445    0.0804845    0.0798754    -0.106686    -0.0915371   -1.01788      0.0849689   -0.118773    -0.287968    -0.0947682   -0.143647    0.000488549  -0.240884    0.0170687    -0.103771     0.136891     0.0104207    0.0151335     0.146085     0.115099    -0.0492151    0.148048     -0.0450847     0.0603971    0.129192    -0.0274723 
  0.115747    -0.0773067    0.252603     -0.0590671   -0.00154352  -0.202677    -0.0527933    0.0690804    0.15449      0.124244    -0.0454758  -0.0563992     0.0335865  -0.0450764    -0.122928     0.141819    -0.0746746    0.0751276    -0.157385     0.048165    -0.00599852   0.000910844  -0.0903641    -0.096476    -0.147524    -0.0807207 
  0.127813    -0.0428977   -0.108228     -0.149901     0.0740411    0.0569332   -0.0333964    0.0563326   -0.0343825   -0.0377493   -0.0281798  -0.191169     -0.0347899  -0.0755789     0.0684097    0.0451588   -0.0191014    0.142694      0.0612655    0.100237    -0.185652     0.103843      0.118584     -0.116953    -0.0889553   -0.0950066 
 -0.0893006    0.0381241    0.0014963     0.0108788    0.0934577    0.0721032   -0.0615972    0.0372824   -0.06822      0.0453107   -0.200334    0.00588334   -0.160285    0.101553      0.076615    -0.0974376    0.0873783   -0.205236      0.0511164    0.0489841    0.0943255    0.168428      0.0254336    -0.0814994   -0.129494     0.00438424
 -0.0201273   -0.0360852    0.0301538    -0.0192571    0.114646    -0.0358524    0.0787411    0.0719391   -0.19623     -0.260667     0.129592   -0.0613361     0.0350405   0.00373667    0.245245     0.211365     0.131847    -0.165087      0.126222     0.0766341   -0.196492     0.0149828     0.228634     -0.108367     0.0906321   -0.0740681 
  0.00600088  -0.0310769    0.106193      0.116102    -0.0647682   -0.0788887   -0.0851212    0.0171767    0.00373314  -0.00144155   0.0449366  -0.206824      0.0375966   0.130844      0.0791627    0.00518201  -0.0434088   -0.0879184     0.0704098   -0.108277     0.0841534    0.0704432    -0.078797     -0.00700312   0.120158    -0.0728186 
  0.0420107   -0.246693     0.0102799     0.082803     0.104046    -0.132354     0.00132194  -0.0927956    0.0828636    0.0911417    0.128873   -0.227517      0.164072   -0.0512224    -0.136097     0.0215858    0.158255     0.0614166     0.00379013  -0.0386846   -0.10975     -0.0755522    -0.014115     -0.190498    -0.060507     0.199359  
  0.0212051    0.0849082    0.0360427    -0.0639773    0.0913739   -0.049742    -0.136578    -0.05387     -0.222984    -0.0736037   -0.0497028  -0.134246     -0.0362651   0.157733     -0.108026     0.0654383   -0.00920678   0.0275349     0.00612751  -0.0521541   -0.118691     0.0628164     0.118951     -0.179789    -0.0666084    0.129758  
 -0.0371033    0.0625176   -0.180992     -0.199826    -0.14627     -0.123028    -0.0989839   -0.228937     0.105907     0.191277     0.0178576   0.018641     -0.018007   -0.132223      0.168542    -0.0548909    0.151424    -0.1989       -0.184143     0.189643    -0.0118481    0.156415      0.0358483    -0.0034058   -0.0245546   -0.153994  
  0.0799564    0.104172    -0.0538563     0.121912    -0.130321     0.0273984   -0.166898    -0.0221615   -0.0715468    0.283777    -0.153898    0.0929098     0.114224   -0.126329     -0.00798316  -0.221806    -0.132366     0.0718407     0.0868875    0.091066    -0.0507796    0.0422842    -0.0879665     0.0292623    0.0189952   -0.20038   
 -0.194999    -0.111074    -0.114039     -0.00379908  -0.120319     0.0424208    0.102305     0.281701    -0.0925996   -0.0494364    0.0370342   0.204862      0.134681    0.0553556     0.093782     0.062088    -0.176419     0.0108627    -0.0792109    0.0503033   -0.210135     0.0447127    -0.081945      0.100155    -0.117156     0.130255  
  0.00673828   0.100993     0.0977014    -0.0660617    0.00978811   0.199365     0.191518     0.0704169   -0.178117    -0.0400753   -0.0996978   0.0198274    -0.0320932   0.0422412     0.0441445    0.0960565    0.0161845   -0.179018      0.0102431    0.20124      0.0547273    0.167007     -0.0519925     0.0279096   -0.0465311    0.0116595 
  0.0935857   -0.279595    -0.153246      0.0388558   -0.069618    -0.00054693  -0.0596956    0.0994587   -0.146524    -0.0749268    0.151533    0.0122583    -0.107536   -0.000491429  -0.0618923    0.131278    -0.0860344    0.238324     -0.0511415    0.0640967    0.0753737   -0.0147102    -0.0372082    -0.0490016   -0.168453    -0.24709   
  0.0229271    0.0168532    0.219011      0.213363     0.165913    -0.0126416   -0.145888     0.13971     -0.0106956   -0.103791    -0.0539575   0.0169484    -0.0812547   0.103851     -0.151841     0.00613336  -0.0160274    0.120388      0.0789873   -0.0773354   -0.0344209    0.144509     -0.101213      0.140154    -0.0903115   -0.0432559 
 -0.180426    -0.00632437  -0.0503423     0.036965     0.208601    -0.0375977   -0.195859    -0.103564     0.121787    -0.0615924   -0.0674743  -0.0521049     0.19154     0.00673045   -0.195735     0.147254     0.122225    -0.0101192    -0.19491      0.140106     0.0364124    0.112894     -0.299813     -0.0120766   -0.00740911   0.0778442 
 -0.123966     0.0479435    0.0830108    -0.0347039    0.122382    -0.0227377    0.0679822   -0.00276941  -0.0580596    0.0749907   -0.0635722   0.0359237     0.111595   -0.0255254    -0.0695605    0.0327274    0.0468159    0.0535219     0.0148733   -0.036331     0.0472208    0.126053     -0.060525      0.272671     0.275872     0.218426  
 -0.187847     0.0037705    0.0326544     0.19484      0.0955938    0.00836358   0.0205033   -0.0542757   -0.199169     0.0350042   -0.083522    0.0688844     0.121514    0.149001     -0.106488     0.123006    -0.0961755    0.0282712    -0.0740087    0.00167844  -0.0365024   -0.098307     -0.0559461     0.0336465    0.0049944    0.090856  
 -0.0117879    0.0188508    0.088537      0.0419286   -0.114418    -0.0326986   -0.0218329   -0.00377161  -0.136902     0.0717684   -0.081697   -0.022498     -0.277437    0.109886     -0.096693    -0.040199    -0.122712     0.0289885    -0.0240077   -0.0174527    0.165823    -0.118288      0.0580859    -0.00781184  -0.085803     0.0502979 
  0.0395944    0.106789     0.0239643    -0.17811      0.139036    -0.0204902    0.0781944    0.119639     0.0891201   -0.2347      -0.0292178   0.0654039     0.0500907   0.0547469    -0.0162745    0.0241188    0.0347739   -0.0500695     0.0430971    0.0163629    0.0570575   -0.0936814     0.0654922     0.0989975   -0.120931     0.0134434 
  0.0210269   -0.0659932   -0.104382     -0.151667     0.00991053  -0.0270178    0.203195    -0.0559375   -0.126451     0.0708815    0.0643755  -0.0652837    -0.0387755  -0.147713      0.0436113    0.0824092   -0.0447152    0.044297      0.0729146    0.0095355    0.0440504   -0.0407084    -0.11589       0.0581832   -0.0191537   -0.0446271 
 -0.0324721    0.0271947    0.183439      0.0908952    0.0560685   -0.0648129    0.0540712    0.134118    -0.162431     0.0128077   -0.108348    0.0383722    -0.217998   -0.180385      0.0637167   -0.0797277    0.0597577    0.00396606   -0.00130864  -0.106098    -0.125165     0.0459092     0.0127372    -0.0407982   -0.0109383   -0.103794  
 -0.117082    -0.159439    -0.022131     -0.0857105    0.042346    -0.10686     -0.149346    -0.043898     0.0494367    0.0809549   -0.0688309  -0.00309454   -0.0607315   0.0694583    -0.0107919   -0.0187634    0.0277955    0.180435      0.190623     0.0191098   -0.0272664    0.212402     -0.0686245     0.042789    -0.0511873    0.0560713 
  0.165556     0.0918489    0.0797407    -0.0523106    0.15118     -0.0478282   -0.0229029    0.0190371    0.0115643    0.10418     -0.196499    0.071201     -0.0987351   0.0845872     0.070273     0.0102927    0.086167     0.0975796    -0.0673679    0.0834091    0.136564    -0.0355767    -0.0549888     0.108812     0.0918857    0.0458528 
 -0.100382     0.0751188    0.00187519   -0.255036     0.0341004   -0.140368     0.0504832    0.0116684    0.00978808   0.065532    -0.0432629  -0.096145     -0.0262016  -0.109061     -0.00741642  -0.167851    -0.19267     -0.017673      0.034882    -0.0843432    0.132446     0.0866814    -0.0810176    -0.166794    -0.0274684    0.0365797 
 -0.0237589    0.111566    -0.0747256     0.0438749    0.00571527   0.0228364   -0.0463772   -0.0603695    0.199147    -0.126834     0.104754    0.0108119     0.0416612  -0.0572897     0.312372    -0.0474585    0.0617445    0.0429426     0.197589     0.203448     0.112485     0.16743       0.18231       0.076404    -0.0749663   -0.0481385 
  0.165102     0.064302    -0.0574523    -0.0191327   -0.0702715   -0.0832331    0.024357    -0.130096    -0.196386    -0.167165    -0.171855   -0.044747     -0.154267    0.00661777   -0.0164005   -0.131648     0.0140426    0.00813478    0.0820987    0.0202707    0.0308567    0.0113533     0.163496     -0.13825      0.210978    -0.025188  
 -0.0285038   -0.171247    -0.000974242  -0.022761     0.134514     0.0695971   -0.111149    -0.0695656    0.0588814   -0.0200524    0.0304113  -0.161507      0.100621    0.170994      0.00801317  -0.0936689   -0.0148087   -0.00392689    0.0480441   -0.0101906   -0.0856724    0.0305938    -0.00111497    0.093732    -0.02125      0.043684  
  0.020074     0.0326135   -0.0265148    -0.136367     0.00490207  -0.041135    -0.0696425    0.0692297    0.189367     0.0196508   -0.0171231   0.0506641     0.105801   -0.0479894    -0.0848354    0.0119705    0.0928493    0.07686       0.0398171   -0.0930519    0.0418142   -0.0680827    -0.0109695    -0.0882766   -0.0490837    0.0758877 
 -0.00412744  -0.0366704    0.0369672    -0.118867    -0.0468255   -0.178738    -0.177867    -0.139443     0.0207555    0.00822161   0.0650065  -0.0712746    -0.0568525  -0.0183364     0.065994     0.0601109   -0.0275529    0.086811     -0.00245104  -0.0374355   -0.0375168    0.000322441   0.00816298    0.00429324  -0.0394838    0.0331733 
 -0.172431    -0.0367601   -0.0103461     0.0039445    0.0202218    0.0542815   -0.0884837   -0.0092507    0.0241263   -0.0325528    0.0445577  -0.00141287   -0.0533332  -0.0535907     0.033704    -0.184763    -0.0134565   -0.0904866     0.0677471    0.129156     0.071914    -0.0422813     0.128174     -0.165697    -0.0988265    0.00531802
 -0.0290088   -0.0108243    0.0419694     0.138125     0.0660284    0.0989845    0.0892682    0.0411909   -0.0792882   -0.0136852   -0.0474082   0.0743126     0.0202884   0.07855       0.158495    -0.118918     0.0708582    0.000954985   0.0178941   -0.0608338   -0.11429     -0.0571013    -0.0686745    -0.0347292   -0.0592915   -0.0207259 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 12 14 20 31
INFO: iteration 1, average log likelihood -1.040346
WARNING: Variances had to be floored 1 3 10 12 14 16 20 21 25 26 31
INFO: iteration 2, average log likelihood -0.999359
WARNING: Variances had to be floored 3 10 12 14 15 20 28 31
INFO: iteration 3, average log likelihood -1.012793
WARNING: Variances had to be floored 1 3 12 14 16 20 21 25 26 31
INFO: iteration 4, average log likelihood -1.015223
WARNING: Variances had to be floored 3 10 12 14 20 31
INFO: iteration 5, average log likelihood -1.021081
WARNING: Variances had to be floored 1 3 10 12 14 15 16 20 21 25 26 28 31
INFO: iteration 6, average log likelihood -0.987311
WARNING: Variances had to be floored 3 12 14 20 31
INFO: iteration 7, average log likelihood -1.040266
WARNING: Variances had to be floored 1 3 10 12 14 16 20 21 25 26 31
INFO: iteration 8, average log likelihood -0.999100
WARNING: Variances had to be floored 3 10 12 14 15 20 28 31
INFO: iteration 9, average log likelihood -1.012621
WARNING: Variances had to be floored 1 3 12 14 16 20 21 25 26 31
INFO: iteration 10, average log likelihood -1.015215
INFO: EM with 100000 data points 10 iterations avll -1.015215
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.103895   -0.164132    -0.0671282    0.0675839   0.0138884   -0.147977      0.0905165   -0.0526488   -0.0749784   -0.170874     0.0662517    0.128042     0.00328541  -0.102787    0.0181216    0.0696966     0.0838999     0.0160314    0.109715    -0.125123     0.0887237     0.0835181    0.0663325    0.0979678    0.079948     0.0639931  
  0.0607397  -0.273566    -0.149673     0.070661   -0.105756    -0.000577746   0.0784197   -0.119207    -0.078808     0.065345     0.0204425    0.182111     0.0289789   -0.067952    0.188543    -0.0318577    -0.089031      0.048964     0.11237     -0.141424     0.00041494    0.197566    -0.0474358   -0.0558682    0.0180789    0.0593122  
 -0.253448   -0.0519498   -0.0371387    0.145801   -0.0461549    0.0732876     0.0366375    0.174877    -0.0720496   -0.0412248   -0.0724779    0.0233441    0.141895    -0.0503088   0.146122    -0.0883955     0.077731     -0.11729     -0.0366041    0.0456395    0.125206     -0.0151913   -0.040093     0.0956106   -0.00886818  -0.141021   
  0.0516014   0.0452088    0.0312639    0.137136    0.00507947   0.0752335     0.0318826    0.094704    -0.0410267    0.00240894  -0.190212     0.0338195   -0.0719284   -0.0270834  -0.0630897    0.0264197     0.163631     -0.0117861   -0.0517167    0.0948408    0.000649185   0.0775226    0.121577    -0.00655052   0.00224021   0.07513    
 -0.0128376   0.0270376    0.057755     0.0228469  -0.0965297   -0.0988211     0.05004      0.110079    -0.142675    -0.00567554  -0.051152    -0.0957622   -0.0605078   -0.011541    0.0655278    0.0543296    -0.00804769   -0.0808249    0.156993     0.0170619   -0.0930109     0.157845    -0.207338    -0.295666    -0.0242885   -0.0758161  
  0.0164     -0.0418764   -0.0176281   -0.0745403  -0.00581158   0.0763722    -0.0191717   -0.00586406   0.00607692   0.12178     -0.0577514   -0.0306596   -0.0835402   -0.0792904   0.00480261   0.0443235    -0.144227     -0.103626     0.00139152  -0.0196742   -0.0562492     0.152953    -0.15175     -0.0615803   -0.077629    -0.000608891
  0.253801    0.177342     0.0240108   -0.0591306   0.136068     0.0610121     0.065079     0.0276442    0.0427645    0.123262    -0.0456384   -0.15002      0.0446015   -0.0634134  -0.00684051  -0.109495      0.0384012    -0.202683     0.103001     0.00666456   0.264465      0.0152572   -0.00679882   0.0624175    0.0219323   -0.147016   
 -0.0713598  -0.135906    -0.182672    -0.019277    0.0165193   -0.0264591    -0.00480061   0.0274249    0.0165616    0.0285733   -0.0525179   -0.150753     0.040831    -0.0211732  -0.0858206    0.021432     -0.186834      0.111103    -0.00362459   0.156752    -0.0267525    -0.0259087   -0.0321257    0.0522023    0.198513    -0.0422295  
 -0.14925     0.0148379    0.0831478    0.0644698  -0.0891653   -0.0213904     0.0790074   -0.0723462   -0.103651    -0.0674183    0.0107702    0.169773    -0.0237121    0.0257907  -0.0780562   -0.108738      0.000203342  -0.00153564  -0.0577151    0.0336406   -0.11565       0.237604    -0.109992     0.0320076   -0.00859446   0.12783    
 -0.0355722  -0.214973    -0.212171     0.105782   -0.0300046    0.0722937     0.134174     0.0705182   -0.154497     0.0109021    0.0380671   -0.0294547    0.0864999   -0.0217097  -0.0376554    0.153738      0.114695     -0.155261     0.123242    -0.0776731   -0.112911      0.0427821    0.0689128    0.0965577    0.0152696    0.0256827  
  0.0303572  -0.0258777   -0.0984686   -0.0405166  -0.228426     0.0906748     0.102242    -0.0287653    0.0974133   -0.157237     0.0515662    0.0567927   -0.0767668    0.0174474  -0.0369196    0.000577716   0.107721      0.00674706  -0.0665199    0.0111002   -0.0289808     0.0297235    0.138928     0.115825     0.0328726    0.0396218  
  0.0118524  -0.00474868  -0.0176586    0.144539    0.0453759   -0.0613996     0.179246     0.0505085   -0.104798    -0.0722488    0.0180931    0.109774     0.0561247    0.106083   -0.122013     0.132489      0.0159679     0.0537254   -0.110277     0.0734227    0.147834      0.149395     0.15454      0.0109941    0.0635651    0.00724182 
 -0.066555   -0.0220709    0.0937421   -0.0360251  -0.0286696   -0.0565787    -0.078409    -0.162257    -0.0479228   -0.0536942    0.0397074   -0.0538975    0.0971217    0.0410961   0.123919     0.0138156    -0.0773934    -0.0700958    0.135111    -0.0519656    0.115245      0.0284298    0.0666965    0.0635647    0.200546     0.0908687  
 -0.0641171   0.0289474   -0.0700195    0.0495059  -0.0674402   -0.0173671    -0.111813     0.0664277   -0.0290143   -0.037133    -0.097636     0.180017     0.042686     0.0149595   0.00276105  -0.0311996    -0.175486      0.0194025   -0.058775    -0.115667     0.0649446    -0.0199594   -0.0794287   -0.0843059   -0.0968067    0.0658531  
 -0.0826743  -0.18007      0.0818092   -0.0267487  -0.0265559   -0.0723925     0.0572042    0.0667029   -0.10383      0.0146452   -0.0900647    0.126898    -0.0129835   -0.152361   -0.0665434   -0.0288434    -0.062871      0.0419667    0.107237     0.0126558    0.0573573     0.0803694   -0.015168     0.0349541    0.0551749    0.0257469  
  0.0704348  -0.0934422   -0.0629215   -0.0106289  -0.0215229    0.15693       0.0249635   -0.0210048    0.18705     -0.0925429   -0.0516354   -0.115101     0.039352    -0.0968782   0.0542626   -0.0226827    -0.232366      0.0276107    0.10087     -0.0613279   -0.158653      0.0594977   -0.0192038    0.0452455   -0.190287     0.0387957  
 -0.0594493  -0.110446    -0.0159805    0.0324336  -0.0593972   -0.0925169    -0.201445     0.00446013  -0.110315    -0.0811735    0.00958506  -0.0045081    0.16513      0.0561674  -0.0636013   -0.0279501    -0.229995     -0.170812     0.0559141   -0.0653978    0.0259091     0.138772    -0.00825532  -0.0466681    0.0809691   -0.058588   
 -0.160118   -0.067574    -0.0406652    0.155907    0.114647     0.196986      0.00163276  -0.0222626    0.114856     0.0227868   -0.0301404   -0.00291689   0.0107332    0.164916    0.140369    -0.147487      0.0309623     0.0255701   -0.230851     0.0119993    0.204295      0.0139595   -0.0759111   -0.00356458  -0.255326    -0.0422779  
 -0.0408674   0.0860124    0.152929    -0.103343   -0.0119448   -0.0467769    -0.0726911   -0.0612995   -0.0897768   -0.0257141    0.0132819   -0.136804    -0.106802     0.0383987   0.0902151   -0.00768352   -0.0433397     0.214153    -0.0598913   -0.0763684   -0.0410812    -0.00940019  -0.0136348   -0.0473485   -0.139168     0.0379622  
  0.130462   -0.0174059   -0.0639775    0.26234     0.0565401    0.0676344     0.051766     0.13462      0.131914    -0.148806    -0.0316507    0.056686    -0.0884976   -0.188961    0.0385285    0.0952134    -0.0926821     0.148861     0.04002     -0.078665     0.0282767    -0.043347    -0.0599994    0.0177358    0.177193     0.142677   
  0.0279462   0.0503218   -0.0499526   -0.0230814   0.0365811   -0.0793295     0.0844046    0.0683446    0.126728     0.0104139   -0.127623    -0.160673    -0.0753632   -0.0893319  -0.100441    -0.00995426    0.0882784    -0.0492213    0.0312261    0.178803    -0.0467597    -0.0200909   -0.0560327   -0.287922     0.220536     0.118243   
 -0.149846    0.078946    -0.00100534  -0.275253   -0.239304     0.0684957    -0.0753536   -0.03791      0.213701    -0.0735765    0.0317971   -0.0404033   -0.105441    -0.0213303  -0.00487851   0.130434     -0.0337445     0.07658     -0.269904     0.0988459    0.00593618   -0.0921058    0.0141892   -0.0573538   -0.0524743    0.0968739  
  0.209532    0.00881265   0.0672777   -0.130085   -0.098056     0.0294003     0.1897       0.157586     0.133622     0.0481514   -0.0448053   -0.0230343   -0.182766    -0.0802473   0.00478879  -0.135008     -0.133571      0.00286668   0.236949     0.13574     -0.010222      0.0651364    0.0607973    0.0950616   -0.106269    -0.183271   
  0.0231589  -0.0010025   -0.0288528   -0.232228   -0.116666     0.0411893    -0.0774523   -0.111002    -0.0683596    0.184387     0.00710566  -0.0263488    0.0255505    0.060962    0.00765919   0.237631      0.0748785     0.0416724   -0.0526045    0.0459798    0.0223334    -0.00340659  -0.0183224   -0.0375813    0.0540106    0.125653   
 -0.0933282  -0.0589187   -0.0693459   -0.113069    0.0890834   -0.00254999    0.0350092   -0.0250562    0.0189501    0.0272041    0.0307325    0.120823     0.0625717    0.0750401  -0.074261     0.0278259    -0.0946164     0.151742     0.0919323   -0.0789482   -0.146227      0.0746115   -0.0193807   -0.00467613   0.0378971   -0.00606583 
 -0.188131    0.0464208    0.0824995    0.0594102  -0.00613155   0.0478933    -0.123426    -0.0129684   -0.104693    -0.0182621   -0.00891542  -0.0829465   -0.111394     0.0792069   0.0851256   -0.169093      0.0676992     0.0423667   -0.0124352    0.18858     -0.0880174     0.0233464    0.17026     -0.0817908   -0.0487303    0.1364     
 -0.0229054   0.101131     0.177083    -0.0816391   0.0625495   -0.0859298     0.0701505    0.0603993   -0.184664    -0.0993611    0.00333575  -0.123516     0.112255     0.038546   -0.0869142   -0.143581     -0.0543719     0.131164    -0.0943001    0.118253    -0.0161164    -0.0847654   -0.0161529    0.00976097   0.00918214   0.0908892  
  0.0769269   0.00379531   0.0883756   -0.0435863  -0.0118744   -0.148371      0.242928    -0.00413593  -0.0339917   -0.00541192   0.0094234    0.00828174   0.0569946    0.0686628   0.171481    -0.127383      0.0481441     0.0430403    0.0131729   -0.0481213    0.0688222    -0.0971224    0.00699534   0.0766181    0.0636217    0.0639682  
 -0.0629993  -0.175713     0.0168802   -0.0824236  -0.0611969   -0.0896622    -0.0731865   -0.0162897    0.190447    -0.0218148   -0.23149     -0.0700101   -0.197651    -0.160285    0.076909     0.01442      -0.116791      0.0271676   -0.119406    -0.0418717    0.148636      0.00660771   0.0655359   -0.057696    -0.164295    -0.141785   
 -0.0136218   0.090046    -0.0672294    0.0397539   0.0766577   -0.0102581     0.0286428    0.00992279   0.11476     -0.0962121   -0.0232657   -0.196744     0.0565481   -0.0354191   0.0628632    0.010055     -0.0287998    -0.0864149    0.158276    -0.0544146    0.0476591     0.115669     0.0057431    0.151141    -0.0738089    0.182446   
  0.160089   -0.0384754    0.0183523    0.059338   -0.108031     0.159601     -0.0363011   -0.235734    -0.0489745    0.178384    -0.0273829    0.109752    -0.0723889    0.0490003  -0.0436271    0.0422138     0.159134     -0.0856009    0.101672     0.10973     -0.0757519    -0.127697     0.0908552    0.0548457   -0.125931    -0.0644642  
 -0.105988    0.159594     0.0763211    0.047496   -0.0160111   -0.0201853    -0.090112    -0.0427618    0.117161    -0.053971    -0.13344     -0.0713573   -0.00445737  -0.15443    -0.008927     0.11599      -0.0158283    -0.0136198    0.10657     -0.0689132    0.132844      0.108358    -0.0486957    0.113022    -0.0867523   -0.109114   kind full, method split
0: avll = -1.4184979440076346
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.418518
INFO: iteration 2, average log likelihood -1.418430
INFO: iteration 3, average log likelihood -1.418363
INFO: iteration 4, average log likelihood -1.418285
INFO: iteration 5, average log likelihood -1.418192
INFO: iteration 6, average log likelihood -1.418073
INFO: iteration 7, average log likelihood -1.417909
INFO: iteration 8, average log likelihood -1.417646
INFO: iteration 9, average log likelihood -1.417185
INFO: iteration 10, average log likelihood -1.416415
INFO: iteration 11, average log likelihood -1.415363
INFO: iteration 12, average log likelihood -1.414315
INFO: iteration 13, average log likelihood -1.413591
INFO: iteration 14, average log likelihood -1.413222
INFO: iteration 15, average log likelihood -1.413066
INFO: iteration 16, average log likelihood -1.413004
INFO: iteration 17, average log likelihood -1.412979
INFO: iteration 18, average log likelihood -1.412969
INFO: iteration 19, average log likelihood -1.412965
INFO: iteration 20, average log likelihood -1.412963
INFO: iteration 21, average log likelihood -1.412962
INFO: iteration 22, average log likelihood -1.412961
INFO: iteration 23, average log likelihood -1.412961
INFO: iteration 24, average log likelihood -1.412961
INFO: iteration 25, average log likelihood -1.412960
INFO: iteration 26, average log likelihood -1.412960
INFO: iteration 27, average log likelihood -1.412960
INFO: iteration 28, average log likelihood -1.412960
INFO: iteration 29, average log likelihood -1.412960
INFO: iteration 30, average log likelihood -1.412960
INFO: iteration 31, average log likelihood -1.412959
INFO: iteration 32, average log likelihood -1.412959
INFO: iteration 33, average log likelihood -1.412959
INFO: iteration 34, average log likelihood -1.412959
INFO: iteration 35, average log likelihood -1.412959
INFO: iteration 36, average log likelihood -1.412959
INFO: iteration 37, average log likelihood -1.412959
INFO: iteration 38, average log likelihood -1.412959
INFO: iteration 39, average log likelihood -1.412959
INFO: iteration 40, average log likelihood -1.412959
INFO: iteration 41, average log likelihood -1.412959
INFO: iteration 42, average log likelihood -1.412959
INFO: iteration 43, average log likelihood -1.412959
INFO: iteration 44, average log likelihood -1.412959
INFO: iteration 45, average log likelihood -1.412959
INFO: iteration 46, average log likelihood -1.412959
INFO: iteration 47, average log likelihood -1.412959
INFO: iteration 48, average log likelihood -1.412959
INFO: iteration 49, average log likelihood -1.412959
INFO: iteration 50, average log likelihood -1.412959
INFO: EM with 100000 data points 50 iterations avll -1.412959
952.4 data points per parameter
1: avll = [-1.41852,-1.41843,-1.41836,-1.41829,-1.41819,-1.41807,-1.41791,-1.41765,-1.41718,-1.41642,-1.41536,-1.41431,-1.41359,-1.41322,-1.41307,-1.413,-1.41298,-1.41297,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412978
INFO: iteration 2, average log likelihood -1.412888
INFO: iteration 3, average log likelihood -1.412819
INFO: iteration 4, average log likelihood -1.412741
INFO: iteration 5, average log likelihood -1.412653
INFO: iteration 6, average log likelihood -1.412559
INFO: iteration 7, average log likelihood -1.412468
INFO: iteration 8, average log likelihood -1.412387
INFO: iteration 9, average log likelihood -1.412323
INFO: iteration 10, average log likelihood -1.412273
INFO: iteration 11, average log likelihood -1.412236
INFO: iteration 12, average log likelihood -1.412208
INFO: iteration 13, average log likelihood -1.412185
INFO: iteration 14, average log likelihood -1.412164
INFO: iteration 15, average log likelihood -1.412144
INFO: iteration 16, average log likelihood -1.412124
INFO: iteration 17, average log likelihood -1.412102
INFO: iteration 18, average log likelihood -1.412078
INFO: iteration 19, average log likelihood -1.412051
INFO: iteration 20, average log likelihood -1.412021
INFO: iteration 21, average log likelihood -1.411988
INFO: iteration 22, average log likelihood -1.411954
INFO: iteration 23, average log likelihood -1.411919
INFO: iteration 24, average log likelihood -1.411884
INFO: iteration 25, average log likelihood -1.411851
INFO: iteration 26, average log likelihood -1.411821
INFO: iteration 27, average log likelihood -1.411794
INFO: iteration 28, average log likelihood -1.411771
INFO: iteration 29, average log likelihood -1.411750
INFO: iteration 30, average log likelihood -1.411733
INFO: iteration 31, average log likelihood -1.411719
INFO: iteration 32, average log likelihood -1.411706
INFO: iteration 33, average log likelihood -1.411696
INFO: iteration 34, average log likelihood -1.411687
INFO: iteration 35, average log likelihood -1.411680
INFO: iteration 36, average log likelihood -1.411674
INFO: iteration 37, average log likelihood -1.411668
INFO: iteration 38, average log likelihood -1.411664
INFO: iteration 39, average log likelihood -1.411660
INFO: iteration 40, average log likelihood -1.411657
INFO: iteration 41, average log likelihood -1.411654
INFO: iteration 42, average log likelihood -1.411652
INFO: iteration 43, average log likelihood -1.411650
INFO: iteration 44, average log likelihood -1.411648
INFO: iteration 45, average log likelihood -1.411647
INFO: iteration 46, average log likelihood -1.411645
INFO: iteration 47, average log likelihood -1.411644
INFO: iteration 48, average log likelihood -1.411643
INFO: iteration 49, average log likelihood -1.411642
INFO: iteration 50, average log likelihood -1.411641
INFO: EM with 100000 data points 50 iterations avll -1.411641
473.9 data points per parameter
2: avll = [-1.41298,-1.41289,-1.41282,-1.41274,-1.41265,-1.41256,-1.41247,-1.41239,-1.41232,-1.41227,-1.41224,-1.41221,-1.41218,-1.41216,-1.41214,-1.41212,-1.4121,-1.41208,-1.41205,-1.41202,-1.41199,-1.41195,-1.41192,-1.41188,-1.41185,-1.41182,-1.41179,-1.41177,-1.41175,-1.41173,-1.41172,-1.41171,-1.4117,-1.41169,-1.41168,-1.41167,-1.41167,-1.41166,-1.41166,-1.41166,-1.41165,-1.41165,-1.41165,-1.41165,-1.41165,-1.41165,-1.41164,-1.41164,-1.41164,-1.41164]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411654
INFO: iteration 2, average log likelihood -1.411590
INFO: iteration 3, average log likelihood -1.411540
INFO: iteration 4, average log likelihood -1.411484
INFO: iteration 5, average log likelihood -1.411419
INFO: iteration 6, average log likelihood -1.411345
INFO: iteration 7, average log likelihood -1.411267
INFO: iteration 8, average log likelihood -1.411190
INFO: iteration 9, average log likelihood -1.411117
INFO: iteration 10, average log likelihood -1.411052
INFO: iteration 11, average log likelihood -1.410994
INFO: iteration 12, average log likelihood -1.410942
INFO: iteration 13, average log likelihood -1.410896
INFO: iteration 14, average log likelihood -1.410854
INFO: iteration 15, average log likelihood -1.410816
INFO: iteration 16, average log likelihood -1.410781
INFO: iteration 17, average log likelihood -1.410749
INFO: iteration 18, average log likelihood -1.410720
INFO: iteration 19, average log likelihood -1.410693
INFO: iteration 20, average log likelihood -1.410667
INFO: iteration 21, average log likelihood -1.410641
INFO: iteration 22, average log likelihood -1.410616
INFO: iteration 23, average log likelihood -1.410590
INFO: iteration 24, average log likelihood -1.410564
INFO: iteration 25, average log likelihood -1.410538
INFO: iteration 26, average log likelihood -1.410512
INFO: iteration 27, average log likelihood -1.410487
INFO: iteration 28, average log likelihood -1.410462
INFO: iteration 29, average log likelihood -1.410439
INFO: iteration 30, average log likelihood -1.410419
INFO: iteration 31, average log likelihood -1.410400
INFO: iteration 32, average log likelihood -1.410383
INFO: iteration 33, average log likelihood -1.410368
INFO: iteration 34, average log likelihood -1.410355
INFO: iteration 35, average log likelihood -1.410343
INFO: iteration 36, average log likelihood -1.410333
INFO: iteration 37, average log likelihood -1.410324
INFO: iteration 38, average log likelihood -1.410316
INFO: iteration 39, average log likelihood -1.410308
INFO: iteration 40, average log likelihood -1.410301
INFO: iteration 41, average log likelihood -1.410294
INFO: iteration 42, average log likelihood -1.410288
INFO: iteration 43, average log likelihood -1.410282
INFO: iteration 44, average log likelihood -1.410277
INFO: iteration 45, average log likelihood -1.410271
INFO: iteration 46, average log likelihood -1.410266
INFO: iteration 47, average log likelihood -1.410261
INFO: iteration 48, average log likelihood -1.410256
INFO: iteration 49, average log likelihood -1.410251
INFO: iteration 50, average log likelihood -1.410246
INFO: EM with 100000 data points 50 iterations avll -1.410246
236.4 data points per parameter
3: avll = [-1.41165,-1.41159,-1.41154,-1.41148,-1.41142,-1.41135,-1.41127,-1.41119,-1.41112,-1.41105,-1.41099,-1.41094,-1.4109,-1.41085,-1.41082,-1.41078,-1.41075,-1.41072,-1.41069,-1.41067,-1.41064,-1.41062,-1.41059,-1.41056,-1.41054,-1.41051,-1.41049,-1.41046,-1.41044,-1.41042,-1.4104,-1.41038,-1.41037,-1.41035,-1.41034,-1.41033,-1.41032,-1.41032,-1.41031,-1.4103,-1.41029,-1.41029,-1.41028,-1.41028,-1.41027,-1.41027,-1.41026,-1.41026,-1.41025,-1.41025]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410250
INFO: iteration 2, average log likelihood -1.410192
INFO: iteration 3, average log likelihood -1.410141
INFO: iteration 4, average log likelihood -1.410085
INFO: iteration 5, average log likelihood -1.410016
INFO: iteration 6, average log likelihood -1.409934
INFO: iteration 7, average log likelihood -1.409837
INFO: iteration 8, average log likelihood -1.409729
INFO: iteration 9, average log likelihood -1.409615
INFO: iteration 10, average log likelihood -1.409501
INFO: iteration 11, average log likelihood -1.409392
INFO: iteration 12, average log likelihood -1.409293
INFO: iteration 13, average log likelihood -1.409205
INFO: iteration 14, average log likelihood -1.409127
INFO: iteration 15, average log likelihood -1.409060
INFO: iteration 16, average log likelihood -1.409001
INFO: iteration 17, average log likelihood -1.408950
INFO: iteration 18, average log likelihood -1.408905
INFO: iteration 19, average log likelihood -1.408865
INFO: iteration 20, average log likelihood -1.408830
INFO: iteration 21, average log likelihood -1.408797
INFO: iteration 22, average log likelihood -1.408768
INFO: iteration 23, average log likelihood -1.408740
INFO: iteration 24, average log likelihood -1.408715
INFO: iteration 25, average log likelihood -1.408691
INFO: iteration 26, average log likelihood -1.408669
INFO: iteration 27, average log likelihood -1.408648
INFO: iteration 28, average log likelihood -1.408628
INFO: iteration 29, average log likelihood -1.408610
INFO: iteration 30, average log likelihood -1.408593
INFO: iteration 31, average log likelihood -1.408576
INFO: iteration 32, average log likelihood -1.408561
INFO: iteration 33, average log likelihood -1.408546
INFO: iteration 34, average log likelihood -1.408533
INFO: iteration 35, average log likelihood -1.408519
INFO: iteration 36, average log likelihood -1.408507
INFO: iteration 37, average log likelihood -1.408495
INFO: iteration 38, average log likelihood -1.408483
INFO: iteration 39, average log likelihood -1.408472
INFO: iteration 40, average log likelihood -1.408461
INFO: iteration 41, average log likelihood -1.408451
INFO: iteration 42, average log likelihood -1.408441
INFO: iteration 43, average log likelihood -1.408431
INFO: iteration 44, average log likelihood -1.408422
INFO: iteration 45, average log likelihood -1.408412
INFO: iteration 46, average log likelihood -1.408404
INFO: iteration 47, average log likelihood -1.408395
INFO: iteration 48, average log likelihood -1.408386
INFO: iteration 49, average log likelihood -1.408378
INFO: iteration 50, average log likelihood -1.408370
INFO: EM with 100000 data points 50 iterations avll -1.408370
118.1 data points per parameter
4: avll = [-1.41025,-1.41019,-1.41014,-1.41008,-1.41002,-1.40993,-1.40984,-1.40973,-1.40961,-1.4095,-1.40939,-1.40929,-1.4092,-1.40913,-1.40906,-1.409,-1.40895,-1.40891,-1.40887,-1.40883,-1.4088,-1.40877,-1.40874,-1.40871,-1.40869,-1.40867,-1.40865,-1.40863,-1.40861,-1.40859,-1.40858,-1.40856,-1.40855,-1.40853,-1.40852,-1.40851,-1.40849,-1.40848,-1.40847,-1.40846,-1.40845,-1.40844,-1.40843,-1.40842,-1.40841,-1.4084,-1.40839,-1.40839,-1.40838,-1.40837]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408370
INFO: iteration 2, average log likelihood -1.408307
INFO: iteration 3, average log likelihood -1.408248
INFO: iteration 4, average log likelihood -1.408179
INFO: iteration 5, average log likelihood -1.408095
INFO: iteration 6, average log likelihood -1.407991
INFO: iteration 7, average log likelihood -1.407866
INFO: iteration 8, average log likelihood -1.407725
INFO: iteration 9, average log likelihood -1.407573
INFO: iteration 10, average log likelihood -1.407420
INFO: iteration 11, average log likelihood -1.407272
INFO: iteration 12, average log likelihood -1.407135
INFO: iteration 13, average log likelihood -1.407010
INFO: iteration 14, average log likelihood -1.406899
INFO: iteration 15, average log likelihood -1.406800
INFO: iteration 16, average log likelihood -1.406713
INFO: iteration 17, average log likelihood -1.406635
INFO: iteration 18, average log likelihood -1.406566
INFO: iteration 19, average log likelihood -1.406505
INFO: iteration 20, average log likelihood -1.406450
INFO: iteration 21, average log likelihood -1.406400
INFO: iteration 22, average log likelihood -1.406355
INFO: iteration 23, average log likelihood -1.406314
INFO: iteration 24, average log likelihood -1.406277
INFO: iteration 25, average log likelihood -1.406242
INFO: iteration 26, average log likelihood -1.406209
INFO: iteration 27, average log likelihood -1.406179
INFO: iteration 28, average log likelihood -1.406150
INFO: iteration 29, average log likelihood -1.406123
INFO: iteration 30, average log likelihood -1.406097
INFO: iteration 31, average log likelihood -1.406072
INFO: iteration 32, average log likelihood -1.406048
INFO: iteration 33, average log likelihood -1.406025
INFO: iteration 34, average log likelihood -1.406003
INFO: iteration 35, average log likelihood -1.405982
INFO: iteration 36, average log likelihood -1.405961
INFO: iteration 37, average log likelihood -1.405941
INFO: iteration 38, average log likelihood -1.405921
INFO: iteration 39, average log likelihood -1.405903
INFO: iteration 40, average log likelihood -1.405884
INFO: iteration 41, average log likelihood -1.405866
INFO: iteration 42, average log likelihood -1.405849
INFO: iteration 43, average log likelihood -1.405832
INFO: iteration 44, average log likelihood -1.405816
INFO: iteration 45, average log likelihood -1.405800
INFO: iteration 46, average log likelihood -1.405784
INFO: iteration 47, average log likelihood -1.405769
INFO: iteration 48, average log likelihood -1.405754
INFO: iteration 49, average log likelihood -1.405740
INFO: iteration 50, average log likelihood -1.405726
INFO: EM with 100000 data points 50 iterations avll -1.405726
59.0 data points per parameter
5: avll = [-1.40837,-1.40831,-1.40825,-1.40818,-1.40809,-1.40799,-1.40787,-1.40772,-1.40757,-1.40742,-1.40727,-1.40713,-1.40701,-1.4069,-1.4068,-1.40671,-1.40664,-1.40657,-1.4065,-1.40645,-1.4064,-1.40636,-1.40631,-1.40628,-1.40624,-1.40621,-1.40618,-1.40615,-1.40612,-1.4061,-1.40607,-1.40605,-1.40603,-1.406,-1.40598,-1.40596,-1.40594,-1.40592,-1.4059,-1.40588,-1.40587,-1.40585,-1.40583,-1.40582,-1.4058,-1.40578,-1.40577,-1.40575,-1.40574,-1.40573]
[-1.4185,-1.41852,-1.41843,-1.41836,-1.41829,-1.41819,-1.41807,-1.41791,-1.41765,-1.41718,-1.41642,-1.41536,-1.41431,-1.41359,-1.41322,-1.41307,-1.413,-1.41298,-1.41297,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41296,-1.41298,-1.41289,-1.41282,-1.41274,-1.41265,-1.41256,-1.41247,-1.41239,-1.41232,-1.41227,-1.41224,-1.41221,-1.41218,-1.41216,-1.41214,-1.41212,-1.4121,-1.41208,-1.41205,-1.41202,-1.41199,-1.41195,-1.41192,-1.41188,-1.41185,-1.41182,-1.41179,-1.41177,-1.41175,-1.41173,-1.41172,-1.41171,-1.4117,-1.41169,-1.41168,-1.41167,-1.41167,-1.41166,-1.41166,-1.41166,-1.41165,-1.41165,-1.41165,-1.41165,-1.41165,-1.41165,-1.41164,-1.41164,-1.41164,-1.41164,-1.41165,-1.41159,-1.41154,-1.41148,-1.41142,-1.41135,-1.41127,-1.41119,-1.41112,-1.41105,-1.41099,-1.41094,-1.4109,-1.41085,-1.41082,-1.41078,-1.41075,-1.41072,-1.41069,-1.41067,-1.41064,-1.41062,-1.41059,-1.41056,-1.41054,-1.41051,-1.41049,-1.41046,-1.41044,-1.41042,-1.4104,-1.41038,-1.41037,-1.41035,-1.41034,-1.41033,-1.41032,-1.41032,-1.41031,-1.4103,-1.41029,-1.41029,-1.41028,-1.41028,-1.41027,-1.41027,-1.41026,-1.41026,-1.41025,-1.41025,-1.41025,-1.41019,-1.41014,-1.41008,-1.41002,-1.40993,-1.40984,-1.40973,-1.40961,-1.4095,-1.40939,-1.40929,-1.4092,-1.40913,-1.40906,-1.409,-1.40895,-1.40891,-1.40887,-1.40883,-1.4088,-1.40877,-1.40874,-1.40871,-1.40869,-1.40867,-1.40865,-1.40863,-1.40861,-1.40859,-1.40858,-1.40856,-1.40855,-1.40853,-1.40852,-1.40851,-1.40849,-1.40848,-1.40847,-1.40846,-1.40845,-1.40844,-1.40843,-1.40842,-1.40841,-1.4084,-1.40839,-1.40839,-1.40838,-1.40837,-1.40837,-1.40831,-1.40825,-1.40818,-1.40809,-1.40799,-1.40787,-1.40772,-1.40757,-1.40742,-1.40727,-1.40713,-1.40701,-1.4069,-1.4068,-1.40671,-1.40664,-1.40657,-1.4065,-1.40645,-1.4064,-1.40636,-1.40631,-1.40628,-1.40624,-1.40621,-1.40618,-1.40615,-1.40612,-1.4061,-1.40607,-1.40605,-1.40603,-1.406,-1.40598,-1.40596,-1.40594,-1.40592,-1.4059,-1.40588,-1.40587,-1.40585,-1.40583,-1.40582,-1.4058,-1.40578,-1.40577,-1.40575,-1.40574,-1.40573]
32×26 Array{Float64,2}:
  0.0668904  -0.110829   -0.167924    -0.128327     0.00110813  -0.329758    0.000659651   0.0422024    0.161297    -0.506901     0.00227036  -0.183189    0.218151    -0.88407     -0.712555    -0.290266    -0.151263     0.208195     0.232123   -0.444217     0.17519      0.0502889   -0.00361806   0.0663637     0.354479    -0.456618  
 -0.371535    0.259018    0.00354308   0.238414     0.343264     0.413707    0.277317     -0.0352551   -0.106138    -0.369981    -0.31206      0.258159    0.230446    -0.132711    -0.009514    -0.690445    -0.29451      0.16948     -0.132442   -0.294439     0.168644     0.0150058    0.137675     0.151809     -0.0836812   -0.90686   
 -0.465963    0.0783928   0.0384019    0.0923149    0.210249     0.0252819  -0.076526      0.0429905   -0.701435    -0.802875    -0.471633    -0.626815    0.0126606    0.502056    -0.0881026    0.0227212    0.080375    -0.0706846    0.459164    0.27725     -0.0294944    0.842025     0.0847807    0.374886      0.599493    -0.27711   
 -0.106449   -0.0583325   0.230599    -0.00422542   0.583069     0.772096   -0.275381     -0.6268      -0.636911    -0.0821015   -0.13816     -0.207018    0.00394016   0.252504    -0.258104    -0.360924     0.233615     0.205685    -0.653169    0.261514     0.00599448   0.366652     0.0931167   -0.115101      0.34625      0.00227363
 -0.255227    0.150916    0.442622     0.198626    -0.0382323   -0.238206    0.0340227    -0.335317     0.178122    -0.475954     0.181853     0.455041   -0.375676    -0.0661971    0.627721    -0.243005     0.140594     0.10391      0.2007      0.318352     0.183698     0.536525     0.048766     0.550354      0.124548    -0.275404  
 -0.316727   -0.231844    0.210148     0.220204     0.133402    -0.424552    0.0154351    -1.10984     -0.150683     0.548401     0.461533    -0.216668   -0.115632    -0.0477358   -0.0550631   -0.313129    -0.304244    -0.0271219    0.266755    0.197159     0.221771     0.282233    -0.545204     0.161637     -0.0901348    0.17371   
 -0.307241   -0.763915   -0.0173028    0.239034     0.155138     0.0350749   0.127833     -0.0383612   -0.256484    -0.104789    -0.254406    -0.515051   -0.339407     0.10361     -0.204243    -0.2522      -0.0446836   -0.268025     0.0400086  -0.144013    -0.0734572    0.0682654   -0.240353    -0.0751493    -0.0129637   -0.00251126
 -0.224655   -0.504932    0.018734     0.468041     0.148845    -0.422016    0.361503      0.271782    -0.00871914  -0.102449     0.132559    -0.203687    0.37325     -0.100928     0.446607     0.191976    -0.265195    -0.474846     0.390344   -0.269069     0.00785061  -0.232312     0.106129    -0.260974     -0.0823563    0.324482  
  0.223301    0.391601   -0.497674    -0.223486    -0.21805      0.0299881   0.110691      0.758969     0.308972    -0.0310505   -0.516618     0.199702    0.112354     0.0828008    0.00545741   0.29381      0.132852     0.0950576    0.0942307  -0.47828     -0.0916122   -0.11732     -0.0499325   -0.308035     -0.386396     0.0538645 
  0.0786338  -0.0874696   0.510901    -0.0172034    0.273648     0.0990604  -0.0574607    -0.280099     0.536195     0.123146     0.15456      0.256347   -0.0362642   -0.0956118   -0.0427197   -0.171122    -0.00975927  -0.00848936  -0.0861027  -0.755006    -0.182831     0.0253407    0.0881228   -0.129957     -0.584242    -0.304937  
  0.114297    0.214153   -0.0484909   -0.00820364  -0.218756    -0.0195472  -0.140965     -0.0527001    0.22273      0.0729399   -0.0751663    0.112677   -0.0524157    0.0178301   -0.111555    -0.0268294    0.346717    -0.289448    -0.127047    0.120047    -0.297196     0.0967535    0.0923722    0.0604639     0.0933227   -0.0459998 
 -0.0603581   0.0588287  -0.0404106   -0.0847463    0.00460726   0.0219426   0.00104557    0.0862843   -0.137749    -0.00929689   0.0121552    0.0191217   0.0605021    0.0231909    0.169423     0.12974     -0.201056     0.222816     0.0579392   0.10462      0.229394     0.033739    -0.0303649   -0.000314543  -0.00310481   0.115244  
  0.0385205   0.208557   -0.254108    -0.417062     0.0606448   -0.0297061   0.388815      0.166849    -0.255425     0.130393    -0.129028    -0.190382    0.0507448    0.150315     0.391687     0.451201    -0.0727598    0.505386    -0.189111    0.598315     0.239406    -0.431627     0.163714    -0.116859      0.793894     0.600062  
  0.608681    0.152066    0.0618341    0.585548    -0.3552       0.346345    0.134758     -0.207588    -0.154261     0.0786289    0.146912     0.381807   -0.233769     0.215247     0.102346     0.0696371    0.0810293    0.106933    -0.258925    0.533277     0.235924    -0.00281374   0.20659     -0.18483       0.318329     0.565599  
  0.3227      0.663795   -0.190923    -0.932062     0.0482633    0.461704   -0.251159     -0.0639472   -0.267882     0.0727355    0.120064     0.271824   -0.0215391   -0.515306     0.0842059    0.0741399   -0.00620105   0.675121    -0.679166    0.0673156    0.0618513   -0.131966     0.288469     0.270944      0.0765086   -0.111758  
  0.126085   -0.157052   -0.297278    -0.24583     -0.477505     0.513555    0.197724      0.0435673   -0.269335     0.0245319    0.405463    -0.166011    0.0122398    0.0570631    0.233004    -0.11301     -0.766383     0.0770875    0.452258   -0.0501545    0.168368    -0.0442351    0.264235     0.51741       0.111884    -0.0971162 
 -0.127257    0.523657   -0.304067    -0.0864156    0.100967    -0.0252835  -0.0305704    -0.0253965    0.201065     0.162844    -0.617652    -0.24485     0.186541    -0.073579    -0.313552    -0.00326711   0.994381    -0.142838    -0.212562   -0.186156    -0.635087     0.104903     0.028747    -0.449292      0.369426    -0.0838489 
  0.0326144  -0.142892   -0.151054    -0.234825     0.0972057   -0.397831    0.0292504    -0.00504937   0.437126     0.170655     0.0847406   -0.0135187  -0.523126     0.338385    -0.218452    -0.18963      0.653086    -0.0778878    0.12961    -0.359013    -0.606576     0.380515    -0.33062      0.416332     -0.339213     0.154259  
  0.118727   -0.185984    0.568072    -0.374343    -0.620108     0.331469   -0.301461     -0.435327     0.282181    -0.00726862  -0.281485     0.278304   -0.158983    -0.564353    -0.47649      0.316533    -0.0918835    0.243589    -0.0938559  -0.518146    -0.0986619    0.496827     0.0677866   -0.426065     -0.499484    -0.673822  
  0.1131     -0.222499    0.0988267   -0.570796     0.265947     1.17345    -0.0191749     0.293957     0.340016     0.134742    -0.391975     0.0470161  -0.0828194    0.652654     0.468373     0.100592     0.0979049    0.139194    -0.32939    -0.536661    -0.854953    -0.54119      0.571044    -0.284536     -0.0748351   -0.128606  
  0.332268   -0.148457    0.376842     0.535806     0.303309    -0.722408   -0.45684       0.280991    -0.121354    -0.918362    -0.152249     0.291536   -0.0748202   -0.666464    -0.28714      0.329445     0.377016    -0.201137     0.0426587   0.357586     0.226427     0.155837    -0.540086    -0.628222     -0.0871536    0.21555   
 -0.0934958  -0.149276    0.524082     0.0528301   -0.188575    -0.0535276  -0.579106      0.168801     0.173896     0.0350929   -0.305319     0.0538498  -0.293114     0.59363     -0.259237     0.598864    -0.252579    -0.215314     0.198882   -0.0281809    0.27719      0.527914    -0.199816    -0.229128     -0.70622      0.259712  
 -0.262701   -0.198605   -0.381308    -0.697709    -0.741653    -0.329177   -0.210059      0.105526     0.17737      0.158039     0.00701465  -0.302142    0.0866199   -0.110246    -0.373545     0.6927      -0.30711      0.164544    -0.254484    0.380723    -0.3715      -0.0459328   -0.254627    -0.0294557    -0.0138024    0.309627  
  0.297535    0.147905   -0.00230562  -0.173004    -0.453513    -0.475605   -0.276725      0.067637     0.7698       0.498243     0.651148     0.328637    0.297693    -0.467566     0.216172     0.509554    -0.00693317  -0.190197     0.0570714   0.00520003   0.0751083   -0.215351    -0.0140926   -0.193896     -0.538798     0.451846  
 -0.537228   -0.501648    0.171624     0.309463     0.644954     0.0880653   0.402289      0.0457726   -0.212094    -0.274824    -0.0599014   -0.362746    0.159635     0.0943724    0.134148    -0.552502    -0.270085    -0.130993     0.224005   -0.880762    -0.333433    -0.0201719   -0.0553732    0.0475489    -0.253355    -0.495041  
  0.117538   -0.259586   -0.0234167    0.591096     0.674311    -0.119188   -0.0817021     0.297088    -0.0918521    0.700196     0.12839      0.0322995   0.0303897   -0.219094     0.171877    -0.947045     0.126788     0.182777     0.0441528  -0.719931     0.557118    -0.47688      0.0571055   -0.0739187    -0.0153981   -0.0964051 
  0.424679    0.190775    0.208605     0.348862    -0.76318      0.164291    0.494316      0.386191     0.0334087    0.182125    -0.190535     0.199774    0.250654    -0.391886     0.217415    -0.00302832  -1.00286     -0.340924     0.621856   -0.107718     0.245127    -0.960007     0.763721    -0.00777374   -0.250615    -0.491506  
 -0.0184208  -0.361341    0.365339     0.612814    -0.0236954   -0.512942   -0.197705      0.132784    -0.0616142   -0.218194     0.00187662  -0.533175    0.411249    -0.0600471    0.348087    -0.408046    -0.616298    -0.424827    -0.0615309   0.876847     0.41063     -0.018264     0.238212     0.406331     -0.288668    -0.292984  
 -0.393839    0.249041   -0.522741     0.285294     0.151663     0.377936    0.695304      0.00836497  -0.741564     0.221399    -0.0619186    0.0877506   0.545465     0.233489     0.600551    -0.341152     0.486033    -0.688613     0.107757    0.771009     0.176083    -0.391768    -0.237487    -0.00825944    0.504583    -0.0813847 
 -0.0474911   0.201784   -0.0987024   -0.0573045   -0.385121     0.4605     -0.173476     -0.0902536   -0.0277659    0.0859199    0.292454     0.416712    0.0654657   -0.494721    -0.343783    -0.132347    -0.301441     0.517138    -0.167347    0.272129     0.764469     0.0213833   -0.161256    -0.319311      0.356833     0.419883  
 -0.18006    -0.697435   -0.251789    -0.0100328   -0.0213519   -0.396438    0.476683      0.145778    -0.362182    -0.312466     0.066335    -0.158091   -0.106451    -0.00184282   0.15097      0.592084    -0.462806    -0.0519201    0.341182    0.360842     0.310584     0.135657    -0.487246     0.148151     -0.181482     0.539001  
  0.292498    0.648738   -0.153833    -0.0326574   -0.244046    -0.146225   -0.121915     -0.14097      0.352997    -0.109872    -0.208239     0.155792   -0.26014      0.223982     0.0132724    0.09517      0.389551    -0.116464    -0.142227    0.317157    -0.366891     0.367665     0.384946     0.437333      0.172767     0.12401   INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405712
INFO: iteration 2, average log likelihood -1.405699
INFO: iteration 3, average log likelihood -1.405685
INFO: iteration 4, average log likelihood -1.405673
INFO: iteration 5, average log likelihood -1.405660
INFO: iteration 6, average log likelihood -1.405648
INFO: iteration 7, average log likelihood -1.405636
INFO: iteration 8, average log likelihood -1.405624
INFO: iteration 9, average log likelihood -1.405613
INFO: iteration 10, average log likelihood -1.405601
INFO: EM with 100000 data points 10 iterations avll -1.405601
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.758545e+05
      1       6.980687e+05      -1.777858e+05 |       32
      2       6.843826e+05      -1.368611e+04 |       32
      3       6.789981e+05      -5.384435e+03 |       32
      4       6.763782e+05      -2.619942e+03 |       32
      5       6.747862e+05      -1.592009e+03 |       32
      6       6.737093e+05      -1.076892e+03 |       32
      7       6.728802e+05      -8.291204e+02 |       32
      8       6.722089e+05      -6.712769e+02 |       32
      9       6.716182e+05      -5.906787e+02 |       32
     10       6.711085e+05      -5.097385e+02 |       32
     11       6.706634e+05      -4.450870e+02 |       32
     12       6.702771e+05      -3.862549e+02 |       32
     13       6.699295e+05      -3.476094e+02 |       32
     14       6.696325e+05      -2.969929e+02 |       32
     15       6.693827e+05      -2.498378e+02 |       32
     16       6.691473e+05      -2.354094e+02 |       32
     17       6.689329e+05      -2.143878e+02 |       32
     18       6.687311e+05      -2.017384e+02 |       32
     19       6.685551e+05      -1.760133e+02 |       32
     20       6.683861e+05      -1.690070e+02 |       32
     21       6.682339e+05      -1.522447e+02 |       32
     22       6.680827e+05      -1.511669e+02 |       32
     23       6.679473e+05      -1.354501e+02 |       32
     24       6.678215e+05      -1.257343e+02 |       32
     25       6.677026e+05      -1.189034e+02 |       32
     26       6.675794e+05      -1.231949e+02 |       32
     27       6.674635e+05      -1.159485e+02 |       32
     28       6.673514e+05      -1.120960e+02 |       32
     29       6.672500e+05      -1.013432e+02 |       32
     30       6.671487e+05      -1.013042e+02 |       32
     31       6.670543e+05      -9.446969e+01 |       32
     32       6.669580e+05      -9.630074e+01 |       32
     33       6.668704e+05      -8.759756e+01 |       32
     34       6.667919e+05      -7.846045e+01 |       32
     35       6.667203e+05      -7.158102e+01 |       32
     36       6.666533e+05      -6.702745e+01 |       32
     37       6.665918e+05      -6.152137e+01 |       32
     38       6.665337e+05      -5.807767e+01 |       32
     39       6.664822e+05      -5.154235e+01 |       32
     40       6.664312e+05      -5.094141e+01 |       32
     41       6.663802e+05      -5.101365e+01 |       32
     42       6.663289e+05      -5.129077e+01 |       32
     43       6.662810e+05      -4.792597e+01 |       32
     44       6.662331e+05      -4.791820e+01 |       32
     45       6.661919e+05      -4.117981e+01 |       32
     46       6.661529e+05      -3.899721e+01 |       32
     47       6.661182e+05      -3.466916e+01 |       32
     48       6.660862e+05      -3.206887e+01 |       32
     49       6.660533e+05      -3.284945e+01 |       32
     50       6.660241e+05      -2.916795e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 666024.1367022838)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.417603
INFO: iteration 2, average log likelihood -1.412464
INFO: iteration 3, average log likelihood -1.410968
INFO: iteration 4, average log likelihood -1.409796
INFO: iteration 5, average log likelihood -1.408659
INFO: iteration 6, average log likelihood -1.407770
INFO: iteration 7, average log likelihood -1.407238
INFO: iteration 8, average log likelihood -1.406945
INFO: iteration 9, average log likelihood -1.406765
INFO: iteration 10, average log likelihood -1.406636
INFO: iteration 11, average log likelihood -1.406533
INFO: iteration 12, average log likelihood -1.406446
INFO: iteration 13, average log likelihood -1.406371
INFO: iteration 14, average log likelihood -1.406304
INFO: iteration 15, average log likelihood -1.406244
INFO: iteration 16, average log likelihood -1.406191
INFO: iteration 17, average log likelihood -1.406143
INFO: iteration 18, average log likelihood -1.406099
INFO: iteration 19, average log likelihood -1.406060
INFO: iteration 20, average log likelihood -1.406024
INFO: iteration 21, average log likelihood -1.405991
INFO: iteration 22, average log likelihood -1.405961
INFO: iteration 23, average log likelihood -1.405933
INFO: iteration 24, average log likelihood -1.405908
INFO: iteration 25, average log likelihood -1.405884
INFO: iteration 26, average log likelihood -1.405863
INFO: iteration 27, average log likelihood -1.405842
INFO: iteration 28, average log likelihood -1.405823
INFO: iteration 29, average log likelihood -1.405805
INFO: iteration 30, average log likelihood -1.405788
INFO: iteration 31, average log likelihood -1.405772
INFO: iteration 32, average log likelihood -1.405757
INFO: iteration 33, average log likelihood -1.405742
INFO: iteration 34, average log likelihood -1.405728
INFO: iteration 35, average log likelihood -1.405714
INFO: iteration 36, average log likelihood -1.405701
INFO: iteration 37, average log likelihood -1.405689
INFO: iteration 38, average log likelihood -1.405677
INFO: iteration 39, average log likelihood -1.405665
INFO: iteration 40, average log likelihood -1.405654
INFO: iteration 41, average log likelihood -1.405643
INFO: iteration 42, average log likelihood -1.405632
INFO: iteration 43, average log likelihood -1.405622
INFO: iteration 44, average log likelihood -1.405612
INFO: iteration 45, average log likelihood -1.405602
INFO: iteration 46, average log likelihood -1.405592
INFO: iteration 47, average log likelihood -1.405583
INFO: iteration 48, average log likelihood -1.405573
INFO: iteration 49, average log likelihood -1.405564
INFO: iteration 50, average log likelihood -1.405556
INFO: EM with 100000 data points 50 iterations avll -1.405556
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.431411    -0.639302    0.179378    0.47242     0.147603    -0.160639    -0.044511   -0.163055   -0.222546     -0.0120809   -0.0945024   -0.613466    -0.0795384    0.0747391  -0.202572   -0.237079    0.0128159   -0.424649    0.152653     0.0225916   0.033019    0.147803   -0.22786    -0.212206   -0.0440527   -0.0450297
 -0.319125    -0.187123   -0.376428   -0.863677   -0.631693    -0.149054    -0.570813    0.281201   -0.0638778     0.159722     0.215893    -0.211143     0.32912     -0.0130669  -0.600164    0.689791   -0.415949     0.616239   -0.320287     0.32031    -0.115097   -0.271381   -0.0951214   0.20283     0.124306     0.388076 
 -0.00509215  -0.193232   -0.0524855   0.773841    0.129746    -0.719684     0.103963   -0.229205   -0.058505      0.0928645    0.62152      0.164027    -0.0105363   -0.276801    0.103726   -0.139093   -0.0813938    0.0714694   0.278279     0.369366    0.672951    0.321411   -0.690364    0.185778   -0.0421899    0.618429 
  0.274464     0.894438   -0.382568   -0.455102   -0.381141     0.138057     0.0262527   0.438776    0.45596       0.11453     -0.210139     0.519748     0.31509     -0.0605075   0.182195    0.534279    0.29416      0.19568     0.0607557   -0.151052   -0.190912   -0.0410603   0.218662   -0.298588   -0.169285     0.130886 
  0.0928057   -0.116911   -0.917592    0.380591   -0.81682     -0.251486    -0.490627    0.90041    -0.091077     -0.102245     0.171729    -0.565482     0.344553    -0.0543587   0.282993   -0.41252    -0.336965    -0.373799    0.0300881    0.465542    0.160163    0.225429    0.176956    0.586447   -0.0675542   -0.332087 
  0.361079     0.464727   -0.0107462  -0.0150088  -0.488158    -0.0385809   -0.0606687  -0.196459    0.300606     -0.175844    -0.194539     0.25021     -0.587087     0.131092    0.0398275   0.34356     0.182979     0.0534079  -0.135094     0.394204   -0.188113    0.496257    0.298833    0.288963    0.0536233    0.216511 
 -0.46644      0.158469   -0.431073    0.220605    0.134605     0.435894     0.766885    0.120257   -0.808685      0.168587    -0.110719     0.00509797   0.384601     0.362148    0.462458   -0.211993    0.136917    -0.459951    0.152671     0.525951    0.282731   -0.297293   -0.209782   -0.111043    0.433648    -0.0069919
  0.00455281  -0.398604   -0.198695   -0.165416   -0.474891    -0.606502     0.0315841   0.0215738   0.382015      0.432622     0.332509    -0.0986533    0.198804    -0.274415    0.117835    0.714942   -0.0807012   -0.340529   -0.0425069   -0.0420964  -0.184565   -0.307156   -0.14913    -0.487465   -0.109191     0.537162 
  0.0353756   -0.124415   -0.133618   -0.0714786  -0.175581     0.247504     0.0115629   0.107016   -0.272537     -0.100562     0.119846    -0.111083     0.165077    -0.102319    0.195954    0.104154   -0.633225     0.0813489   0.155732     0.0570358   0.224115   -0.0517402   0.232494    0.0668464   0.00192522   0.014204 
  0.139076    -0.0170633  -0.0896637   0.0624689   0.106242    -0.175436    -0.304355    0.0884438   0.107841      0.207838     0.0360648    0.0871645    0.374474    -0.517215   -0.45999    -0.433589    0.272443    -0.0641617   0.122721    -0.346642    0.33898    -0.199251   -0.216392   -0.208851    0.00505888  -0.10172  
 -0.0899251   -0.410209    0.463446    0.730965    0.394476    -0.145445     0.270818    0.285284   -0.143797     -0.261107    -0.00346449  -0.703522     0.494205    -0.0508716   0.608389   -0.144182   -0.838687    -0.149894    0.493683    -0.04358     0.080295   -0.292552    0.538381   -0.143327   -0.299789    -0.180372 
 -0.0407143   -0.117627    0.607095    0.0177047  -0.132789    -0.0662728   -0.647217    0.0248361   0.138717      0.0337117   -0.313168    -0.0621704   -0.228566     0.597387   -0.29724     0.64118    -0.00579007  -0.209856    0.119419     0.0640988   0.240628    0.734839   -0.244907   -0.357393   -0.634675     0.270694 
  0.52248     -0.189767    0.506566    0.365743    0.383781    -0.36502     -0.548418    0.457869   -0.182881     -1.1274      -0.285718     0.309583     0.00462948  -0.621801   -0.15993     0.293496    0.332167    -0.124861   -0.0666278    0.226854   -0.0572267  -0.0471176  -0.221274   -0.615457    0.0288456    0.0994106
  0.20303     -0.238478    0.646671   -0.211258   -0.496039     0.559858    -0.190863   -0.107117    0.172507      0.375936     0.145641     0.462964    -0.0810911   -0.479594   -0.326655    0.368559   -0.807708     0.447609   -0.0900038   -0.466852    0.327719    0.0454453   0.177362   -0.52308    -0.429797    -0.0887987
 -0.340591     0.221686   -0.446716   -0.0158516   0.395938    -0.225334     0.0333756  -0.244465    0.000561913  -0.270818    -0.789254    -0.667597     0.42607      0.170533   -0.192593    0.0628081   0.853726    -0.634376   -0.106291    -0.157545   -0.691667    0.236283    0.183415    0.132551    0.201152    -0.293874 
 -0.0168742    0.0524174   0.137184    0.0276313   0.0534203   -0.0467138    0.0130324  -0.114631    0.097756     -0.00580015   0.124506     0.204351    -0.0749843    0.0569868   0.173752   -0.0331859   0.0979258    0.0421763   0.00170403  -0.0114384   0.0351843   0.155352   -0.0412538   0.0626764  -0.0630363    0.078928 
  0.220014     0.167587   -0.164129   -0.131975   -0.0576919    0.114701     0.328066    0.110659   -0.306432      0.0730838   -0.12605     -0.154876    -0.142763     0.239433    0.372614    0.491073    0.0483643    0.339346   -0.118256     0.672677    0.165926   -0.221915    0.0847836  -0.0973472   0.623599     0.621151 
 -0.514575    -0.317487   -0.275374   -0.358649   -0.249623    -0.392061     0.274456    0.236113   -0.398001     -0.997152    -0.409606    -0.0138053   -0.188002    -0.222624    0.172903    0.865294   -0.778353     0.0761193   0.189214     0.470683    0.22084     0.713221   -0.545627    0.0432347  -0.179758     0.107815 
 -0.0751825   -0.242582   -0.315793    0.320508    0.385334    -0.256211     0.535287    0.293684    0.14262       0.0621109   -0.252417     0.0662437    0.358415    -0.0600069   0.258406   -0.346794   -0.0841869   -0.195784    0.322681    -0.601185   -0.0120619  -0.540899   -0.0299401  -0.23312    -0.206574     0.119127 
 -0.0752241   -0.939693    0.272221    0.179874    0.409107     0.284456     0.115693   -0.449432   -0.72775      -0.12015      0.0990055   -0.298625    -0.461858    -0.0126553  -0.0238848  -0.721842   -0.596498     0.164387   -0.220345    -0.0224223   0.256497   -0.111134   -0.237233    0.425066    0.17326     -0.258378 
 -0.282433     0.0130056  -0.331754   -0.399709   -0.358864     0.145583     0.366437   -0.349535    0.149295      0.726024     0.31129     -0.327207    -0.13354      0.534984    0.200906   -0.1151     -0.365606     0.137721    0.332371    -0.236884   -0.0587952   0.270402    0.0683219   0.622475   -0.100903     0.11833  
  0.0597845    0.502061   -0.235879   -0.468817    0.255082     0.600488    -0.299451   -0.3982     -0.368823      0.190826     0.219869    -0.0550843    0.0955755   -0.331909    0.10825    -0.0700865   0.28482      0.808653   -1.10254      0.262173    0.0569285   0.251474    0.229662   -0.0336637   0.275587     0.0452529
  0.147533    -0.237064    0.349295    0.194833   -0.393211    -0.25341      0.114427    0.540024    0.492567     -0.211882     0.0494271    0.531587    -0.580271     0.217847    0.20927     0.15628    -0.369124    -0.83844     0.763736    -0.218059    0.166064   -0.397375    0.0549409   0.705075   -0.758429     0.180377 
 -0.5287       0.0973339   0.42887     0.254579    0.307662     0.0929643   -0.119854   -0.33078    -0.441459     -0.609649    -0.174473    -0.0864979   -0.122564     0.303288    0.0805829  -0.374223    0.188587     0.0610616   0.357282     0.314311    0.100818    0.761437    0.25579     0.510203    0.667601    -0.344823 
 -0.107933    -0.374931   -0.148581   -0.468852    0.242805    -0.414824     0.235058   -0.136348    0.368336      0.00234071   0.0452394   -0.293016    -0.609883     0.0531644  -0.309431    0.0569657   0.521184     0.109753    0.277468    -0.535849   -0.671153    0.518536   -0.662885    0.0309125  -0.356369     0.234688 
  0.396314     0.211851   -0.0159431   0.0561993  -0.173408     0.0655265   -0.303242   -0.266239    0.312297      0.23626      0.201387     0.327486    -0.212318     0.122144   -0.2447     -0.347984    0.782982    -0.397836   -0.237135     0.0789417  -0.420456    0.125457    0.130065    0.262953    0.132542     0.150168 
  0.228351     0.265881   -0.306324   -0.282105    0.0487641    0.208445     0.371743    0.0703808  -0.0683341    -0.320061     0.114807     0.00372499   0.110697    -0.5752     -0.185092   -0.532463   -0.219321     0.307548    0.330924    -0.287039    0.244798   -0.208093    0.157884    0.418558    0.476362    -0.585651 
 -0.0751378    0.21062    -0.212503   -0.194957   -0.00252021   0.050825    -0.162955    0.207462    0.035547     -0.0583293   -0.602965    -0.105006    -0.107499    -0.0797112  -0.434872    0.0549749   0.258621     0.0553036  -0.170058    -0.175592   -0.235416    0.0809854  -0.102725   -0.183358   -0.00721426  -0.264499 
  0.00208087  -0.154483    0.0971428  -0.380486    0.491618     1.05088      0.025381    0.249931    0.182733      0.179988    -0.464327    -0.0119581   -0.201801     0.650755    0.432461   -0.0972601   0.14349      0.231352   -0.338565    -0.667406   -0.706993   -0.559189    0.546056   -0.283833    0.00394085  -0.132976 
  0.125365     0.315715    0.739429    0.0824058  -0.322455    -0.33475     -0.218769   -0.350948    0.369534      0.165928     0.279622     0.299579     0.345626    -0.358607    0.356225   -0.0793338  -0.411921    -0.231155   -0.190275     0.803728    0.281756   -0.291657    0.273381    0.333307   -0.204332    -0.329992 
  0.479389     0.0906405   0.133563    0.768585   -0.480437     0.536682     0.429479   -0.375433   -0.150484     -0.0743418   -0.0912751    0.651037    -0.11082      0.0542154  -0.257079   -0.557735   -0.0852471    0.109401   -0.350727     0.46697     0.4155     -0.401976    0.239326   -0.539398    0.477718     0.200934 
 -0.35894     -0.0922992   0.345886    0.125499    0.173761    -0.00391334   0.0975092  -0.156699    0.513682     -0.0941521    0.0484316    0.258728    -0.0813402   -0.124028    0.0294894  -0.562633   -0.0353292    0.0954774  -0.165866    -0.893612   -0.190601    0.455692   -0.0041024   0.0501128  -0.769396    -0.784661 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405547
INFO: iteration 2, average log likelihood -1.405539
INFO: iteration 3, average log likelihood -1.405530
INFO: iteration 4, average log likelihood -1.405522
INFO: iteration 5, average log likelihood -1.405514
INFO: iteration 6, average log likelihood -1.405507
INFO: iteration 7, average log likelihood -1.405499
INFO: iteration 8, average log likelihood -1.405492
INFO: iteration 9, average log likelihood -1.405484
INFO: iteration 10, average log likelihood -1.405477
INFO: EM with 100000 data points 10 iterations avll -1.405477
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
