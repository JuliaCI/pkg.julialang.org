>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.1.1
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-98-generic #145-Ubuntu SMP Sat Oct 8 20:13:07 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (683.65234375 MB free)
Uptime: 23344.0 sec
Load Avg:  0.91943359375  0.9755859375  1.02734375
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3507 MHz    1325436 s       5862 s     139806 s     605074 s         69 s
#2  3507 MHz     529875 s        387 s      79275 s    1634856 s          2 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.1.1
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.5687729787886092e6,[100000.0,5.86573e-30],
[409.431 -38.0576 353.58; 2.17625e-29 1.92624e-30 2.19922e-29],

Array{Float64,2}[
[99931.8 74.4966 206.774; 74.4966 99978.4 -449.954; 206.774 -449.954 99965.9],

[8.07414e-29 7.14658e-30 8.15937e-29; 7.14658e-30 6.32557e-31 7.22201e-30; 8.15937e-29 7.22201e-30 8.24549e-29]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.457951e+03
      1       9.413824e+02      -5.165690e+02 |        7
      2       9.155465e+02      -2.583597e+01 |        0
      3       9.155465e+02       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 915.546474620407)
INFO: K-means with 272 data points using 3 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.071535
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.793331
INFO: iteration 2, lowerbound -3.633297
INFO: iteration 3, lowerbound -3.466732
INFO: iteration 4, lowerbound -3.295538
INFO: iteration 5, lowerbound -3.139266
INFO: iteration 6, lowerbound -3.015073
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.930034
INFO: dropping number of Gaussions to 5
INFO: iteration 8, lowerbound -2.865091
INFO: dropping number of Gaussions to 4
INFO: iteration 9, lowerbound -2.805515
INFO: iteration 10, lowerbound -2.766235
INFO: iteration 11, lowerbound -2.741238
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.705061
INFO: iteration 13, lowerbound -2.655737
INFO: iteration 14, lowerbound -2.595966
INFO: iteration 15, lowerbound -2.527605
INFO: iteration 16, lowerbound -2.461302
INFO: iteration 17, lowerbound -2.406136
INFO: iteration 18, lowerbound -2.364161
INFO: iteration 19, lowerbound -2.333337
INFO: iteration 20, lowerbound -2.313594
INFO: iteration 21, lowerbound -2.307421
INFO: dropping number of Gaussions to 2
INFO: iteration 22, lowerbound -2.302932
INFO: iteration 23, lowerbound -2.299261
INFO: iteration 24, lowerbound -2.299257
INFO: iteration 25, lowerbound -2.299255
INFO: iteration 26, lowerbound -2.299254
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Tue 11 Oct 2016 10:58:51 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Tue 11 Oct 2016 10:58:53 AM UTC: K-means with 272 data points using 3 iterations
11.3 data points per parameter
,Tue 11 Oct 2016 10:58:55 AM UTC: EM with 272 data points 0 iterations avll -2.071535
5.8 data points per parameter
,Tue 11 Oct 2016 10:58:56 AM UTC: GMM converted to Variational GMM
,Tue 11 Oct 2016 10:58:58 AM UTC: iteration 1, lowerbound -3.793331
,Tue 11 Oct 2016 10:58:58 AM UTC: iteration 2, lowerbound -3.633297
,Tue 11 Oct 2016 10:58:58 AM UTC: iteration 3, lowerbound -3.466732
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 4, lowerbound -3.295538
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 5, lowerbound -3.139266
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 6, lowerbound -3.015073
,Tue 11 Oct 2016 10:58:59 AM UTC: dropping number of Gaussions to 7
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 7, lowerbound -2.930034
,Tue 11 Oct 2016 10:58:59 AM UTC: dropping number of Gaussions to 5
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 8, lowerbound -2.865091
,Tue 11 Oct 2016 10:58:59 AM UTC: dropping number of Gaussions to 4
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 9, lowerbound -2.805515
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 10, lowerbound -2.766235
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 11, lowerbound -2.741238
,Tue 11 Oct 2016 10:58:59 AM UTC: dropping number of Gaussions to 3
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 12, lowerbound -2.705061
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 13, lowerbound -2.655737
,Tue 11 Oct 2016 10:58:59 AM UTC: iteration 14, lowerbound -2.595966
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 15, lowerbound -2.527605
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 16, lowerbound -2.461302
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 17, lowerbound -2.406136
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 18, lowerbound -2.364161
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 19, lowerbound -2.333337
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 20, lowerbound -2.313594
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 21, lowerbound -2.307421
,Tue 11 Oct 2016 10:59:00 AM UTC: dropping number of Gaussions to 2
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 22, lowerbound -2.302932
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 23, lowerbound -2.299261
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 24, lowerbound -2.299257
,Tue 11 Oct 2016 10:59:00 AM UTC: iteration 25, lowerbound -2.299255
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 26, lowerbound -2.299254
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 27, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 28, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 29, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 30, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 31, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 32, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 33, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 34, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 35, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 36, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 37, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 38, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:01 AM UTC: iteration 39, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 40, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 41, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 42, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 43, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 44, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 45, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 46, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 47, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 48, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 49, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: iteration 50, lowerbound -2.299253
,Tue 11 Oct 2016 10:59:02 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999999
avll from stats: -0.9780697931001998
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9780697931000819
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9780697931000819
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -1.0038164421524223
avll from llpg:  -1.0038164421524223
avll direct:     -1.0038164421524223
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.189482    -0.0578747   -0.0450574     0.00544147   0.0262633    0.0771178     0.180506    -0.0478738  -0.0765257    0.0286824    0.0272573   -0.045185     0.0714484    0.0872698   0.0800954    0.0839856   -0.0943538  -0.00462356  -0.120724    -0.102404    -0.0326574   -0.105941     0.0373801    0.182307    -0.0313019   -0.0427528 
 -0.116554     0.110485    -0.0238793    -0.0647112    0.0612588   -0.0659839     0.134046    -0.249512   -0.215505    -0.0310467   -0.0839973   -0.0776729   -0.252194    -0.0741679  -0.173263    -0.139732     0.157612   -0.0670025   -0.121374     0.0824987    0.0694708    0.141231    -0.142815     0.0901035    0.197592     0.00344521
 -0.0670625   -0.115894    -0.102034     -0.0121023    0.0422061    0.185315      0.0671421    0.023916   -0.0123586    0.0850112    0.0670308    0.0254356   -0.00102937  -0.0554507   0.0327882   -0.240707     0.018376    0.107308     0.128939    -0.0304871   -0.107355    -0.0745289   -0.0664485    0.0795726    0.0101491    0.0679894 
  0.0361646    0.251605    -0.00924427   -0.0413107   -0.0976561   -0.0160747    -0.127107    -0.0105494  -0.0301154   -0.00667766  -0.0943943   -0.0368898    0.1483       0.0652097  -0.0147831   -0.11336     -0.131407    0.0692901   -0.010333    -0.0933308   -0.109118     0.121788    -0.00967709  -0.0359438    0.045349    -0.129078  
  0.00617861  -0.00944402   0.11388       0.0788408    0.115308     0.113221      0.0610559   -0.0151497   0.102641     0.0898098   -0.0229057   -0.0163671   -0.0396669    0.0894553   0.00351553   0.280493     0.181058   -0.0911856   -0.0307232   -0.16349      0.0582159    0.0422891    0.143667     0.0111892    0.0484504   -0.0343121 
 -0.278037    -0.0191089    0.0827347     0.0978169   -0.0851208    0.0129431    -0.0779943   -0.0754034  -0.034396    -0.0433344   -0.0289744   -0.120774     0.0530104   -0.10563    -0.0169226    0.0627139   -0.0532135  -0.0507899   -0.137051     0.142969    -0.0821735    0.0429441   -0.0954354    0.0301883    0.140143    -0.0956353 
  0.0115163    0.131065    -0.0150707     0.122105    -0.206896     0.151956     -0.0387053   -0.0447335   0.0378203   -0.119077    -0.0882032   -0.00176599  -0.0135574   -0.0535015  -0.0250736   -0.140991    -0.102944    0.0849528    0.0120347    0.0518772   -0.160265    -0.0923067   -0.102674     0.0055932   -0.0288999   -0.0200186 
 -0.0503483    0.196309     0.0729762    -0.0190149    0.0710193   -0.0817132     0.0563152    0.0683396   0.202768     0.0174043    0.0390841   -0.133222     0.11533      0.0108386   0.0298146   -0.00411246   0.0292197   0.04347     -0.121578    -0.0049956   -0.0313899    0.0192834    0.136369    -0.00908094   0.0516521   -0.0322118 
  0.00706063   0.101776     0.104254     -0.0700755    0.0953997    0.284424      0.00598253  -0.134316   -0.135853    -0.0613335   -0.0522058    0.0254965    0.0147878    0.0979585   0.0114006   -0.0147476   -0.0176514  -0.042864    -0.156227     0.0749575   -0.210048     0.0581804    0.032013     0.0161546    0.174835    -0.00353587
  0.0108377   -0.175126    -0.116427      0.0724698    0.0582865    0.0525979    -0.0491032   -0.0323814  -0.165261     0.00352732   0.0733939    0.157229    -0.152535    -0.127932    0.22541     -0.106288    -0.0445948   0.136136    -0.0359947    0.0121009    0.0522138    0.025966     0.056911    -0.0845109   -0.0537751   -0.112958  
  0.00283346  -0.0470003   -0.00830099    0.121053    -0.00977939  -0.0136159     0.0040391   -0.0239113  -0.126495    -0.0375848   -0.0940306    0.160669     0.124859     0.0233175   0.0654404   -0.117648     0.0136257   0.075498     0.102133    -0.0785004   -0.00919308  -0.0148935    0.0817655    0.00125239  -0.127586    -0.156305  
 -0.130362     0.0419251   -0.0227516     0.057147    -0.00202461  -0.0594186     0.0558242    0.115741    0.0188421   -0.100933    -0.0483524   -0.0324351    0.00558348  -0.116773    0.0592949   -0.0373895   -0.0175936   0.0987043   -0.0361248    0.0428023    0.0774441    0.108016    -0.244953    -0.095083    -0.0177754    0.0893551 
  0.102768     0.00276077   0.161134      0.0177589    0.0742744   -0.0580611     0.00172968  -0.0758586  -0.0204419    0.0599339    0.245431     0.0625775    0.0241816    0.155969    0.0334067   -0.0766393   -0.199146   -0.0628377    0.0745631   -0.00761206   0.044471     0.129116     0.00997403   0.0542862    0.0488757   -0.135607  
 -0.0295814   -0.0108963   -0.000261061   0.120124     0.022652     0.063072     -0.0949203   -0.0192042  -0.0566398    0.00780419  -0.0984416    0.0379594   -0.182745    -0.0686384  -0.0850032    0.184888     0.091398    0.0336932    0.0164187    0.00474654  -0.0444141    0.0710294   -0.0302368    0.130523     0.140765     0.0709102 
  0.104687    -0.0517783    0.080242     -0.0816826   -0.192361     0.0560176    -0.161164    -0.070163   -0.141997     0.0874154   -0.0149333    0.0860234    0.0614694   -0.140232    0.0478565    0.00225363  -0.0429086  -0.143805     0.108825    -0.102294    -0.103652     0.114909     0.0957008   -0.296701     0.00667779  -0.0371478 
 -0.0167046   -0.0531475    0.149055     -0.101323    -0.0442985    0.0977106     0.105748     0.0522476  -0.00814923  -0.0629142   -0.130278     0.0946278    0.0507236    0.0285259   0.0218919   -0.0283637    0.128395    0.0137189   -0.0199239    0.0784769    0.0153102   -0.128034     0.0901734   -0.00283252  -0.0492747    0.1465    
 -0.0840498   -0.0610154   -0.0949327    -0.117134    -0.0198939   -0.00211104    0.0364272   -0.0456927   0.0570205    0.0239021    0.0341081   -0.127601     0.0333193   -0.0080278   0.0803031    0.144037    -0.01979     0.0998418    0.149512     0.0981545   -0.143323    -0.0148582    0.122485     0.171428    -0.0403907   -0.0284331 
  0.106083     0.00542634   0.194872      0.149216    -0.0268346    0.0456273    -0.200235     0.108071   -0.176685     0.0426178    0.113155    -0.0284782    0.0727107   -0.228904    0.051265    -0.0400379    0.045255   -0.0421945    0.0367596   -0.0479536    0.176515    -0.00776195   0.0167317    0.128223    -0.053994    -0.160274  
 -0.0455633   -0.0447125   -0.088439      0.131144    -0.107497    -0.018092     -0.139686     0.11453     0.0304719   -0.0257072   -0.169719    -0.212378     0.0467736    0.133809   -0.0376733    0.0969948   -0.0695533   0.0180039   -0.0252644    0.264247    -0.0702995    0.0670801   -0.0329289    0.187679    -0.0451513    0.0176006 
 -0.153036    -0.0354121   -0.0363619    -0.0889348   -0.0362225   -0.124572      0.0119286   -0.0830747   0.124567     0.00107469  -0.0184154   -0.0417735   -0.0210662   -0.0293064   0.0745218    0.203411    -0.0851738   0.0570837    0.0843111    0.115623     0.0763996   -0.0511797    0.203077     0.221423    -0.170011    -0.00999558
  0.0691441   -0.105629     0.139554     -0.0614594   -0.0917151    0.240942     -0.0688286   -0.0754673  -0.0518396    0.140377     0.0941187    0.0387657    0.0328396    0.0851236  -0.131737     0.0288217    0.129118   -0.0956828    0.00290162  -0.00965612  -0.0461096   -0.0112814    0.161916     0.117525    -0.0568049   -0.141035  
 -0.0292694   -0.0869298   -0.0664338     0.0229557   -0.108497     0.104755      0.0899859   -0.012016    0.08866      0.00397522   0.0338851   -0.173284    -0.118015    -0.0643362  -0.115255     0.0777631   -0.100185   -0.151841    -0.140972    -0.0964253   -0.145744     0.0208166   -0.0133681    0.129703    -0.151094    -0.0742606 
  0.080624     0.0293477    0.190263     -0.0162213   -0.0925119    0.110151      0.0535037   -0.0133533   0.055501     0.117229     0.0586497   -0.00312442  -0.0710764    0.073746   -0.0368433    0.0503239    0.12962     0.116733     0.00541776  -0.099579    -0.0251781    0.0526176   -0.0919791    0.138242     0.0970043   -0.064497  
  0.0280424   -0.0657389    0.0225652    -0.0570259   -0.100095     0.223121     -0.0990957    0.150417   -0.0608283    0.150645    -0.0254133   -0.126472     0.301644    -0.0678371   0.0412078    0.0630398   -0.0263975   0.0106596   -0.0980528   -0.121934    -0.107353     0.0636685   -0.0378361    0.0761028   -0.0366635    0.0430217 
 -0.0513778   -0.0192061    0.138104      0.0246821    0.134046    -0.147125     -0.00272764  -0.0255553  -0.177165    -0.0905793    0.0299584   -0.085602    -0.233104     0.111589   -0.107627     0.0614872    0.126781    0.14114     -0.034755     0.0381032   -0.108767    -0.0746151    0.0072132    0.0887922   -0.0536701    0.0692608 
 -0.0790726    0.0991563   -0.247314      0.228112     0.0520029   -0.0165435     0.0971138   -0.127784   -0.044915     0.00810771  -0.00812339   0.0109644   -0.0909871   -0.12427     0.0306279   -0.237692    -0.136673   -0.17655     -0.184616     0.119756     0.0134486   -0.102292     0.0280697   -0.029046    -0.0909525    0.157764  
 -0.0167864    0.127258    -0.0493191    -0.253625     0.0582392    0.000590178   0.133748    -0.0310812   0.192667     0.0888099    0.0622319   -0.111907     0.039575     0.096837   -0.0945003    0.0136681    0.0788845  -0.0180517    0.0126099    0.161896     0.0524544    0.134999     0.12796     -0.0930011    0.100111    -0.0300899 
 -0.16571      0.00851479   0.0491483     0.077574     0.0657552   -0.0341103    -0.173311    -0.17205     0.130868     0.123799    -0.0801351    0.110511    -0.00600691  -0.02936     0.133263    -0.0994427    0.0647196   0.084538    -0.122942     0.0277914    0.0608635   -0.090992    -0.0317849   -0.0761259    0.0867952   -0.19447   
  0.0163851   -0.00667391   0.124925     -0.0145473   -0.0708852    0.183673      0.151636    -0.0888299  -0.0360866    0.180756     0.16019      0.0396813    0.103889     0.121417   -0.0586506    0.0528595   -0.135667   -0.165178     0.140617     0.0230713    0.150651    -0.0536559    0.0729897   -0.0191564    0.043984    -0.0751806 
 -0.141993    -0.0125016   -0.0207378    -0.135876    -0.0556278   -0.105105      0.0717618   -0.0294023  -0.00545781   0.031763    -0.0251783   -0.101119    -0.0511213    0.0616993   0.109404     0.0496608   -0.017092    0.0241254   -0.074453     0.0140986   -0.00110854   0.0968135   -0.035522    -0.00540857   0.119756    -0.0717272 
  0.15198     -0.0322291   -0.0821611    -0.0206879    0.0634762   -0.0983805    -0.1323      -0.029542   -0.141434     0.0969459   -0.0520916   -0.0518178   -0.164727    -0.0200894  -0.111917    -0.00612933  -0.0231626  -0.029229    -0.0750072   -0.0728351   -0.115656     0.0218005   -0.153113     0.0825505   -0.063141     0.102726  
 -0.133974    -0.181026    -0.0477811     0.104184    -0.079323     0.0158638    -0.110469     0.0296016  -0.00266723  -0.11846     -0.203844     0.041348     0.147151     0.018973    0.00220534  -0.0202579   -0.0290389  -0.105225    -0.0336157   -0.0204724   -0.0861969    0.134124     0.00372887   0.11939      0.0478253   -0.00996899kind diag, method split
0: avll = -1.410370058259572
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.410466
INFO: iteration 2, average log likelihood -1.410392
INFO: iteration 3, average log likelihood -1.410071
INFO: iteration 4, average log likelihood -1.406627
INFO: iteration 5, average log likelihood -1.394189
INFO: iteration 6, average log likelihood -1.384493
INFO: iteration 7, average log likelihood -1.381729
INFO: iteration 8, average log likelihood -1.380434
INFO: iteration 9, average log likelihood -1.379578
INFO: iteration 10, average log likelihood -1.378929
INFO: iteration 11, average log likelihood -1.378384
INFO: iteration 12, average log likelihood -1.377899
INFO: iteration 13, average log likelihood -1.377489
INFO: iteration 14, average log likelihood -1.377165
INFO: iteration 15, average log likelihood -1.376917
INFO: iteration 16, average log likelihood -1.376730
INFO: iteration 17, average log likelihood -1.376592
INFO: iteration 18, average log likelihood -1.376494
INFO: iteration 19, average log likelihood -1.376423
INFO: iteration 20, average log likelihood -1.376373
INFO: iteration 21, average log likelihood -1.376338
INFO: iteration 22, average log likelihood -1.376314
INFO: iteration 23, average log likelihood -1.376297
INFO: iteration 24, average log likelihood -1.376285
INFO: iteration 25, average log likelihood -1.376275
INFO: iteration 26, average log likelihood -1.376268
INFO: iteration 27, average log likelihood -1.376262
INFO: iteration 28, average log likelihood -1.376257
INFO: iteration 29, average log likelihood -1.376253
INFO: iteration 30, average log likelihood -1.376250
INFO: iteration 31, average log likelihood -1.376247
INFO: iteration 32, average log likelihood -1.376244
INFO: iteration 33, average log likelihood -1.376243
INFO: iteration 34, average log likelihood -1.376241
INFO: iteration 35, average log likelihood -1.376239
INFO: iteration 36, average log likelihood -1.376238
INFO: iteration 37, average log likelihood -1.376237
INFO: iteration 38, average log likelihood -1.376236
INFO: iteration 39, average log likelihood -1.376236
INFO: iteration 40, average log likelihood -1.376235
INFO: iteration 41, average log likelihood -1.376234
INFO: iteration 42, average log likelihood -1.376234
INFO: iteration 43, average log likelihood -1.376234
INFO: iteration 44, average log likelihood -1.376233
INFO: iteration 45, average log likelihood -1.376233
INFO: iteration 46, average log likelihood -1.376233
INFO: iteration 47, average log likelihood -1.376232
INFO: iteration 48, average log likelihood -1.376232
INFO: iteration 49, average log likelihood -1.376232
INFO: iteration 50, average log likelihood -1.376232
INFO: EM with 100000 data points 50 iterations avll -1.376232
952.4 data points per parameter
1: avll = [-1.41047,-1.41039,-1.41007,-1.40663,-1.39419,-1.38449,-1.38173,-1.38043,-1.37958,-1.37893,-1.37838,-1.3779,-1.37749,-1.37716,-1.37692,-1.37673,-1.37659,-1.37649,-1.37642,-1.37637,-1.37634,-1.37631,-1.3763,-1.37628,-1.37628,-1.37627,-1.37626,-1.37626,-1.37625,-1.37625,-1.37625,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.376407
INFO: iteration 2, average log likelihood -1.376250
INFO: iteration 3, average log likelihood -1.375654
INFO: iteration 4, average log likelihood -1.370080
INFO: iteration 5, average log likelihood -1.353766
INFO: iteration 6, average log likelihood -1.344244
INFO: iteration 7, average log likelihood -1.341436
INFO: iteration 8, average log likelihood -1.339741
INFO: iteration 9, average log likelihood -1.338051
INFO: iteration 10, average log likelihood -1.336304
INFO: iteration 11, average log likelihood -1.334780
INFO: iteration 12, average log likelihood -1.333665
INFO: iteration 13, average log likelihood -1.332803
INFO: iteration 14, average log likelihood -1.332160
INFO: iteration 15, average log likelihood -1.331699
INFO: iteration 16, average log likelihood -1.331362
INFO: iteration 17, average log likelihood -1.331102
INFO: iteration 18, average log likelihood -1.330896
INFO: iteration 19, average log likelihood -1.330742
INFO: iteration 20, average log likelihood -1.330634
INFO: iteration 21, average log likelihood -1.330558
INFO: iteration 22, average log likelihood -1.330504
INFO: iteration 23, average log likelihood -1.330464
INFO: iteration 24, average log likelihood -1.330433
INFO: iteration 25, average log likelihood -1.330406
INFO: iteration 26, average log likelihood -1.330381
INFO: iteration 27, average log likelihood -1.330354
INFO: iteration 28, average log likelihood -1.330323
INFO: iteration 29, average log likelihood -1.330285
INFO: iteration 30, average log likelihood -1.330235
INFO: iteration 31, average log likelihood -1.330169
INFO: iteration 32, average log likelihood -1.330076
INFO: iteration 33, average log likelihood -1.329941
INFO: iteration 34, average log likelihood -1.329748
INFO: iteration 35, average log likelihood -1.329506
INFO: iteration 36, average log likelihood -1.329249
INFO: iteration 37, average log likelihood -1.329015
INFO: iteration 38, average log likelihood -1.328811
INFO: iteration 39, average log likelihood -1.328631
INFO: iteration 40, average log likelihood -1.328465
INFO: iteration 41, average log likelihood -1.328302
INFO: iteration 42, average log likelihood -1.328135
INFO: iteration 43, average log likelihood -1.327950
INFO: iteration 44, average log likelihood -1.327760
INFO: iteration 45, average log likelihood -1.327587
INFO: iteration 46, average log likelihood -1.327441
INFO: iteration 47, average log likelihood -1.327321
INFO: iteration 48, average log likelihood -1.327223
INFO: iteration 49, average log likelihood -1.327142
INFO: iteration 50, average log likelihood -1.327076
INFO: EM with 100000 data points 50 iterations avll -1.327076
473.9 data points per parameter
2: avll = [-1.37641,-1.37625,-1.37565,-1.37008,-1.35377,-1.34424,-1.34144,-1.33974,-1.33805,-1.3363,-1.33478,-1.33367,-1.3328,-1.33216,-1.3317,-1.33136,-1.3311,-1.3309,-1.33074,-1.33063,-1.33056,-1.3305,-1.33046,-1.33043,-1.33041,-1.33038,-1.33035,-1.33032,-1.33028,-1.33024,-1.33017,-1.33008,-1.32994,-1.32975,-1.32951,-1.32925,-1.32901,-1.32881,-1.32863,-1.32846,-1.3283,-1.32813,-1.32795,-1.32776,-1.32759,-1.32744,-1.32732,-1.32722,-1.32714,-1.32708]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.327190
INFO: iteration 2, average log likelihood -1.326950
INFO: iteration 3, average log likelihood -1.326165
INFO: iteration 4, average log likelihood -1.320011
INFO: iteration 5, average log likelihood -1.303941
INFO: iteration 6, average log likelihood -1.291503
INFO: iteration 7, average log likelihood -1.284777
INFO: iteration 8, average log likelihood -1.280341
INFO: iteration 9, average log likelihood -1.277252
INFO: iteration 10, average log likelihood -1.275391
INFO: iteration 11, average log likelihood -1.274252
INFO: iteration 12, average log likelihood -1.273403
INFO: iteration 13, average log likelihood -1.272593
INFO: iteration 14, average log likelihood -1.271692
INFO: iteration 15, average log likelihood -1.270758
INFO: iteration 16, average log likelihood -1.270008
INFO: iteration 17, average log likelihood -1.269332
INFO: iteration 18, average log likelihood -1.268505
INFO: iteration 19, average log likelihood -1.267459
INFO: iteration 20, average log likelihood -1.266356
INFO: iteration 21, average log likelihood -1.265392
INFO: iteration 22, average log likelihood -1.264597
INFO: iteration 23, average log likelihood -1.264044
INFO: iteration 24, average log likelihood -1.263698
INFO: iteration 25, average log likelihood -1.263463
INFO: iteration 26, average log likelihood -1.263273
INFO: iteration 27, average log likelihood -1.263096
INFO: iteration 28, average log likelihood -1.262920
INFO: iteration 29, average log likelihood -1.262737
INFO: iteration 30, average log likelihood -1.262547
INFO: iteration 31, average log likelihood -1.262355
INFO: iteration 32, average log likelihood -1.262173
INFO: iteration 33, average log likelihood -1.262024
INFO: iteration 34, average log likelihood -1.261921
INFO: iteration 35, average log likelihood -1.261854
INFO: iteration 36, average log likelihood -1.261809
INFO: iteration 37, average log likelihood -1.261779
INFO: iteration 38, average log likelihood -1.261758
INFO: iteration 39, average log likelihood -1.261743
INFO: iteration 40, average log likelihood -1.261733
INFO: iteration 41, average log likelihood -1.261726
INFO: iteration 42, average log likelihood -1.261722
INFO: iteration 43, average log likelihood -1.261719
INFO: iteration 44, average log likelihood -1.261717
INFO: iteration 45, average log likelihood -1.261716
INFO: iteration 46, average log likelihood -1.261716
INFO: iteration 47, average log likelihood -1.261715
INFO: iteration 48, average log likelihood -1.261715
INFO: iteration 49, average log likelihood -1.261715
INFO: iteration 50, average log likelihood -1.261715
INFO: EM with 100000 data points 50 iterations avll -1.261715
236.4 data points per parameter
3: avll = [-1.32719,-1.32695,-1.32616,-1.32001,-1.30394,-1.2915,-1.28478,-1.28034,-1.27725,-1.27539,-1.27425,-1.2734,-1.27259,-1.27169,-1.27076,-1.27001,-1.26933,-1.26851,-1.26746,-1.26636,-1.26539,-1.2646,-1.26404,-1.2637,-1.26346,-1.26327,-1.2631,-1.26292,-1.26274,-1.26255,-1.26236,-1.26217,-1.26202,-1.26192,-1.26185,-1.26181,-1.26178,-1.26176,-1.26174,-1.26173,-1.26173,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26171]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.261948
INFO: iteration 2, average log likelihood -1.261669
INFO: iteration 3, average log likelihood -1.260088
INFO: iteration 4, average log likelihood -1.243732
WARNING: Variances had to be floored 10
INFO: iteration 5, average log likelihood -1.205749
WARNING: Variances had to be floored 7
INFO: iteration 6, average log likelihood -1.193738
WARNING: Variances had to be floored 8 10
INFO: iteration 7, average log likelihood -1.189585
WARNING: Variances had to be floored 7
INFO: iteration 8, average log likelihood -1.193762
WARNING: Variances had to be floored 10
INFO: iteration 9, average log likelihood -1.187409
WARNING: Variances had to be floored 3 7 8
INFO: iteration 10, average log likelihood -1.180967
INFO: iteration 11, average log likelihood -1.197862
WARNING: Variances had to be floored 7
INFO: iteration 12, average log likelihood -1.179633
WARNING: Variances had to be floored 8 9
INFO: iteration 13, average log likelihood -1.175368
WARNING: Variances had to be floored 7
INFO: iteration 14, average log likelihood -1.184147
INFO: iteration 15, average log likelihood -1.177834
WARNING: Variances had to be floored 7 8 10
INFO: iteration 16, average log likelihood -1.166640
WARNING: Variances had to be floored 9
INFO: iteration 17, average log likelihood -1.186004
WARNING: Variances had to be floored 7
INFO: iteration 18, average log likelihood -1.180554
WARNING: Variances had to be floored 8 9
INFO: iteration 19, average log likelihood -1.174387
WARNING: Variances had to be floored 7
INFO: iteration 20, average log likelihood -1.181982
WARNING: Variances had to be floored 9
INFO: iteration 21, average log likelihood -1.176006
WARNING: Variances had to be floored 7 8
INFO: iteration 22, average log likelihood -1.172973
WARNING: Variances had to be floored 9
INFO: iteration 23, average log likelihood -1.179988
WARNING: Variances had to be floored 7
INFO: iteration 24, average log likelihood -1.175982
WARNING: Variances had to be floored 8 9
INFO: iteration 25, average log likelihood -1.171069
WARNING: Variances had to be floored 7
INFO: iteration 26, average log likelihood -1.178380
WARNING: Variances had to be floored 9
INFO: iteration 27, average log likelihood -1.173146
WARNING: Variances had to be floored 7 8
INFO: iteration 28, average log likelihood -1.170155
WARNING: Variances had to be floored 9
INFO: iteration 29, average log likelihood -1.178451
WARNING: Variances had to be floored 7
INFO: iteration 30, average log likelihood -1.175079
WARNING: Variances had to be floored 8 9
INFO: iteration 31, average log likelihood -1.171077
WARNING: Variances had to be floored 7
INFO: iteration 32, average log likelihood -1.178109
WARNING: Variances had to be floored 9
INFO: iteration 33, average log likelihood -1.172949
WARNING: Variances had to be floored 7 8
INFO: iteration 34, average log likelihood -1.169559
WARNING: Variances had to be floored 9
INFO: iteration 35, average log likelihood -1.177803
WARNING: Variances had to be floored 7
INFO: iteration 36, average log likelihood -1.174232
WARNING: Variances had to be floored 8 9
INFO: iteration 37, average log likelihood -1.170300
WARNING: Variances had to be floored 7
INFO: iteration 38, average log likelihood -1.177387
WARNING: Variances had to be floored 9
INFO: iteration 39, average log likelihood -1.172364
WARNING: Variances had to be floored 7 8
INFO: iteration 40, average log likelihood -1.169199
WARNING: Variances had to be floored 9
INFO: iteration 41, average log likelihood -1.177621
WARNING: Variances had to be floored 7
INFO: iteration 42, average log likelihood -1.174197
WARNING: Variances had to be floored 8 9
INFO: iteration 43, average log likelihood -1.170264
WARNING: Variances had to be floored 7
INFO: iteration 44, average log likelihood -1.177391
WARNING: Variances had to be floored 9
INFO: iteration 45, average log likelihood -1.172341
WARNING: Variances had to be floored 7 8
INFO: iteration 46, average log likelihood -1.169203
WARNING: Variances had to be floored 9
INFO: iteration 47, average log likelihood -1.177606
WARNING: Variances had to be floored 7
INFO: iteration 48, average log likelihood -1.174199
WARNING: Variances had to be floored 8 9
INFO: iteration 49, average log likelihood -1.170255
WARNING: Variances had to be floored 7
INFO: iteration 50, average log likelihood -1.177392
INFO: EM with 100000 data points 50 iterations avll -1.177392
118.1 data points per parameter
4: avll = [-1.26195,-1.26167,-1.26009,-1.24373,-1.20575,-1.19374,-1.18958,-1.19376,-1.18741,-1.18097,-1.19786,-1.17963,-1.17537,-1.18415,-1.17783,-1.16664,-1.186,-1.18055,-1.17439,-1.18198,-1.17601,-1.17297,-1.17999,-1.17598,-1.17107,-1.17838,-1.17315,-1.17015,-1.17845,-1.17508,-1.17108,-1.17811,-1.17295,-1.16956,-1.1778,-1.17423,-1.1703,-1.17739,-1.17236,-1.1692,-1.17762,-1.1742,-1.17026,-1.17739,-1.17234,-1.1692,-1.17761,-1.1742,-1.17026,-1.17739]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 17 18
INFO: iteration 1, average log likelihood -1.172615
WARNING: Variances had to be floored 13 14 15 16 17 18
INFO: iteration 2, average log likelihood -1.162764
WARNING: Variances had to be floored 17 18
INFO: iteration 3, average log likelihood -1.170228
WARNING: Variances had to be floored 7 8 13 14 15 16 17 18 30
INFO: iteration 4, average log likelihood -1.137274
WARNING: Variances had to be floored 13 14 17 18 19 20 29 30
INFO: iteration 5, average log likelihood -1.116165
WARNING: Variances had to be floored 5 7 8 13 14 15 16 17 18 20
INFO: iteration 6, average log likelihood -1.099230
WARNING: Variances had to be floored 13 14 17 18 19 20
INFO: iteration 7, average log likelihood -1.105802
WARNING: Variances had to be floored 7 8 13 14 15 16 17 18 20 29 30
INFO: iteration 8, average log likelihood -1.083618
WARNING: Variances had to be floored 5 11 13 14 15 17 18 19 20
INFO: iteration 9, average log likelihood -1.095556
WARNING: Variances had to be floored 7 8 16 17 18 20
INFO: iteration 10, average log likelihood -1.105528
WARNING: Variances had to be floored 13 14 15 17 18 19 20 29 30
INFO: iteration 11, average log likelihood -1.086882
WARNING: Variances had to be floored 5 7 8 13 14 16 17 18 20
INFO: iteration 12, average log likelihood -1.088863
WARNING: Variances had to be floored 13 14 15 17 18 19 20
INFO: iteration 13, average log likelihood -1.100239
WARNING: Variances had to be floored 7 8 13 14 16 17 18 20 29 30
INFO: iteration 14, average log likelihood -1.079584
WARNING: Variances had to be floored 5 13 14 15 16 17 18 19 20
INFO: iteration 15, average log likelihood -1.091837
WARNING: Variances had to be floored 7 8 13 14 16 17 18 20
INFO: iteration 16, average log likelihood -1.089862
WARNING: Variances had to be floored 5 11 13 14 15 17 18 19 20 29 30
INFO: iteration 17, average log likelihood -1.079613
WARNING: Variances had to be floored 7 8 16 17 18 20
INFO: iteration 18, average log likelihood -1.094563
WARNING: Variances had to be floored 5 13 14 17 18 19 20
INFO: iteration 19, average log likelihood -1.078571
WARNING: Variances had to be floored 7 8 13 14 16 17 18 20 29 30
INFO: iteration 20, average log likelihood -1.070854
WARNING: Variances had to be floored 5 11 13 14 15 16 17 18 19 20
INFO: iteration 21, average log likelihood -1.079551
WARNING: Variances had to be floored 7 8 17 18 20
INFO: iteration 22, average log likelihood -1.092919
WARNING: Variances had to be floored 5 13 14 15 16 17 18 19 20 29 30
INFO: iteration 23, average log likelihood -1.066820
WARNING: Variances had to be floored 7 8 13 14 16 17 18 20
INFO: iteration 24, average log likelihood -1.082092
WARNING: Variances had to be floored 5 11 13 14 15 17 18 19 20 30
INFO: iteration 25, average log likelihood -1.076682
WARNING: Variances had to be floored 7 8 16 17 18 20 29
INFO: iteration 26, average log likelihood -1.087726
WARNING: Variances had to be floored 5 13 14 15 17 18 19 20 30
INFO: iteration 27, average log likelihood -1.075238
WARNING: Variances had to be floored 7 8 13 14 16 17 18 20 29 30
INFO: iteration 28, average log likelihood -1.074353
WARNING: Variances had to be floored 5 11 13 14 15 16 17 18 19 20
INFO: iteration 29, average log likelihood -1.079535
WARNING: Variances had to be floored 7 8 17 18 20
INFO: iteration 30, average log likelihood -1.092894
WARNING: Variances had to be floored 5 13 14 15 16 17 18 19 20 29 30
INFO: iteration 31, average log likelihood -1.066796
WARNING: Variances had to be floored 7 8 13 14 16 17 18 20
INFO: iteration 32, average log likelihood -1.082071
WARNING: Variances had to be floored 5 11 13 14 15 17 18 19 20 30
INFO: iteration 33, average log likelihood -1.076952
WARNING: Variances had to be floored 7 8 16 17 18 20 29
INFO: iteration 34, average log likelihood -1.087360
WARNING: Variances had to be floored 5 13 14 17 18 19 20 30
INFO: iteration 35, average log likelihood -1.075315
WARNING: Variances had to be floored 7 8 13 14 15 16 17 18 20 29 30
INFO: iteration 36, average log likelihood -1.074074
WARNING: Variances had to be floored 5 11 13 14 16 17 18 19 20
INFO: iteration 37, average log likelihood -1.079578
WARNING: Variances had to be floored 7 8 15 17 18 20
INFO: iteration 38, average log likelihood -1.091778
WARNING: Variances had to be floored 5 13 14 16 17 18 19 20 29 30
INFO: iteration 39, average log likelihood -1.068208
WARNING: Variances had to be floored 7 8 13 14 15 17 18 20
INFO: iteration 40, average log likelihood -1.081067
WARNING: Variances had to be floored 5 11 13 14 16 17 18 19 20 30
INFO: iteration 41, average log likelihood -1.075506
WARNING: Variances had to be floored 7 8 15 17 18 20 29
INFO: iteration 42, average log likelihood -1.089300
WARNING: Variances had to be floored 5 13 14 16 17 18 19 20 30
INFO: iteration 43, average log likelihood -1.073568
WARNING: Variances had to be floored 7 8 13 14 15 17 18 20 29 30
INFO: iteration 44, average log likelihood -1.076879
WARNING: Variances had to be floored 5 11 13 14 16 17 18 19 20
INFO: iteration 45, average log likelihood -1.076824
WARNING: Variances had to be floored 7 8 15 17 18 20
INFO: iteration 46, average log likelihood -1.092557
WARNING: Variances had to be floored 5 13 14 16 17 18 19 20 29 30
INFO: iteration 47, average log likelihood -1.067298
WARNING: Variances had to be floored 7 8 13 14 15 16 17 18 20
INFO: iteration 48, average log likelihood -1.081900
WARNING: Variances had to be floored 5 11 13 14 15 16 17 18 19 20 30
INFO: iteration 49, average log likelihood -1.077720
WARNING: Variances had to be floored 7 8 17 18 20 29
INFO: iteration 50, average log likelihood -1.090007
INFO: EM with 100000 data points 50 iterations avll -1.090007
59.0 data points per parameter
5: avll = [-1.17262,-1.16276,-1.17023,-1.13727,-1.11617,-1.09923,-1.1058,-1.08362,-1.09556,-1.10553,-1.08688,-1.08886,-1.10024,-1.07958,-1.09184,-1.08986,-1.07961,-1.09456,-1.07857,-1.07085,-1.07955,-1.09292,-1.06682,-1.08209,-1.07668,-1.08773,-1.07524,-1.07435,-1.07953,-1.09289,-1.0668,-1.08207,-1.07695,-1.08736,-1.07532,-1.07407,-1.07958,-1.09178,-1.06821,-1.08107,-1.07551,-1.0893,-1.07357,-1.07688,-1.07682,-1.09256,-1.0673,-1.0819,-1.07772,-1.09001]
[-1.41037,-1.41047,-1.41039,-1.41007,-1.40663,-1.39419,-1.38449,-1.38173,-1.38043,-1.37958,-1.37893,-1.37838,-1.3779,-1.37749,-1.37716,-1.37692,-1.37673,-1.37659,-1.37649,-1.37642,-1.37637,-1.37634,-1.37631,-1.3763,-1.37628,-1.37628,-1.37627,-1.37626,-1.37626,-1.37625,-1.37625,-1.37625,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37624,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37623,-1.37641,-1.37625,-1.37565,-1.37008,-1.35377,-1.34424,-1.34144,-1.33974,-1.33805,-1.3363,-1.33478,-1.33367,-1.3328,-1.33216,-1.3317,-1.33136,-1.3311,-1.3309,-1.33074,-1.33063,-1.33056,-1.3305,-1.33046,-1.33043,-1.33041,-1.33038,-1.33035,-1.33032,-1.33028,-1.33024,-1.33017,-1.33008,-1.32994,-1.32975,-1.32951,-1.32925,-1.32901,-1.32881,-1.32863,-1.32846,-1.3283,-1.32813,-1.32795,-1.32776,-1.32759,-1.32744,-1.32732,-1.32722,-1.32714,-1.32708,-1.32719,-1.32695,-1.32616,-1.32001,-1.30394,-1.2915,-1.28478,-1.28034,-1.27725,-1.27539,-1.27425,-1.2734,-1.27259,-1.27169,-1.27076,-1.27001,-1.26933,-1.26851,-1.26746,-1.26636,-1.26539,-1.2646,-1.26404,-1.2637,-1.26346,-1.26327,-1.2631,-1.26292,-1.26274,-1.26255,-1.26236,-1.26217,-1.26202,-1.26192,-1.26185,-1.26181,-1.26178,-1.26176,-1.26174,-1.26173,-1.26173,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26172,-1.26171,-1.26195,-1.26167,-1.26009,-1.24373,-1.20575,-1.19374,-1.18958,-1.19376,-1.18741,-1.18097,-1.19786,-1.17963,-1.17537,-1.18415,-1.17783,-1.16664,-1.186,-1.18055,-1.17439,-1.18198,-1.17601,-1.17297,-1.17999,-1.17598,-1.17107,-1.17838,-1.17315,-1.17015,-1.17845,-1.17508,-1.17108,-1.17811,-1.17295,-1.16956,-1.1778,-1.17423,-1.1703,-1.17739,-1.17236,-1.1692,-1.17762,-1.1742,-1.17026,-1.17739,-1.17234,-1.1692,-1.17761,-1.1742,-1.17026,-1.17739,-1.17262,-1.16276,-1.17023,-1.13727,-1.11617,-1.09923,-1.1058,-1.08362,-1.09556,-1.10553,-1.08688,-1.08886,-1.10024,-1.07958,-1.09184,-1.08986,-1.07961,-1.09456,-1.07857,-1.07085,-1.07955,-1.09292,-1.06682,-1.08209,-1.07668,-1.08773,-1.07524,-1.07435,-1.07953,-1.09289,-1.0668,-1.08207,-1.07695,-1.08736,-1.07532,-1.07407,-1.07958,-1.09178,-1.06821,-1.08107,-1.07551,-1.0893,-1.07357,-1.07688,-1.07682,-1.09256,-1.0673,-1.0819,-1.07772,-1.09001]
32×26 Array{Float64,2}:
  0.109539     0.00576494   0.195501    0.136826    -0.0872005    0.0458408   -0.19487      0.108208    -0.15613      0.0431003    0.120004     -0.0329071     0.0450422   -0.230362      0.0477087   -0.0250155    0.0646032    -0.0592258    0.0536219    -0.0472213    0.181316   -0.0155557   0.0128373    0.104198    -0.0656644  -0.17903   
  0.00551158   0.0155571    0.0738871   0.11163      0.15525      0.12961      0.0525399   -0.0248596    0.0981509    0.091775    -0.0260217    -0.0366603    -0.0425444    0.0857558     0.0488482    0.276669     0.135564     -0.103363    -0.0300212    -0.178031     0.0582276   0.0911284   0.135564     0.00935257   0.0734606  -0.0458315 
 -0.0289246    0.0281321    0.0185128   0.134513     0.00615878   0.107589     0.0300699   -0.01663     -0.229644     0.0697462   -0.111586     -0.000813588  -0.220861    -0.0575203    -0.0934442    0.18131      0.0990489     0.132142     0.0225308     1.52932e-5  -0.0437962   0.0910812  -0.298151     0.131515     0.128819    0.0592334 
 -0.0263472   -0.0662596   -0.0217918   0.0944476    0.0367966    0.0289729   -0.166001    -0.0365197    0.170811    -0.0837023   -0.0717195     0.0798922    -0.150043    -0.0853722    -0.0300976    0.197709     0.095024     -0.0269069   -0.0340004     0.0106715   -0.0348862   0.0449116   0.201975     0.147825     0.155724    0.0495498 
  0.00400088  -0.0197261    0.137023   -0.0141782    0.112431    -0.142199     0.00170062  -0.0414973   -0.180359    -0.0998829    0.0733927    -0.0936249    -0.250205     0.106449     -0.110243     0.0609985    0.118033      0.136369    -0.000975872   0.0442535   -0.105514   -0.0708023   0.00488514   0.0775649   -0.0591943   0.0633918 
 -0.0115549   -0.00369137   0.129557   -0.0321498   -0.0556785    0.181566     0.153965    -0.0994159   -0.0372839    0.186767     0.159592      0.0306799     0.114061     0.141835     -0.0514675    0.0316116   -0.127256     -0.118689     0.180937      0.0133835    0.159821   -0.0782673   0.0913659   -0.0153635    0.0540495  -0.0761145 
 -0.0512751   -0.0890343   -0.0623794   0.0192657   -0.10899      0.104643     0.0908781   -0.0900925    0.0642076    0.00559363   0.0119954    -0.168897     -0.120103    -0.0717921    -0.111792     0.081957    -0.104417     -0.162797    -0.140362     -0.107482    -0.151525    0.0328276   0.00192763   0.129479    -0.146925   -0.0798603 
  0.0017299   -0.0471712   -0.0164663   0.105872    -0.00985238  -0.0138569   -0.0125578   -0.00114826  -0.122138    -0.0395939   -0.0882346     0.174329      0.0527286    0.0250137     0.0549816   -0.115675     0.0144358     0.0776081    0.102041     -0.0605237   -0.0801613  -0.0203104   0.0758472   -0.00561989  -0.124855   -0.159126  
 -0.138296    -0.0348327    0.105845    0.0584028    0.0669045   -0.0043331   -0.185362    -0.199171     0.169436     0.135431    -0.0707979     0.129569      0.00130546  -0.0345667     0.14063     -0.176008     0.111282      0.13991     -0.119842      0.100601     0.0739436  -0.0911139  -0.07016     -0.0883097    0.0725784  -0.442046  
  0.0860944    0.0695984   -0.0160138   0.112734     0.0678661   -0.00358858  -0.167711    -0.148251     0.107182     0.086683    -0.0857242     0.0236101    -0.0206723   -0.0295556     0.116907    -0.0273633    0.0392117     0.0159867   -0.120687     -0.102395     0.0364963   0.0352827  -0.0188414   -0.0263198    0.108445    0.0260957 
  0.00322608  -0.134516    -0.0194301   0.00575013   0.0420872    0.0770995    0.00801801  -0.00614704  -0.10721     -0.0219614    0.0115459     0.135085     -0.0717141   -0.0731005     0.156823    -0.0787004    0.00187291    0.0944538   -0.0399708     0.0328017    0.0574792  -0.02273     0.0703434   -0.0389225   -0.0884094  -0.0102069 
 -0.194642    -0.0846326   -0.0448812   0.0222939    0.0278452    0.0822318    0.153329    -0.0443719   -0.0331313    0.0677237    0.018788     -0.0898931     0.0797266    0.0854232     0.100914     0.0160342   -0.0936847    -0.0186179   -0.131805     -0.100567    -0.0505841  -0.0960107   0.0391256    0.150686    -0.0247643  -0.0451579 
  0.0894652    0.00567493   0.168178   -0.0734841    0.0457004   -0.04028      0.00335853  -0.0696561   -0.0375169    0.0588826    0.405021      0.0692088     0.0063545    0.144443      0.0285666   -0.0609822   -0.348427     -0.143951     0.0622208    -0.543142     0.0252341   0.12439     0.0102722    0.0649119    0.0343753  -0.122971  
  0.0875331   -0.016678     0.016897    0.182402     0.112145    -0.0280668    0.0042141   -0.0660248   -0.0295439    0.0593574   -0.104603      0.0667248     0.0703948    0.140703      0.0625593   -0.0707109    0.227524      0.0804987    0.0398462     0.963307     0.0302099   0.108768    0.0472744    0.0135762    0.0585712  -0.145024  
 -0.20622     -0.0217321    0.123306   -0.152242    -0.0948976    0.195143    -0.0540073    0.252132    -0.176057    -0.0455779   -0.0475112     0.0736867     0.0517575   -0.116902      0.0276611    0.163241     0.00568431   -0.428461    -0.101545      0.144045    -0.460221   -0.386176   -0.10459      0.278849    -0.0238197  -0.196112  
 -0.275423    -0.0206065    0.0792401   0.15432     -0.088633    -0.00539348  -0.0662984   -0.0639974   -0.0279219   -0.045144    -0.0252894    -0.148408      0.0517759   -0.105325     -0.0120286    0.0382395   -0.0423716     0.0414766   -0.143388      0.142208     0.0408179   0.161489   -0.105808     0.0185087    0.146646   -0.0881986 
 -0.376807    -0.0790802   -0.100229    0.0778163    0.064152    -0.0983999   -0.142688    -0.0520637   -0.141332    -0.042721    -0.0986289     0.281616     -0.178949    -0.0587031    -0.115659     0.051364    -0.122814      0.0192706    0.0225568    -0.0730863   -0.111244    0.023162   -0.152611     0.0327414   -0.0325604   0.101245  
  0.662347     0.0575542   -0.087076   -0.112733     0.0624571   -0.0990318   -0.13775     -0.0155666   -0.123346     0.208146    -0.0409535    -0.383583     -0.159339     0.0382503    -0.114924    -0.040853     0.0168714    -0.0999639   -0.188985     -0.0748984   -0.1104      0.020468   -0.153171     0.121322    -0.0536465   0.100919  
 -0.0782105   -0.0715499   -0.0479724  -0.110624    -0.0166124   -0.0247537    0.0288257   -0.0370427    0.0630447    0.0250496    0.104054     -0.108943      0.0278255    0.00770081    0.090017     0.133459    -0.0182289     0.102089     0.146152      0.098978    -0.182678   -0.0275355   0.113648     0.168141    -0.0507202  -0.0302677 
 -0.11018      0.103742    -0.0439311  -0.0585908    0.0595904   -0.00833385   0.128128    -0.25153     -0.222754    -0.0234612   -0.0873406    -0.044619     -0.206238    -0.108882     -0.164525    -0.12487      0.130194     -0.0753349   -0.142047      0.0757607    0.0465111   0.155036   -0.132183     0.0764238    0.212433   -0.00124447
  0.0382412    0.243587     0.0371768  -0.0410451   -0.0749421   -0.00944188  -0.135692    -0.0314612   -0.0428933    0.0521081   -0.0673486    -0.0514509     0.141856     0.0480441     0.00579225  -0.100381    -0.194921      0.0665919   -0.0396468    -0.0853476   -0.111104    0.118254   -0.0150501   -0.0328809    0.0302995  -0.132999  
 -0.0189656   -0.0179083    0.0149841  -0.0289311   -0.0229809    0.127909    -0.0189444   -0.0225936   -0.0644601    0.00617768  -0.0233144     0.0191255     0.00919975  -0.0646819     0.0534301   -0.0831406   -0.0076064     0.00626449   0.010062     -0.00680321  -0.085718    0.0379547  -0.0441567   -0.0840981    0.0375983   0.0393895 
 -0.0608217    0.1057      -0.0175433  -0.336523     0.11306      0.0401528    0.0979333   -0.0215455    0.0696109    0.145507     0.0103808    -0.114751     -0.00884783   0.0937587    -0.176101    -0.320824     0.0652991    -0.0220408   -0.180168      0.16035      0.0319902   0.0644811   0.114829    -0.0977885    0.10308     0.0547524 
  0.0731532    0.111834    -0.0792402  -0.205873    -0.0218145   -0.0494951    0.18278     -0.121517     0.24207     -0.0286395    0.115771     -0.107894      0.0486013    0.0979865    -0.0476266    0.415559     0.133666     -0.00731596   0.225319      0.160718     0.0524871   0.162859    0.147714    -0.0877829    0.0699968  -0.0960271 
 -0.0305073   -0.0811845   -0.0124666   0.153649    -0.109659    -0.0603425   -0.131638     0.118566    -0.0245302   -0.0249758   -0.159695     -0.212157      0.0474308    0.132481     -0.0390733    0.0957904   -0.0649896     0.0390995   -0.0490229     0.249066    -0.0774014   0.0591435  -0.0324632    0.18769     -0.0448964   0.0241946 
  0.0428418   -0.0248572    0.112073   -0.0240111   -0.0971252    0.15245     -0.0438448    0.0619521   -0.0240903    0.132543     0.000462788  -0.0632006     0.116461     0.0119972     0.014818     0.0563252    0.0502246     0.0581212   -0.0614118    -0.11372     -0.0756419   0.062726   -0.0698144    0.108505     0.0684764   0.00199414
 -0.0634474    0.108865    -0.259449    0.202806     0.0474414   -0.0065721    0.0941794   -0.0846218   -0.0606545    0.00982188  -0.0375774     0.0100885    -0.0867751   -0.12511       0.0274011   -0.22602     -0.133545     -0.155735    -0.163648      0.0727786    0.0423722  -0.0999869   0.0735599   -0.0219294   -0.0826439   0.178451  
 -0.0642504    0.0478317   -0.0150404  -0.0137415   -0.122767     0.0355536    0.0263604   -0.0283788    0.0178124   -0.0479841   -0.0572846    -0.0503397     0.0065801    0.0333621     0.0440063   -0.0441081   -0.0522181     0.0372547   -0.0388207     0.0309426   -0.0624999   0.0022238  -0.0543781   -0.0037893    0.0265737  -0.0369134 
 -0.142295    -0.0338882   -0.0313672  -0.0887083   -0.0349491   -0.11059      0.00630865  -0.069402     0.108285    -0.00597329  -0.0399676    -0.0413851    -0.0934062   -0.0297321     0.0571882    0.204968    -0.0838226     0.0270022    0.0887433     0.127633     0.0909492  -0.0389498   0.195372     0.269412    -0.184659   -0.00761089
 -0.0493742    0.240029     0.0777916  -0.0228273    0.0816064   -0.109323     0.0525291    0.0773199    0.196841     0.0316679    0.0395785    -0.139392      0.10668      0.000377481   0.018996    -0.00705184   0.0171713     0.0421941   -0.108763     -0.00893859  -0.0165243   0.0402245   0.136048    -0.0107951    0.0554014  -0.0319605 
 -0.115323    -0.180326    -0.0508534   0.109893    -0.0778723    0.0135755   -0.10193      0.0257795   -0.00436526  -0.0936927   -0.203635      0.0506653     0.139245     0.0153519    -0.0070789   -0.0131433   -0.000768997  -0.104766    -0.0507406    -0.0216344   -0.0823239   0.118285    0.00271904   0.144965     0.0575248  -0.0268394 
  0.0732282   -0.112039     0.136688   -0.0908868   -0.0943883    0.228888    -0.0505361   -0.0804923   -0.072265     0.145566     0.0928302     0.0346769    -0.0547026    0.124351     -0.148358     0.026997     0.124319     -0.0994753   -0.0821428    -0.0371063   -0.0469762  -0.0100809   0.155747     0.118843    -0.0591931  -0.134805  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 13 14 15 16 17 18 19 20 30
INFO: iteration 1, average log likelihood -1.072473
WARNING: Variances had to be floored 5 7 8 13 14 15 16 17 18 19 20 29 30
INFO: iteration 2, average log likelihood -1.051483
WARNING: Variances had to be floored 5 11 13 14 15 16 17 18 19 20 30
INFO: iteration 3, average log likelihood -1.066837
WARNING: Variances had to be floored 5 7 8 13 14 15 16 17 18 19 20 29 30
INFO: iteration 4, average log likelihood -1.054912
WARNING: Variances had to be floored 5 13 14 15 16 17 18 19 20 30
INFO: iteration 5, average log likelihood -1.068706
WARNING: Variances had to be floored 5 7 8 11 13 14 15 16 17 18 19 20 29 30
INFO: iteration 6, average log likelihood -1.049628
WARNING: Variances had to be floored 5 13 14 15 16 17 18 19 20 30
INFO: iteration 7, average log likelihood -1.072221
WARNING: Variances had to be floored 5 7 8 13 14 15 16 17 18 19 20 29 30
INFO: iteration 8, average log likelihood -1.051476
WARNING: Variances had to be floored 5 11 13 14 15 16 17 18 19 20 30
INFO: iteration 9, average log likelihood -1.066954
WARNING: Variances had to be floored 5 7 8 13 14 15 16 17 18 19 20 29 30
INFO: iteration 10, average log likelihood -1.054889
INFO: EM with 100000 data points 10 iterations avll -1.054889
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.959824e+05
      1       6.670521e+05      -2.289303e+05 |       32
      2       6.423512e+05      -2.470095e+04 |       32
      3       6.295503e+05      -1.280086e+04 |       32
      4       6.220029e+05      -7.547431e+03 |       32
      5       6.166938e+05      -5.309035e+03 |       32
      6       6.131228e+05      -3.571069e+03 |       32
      7       6.110000e+05      -2.122816e+03 |       32
      8       6.096083e+05      -1.391690e+03 |       32
      9       6.087826e+05      -8.256480e+02 |       32
     10       6.081509e+05      -6.316730e+02 |       32
     11       6.077049e+05      -4.459924e+02 |       32
     12       6.073743e+05      -3.306658e+02 |       32
     13       6.070969e+05      -2.774317e+02 |       32
     14       6.068540e+05      -2.428074e+02 |       32
     15       6.066327e+05      -2.213070e+02 |       32
     16       6.064228e+05      -2.099348e+02 |       32
     17       6.061392e+05      -2.835944e+02 |       32
     18       6.058439e+05      -2.953324e+02 |       32
     19       6.055591e+05      -2.847860e+02 |       32
     20       6.053145e+05      -2.446348e+02 |       32
     21       6.051393e+05      -1.751150e+02 |       32
     22       6.049818e+05      -1.575637e+02 |       32
     23       6.048143e+05      -1.674262e+02 |       32
     24       6.045754e+05      -2.389641e+02 |       32
     25       6.041421e+05      -4.333266e+02 |       32
     26       6.033999e+05      -7.421140e+02 |       32
     27       6.025346e+05      -8.653764e+02 |       32
     28       6.020727e+05      -4.618378e+02 |       32
     29       6.019281e+05      -1.446361e+02 |       32
     30       6.018822e+05      -4.586837e+01 |       32
     31       6.018614e+05      -2.084507e+01 |       31
     32       6.018514e+05      -1.000038e+01 |       23
     33       6.018457e+05      -5.667731e+00 |       28
     34       6.018402e+05      -5.495921e+00 |       26
     35       6.018361e+05      -4.095605e+00 |       25
     36       6.018333e+05      -2.864233e+00 |       20
     37       6.018317e+05      -1.589965e+00 |       19
     38       6.018296e+05      -2.030476e+00 |       22
     39       6.018271e+05      -2.541204e+00 |       20
     40       6.018253e+05      -1.839705e+00 |       19
     41       6.018242e+05      -1.082251e+00 |       13
     42       6.018235e+05      -7.146922e-01 |       13
     43       6.018228e+05      -6.956004e-01 |       12
     44       6.018223e+05      -4.425466e-01 |        6
     45       6.018220e+05      -2.803712e-01 |        6
     46       6.018218e+05      -2.569681e-01 |        7
     47       6.018215e+05      -2.442855e-01 |        6
     48       6.018213e+05      -2.385559e-01 |        5
     49       6.018211e+05      -1.679546e-01 |        5
     50       6.018209e+05      -2.456611e-01 |        7
K-means terminated without convergence after 50 iterations (objv = 601820.8858920037)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.326219
INFO: iteration 2, average log likelihood -1.300411
INFO: iteration 3, average log likelihood -1.270035
INFO: iteration 4, average log likelihood -1.225874
INFO: iteration 5, average log likelihood -1.174652
WARNING: Variances had to be floored 6 8 31
INFO: iteration 6, average log likelihood -1.113862
WARNING: Variances had to be floored 23
INFO: iteration 7, average log likelihood -1.101465
WARNING: Variances had to be floored 2 9 12 17 19 30 32
INFO: iteration 8, average log likelihood -1.043613
WARNING: Variances had to be floored 1 6 15 22
INFO: iteration 9, average log likelihood -1.099825
WARNING: Variances had to be floored 8 31
INFO: iteration 10, average log likelihood -1.116688
WARNING: Variances had to be floored 23
INFO: iteration 11, average log likelihood -1.103976
WARNING: Variances had to be floored 6 9 25 30 32
INFO: iteration 12, average log likelihood -1.048728
WARNING: Variances had to be floored 1 2 8 12 17 19 22 31
INFO: iteration 13, average log likelihood -1.061156
INFO: iteration 14, average log likelihood -1.158562
WARNING: Variances had to be floored 23
INFO: iteration 15, average log likelihood -1.096807
WARNING: Variances had to be floored 6 9 25 30 31 32
INFO: iteration 16, average log likelihood -1.049325
WARNING: Variances had to be floored 1 8 12
INFO: iteration 17, average log likelihood -1.087506
WARNING: Variances had to be floored 17 19
INFO: iteration 18, average log likelihood -1.104306
WARNING: Variances had to be floored 2 6 22 23
INFO: iteration 19, average log likelihood -1.072375
WARNING: Variances had to be floored 9 12 25 30 31 32
INFO: iteration 20, average log likelihood -1.067368
WARNING: Variances had to be floored 1 8
INFO: iteration 21, average log likelihood -1.102073
WARNING: Variances had to be floored 6 15 17
INFO: iteration 22, average log likelihood -1.094083
WARNING: Variances had to be floored 19 23
INFO: iteration 23, average log likelihood -1.079817
WARNING: Variances had to be floored 1 2 9 12 22 25 30 31 32
INFO: iteration 24, average log likelihood -1.039882
WARNING: Variances had to be floored 6 8
INFO: iteration 25, average log likelihood -1.129694
WARNING: Variances had to be floored 7 17
INFO: iteration 26, average log likelihood -1.107918
WARNING: Variances had to be floored 19 23
INFO: iteration 27, average log likelihood -1.076941
WARNING: Variances had to be floored 1 6 8 9 12 15 31 32
INFO: iteration 28, average log likelihood -1.037107
WARNING: Variances had to be floored 22 25
INFO: iteration 29, average log likelihood -1.138387
WARNING: Variances had to be floored 7 17
INFO: iteration 30, average log likelihood -1.097478
WARNING: Variances had to be floored 2 6 19 23 30 31
INFO: iteration 31, average log likelihood -1.053452
WARNING: Variances had to be floored 1 8 9 12 15
INFO: iteration 32, average log likelihood -1.083979
WARNING: Variances had to be floored 22 32
INFO: iteration 33, average log likelihood -1.122061
WARNING: Variances had to be floored 7 17 25
INFO: iteration 34, average log likelihood -1.088081
WARNING: Variances had to be floored 6 8 23 31
INFO: iteration 35, average log likelihood -1.069706
WARNING: Variances had to be floored 1 2 9 12 32
INFO: iteration 36, average log likelihood -1.080909
WARNING: Variances had to be floored 15 17 19
INFO: iteration 37, average log likelihood -1.104031
WARNING: Variances had to be floored 6 7 8 25
INFO: iteration 38, average log likelihood -1.086504
WARNING: Variances had to be floored 23 31
INFO: iteration 39, average log likelihood -1.088533
WARNING: Variances had to be floored 1 2 9 12 22 32
INFO: iteration 40, average log likelihood -1.055496
WARNING: Variances had to be floored 6 17
INFO: iteration 41, average log likelihood -1.109078
WARNING: Variances had to be floored 8 15 19 25
INFO: iteration 42, average log likelihood -1.083004
WARNING: Variances had to be floored 7 12 23
INFO: iteration 43, average log likelihood -1.078561
WARNING: Variances had to be floored 1 2 6 22 31 32
INFO: iteration 44, average log likelihood -1.055209
WARNING: Variances had to be floored 9 17 30
INFO: iteration 45, average log likelihood -1.112089
WARNING: Variances had to be floored 8
INFO: iteration 46, average log likelihood -1.113389
WARNING: Variances had to be floored 12 19 23
INFO: iteration 47, average log likelihood -1.065362
WARNING: Variances had to be floored 1 2 6 7 15 31 32
INFO: iteration 48, average log likelihood -1.039753
WARNING: Variances had to be floored 9 17 22
INFO: iteration 49, average log likelihood -1.111638
WARNING: Variances had to be floored 8 25
INFO: iteration 50, average log likelihood -1.109939
INFO: EM with 100000 data points 50 iterations avll -1.109939
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.115739    -0.0320486   -0.107346    -0.0369539    0.0440326   -0.107667    -0.108522    -0.0384364   -0.117071     0.103701    -0.144295   -0.0558158   -0.15803     -0.00502143  -0.079397     0.0329971   -0.0309012    -0.005408    -0.0478213   -0.0325681   -0.124181     0.0127211   -0.0837637    0.0982951   -0.0462177    0.0687999
 -0.11864      0.0672773   -0.0462643   -0.0866472    0.00768821  -0.147416     0.124821    -0.0706331   -0.0627764   -0.00993055  -0.0561461  -0.110205     0.109831     0.104641     0.011372    -0.0134379   -0.0254264    -0.00645627  -0.0901044   -0.0367069    0.00481208   0.0743226   -0.0456111    0.0507569    0.137975    -0.0717292
  0.00557782   0.0133827    0.0726497    0.117665     0.158184     0.130211     0.0556906   -0.0250923    0.0977851    0.0916642   -0.0265662  -0.0362471   -0.0416868    0.0875613    0.0468525    0.278536     0.137207     -0.104765    -0.0329014   -0.179363     0.0580495    0.0875232    0.133287     0.0124476    0.0721516   -0.0458638
 -0.120414    -0.176497    -0.0505105    0.101307    -0.070343     0.0124702   -0.103318     0.0212162   -0.00496481  -0.0980344   -0.188576    0.0509039    0.132448     0.00059274  -0.00684472  -0.0128593   -0.000556423  -0.0971677   -0.043687    -0.0218292   -0.0895646    0.11716      0.00447947   0.139937     0.0509656   -0.0207658
  0.00330314   0.109817    -0.0489214   -0.277249     0.0467908   -0.0053255    0.142896    -0.0726389    0.158968     0.0590005    0.0615049  -0.111933     0.0192246    0.0956407   -0.114134     0.045229     0.0989158    -0.0155092    0.0229261    0.160288     0.0421755    0.112012     0.130861    -0.0932995    0.0857303   -0.0196226
  0.00261461  -0.152838    -0.0995852    0.0311983    0.0572902    0.0600705   -0.0248314   -0.0269841   -0.13826      0.00284528   0.0418377   0.140837    -0.114746    -0.118559     0.194655    -0.109549    -0.0438836     0.131039    -0.0415079    0.0181219    0.0629141    0.0276489    0.0553311   -0.0469669   -0.0546283   -0.103882 
 -0.0269334   -0.0221763   -0.060197    -0.068462     0.0387595   -0.0926172   -0.0560363   -0.0581911   -0.0770778    0.0233358    0.210875   -0.106307    -0.093289    -0.0385636   -0.0530557    0.0278406   -0.055384      0.0138425    0.049149     0.00348516  -0.13418      0.0207396   -0.0690522    0.146037    -0.00733657   0.046986 
 -0.274225    -0.0199689    0.0888977    0.110224    -0.0895144    0.0249092   -0.0719954   -0.0124575   -0.0485698   -0.0437223   -0.0259343  -0.122569     0.0519524   -0.105598    -0.00982201   0.0639647   -0.0392611    -0.0392929   -0.137149     0.141951    -0.0435553    0.077373    -0.11002      0.0647137    0.114677    -0.117387 
  0.00183449   0.128925     0.0923971   -0.0622892    0.122577     0.26834     -0.00947706  -0.114858    -0.121367    -0.0698085   -0.0600164   0.020987     0.00968663   0.161883     0.0268242   -0.0142016   -0.0331374    -0.0263379   -0.145251     0.0793107   -0.209188     0.022871    -0.00101534   0.00564353   0.14581     -0.0132157
 -0.0289857   -0.016986    -0.00155894   0.11141      0.0196456    0.0693103   -0.067765    -0.0258546   -0.0318458   -0.00458545  -0.092102    0.03721     -0.184163    -0.0721861   -0.0594802    0.190806     0.095102      0.0572139   -0.00284615   0.00522161  -0.0405675    0.0672216   -0.0505179    0.138608     0.137995     0.0500752
 -0.0367836    0.010547     0.0521002    0.08587      0.0647228   -0.0057166   -0.177662    -0.176347     0.14414      0.119946    -0.077718    0.0801142   -0.00678488  -0.0336255    0.130464    -0.11253      0.0779287     0.0860773   -0.119316     0.00904182   0.0620516   -0.0357303   -0.0460568   -0.0632252    0.0873246   -0.236523 
 -0.0527259   -0.0879978   -0.0578147    0.0137394   -0.100999     0.101672     0.0870932   -0.0916822    0.05319      0.00352765   0.0152     -0.163494    -0.114792    -0.0572693   -0.111638     0.0808422   -0.0971047    -0.154378    -0.137615    -0.104743    -0.151632     0.0356104    0.0069681    0.119361    -0.140549    -0.0757421
 -0.00100431  -0.00493333   0.116768    -0.00668526   0.0308387    0.0206994    0.0775753   -0.0703498   -0.102826     0.0396519    0.107219   -0.0247489   -0.0659239    0.125       -0.0789252    0.0466553   -0.00532141    0.0104716    0.0856713    0.0208467    0.0189634   -0.0684279    0.0434519    0.0361882   -0.00699838  -0.0122062
  0.0354233    0.237227     0.0352969   -0.0412328   -0.0708706   -0.0112595   -0.137441    -0.0328546   -0.0432256    0.0491881   -0.058689   -0.0518858    0.139713     0.0439251   -0.0011465   -0.0846343   -0.191377      0.0672345   -0.0419914   -0.0833967   -0.11131      0.114026    -0.0154602   -0.0327469    0.0280706   -0.123212 
 -0.126572     0.0341472   -0.0265395   -0.108319     0.0153673   -0.0915658    0.0857482   -0.117311    -0.0642       0.00636447  -0.0431665  -0.0848809   -0.106524    -0.0183997   -0.0278281   -0.0284878    0.048674     -0.0218087   -0.102753     0.0341577    0.0334637    0.12545     -0.084722     0.0337615    0.159979    -0.0429546
 -0.0641662    0.111722    -0.269065     0.208934     0.0525515   -0.00933124   0.0944165   -0.082944    -0.058752     0.0112174   -0.0357474   0.00942964  -0.0872872   -0.129234     0.0275991   -0.229701    -0.137562     -0.15692     -0.163808     0.0662037    0.0391156   -0.0977443    0.07151     -0.0243831   -0.0793203    0.176132 
 -0.00177529   0.132892    -0.0367891    0.164392    -0.43438      0.182792    -0.0181273   -0.0523796    0.0298569   -0.103628    -0.0711443  -0.01418      0.0163069   -0.0740455    0.00533966  -0.191228    -0.0973579     0.0551854   -0.0136251    0.0376915   -0.126112    -0.0767289   -0.0961887    0.0114507   -0.0642831   -0.0179524
 -0.183609    -0.0874693   -0.0441703    0.0197738    0.027767     0.0833074    0.149566    -0.0424512   -0.0405266    0.0609133    0.0223871  -0.0789953    0.07479      0.0856576    0.104704     0.0153482   -0.0931194    -0.0154012   -0.126423    -0.0949848   -0.0434984   -0.0914624    0.0405981    0.14448     -0.0264811   -0.0467101
 -0.133416     0.0148785   -0.018186     0.0530503    0.0128235   -0.0569388    0.0665697    0.106933     0.00948767  -0.0960813   -0.0530996  -0.0399227   -0.0262595   -0.113608     0.0858176   -0.0369641   -0.00107842    0.0962505   -0.0207787    0.0376778    0.0724082    0.11605     -0.199242    -0.087133    -0.0391629    0.0843394
  0.111713     0.00566627   0.194999     0.139622    -0.0836887    0.0449195   -0.19827      0.108541    -0.158077     0.0433882    0.120935   -0.0377775    0.0442332   -0.229869     0.0482583   -0.0255052    0.0633681    -0.0570927    0.0543576   -0.0464282    0.180287    -0.0133444    0.0105195    0.105458    -0.064534    -0.183403 
 -0.0711336   -0.109638    -0.112269    -0.00903175   0.0392856    0.22572      0.0517097   -0.0162767   -0.0094594    0.0784184    0.0364312   0.0444605   -0.0279547   -0.0809952    0.0366912   -0.238952     0.00520056    0.101247     0.0865776   -0.029487    -0.107453    -0.0745185   -0.0810179    0.0652535    0.0139046    0.074037 
 -0.00332387  -0.0212292   -0.0189442    0.207111    -0.0116035   -0.0227993   -0.0024919    0.0211216   -0.113637    -0.0303334   -0.118085    0.207506     0.0387452    0.024269     0.0822618   -0.168703     0.00826847    0.07527      0.103538    -0.0511467   -0.0584554   -0.00864744   0.0742788    0.00958815  -0.107656    -0.15696  
  0.0656263    0.0289051    0.183694    -0.0359205   -0.110778     0.107464     0.0526479   -0.0243629    0.0385439    0.0926967    0.0399899   0.00371132  -0.0813636    0.0591591   -0.005572     0.0486155    0.154333      0.105659    -0.00651161  -0.119548    -0.00433661   0.0791574   -0.0904206    0.137526     0.134426    -0.0545863
 -0.0955328    0.0881958    0.0139211   -0.0647251    0.0164974   -0.105929     0.0323643   -0.00199875   0.151401     0.0150906    0.0136958  -0.0968638    0.00441607  -0.0132674    0.0430641    0.109494    -0.0328827     0.0401748    0.0108112    0.0631088    0.00891626  -0.00238987   0.159481     0.138788    -0.0615647   -0.0207723
  0.0999149   -0.00194604   0.131245     0.0250539    0.0656429   -0.0414512   -0.00264575  -0.078818    -0.0312692    0.0590273    0.222839    0.0630829    0.0284894    0.151484     0.0319383   -0.0691414   -0.157097     -0.0912334    0.0585623   -0.0239265    0.0368819    0.133894     0.0127708    0.0457516    0.0531702   -0.127393 
  0.120452    -0.0472335    0.1087      -0.0831216   -0.178998     0.0726043   -0.179997    -0.0730219   -0.136448     0.0865009   -0.0116061   0.0426945    0.0663036   -0.140317     0.0528412   -0.0100114   -0.0466414    -0.146756     0.107303    -0.102757    -0.103904     0.0880651    0.0902271   -0.293986     0.0158285   -0.037165 
  0.0493449   -0.112468     0.106685    -0.0752193   -0.0706461    0.166183    -0.0526478   -0.0654324   -0.0784649    0.0963822    0.0623171   0.0612717   -0.0156307    0.10355     -0.0966769   -0.00255736   0.098511     -0.0536459   -0.0268448   -0.0374653   -0.0824368   -0.0129921    0.139749     0.081944    -0.0729284   -0.139068 
  0.0189306   -0.0630807    0.038549    -0.0148005   -0.106607     0.189537    -0.121156     0.126316    -0.0785848    0.146491    -0.0425249  -0.113486     0.271132    -0.0498784    0.0324243    0.0637028   -0.0506998     0.0199956   -0.0944118   -0.0911379   -0.143327     0.0315387   -0.0524068    0.0768244    0.00474214   0.0397447
 -0.0295554   -0.0793156   -0.0148236    0.153821    -0.10867     -0.0556962   -0.13084      0.115817    -0.0242323   -0.0248136   -0.159075   -0.212071     0.046929     0.132651    -0.0394975    0.0959168   -0.0659246     0.0386199   -0.0485258    0.249959    -0.0776106    0.0566646   -0.0325871    0.187475    -0.0451234    0.0235544
 -0.101883     0.0863171   -0.133972    -0.0814105    0.0373675    0.0466395    0.0621349   -0.251128    -0.139156    -0.0305301   -0.0629558  -0.118478    -0.146582    -0.106019    -0.106928     0.00178709   0.172708      0.022511    -0.0973985    0.17739     -0.0388244    0.0914426   -0.068984     0.0552728    0.131978     0.0175285
  0.00969737   0.0194291    0.150245    -0.0337872    0.0783277   -0.0654255    0.0510137   -0.137811    -0.096166     0.0274755    0.404277    0.0562888    0.0422212    0.0889765    0.0275152   -0.0170806   -0.224875     -0.063952     0.0489553   -0.0400346    0.0234303    0.0967316    0.021002     0.109801     0.0773328   -0.185969 
 -0.0214522   -0.0374869    0.15042     -0.105289    -0.0308357    0.103168     0.105182     0.0329746   -0.00618376  -0.0667804   -0.117649    0.0519018    0.0546443    0.0950939    0.0303971   -0.0025008    0.106311      0.00360716  -0.00614955   0.117437    -0.0218607   -0.139169     0.101459     0.0247604   -0.0708879    0.167413 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 12 23
INFO: iteration 1, average log likelihood -1.078772
WARNING: Variances had to be floored 1 6 9 12 17 19 23 30 31 32
INFO: iteration 2, average log likelihood -1.013758
WARNING: Variances had to be floored 6 7 8 12 15 19 22 23
INFO: iteration 3, average log likelihood -1.028934
WARNING: Variances had to be floored 1 6 9 12 23 30 31 32
INFO: iteration 4, average log likelihood -1.032471
WARNING: Variances had to be floored 6 7 12 15 17 19 23 30
INFO: iteration 5, average log likelihood -1.035281
WARNING: Variances had to be floored 1 6 9 12 19 22 23 25 31 32
INFO: iteration 6, average log likelihood -1.015123
WARNING: Variances had to be floored 6 7 8 12 19 23 30
INFO: iteration 7, average log likelihood -1.037092
WARNING: Variances had to be floored 1 6 9 12 15 17 19 23 31 32
INFO: iteration 8, average log likelihood -1.023227
WARNING: Variances had to be floored 6 7 12 19 22 23 30
INFO: iteration 9, average log likelihood -1.042033
WARNING: Variances had to be floored 1 6 8 9 12 15 19 23 31 32
INFO: iteration 10, average log likelihood -1.014733
INFO: EM with 100000 data points 10 iterations avll -1.014733
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.00208164   0.138883    -0.0331461   -0.150738    0.157393      0.00172134   0.0511841  -0.0742394     0.00355345   0.137521     0.117916    -0.19542      0.147143   -0.22577      -0.0503703     0.189883    -0.0367726   -0.0978249   -0.1701       0.0768642   -0.0379716   -0.141279    -0.137213     -0.098423      0.104497     0.00259369 
 -0.175672    -0.0736256    0.0323814    0.13692     0.0259321     0.0482843    0.164956   -0.0860768     0.060226    -0.149426     0.0827092   -0.087505    -0.105062   -0.236081      0.114872      0.0831117   -0.131045     0.0336067   -0.0682832    0.0251946    0.080692     0.0960966    0.0381265    -0.0558623    -0.0658692   -0.189076   
 -0.0199825   -0.085193    -0.0413398   -0.0505038  -0.171812     -0.147094    -0.0204798  -0.0361056    -0.00137889   0.0538022    0.0246603   -0.00654918   0.114917   -0.0959648    -0.0668667     0.111821     0.0666371   -0.0902885   -0.151151    -0.166697    -0.136809     0.071862    -0.112457      0.180076     -0.0738573    0.00450332 
 -0.024734     0.0835077   -0.0348706   -0.0784539  -0.00488489    0.118667     0.0130906   0.0701123     0.207155    -0.0146132    0.211248     0.0372914   -0.0785446  -0.0706974     0.180058      0.0833703   -0.0439547   -0.0692532    0.195755     0.139665     0.0554007   -0.067394    -0.072989     -0.00874148   -0.0809474   -0.0542813  
  0.0880363    0.0276703   -0.11335     -0.0661913   0.0297294    -0.0426329   -0.203197   -0.0230634     0.100479     0.213717     0.0276525   -0.0448156   -0.091976   -0.0114618     0.125819     -0.0528207    0.0670658    0.0261341    0.00461229  -0.172589     0.0689301   -0.0557877    0.0208113    -0.100619      0.144538     0.0142107  
  0.00480454  -0.26076      0.161727    -0.0101489  -0.0331977    -0.104409    -0.0855034  -0.0044909    -0.146458    -0.0384618   -0.0280028    0.0972256   -0.115016    0.22328       0.0484615    -0.10682     -0.0900809   -0.0123977   -0.0884551   -0.0393373    0.0337447    0.167593     0.0286879     0.0103813    -0.202157    -0.0119235  
  0.134536     0.0904436   -0.0951635   -0.072484    0.00964052    0.0668821   -0.039878    0.0968411    -0.0390458    0.0831077   -0.0896553    0.0322799   -0.0613715   0.00351993   -0.00659862    0.0935077   -0.141031     0.0943771    0.0665616    0.0520592   -0.206699     0.0271792    0.0615794     0.0820106     0.00630488   0.0125124  
  0.0704363   -0.0526332   -0.0291554    0.0310866  -0.135556      0.00970725   0.0034489   0.108173     -0.0174161    0.105857    -0.156721    -0.0364311   -0.122573    0.0900296    -0.0841866     0.0628779    0.085773    -0.0177429    0.0180769    0.0715093   -0.0439445   -0.0421125    0.0366623     0.022097      0.0180073    0.0991323  
  0.104598    -0.118396     0.0845599   -0.0553865  -0.250423      0.0772216    0.134857   -0.20223      -0.0118785    0.0766358    0.178042    -0.0669993    0.0515708  -0.0731666     0.0465769    -0.00221425  -0.0838858    0.05664      0.0486641    0.016638     0.0422795    0.0247117   -0.0635128     0.0704435    -0.125724     0.246614   
  0.153513     0.190637     0.0552684    0.103383   -0.0587735    -0.069162    -0.0296264  -0.133558      0.138372    -0.238353     0.130304     0.0829979    0.0160348   0.0479477     0.108022      0.0438839    0.0134088    0.0582723   -0.120437     0.0909362    0.0404587    0.08635     -0.12131       0.0922051    -0.133561     0.0917525  
  0.0136364   -0.0915758    0.00684689  -0.0675377   0.0357858     0.172578     0.0899974   0.0649322    -0.0571394    0.00463588  -0.00219495  -0.0182023    0.217357   -0.211738     -0.0693802     0.00932092   0.166348     0.084728    -0.0216658   -0.112499     0.0986074   -0.0545383   -0.0431328     0.0344265    -0.00524402   0.0325854  
 -0.0621281    0.137058     0.126841     0.22959     0.0111892     0.0303503    0.0330273   0.0700142    -0.0511154   -0.0448878   -0.0225866    0.104111    -0.0345696   0.134731      0.137196      0.0353747   -0.0545056   -0.155579     0.1079       0.0818145    0.0247178   -0.00622108   0.115042      0.0205391     0.0332828    0.0545494  
 -0.0768715   -0.0467737   -0.169449    -0.0899546  -0.0874128    -0.0489651   -0.0316656  -0.0487916     0.0621512    0.0431564   -0.0183454    0.0671548    0.0312918  -0.135543     -0.0453837     0.0659596   -0.098128    -0.04338      0.0235548   -0.0428497    0.0882688   -0.213572    -0.0397452     0.0728558     0.0409911    0.183575   
 -0.119796    -0.0603896   -0.0150656    0.0559559   0.0577595    -0.0241828    0.049194   -0.108629      0.0883973   -0.00753776   0.122055     0.107918    -0.0360247   0.0102998    -0.102078      0.142053     0.235828     0.00578769   0.075872     0.0925196    0.0120375   -0.0044558    0.126944     -0.250741      0.115386     0.0105892  
  0.108922    -0.0276219    0.126226     0.0581343   0.0818738     0.0287977   -0.109794   -0.0482293    -0.0145129    0.168262    -0.206923     0.034532     0.0455825  -0.127328      0.0190895     0.0496908   -0.0412955    0.0709332   -0.0772      -0.0542384   -0.0346029    0.146269     0.116705      0.0711981    -0.0513329    0.0304931  
  0.10395     -0.0641164    0.0751318   -0.15111     0.0985455    -0.0201132   -0.120746   -0.0985046    -0.0266937   -0.0407244    0.0148433    0.0723466    0.0166103  -0.0757335    -0.163678      0.16109      0.00178722  -0.014668     0.0433597   -0.0397188   -0.0427635    0.0369235   -0.215358     -0.0773579     0.011019    -0.10233    
  0.0530843    0.0577716    0.148386    -0.0166803   0.00950578    0.0846334   -0.121764    0.0257835     0.0541223    0.02723     -0.0989101    0.00214731  -0.0527732   0.0572381    -0.0395854    -0.0739644   -0.0158214   -0.0400197   -0.148767    -0.00794302  -0.0761376   -0.126058    -0.0926591    -0.0113476    -0.00772332   0.0302255  
  0.0703266   -0.124083    -0.024148    -0.147588   -0.0243247    -0.00297714   0.0118984   0.10539       0.0793655   -0.102748     0.0152767    0.058942     0.0666495   0.13506      -0.0194188     0.0618445    0.0144375   -0.0412181    0.242329    -0.0770443    0.0777482   -0.102055    -0.056934      0.239523      0.0224014   -0.0312263  
 -0.0435279    0.0613451   -0.0271428   -0.0188409   0.0829745    -0.139214     0.0471802   0.0232211     0.0966743   -0.0638214   -0.0790301    0.118461    -0.129863   -0.000299439  -0.0561221     0.0628878   -0.00847208  -0.145453    -0.0480593   -0.0386997   -0.0811139   -0.0565629   -0.0605216     0.000372878   0.0477212   -0.078894   
  0.197402     0.0247317    0.0785282    0.15597     0.103125     -0.128382     0.010206    0.0587107     0.0396871   -0.169378     0.0938703    0.0594466   -0.249351    0.0330816    -0.0517393    -0.00323767  -0.035271    -0.065749     0.0366569   -0.093532     0.0518809    0.10164     -0.14222       0.00431642   -0.0588761   -0.104819   
 -0.0853189    0.0403898   -0.0459295   -0.162526   -0.17334      -0.0968941   -0.0638486   0.0702624     0.0322118   -0.0226813    0.0521127   -0.135226    -0.0271331   0.097809      0.0717227    -0.169966    -0.0574887   -0.105862    -0.0755678    0.0786388    0.0512829    0.00265439   3.28996e-5    0.00541632    0.0238625   -0.0386511  
 -0.0269555   -0.0471116   -0.0892987    0.0534359   0.168034     -0.088189    -0.0190495   0.0916874     0.179181    -0.116357     0.0875929    0.0198313   -0.0544236   0.298134      0.047098     -0.177127     0.0313475   -0.0250883    0.0551363   -0.0257765    0.00960772  -0.0301485   -0.0580164     0.0605581     0.119897    -0.000701822
 -0.180961     0.125625    -0.0137962   -0.0480865   0.0739496     0.00538844  -0.0353771  -0.0580772    -0.0140805    0.0731795    0.0324237   -0.0388803   -0.0385615   0.0556709     0.000670661   0.044767    -0.0777858   -0.0357479    0.04167     -0.0687035   -0.159966    -0.132815    -0.000928822  -0.0591206    -0.131786    -0.0131804  
  0.023394     0.170048    -0.0733034   -0.05363    -0.0208927     0.0968954   -0.0448454  -0.021957     -0.017006     0.141894     0.00160231   0.00377851  -0.18953     0.0725622    -0.249973     -0.0196704    0.0813269   -0.0360624   -0.0492415   -0.00693189   0.102851     0.162659    -0.0545097     0.0664546    -0.0022096    0.0329127  
 -0.141543     0.238252     0.0985892   -0.103791    0.190032     -0.13786      0.111258    0.117647     -0.101805     0.176392     0.139047    -0.086088    -0.0686325   0.138856     -0.209631     -0.0565577   -0.010629    -0.00833068  -0.0131057   -0.0144241   -0.0890447   -0.0595529   -0.0385741     0.114805      0.125033    -0.0570167  
 -0.0158518   -0.210518     0.156458    -0.0281375  -0.0991091    -0.00339574   0.131462    0.0212079     0.0753479    0.0336051   -0.0231075    0.128021    -0.147923    0.145123      0.150354      0.0645427   -0.050582     0.0433757    0.0518979    0.180599     0.181669     0.0844317    0.119125      0.101093     -0.240489    -0.110626   
  0.0434415   -0.00443782   0.0803178   -0.0179765   0.000770042  -0.0101951    0.0117728   0.000455162  -0.0443525    0.122644    -0.120784    -0.076463     0.026409    0.201198      0.0489692     0.0868954    0.0103513   -0.0338464    0.0219999   -0.24226     -0.0356082    0.00883193  -0.0925546     0.0581663    -0.0293319    0.246122   
  0.0134604   -0.111665     0.0306082   -0.109354   -0.134094      0.0125598    0.0179784  -0.0137292     0.0503075   -0.100556     0.0793692   -0.017524    -0.0513837  -0.0876449    -0.161445     -0.173773     0.0293863   -0.120938     0.045782    -0.187265    -0.0150471   -0.0287527    0.209571     -0.0879532    -0.0496609    0.0058695  
 -0.0901368   -0.0313301    0.239051    -0.105675   -0.049946     -0.162786    -0.159906   -0.116574     -0.203642    -0.0271761   -0.140306     0.0814587    0.0432971   0.0191473    -0.161805     -0.0348727    0.283247    -0.213299     0.127276     0.12109      0.243255     0.0167941   -0.112796      0.147222     -0.0962533   -0.169545   
 -0.111443     0.0426112    0.0342113    0.244903   -0.0527473     0.126493     0.0465012  -0.103009     -0.016723     0.0784404    0.0321518    0.0912985   -0.0263959  -0.212588     -0.115643      0.132952     0.0919081    0.0674608   -0.15409     -0.0580758    0.0946395   -0.0865321   -0.0136317    -0.160429     -0.0507701    0.031641   
 -0.105094    -0.0233485   -0.0239188   -0.051761    0.00102245   -0.167009    -0.0625358   0.146138      0.0123171    0.0594157    0.0490308    0.0809278    0.108521    0.160568     -0.0252485     0.15818      0.193149     0.0736125    0.0444998   -0.0228725   -0.0713641    0.0995651   -0.0754433    -0.0844301    -0.0158831   -0.015834   
 -0.0654189    0.115624     0.137419    -0.024368    0.115405      0.0830483   -0.0155044  -0.090184     -0.00555869  -0.122453    -0.0168638    0.00624025   0.160476    0.161271      0.00192916    0.0475217    0.138089    -0.162512     0.157782    -0.016634     0.105703    -0.0371584    0.0312281     0.0993411     0.0310376   -0.0294615  kind full, method split
0: avll = -1.4154826432696632
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415503
INFO: iteration 2, average log likelihood -1.415405
INFO: iteration 3, average log likelihood -1.415323
INFO: iteration 4, average log likelihood -1.415228
INFO: iteration 5, average log likelihood -1.415115
INFO: iteration 6, average log likelihood -1.414984
INFO: iteration 7, average log likelihood -1.414839
INFO: iteration 8, average log likelihood -1.414668
INFO: iteration 9, average log likelihood -1.414435
INFO: iteration 10, average log likelihood -1.414065
INFO: iteration 11, average log likelihood -1.413463
INFO: iteration 12, average log likelihood -1.412585
INFO: iteration 13, average log likelihood -1.411568
INFO: iteration 14, average log likelihood -1.410727
INFO: iteration 15, average log likelihood -1.410232
INFO: iteration 16, average log likelihood -1.410002
INFO: iteration 17, average log likelihood -1.409907
INFO: iteration 18, average log likelihood -1.409868
INFO: iteration 19, average log likelihood -1.409852
INFO: iteration 20, average log likelihood -1.409846
INFO: iteration 21, average log likelihood -1.409843
INFO: iteration 22, average log likelihood -1.409841
INFO: iteration 23, average log likelihood -1.409840
INFO: iteration 24, average log likelihood -1.409839
INFO: iteration 25, average log likelihood -1.409839
INFO: iteration 26, average log likelihood -1.409838
INFO: iteration 27, average log likelihood -1.409838
INFO: iteration 28, average log likelihood -1.409837
INFO: iteration 29, average log likelihood -1.409837
INFO: iteration 30, average log likelihood -1.409837
INFO: iteration 31, average log likelihood -1.409837
INFO: iteration 32, average log likelihood -1.409836
INFO: iteration 33, average log likelihood -1.409836
INFO: iteration 34, average log likelihood -1.409836
INFO: iteration 35, average log likelihood -1.409836
INFO: iteration 36, average log likelihood -1.409836
INFO: iteration 37, average log likelihood -1.409835
INFO: iteration 38, average log likelihood -1.409835
INFO: iteration 39, average log likelihood -1.409835
INFO: iteration 40, average log likelihood -1.409835
INFO: iteration 41, average log likelihood -1.409835
INFO: iteration 42, average log likelihood -1.409835
INFO: iteration 43, average log likelihood -1.409835
INFO: iteration 44, average log likelihood -1.409835
INFO: iteration 45, average log likelihood -1.409835
INFO: iteration 46, average log likelihood -1.409835
INFO: iteration 47, average log likelihood -1.409835
INFO: iteration 48, average log likelihood -1.409835
INFO: iteration 49, average log likelihood -1.409835
INFO: iteration 50, average log likelihood -1.409834
INFO: EM with 100000 data points 50 iterations avll -1.409834
952.4 data points per parameter
1: avll = [-1.4155,-1.4154,-1.41532,-1.41523,-1.41511,-1.41498,-1.41484,-1.41467,-1.41443,-1.41406,-1.41346,-1.41259,-1.41157,-1.41073,-1.41023,-1.41,-1.40991,-1.40987,-1.40985,-1.40985,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.409854
INFO: iteration 2, average log likelihood -1.409753
INFO: iteration 3, average log likelihood -1.409668
INFO: iteration 4, average log likelihood -1.409568
INFO: iteration 5, average log likelihood -1.409452
INFO: iteration 6, average log likelihood -1.409329
INFO: iteration 7, average log likelihood -1.409213
INFO: iteration 8, average log likelihood -1.409118
INFO: iteration 9, average log likelihood -1.409049
INFO: iteration 10, average log likelihood -1.409002
INFO: iteration 11, average log likelihood -1.408971
INFO: iteration 12, average log likelihood -1.408949
INFO: iteration 13, average log likelihood -1.408934
INFO: iteration 14, average log likelihood -1.408923
INFO: iteration 15, average log likelihood -1.408915
INFO: iteration 16, average log likelihood -1.408908
INFO: iteration 17, average log likelihood -1.408902
INFO: iteration 18, average log likelihood -1.408897
INFO: iteration 19, average log likelihood -1.408893
INFO: iteration 20, average log likelihood -1.408888
INFO: iteration 21, average log likelihood -1.408884
INFO: iteration 22, average log likelihood -1.408880
INFO: iteration 23, average log likelihood -1.408876
INFO: iteration 24, average log likelihood -1.408871
INFO: iteration 25, average log likelihood -1.408867
INFO: iteration 26, average log likelihood -1.408863
INFO: iteration 27, average log likelihood -1.408858
INFO: iteration 28, average log likelihood -1.408854
INFO: iteration 29, average log likelihood -1.408849
INFO: iteration 30, average log likelihood -1.408844
INFO: iteration 31, average log likelihood -1.408840
INFO: iteration 32, average log likelihood -1.408835
INFO: iteration 33, average log likelihood -1.408831
INFO: iteration 34, average log likelihood -1.408826
INFO: iteration 35, average log likelihood -1.408822
INFO: iteration 36, average log likelihood -1.408818
INFO: iteration 37, average log likelihood -1.408814
INFO: iteration 38, average log likelihood -1.408810
INFO: iteration 39, average log likelihood -1.408806
INFO: iteration 40, average log likelihood -1.408802
INFO: iteration 41, average log likelihood -1.408799
INFO: iteration 42, average log likelihood -1.408796
INFO: iteration 43, average log likelihood -1.408793
INFO: iteration 44, average log likelihood -1.408790
INFO: iteration 45, average log likelihood -1.408787
INFO: iteration 46, average log likelihood -1.408784
INFO: iteration 47, average log likelihood -1.408782
INFO: iteration 48, average log likelihood -1.408780
INFO: iteration 49, average log likelihood -1.408778
INFO: iteration 50, average log likelihood -1.408776
INFO: EM with 100000 data points 50 iterations avll -1.408776
473.9 data points per parameter
2: avll = [-1.40985,-1.40975,-1.40967,-1.40957,-1.40945,-1.40933,-1.40921,-1.40912,-1.40905,-1.409,-1.40897,-1.40895,-1.40893,-1.40892,-1.40891,-1.40891,-1.4089,-1.4089,-1.40889,-1.40889,-1.40888,-1.40888,-1.40888,-1.40887,-1.40887,-1.40886,-1.40886,-1.40885,-1.40885,-1.40884,-1.40884,-1.40884,-1.40883,-1.40883,-1.40882,-1.40882,-1.40881,-1.40881,-1.40881,-1.4088,-1.4088,-1.4088,-1.40879,-1.40879,-1.40879,-1.40878,-1.40878,-1.40878,-1.40878,-1.40878]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408790
INFO: iteration 2, average log likelihood -1.408721
INFO: iteration 3, average log likelihood -1.408663
INFO: iteration 4, average log likelihood -1.408596
INFO: iteration 5, average log likelihood -1.408515
INFO: iteration 6, average log likelihood -1.408422
INFO: iteration 7, average log likelihood -1.408321
INFO: iteration 8, average log likelihood -1.408221
INFO: iteration 9, average log likelihood -1.408125
INFO: iteration 10, average log likelihood -1.408037
INFO: iteration 11, average log likelihood -1.407956
INFO: iteration 12, average log likelihood -1.407882
INFO: iteration 13, average log likelihood -1.407814
INFO: iteration 14, average log likelihood -1.407752
INFO: iteration 15, average log likelihood -1.407695
INFO: iteration 16, average log likelihood -1.407641
INFO: iteration 17, average log likelihood -1.407591
INFO: iteration 18, average log likelihood -1.407544
INFO: iteration 19, average log likelihood -1.407499
INFO: iteration 20, average log likelihood -1.407456
INFO: iteration 21, average log likelihood -1.407416
INFO: iteration 22, average log likelihood -1.407379
INFO: iteration 23, average log likelihood -1.407346
INFO: iteration 24, average log likelihood -1.407317
INFO: iteration 25, average log likelihood -1.407292
INFO: iteration 26, average log likelihood -1.407270
INFO: iteration 27, average log likelihood -1.407251
INFO: iteration 28, average log likelihood -1.407235
INFO: iteration 29, average log likelihood -1.407221
INFO: iteration 30, average log likelihood -1.407208
INFO: iteration 31, average log likelihood -1.407197
INFO: iteration 32, average log likelihood -1.407187
INFO: iteration 33, average log likelihood -1.407177
INFO: iteration 34, average log likelihood -1.407169
INFO: iteration 35, average log likelihood -1.407160
INFO: iteration 36, average log likelihood -1.407152
INFO: iteration 37, average log likelihood -1.407144
INFO: iteration 38, average log likelihood -1.407137
INFO: iteration 39, average log likelihood -1.407129
INFO: iteration 40, average log likelihood -1.407122
INFO: iteration 41, average log likelihood -1.407114
INFO: iteration 42, average log likelihood -1.407107
INFO: iteration 43, average log likelihood -1.407099
INFO: iteration 44, average log likelihood -1.407091
INFO: iteration 45, average log likelihood -1.407084
INFO: iteration 46, average log likelihood -1.407076
INFO: iteration 47, average log likelihood -1.407068
INFO: iteration 48, average log likelihood -1.407060
INFO: iteration 49, average log likelihood -1.407052
INFO: iteration 50, average log likelihood -1.407043
INFO: EM with 100000 data points 50 iterations avll -1.407043
236.4 data points per parameter
3: avll = [-1.40879,-1.40872,-1.40866,-1.4086,-1.40852,-1.40842,-1.40832,-1.40822,-1.40813,-1.40804,-1.40796,-1.40788,-1.40781,-1.40775,-1.40769,-1.40764,-1.40759,-1.40754,-1.4075,-1.40746,-1.40742,-1.40738,-1.40735,-1.40732,-1.40729,-1.40727,-1.40725,-1.40723,-1.40722,-1.40721,-1.4072,-1.40719,-1.40718,-1.40717,-1.40716,-1.40715,-1.40714,-1.40714,-1.40713,-1.40712,-1.40711,-1.40711,-1.4071,-1.40709,-1.40708,-1.40708,-1.40707,-1.40706,-1.40705,-1.40704]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407044
INFO: iteration 2, average log likelihood -1.406976
INFO: iteration 3, average log likelihood -1.406914
INFO: iteration 4, average log likelihood -1.406845
INFO: iteration 5, average log likelihood -1.406763
INFO: iteration 6, average log likelihood -1.406669
INFO: iteration 7, average log likelihood -1.406563
INFO: iteration 8, average log likelihood -1.406452
INFO: iteration 9, average log likelihood -1.406342
INFO: iteration 10, average log likelihood -1.406236
INFO: iteration 11, average log likelihood -1.406138
INFO: iteration 12, average log likelihood -1.406049
INFO: iteration 13, average log likelihood -1.405967
INFO: iteration 14, average log likelihood -1.405894
INFO: iteration 15, average log likelihood -1.405829
INFO: iteration 16, average log likelihood -1.405772
INFO: iteration 17, average log likelihood -1.405721
INFO: iteration 18, average log likelihood -1.405677
INFO: iteration 19, average log likelihood -1.405639
INFO: iteration 20, average log likelihood -1.405606
INFO: iteration 21, average log likelihood -1.405577
INFO: iteration 22, average log likelihood -1.405551
INFO: iteration 23, average log likelihood -1.405529
INFO: iteration 24, average log likelihood -1.405508
INFO: iteration 25, average log likelihood -1.405490
INFO: iteration 26, average log likelihood -1.405473
INFO: iteration 27, average log likelihood -1.405458
INFO: iteration 28, average log likelihood -1.405443
INFO: iteration 29, average log likelihood -1.405430
INFO: iteration 30, average log likelihood -1.405417
INFO: iteration 31, average log likelihood -1.405406
INFO: iteration 32, average log likelihood -1.405394
INFO: iteration 33, average log likelihood -1.405383
INFO: iteration 34, average log likelihood -1.405373
INFO: iteration 35, average log likelihood -1.405363
INFO: iteration 36, average log likelihood -1.405353
INFO: iteration 37, average log likelihood -1.405344
INFO: iteration 38, average log likelihood -1.405334
INFO: iteration 39, average log likelihood -1.405325
INFO: iteration 40, average log likelihood -1.405316
INFO: iteration 41, average log likelihood -1.405307
INFO: iteration 42, average log likelihood -1.405298
INFO: iteration 43, average log likelihood -1.405290
INFO: iteration 44, average log likelihood -1.405281
INFO: iteration 45, average log likelihood -1.405272
INFO: iteration 46, average log likelihood -1.405263
INFO: iteration 47, average log likelihood -1.405255
INFO: iteration 48, average log likelihood -1.405246
INFO: iteration 49, average log likelihood -1.405237
INFO: iteration 50, average log likelihood -1.405228
INFO: EM with 100000 data points 50 iterations avll -1.405228
118.1 data points per parameter
4: avll = [-1.40704,-1.40698,-1.40691,-1.40684,-1.40676,-1.40667,-1.40656,-1.40645,-1.40634,-1.40624,-1.40614,-1.40605,-1.40597,-1.40589,-1.40583,-1.40577,-1.40572,-1.40568,-1.40564,-1.40561,-1.40558,-1.40555,-1.40553,-1.40551,-1.40549,-1.40547,-1.40546,-1.40544,-1.40543,-1.40542,-1.40541,-1.40539,-1.40538,-1.40537,-1.40536,-1.40535,-1.40534,-1.40533,-1.40533,-1.40532,-1.40531,-1.4053,-1.40529,-1.40528,-1.40527,-1.40526,-1.40525,-1.40525,-1.40524,-1.40523]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.405228
INFO: iteration 2, average log likelihood -1.405165
INFO: iteration 3, average log likelihood -1.405104
INFO: iteration 4, average log likelihood -1.405034
INFO: iteration 5, average log likelihood -1.404946
INFO: iteration 6, average log likelihood -1.404836
INFO: iteration 7, average log likelihood -1.404701
INFO: iteration 8, average log likelihood -1.404544
INFO: iteration 9, average log likelihood -1.404372
INFO: iteration 10, average log likelihood -1.404197
INFO: iteration 11, average log likelihood -1.404027
INFO: iteration 12, average log likelihood -1.403871
INFO: iteration 13, average log likelihood -1.403734
INFO: iteration 14, average log likelihood -1.403615
INFO: iteration 15, average log likelihood -1.403514
INFO: iteration 16, average log likelihood -1.403428
INFO: iteration 17, average log likelihood -1.403355
INFO: iteration 18, average log likelihood -1.403292
INFO: iteration 19, average log likelihood -1.403237
INFO: iteration 20, average log likelihood -1.403189
INFO: iteration 21, average log likelihood -1.403146
INFO: iteration 22, average log likelihood -1.403107
INFO: iteration 23, average log likelihood -1.403072
INFO: iteration 24, average log likelihood -1.403039
INFO: iteration 25, average log likelihood -1.403008
INFO: iteration 26, average log likelihood -1.402979
INFO: iteration 27, average log likelihood -1.402951
INFO: iteration 28, average log likelihood -1.402925
INFO: iteration 29, average log likelihood -1.402900
INFO: iteration 30, average log likelihood -1.402876
INFO: iteration 31, average log likelihood -1.402853
INFO: iteration 32, average log likelihood -1.402830
INFO: iteration 33, average log likelihood -1.402809
INFO: iteration 34, average log likelihood -1.402788
INFO: iteration 35, average log likelihood -1.402767
INFO: iteration 36, average log likelihood -1.402748
INFO: iteration 37, average log likelihood -1.402728
INFO: iteration 38, average log likelihood -1.402710
INFO: iteration 39, average log likelihood -1.402691
INFO: iteration 40, average log likelihood -1.402673
INFO: iteration 41, average log likelihood -1.402656
INFO: iteration 42, average log likelihood -1.402638
INFO: iteration 43, average log likelihood -1.402621
INFO: iteration 44, average log likelihood -1.402605
INFO: iteration 45, average log likelihood -1.402588
INFO: iteration 46, average log likelihood -1.402572
INFO: iteration 47, average log likelihood -1.402556
INFO: iteration 48, average log likelihood -1.402541
INFO: iteration 49, average log likelihood -1.402526
INFO: iteration 50, average log likelihood -1.402511
INFO: EM with 100000 data points 50 iterations avll -1.402511
59.0 data points per parameter
5: avll = [-1.40523,-1.40516,-1.4051,-1.40503,-1.40495,-1.40484,-1.4047,-1.40454,-1.40437,-1.4042,-1.40403,-1.40387,-1.40373,-1.40361,-1.40351,-1.40343,-1.40335,-1.40329,-1.40324,-1.40319,-1.40315,-1.40311,-1.40307,-1.40304,-1.40301,-1.40298,-1.40295,-1.40292,-1.4029,-1.40288,-1.40285,-1.40283,-1.40281,-1.40279,-1.40277,-1.40275,-1.40273,-1.40271,-1.40269,-1.40267,-1.40266,-1.40264,-1.40262,-1.4026,-1.40259,-1.40257,-1.40256,-1.40254,-1.40253,-1.40251]
[-1.41548,-1.4155,-1.4154,-1.41532,-1.41523,-1.41511,-1.41498,-1.41484,-1.41467,-1.41443,-1.41406,-1.41346,-1.41259,-1.41157,-1.41073,-1.41023,-1.41,-1.40991,-1.40987,-1.40985,-1.40985,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40984,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40983,-1.40985,-1.40975,-1.40967,-1.40957,-1.40945,-1.40933,-1.40921,-1.40912,-1.40905,-1.409,-1.40897,-1.40895,-1.40893,-1.40892,-1.40891,-1.40891,-1.4089,-1.4089,-1.40889,-1.40889,-1.40888,-1.40888,-1.40888,-1.40887,-1.40887,-1.40886,-1.40886,-1.40885,-1.40885,-1.40884,-1.40884,-1.40884,-1.40883,-1.40883,-1.40882,-1.40882,-1.40881,-1.40881,-1.40881,-1.4088,-1.4088,-1.4088,-1.40879,-1.40879,-1.40879,-1.40878,-1.40878,-1.40878,-1.40878,-1.40878,-1.40879,-1.40872,-1.40866,-1.4086,-1.40852,-1.40842,-1.40832,-1.40822,-1.40813,-1.40804,-1.40796,-1.40788,-1.40781,-1.40775,-1.40769,-1.40764,-1.40759,-1.40754,-1.4075,-1.40746,-1.40742,-1.40738,-1.40735,-1.40732,-1.40729,-1.40727,-1.40725,-1.40723,-1.40722,-1.40721,-1.4072,-1.40719,-1.40718,-1.40717,-1.40716,-1.40715,-1.40714,-1.40714,-1.40713,-1.40712,-1.40711,-1.40711,-1.4071,-1.40709,-1.40708,-1.40708,-1.40707,-1.40706,-1.40705,-1.40704,-1.40704,-1.40698,-1.40691,-1.40684,-1.40676,-1.40667,-1.40656,-1.40645,-1.40634,-1.40624,-1.40614,-1.40605,-1.40597,-1.40589,-1.40583,-1.40577,-1.40572,-1.40568,-1.40564,-1.40561,-1.40558,-1.40555,-1.40553,-1.40551,-1.40549,-1.40547,-1.40546,-1.40544,-1.40543,-1.40542,-1.40541,-1.40539,-1.40538,-1.40537,-1.40536,-1.40535,-1.40534,-1.40533,-1.40533,-1.40532,-1.40531,-1.4053,-1.40529,-1.40528,-1.40527,-1.40526,-1.40525,-1.40525,-1.40524,-1.40523,-1.40523,-1.40516,-1.4051,-1.40503,-1.40495,-1.40484,-1.4047,-1.40454,-1.40437,-1.4042,-1.40403,-1.40387,-1.40373,-1.40361,-1.40351,-1.40343,-1.40335,-1.40329,-1.40324,-1.40319,-1.40315,-1.40311,-1.40307,-1.40304,-1.40301,-1.40298,-1.40295,-1.40292,-1.4029,-1.40288,-1.40285,-1.40283,-1.40281,-1.40279,-1.40277,-1.40275,-1.40273,-1.40271,-1.40269,-1.40267,-1.40266,-1.40264,-1.40262,-1.4026,-1.40259,-1.40257,-1.40256,-1.40254,-1.40253,-1.40251]
32×26 Array{Float64,2}:
 -0.167255     0.0411563    0.393947   -0.582276   -0.181712   -0.548491    -0.175796    -0.216005     0.343493    0.609529   -0.30715      0.153301    -0.134367    -0.157504     0.366613    0.265514      0.472022     0.0141483   0.427244   -0.0833351   -0.0712036  -0.163129    0.074651    -0.242638     -0.343431     0.246668  
  0.0637525    0.380601     0.0597683  -0.0715593   0.267097    0.0689495   -0.33739      0.838178     0.495539    0.503722   -0.179328     0.0854548    0.0899893   -0.110753     0.38988     0.335606      0.163764     0.0914908  -0.0195966  -0.0196332   -0.0143304  -0.354616    0.00407663  -0.178123     -0.301132     0.172954  
  0.5513      -0.102639     0.519526    0.43785    -0.314493    0.415011     0.0834209    0.158746     0.36912     0.133892    0.040532     0.153807    -0.173228     0.409384    -0.831816    0.563505     -0.114534     0.0318725   0.0581401  -0.267617     0.307594    0.102416    0.0688373   -0.167211     -0.00306711   0.337436  
  0.4375      -0.182574     0.171043    0.27482     0.574081    0.171907    -0.170092     0.465793     0.0836051   0.0187496  -0.0585564    0.170422    -0.450135    -0.313271    -0.168556    0.606297      0.317294     0.21976    -0.179778   -0.308558     0.100757    0.576205   -0.157678    -0.0605902    -0.52947     -0.162966  
 -0.223378     0.163364    -0.168934   -0.181404    0.0194844   0.029867    -0.595376    -0.337282     0.448316   -0.370022    0.180479    -0.0492474   -0.495892     0.0398929    0.0880695  -0.0975071    -0.0626525   -0.308033    0.188318    0.41019     -0.135434    0.395874   -0.163493    -0.178018      0.760032    -0.0981833 
 -0.384589    -0.163198    -0.248836    0.0883017   0.439943    0.336062    -0.183439    -0.311542     0.135451   -0.0387168  -0.151807    -0.213703    -0.31197      0.0278285   -0.360882    0.0748287    -0.494898     0.74106    -0.0668072   0.508183     0.0377996   0.0470814  -0.49273     -0.431853      0.657626    -0.918392  
  0.189467     0.24314     -0.233071   -0.769175    0.107265    0.546135    -0.0630479    0.533283     0.527926    0.647412    0.0193957   -0.0992659    0.0883033   -0.462019    -1.13437     0.491461      0.0774416   -0.0527736   0.453681    0.181251    -0.463186   -0.0371677  -0.302746     0.402218     -0.0239256   -0.159802  
  0.255248     0.189185     0.10446    -0.200321    0.134837    0.0189361   -0.231803    -0.321292    -0.112114    0.331392   -0.0282958   -0.235907     0.0527376   -0.40246     -0.721502   -0.0392837    -0.390997    -0.0995862   0.562912    0.51478      0.149192    0.81135     0.32451      0.0761207    -0.456238    -0.734354  
  0.228341    -0.07677     -0.533031    0.0191138   0.0315084   0.0231651    0.0632416    0.0654165   -0.0631115  -0.192494   -0.674153     0.299059    -0.403576     0.501917     0.489999    0.0770506     0.133537     0.0167119  -0.714085   -0.0245244   -0.259349   -0.360581   -0.396452    -0.0443594     0.201505     0.488605  
 -0.476759    -0.193866     0.151917    0.437002   -0.0274881  -0.158091     0.36547     -0.800754     0.378946   -0.0953046  -0.00106905   0.0682566   -0.531765    -0.20267      0.253404    0.0423886    -0.00394765   0.0577624  -0.48401    -0.544344    -0.300228   -0.744162    0.600046     0.23221       0.201095    -0.0643515 
 -0.0614373   -0.298046     0.0314996  -0.768552   -0.431785   -0.384054     0.142203    -0.335069    -0.196979   -0.223765    0.472499     0.435779     0.255676     0.132685    -0.0427482  -0.427583     -0.363164     0.178291    0.214752   -0.0797645   -0.11675    -0.787124   -0.082282    -0.591461      0.86929      0.120636  
 -0.329962     0.336485     0.16717    -0.0577208   0.307683   -0.0690297    0.0912986   -0.0111485    0.0841718   0.100228   -0.0810205    0.46674     -0.231216     0.372451    -0.324769   -0.0788463    -0.329699    -0.324421    0.56755    -0.521119    -0.405829   -0.151363    0.0598355   -0.498096      0.346352    -0.191535  
  0.00412858   0.00924883   0.0449663  -0.0827138  -0.0151331   0.147904    -0.107007    -0.0779225    0.149774    0.0471654   0.0670316    0.106891     0.00465979  -0.00487452  -0.306052    0.10651       0.037271    -0.0357882   0.158138   -0.0080348    0.0564814   0.0142485   0.0159658   -0.000659327   0.111593    -0.0340599 
 -0.104912     0.262979     0.13135     0.178976    0.0354834  -0.189795     0.158192     0.226165    -0.0191951  -0.0378242   0.0641832   -0.135708     0.150941     0.0234961    0.389674   -0.15532      -0.0578045   -0.126095   -0.134624   -0.0653424    0.0351755  -0.0541882  -0.0897162   -0.0914551    -0.145803     0.0826245 
  0.0638004   -0.362957    -0.425516   -0.322443   -0.0786735  -0.101276     0.153804    -0.211256    -0.352327   -0.184342   -0.115038     0.163611    -0.16329      0.0828052    0.0557455  -0.460959     -0.250913    -0.0486532  -0.178214    0.111401    -0.172235    0.0605329   0.0618508    0.210407      0.00934002  -0.0797984 
 -0.0829261   -0.488805     0.237678    0.252272   -0.156515   -0.202459    -0.00420499  -0.160242    -0.387015   -0.0809948   0.145882    -0.0935115    0.0401165    0.198769     0.0328501   0.378388      0.335566     0.0944337  -0.0286116   0.138494     0.055773   -0.127353   -0.0604226    0.58555      -0.0643984   -0.202815  
  0.231771     0.323605     0.211736    0.0339875   0.125616    0.427694    -0.250674     0.378741     0.357143    0.241757    0.0773107    0.199296    -0.206621     0.0663549   -0.497333    0.705146      0.143438    -0.0130397   0.0839055   0.0281326    0.177314    0.0328743   0.0497932   -0.181879      0.0576587   -0.155296  
  0.143908    -0.017813     0.452731    0.171356   -0.071267   -1.0691      -0.0531063   -0.0728871    0.121275    0.09824    -0.127233     0.4861      -0.403481    -0.228542     0.641664    0.035926     -0.205049    -0.122184    0.390711    0.151493    -0.0935541  -0.167946    0.417545    -0.734956     -0.181913     0.22433   
  0.409366    -0.147104     0.255725    0.0562334  -0.440068   -0.157147    -0.190297     0.782076    -0.317209    0.201786   -0.412408     0.463841     0.387641     0.683112    -0.18397    -0.379948     -0.362781     0.154141    0.117911    0.774654     0.368706    0.384914   -0.113252    -0.278172     -0.646941     0.107562  
  0.210454     0.403432     0.93814     0.573752    0.200205   -0.258886    -0.0580099   -0.121474    -1.05321     0.0324138  -0.136432     0.470897    -0.219473     0.468938     0.4992      0.400893     -0.49214     -0.609918    0.0686702   0.231991    -0.252646    0.226908    0.652224     0.499996     -0.257294    -0.341297  
 -0.0321242   -0.43689     -0.538164   -0.264668   -0.193764    0.460492    -0.094855    -0.694185    -0.145941    0.496084   -0.680167     0.00447494  -0.466636    -0.139825    -0.841169    0.187367      0.193142     0.242121    0.0917555  -0.56952      0.125312    0.44585     0.0298571    0.0870157     0.00948676   0.865636  
 -0.200992    -0.20937     -0.458648    0.246703   -0.172851    0.634217     0.0281112   -0.00582176  -0.398304   -0.133879    0.162515     0.327164    -0.178685    -0.122208    -1.35648    -0.342844     -0.327899    -0.104431   -0.36679     0.00356672   0.188396    0.189628   -0.106468     0.806535      0.490458    -0.279349  
 -0.371181     0.0255099   -0.44251     0.168653    0.0511291  -0.195472     0.144398    -0.0414743   -0.0554065  -0.321802    0.0795996   -0.312327     0.0530096    0.426346     0.663713   -0.324523      0.0811376   -0.78132    -0.0634682  -0.394947     0.190692   -0.0331358  -0.312394     0.139616     -0.00249519   0.531465  
 -0.322875    -0.502584    -0.231721   -0.157129    0.0639488   0.0104861   -0.0244888   -0.325333    -0.250798    0.10439    -0.250204    -0.072677    -0.20558     -0.0271984   -0.097389   -0.207445      0.0134429    0.573894   -0.46705    -0.115579    -0.605718    0.122869   -0.213556     0.477792      0.0702809   -0.110074  
 -0.147696     0.796714     0.222943    0.0841381   0.380699    0.269406     0.444085    -0.58459      0.118289   -0.0241615   0.509122    -0.458228     0.260057    -1.14965      0.102097   -0.000306937   0.446037    -0.190067    0.40265    -0.289609     0.583255   -0.0899203  -0.504696    -0.178159      0.316831    -0.15368   
 -0.533015     0.222664     0.439543    0.0450922  -0.176349    0.428714    -0.097528     0.138648     0.270695   -0.264061    0.810868    -0.595283     0.683982     0.146424    -0.403637    0.0376157     0.288893    -0.3943      0.349242   -0.151012     0.255165    0.144063    0.0086493    0.20433       0.0773103   -0.353976  
  0.0124398    0.282206     0.229517    0.556      -0.0987466  -0.290068    -0.200707    -0.0805552   -0.105365   -0.646961    0.655275    -0.0978558    0.247516     0.632022     0.506114   -0.290034     -0.0063905    0.0831648  -0.135718    0.0565591    0.183282    0.172479    0.197009    -0.821247     -0.0538394   -0.397514  
 -0.493168     0.220895    -0.0361481   0.102893    0.196394   -0.649368     0.323773     0.273288    -0.297248   -0.48835     0.70177     -0.0903513    0.599534    -0.0320781    0.912911   -0.661353     -0.0459158   -0.0887209   0.0534247   0.189778    -0.189273   -0.14778    -0.473483     0.52184       0.0380499   -0.271582  
  0.233534    -0.0472331    0.16314     0.0536336   0.0983899   0.00955747   0.736502     0.388992    -0.410153    0.131067   -0.18868     -0.0701783    0.24982      0.0988636    0.102455    0.232534      0.111776    -0.206105   -0.325838   -0.906004    -0.148673   -0.0441448   0.12654      0.235384     -1.14953      0.337867  
  0.666969    -0.402284     0.219279   -0.228584   -0.294992   -0.125328     0.498408    -0.256794    -0.346891   -0.011246   -0.208719    -0.0745206    0.882533    -0.362702     0.142      -0.23693       0.0868507    0.196699    0.153361   -0.354574     0.477657   -0.59829     0.413103     0.248499     -0.570152     0.571219  
  0.162291    -0.388639    -0.51865    -0.272062   -0.403026    0.184476    -0.0499696    0.019623     0.288443   -0.251692    0.462461    -0.677889     0.367578    -0.175676     0.334121   -0.0728707     0.482875    -0.0190586  -0.0908835   0.152197     0.310323    0.441426   -0.0192154   -0.0794098    -0.52537      0.407858  
  0.0376511    0.163134    -0.208227   -0.344581   -0.0565659  -0.100773    -0.122659     0.0592813   -0.0697129   0.0400352  -0.308327    -0.428451     0.0242918   -0.610965     0.258434   -0.089602      0.231861    -0.133913   -0.326668    0.823375     0.314051   -0.0612274  -0.240525     0.915027      0.0865774    0.00896207INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402496
INFO: iteration 2, average log likelihood -1.402482
INFO: iteration 3, average log likelihood -1.402468
INFO: iteration 4, average log likelihood -1.402454
INFO: iteration 5, average log likelihood -1.402440
INFO: iteration 6, average log likelihood -1.402427
INFO: iteration 7, average log likelihood -1.402414
INFO: iteration 8, average log likelihood -1.402401
INFO: iteration 9, average log likelihood -1.402389
INFO: iteration 10, average log likelihood -1.402377
INFO: EM with 100000 data points 10 iterations avll -1.402377
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.454796e+05
      1       6.964111e+05      -2.490685e+05 |       32
      2       6.816001e+05      -1.481099e+04 |       32
      3       6.764417e+05      -5.158405e+03 |       32
      4       6.738750e+05      -2.566749e+03 |       32
      5       6.722557e+05      -1.619311e+03 |       32
      6       6.710045e+05      -1.251176e+03 |       32
      7       6.700381e+05      -9.663737e+02 |       32
      8       6.692828e+05      -7.553126e+02 |       32
      9       6.686930e+05      -5.898052e+02 |       32
     10       6.682156e+05      -4.773378e+02 |       32
     11       6.678204e+05      -3.952174e+02 |       32
     12       6.674738e+05      -3.466671e+02 |       32
     13       6.671967e+05      -2.770518e+02 |       32
     14       6.669594e+05      -2.372893e+02 |       32
     15       6.667495e+05      -2.099306e+02 |       32
     16       6.665630e+05      -1.864829e+02 |       32
     17       6.663997e+05      -1.632844e+02 |       32
     18       6.662526e+05      -1.470766e+02 |       32
     19       6.661250e+05      -1.276638e+02 |       32
     20       6.660004e+05      -1.245732e+02 |       32
     21       6.658850e+05      -1.153684e+02 |       32
     22       6.657866e+05      -9.846562e+01 |       32
     23       6.657072e+05      -7.938052e+01 |       32
     24       6.656428e+05      -6.439561e+01 |       32
     25       6.655831e+05      -5.972963e+01 |       32
     26       6.655209e+05      -6.216305e+01 |       32
     27       6.654575e+05      -6.341971e+01 |       32
     28       6.653968e+05      -6.065789e+01 |       32
     29       6.653348e+05      -6.206344e+01 |       32
     30       6.652739e+05      -6.091507e+01 |       32
     31       6.652220e+05      -5.188314e+01 |       32
     32       6.651717e+05      -5.028786e+01 |       32
     33       6.651222e+05      -4.951784e+01 |       32
     34       6.650812e+05      -4.100021e+01 |       32
     35       6.650472e+05      -3.397277e+01 |       32
     36       6.650135e+05      -3.373055e+01 |       32
     37       6.649848e+05      -2.864377e+01 |       32
     38       6.649537e+05      -3.113427e+01 |       32
     39       6.649219e+05      -3.178216e+01 |       32
     40       6.648916e+05      -3.029546e+01 |       32
     41       6.648649e+05      -2.674224e+01 |       32
     42       6.648391e+05      -2.577337e+01 |       32
     43       6.648124e+05      -2.672460e+01 |       32
     44       6.647861e+05      -2.623676e+01 |       32
     45       6.647620e+05      -2.408475e+01 |       32
     46       6.647365e+05      -2.550826e+01 |       32
     47       6.647121e+05      -2.444305e+01 |       32
     48       6.646846e+05      -2.750364e+01 |       32
     49       6.646594e+05      -2.520073e+01 |       32
     50       6.646418e+05      -1.755159e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 664641.8360097636)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414464
INFO: iteration 2, average log likelihood -1.409453
INFO: iteration 3, average log likelihood -1.408044
INFO: iteration 4, average log likelihood -1.406936
INFO: iteration 5, average log likelihood -1.405754
INFO: iteration 6, average log likelihood -1.404697
INFO: iteration 7, average log likelihood -1.404010
INFO: iteration 8, average log likelihood -1.403641
INFO: iteration 9, average log likelihood -1.403436
INFO: iteration 10, average log likelihood -1.403302
INFO: iteration 11, average log likelihood -1.403201
INFO: iteration 12, average log likelihood -1.403119
INFO: iteration 13, average log likelihood -1.403049
INFO: iteration 14, average log likelihood -1.402988
INFO: iteration 15, average log likelihood -1.402933
INFO: iteration 16, average log likelihood -1.402884
INFO: iteration 17, average log likelihood -1.402838
INFO: iteration 18, average log likelihood -1.402797
INFO: iteration 19, average log likelihood -1.402759
INFO: iteration 20, average log likelihood -1.402723
INFO: iteration 21, average log likelihood -1.402690
INFO: iteration 22, average log likelihood -1.402658
INFO: iteration 23, average log likelihood -1.402629
INFO: iteration 24, average log likelihood -1.402601
INFO: iteration 25, average log likelihood -1.402575
INFO: iteration 26, average log likelihood -1.402550
INFO: iteration 27, average log likelihood -1.402527
INFO: iteration 28, average log likelihood -1.402505
INFO: iteration 29, average log likelihood -1.402484
INFO: iteration 30, average log likelihood -1.402464
INFO: iteration 31, average log likelihood -1.402445
INFO: iteration 32, average log likelihood -1.402428
INFO: iteration 33, average log likelihood -1.402411
INFO: iteration 34, average log likelihood -1.402394
INFO: iteration 35, average log likelihood -1.402379
INFO: iteration 36, average log likelihood -1.402364
INFO: iteration 37, average log likelihood -1.402350
INFO: iteration 38, average log likelihood -1.402337
INFO: iteration 39, average log likelihood -1.402324
INFO: iteration 40, average log likelihood -1.402311
INFO: iteration 41, average log likelihood -1.402299
INFO: iteration 42, average log likelihood -1.402287
INFO: iteration 43, average log likelihood -1.402276
INFO: iteration 44, average log likelihood -1.402265
INFO: iteration 45, average log likelihood -1.402255
INFO: iteration 46, average log likelihood -1.402245
INFO: iteration 47, average log likelihood -1.402235
INFO: iteration 48, average log likelihood -1.402226
INFO: iteration 49, average log likelihood -1.402217
INFO: iteration 50, average log likelihood -1.402208
INFO: EM with 100000 data points 50 iterations avll -1.402208
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.175165    0.258953    -0.465517   -0.0675866   0.0254215    0.192898     0.0924371   -0.344872     0.789836   -0.546823     0.549616   -0.482315    -0.41128    -0.617659     0.448434    0.198368     0.00848274  -0.109423   -0.00471842  -0.161459     0.124053    -0.100982   -0.320488    -0.0760067    0.693276    0.166611 
 -0.172771    0.169065     0.137043   -0.0118621   0.0728234   -0.00389981  -0.149944    -0.0286068    0.173706    0.133433     0.0380749   0.119749    -0.264198    0.0733501   -0.0551577   0.213784     0.0319214   -0.050382    0.0342659   -0.11583     -0.113117    -0.0474177  -0.0492763   -0.152631     0.113489   -0.111057 
  0.322628   -0.00628044   0.111971    0.0440042   0.205915     0.218027    -0.0404228    0.333611     0.180108    0.131483     0.0327192   0.0392186   -0.124765   -0.0382962   -0.269616    0.376594     0.0636108   -0.0180548   0.0438938   -0.189191     0.0989913    0.283469   -0.137294    -0.00784215  -0.321353    0.0588894
 -0.341221   -0.0460872    0.308749   -0.201984   -0.0858036   -0.166235    -0.304       -0.0615475    0.1945      0.141187     0.0104373   0.153729    -0.0785031   0.176233     0.133318    0.348614     0.357787     0.0463047   0.231232    -0.202786    -0.078415    -0.356349   -0.0833895   -0.290142     0.246895    0.0644037
  0.477557   -0.195166     0.471886    0.511576   -0.243866     0.548003     0.00377199   0.140652     0.508778    0.0822627   -0.358921    0.227028    -0.629613    0.565455    -0.856741    0.797344    -0.12898     -0.0414362  -0.0793241   -0.240757     0.210777     0.135311    0.0280541   -0.242082     0.183155    0.418321 
  0.225394    0.395827     0.942099    0.561232    0.255902    -0.200245    -0.0696852   -0.131651    -1.03026     0.0250755   -0.160194    0.440266    -0.2499      0.410785     0.421775    0.475956    -0.482074    -0.653448    0.0834893    0.140936    -0.222723     0.247529    0.648829     0.47448     -0.284176   -0.329132 
  0.184766   -0.252589     0.0918996  -0.12058    -0.384814    -0.102067     0.0512022   -0.00213703  -0.141544    0.0733137    0.0520798  -0.426673     0.435892   -0.0847032    0.137446    0.158936     0.368219     0.0136051  -0.114198     0.205479     0.407312    -0.118498    0.00914474   0.45696     -0.460181    0.192301 
 -0.0968967  -0.3707      -0.452729    0.0829282  -0.200424     0.669089     0.009773    -0.113891    -0.262403   -0.0623588    0.141797    0.0295959   -0.103631   -0.026874    -1.26513    -0.228499    -0.269918     0.0692444  -0.389932     0.00274064   0.174795     0.28979    -0.235065     0.835153     0.398174   -0.131275 
  0.505538   -0.227181     0.473717    0.0388412  -0.0263084   -0.584861     0.135655     0.0884137    0.497276    0.33848      0.0840405  -0.00527283   0.167521   -0.00301973   0.789558    0.322715     0.225135    -0.0617824   0.363939    -0.373192     0.138569    -0.279814    0.239025    -0.465479    -0.85256     0.68932  
 -0.296585    0.0939116   -0.0596491  -0.387108    0.41476     -0.0958738    0.509569     0.166303    -0.0315778   0.80771     -0.0524093   0.181789    -0.227204   -0.706961    -0.504294    0.476335     0.271332    -0.148677    0.347404    -0.12285     -0.522423    -0.364687   -0.194281     0.710274     0.0659001   0.0721886
 -0.557016    0.872777    -0.452163    0.348664    0.630979     0.438063    -0.497081     0.73013      0.444486    0.516661    -0.346718   -0.757077    -0.13465    -0.165254     0.562589   -0.147192     0.0437129   -0.290811   -0.264175     0.416889     0.347172     0.181899   -0.262334     0.305664    -0.0716891   0.211655 
 -0.418595    0.0911852   -0.354599   -0.476626    0.0861561   -0.332848    -0.0184991   -0.173867    -0.201639   -0.329318     0.450346   -0.510705     0.674173   -0.355415     0.779073   -0.695372     0.446238    -0.116429    0.0980198    0.390625     0.148242    -0.0448612  -0.459323     0.341876    -0.040512   -0.0160403
 -0.398491   -0.43104      0.105821    0.510605   -0.240459    -0.32993      0.665502    -0.361609    -0.31643    -0.233357     0.125764   -0.304892    -0.147267    0.0672361    0.675538   -0.429938    -0.228876    -0.173047   -0.502941    -0.258345    -0.289493    -0.229436   -0.00990225   0.716181    -0.106083   -0.15786  
 -0.100172   -0.257014     0.0943918  -0.514869   -0.292307    -0.427371     0.336027    -0.377721    -0.258071   -0.256135     0.401916    0.524324     0.243282    0.148334    -0.0790812  -0.546771    -0.525        0.169347    0.178636    -0.182336    -0.134276    -0.849276    0.0857688   -0.448277     0.783694    0.101331 
  0.67029     0.67723      0.0713637  -0.525915    0.177772     0.556264    -0.709782     0.592196     0.546517    0.605227    -0.158435    0.156285     0.171601   -0.165852    -0.452314    0.823253     0.17383      0.079444    0.295859     0.34403     -0.0509785   -0.155786    0.308613    -0.257751    -0.195511   -0.0932145
  0.507616   -0.31941     -0.147662   -0.643368    0.0824341    0.303317     0.498611    -0.298646    -0.470904   -0.08387     -0.932983   -0.0105556    0.560183   -0.273238     0.189528   -0.154234     0.103963     0.253015   -0.489379    -0.403832     0.341708    -0.330764    0.229798     0.215787    -0.501323    0.788866 
 -0.0691382  -0.0626888   -0.106178    0.0542308  -0.0611463   -0.0697343    0.0776727   -0.195567    -0.140796   -0.216745     0.119868    0.00950194   0.0408741   0.0630867   -0.0286063  -0.21362     -0.103725    -0.0799128  -0.0434865    0.129333    -0.00500037   0.0171142   0.0281546    0.116844     0.160176   -0.0736917
 -0.07861     0.7005       0.459756    0.233814   -0.171175    -0.77178     -0.0959418    0.190016    -0.0117809  -0.475262     0.551759   -0.248938     0.507828    0.534796     0.684183    0.101973    -0.046309    -0.339719    0.137684     0.232722    -0.19818     -0.0528468  -0.0867933   -0.276146     0.0710649  -0.424825 
  0.166753    0.494588     0.588551    0.39129     0.107088     0.0323975    0.255287     0.0418435   -0.0485101   0.0441559    0.709863   -0.163183     0.658378   -0.449483    -0.491647    0.143648     0.164651    -0.0488825   0.594868    -0.153918     0.588638     0.104341   -0.152216    -0.111482     0.0149673  -0.230599 
  0.40645    -0.166715    -0.70261     0.0248596   0.0285519   -0.266822     0.0137493    0.0412442   -0.209121   -0.146883    -0.587234    0.365912    -0.6962      0.459595     0.53193     0.0560268    0.14247      0.144048   -0.656935     0.236989    -0.465988    -0.236345   -0.531747     3.99761e-5   0.188688    0.434846 
  0.277238   -0.412133     0.0529958  -0.125097   -0.114463     0.149582     0.406294    -0.153889    -0.309952    0.400447    -0.124002    0.466151    -0.182679    0.251442    -0.523718   -0.0275708   -0.110665     0.0784636   0.265841    -0.37478     -0.34977      0.0444      0.359736     0.016518    -0.618501   -0.261293 
 -0.60491     0.278359     0.283167   -0.191081    0.0596977    1.00559     -0.0922614   -0.096661     0.473577   -0.397375     0.639073   -0.422914     0.159289    0.102195    -0.494471    0.00945417   0.295838    -0.505664    0.162326    -0.127382     0.190002     0.235016    0.427922     0.0509322    0.15818    -0.470796 
  0.444067   -0.452369    -0.348202   -0.169393   -0.0670208    0.026006    -0.518596    -0.00257462  -0.0656196  -0.0856714   -0.0376746  -0.24177     -0.328765   -0.367228    -0.165868    0.254638     0.408876     0.0916454   0.0374439    0.520623     0.157606     0.883627   -0.190826     0.344369    -0.447821   -0.312809 
 -0.0811613   0.111271    -0.0708706  -0.0438762   0.052921     0.0082175   -0.370161    -0.100256     0.210067   -0.236761     0.384526    0.0834895   -0.0451521   0.659486    -0.154761   -0.62113     -0.388867    -0.211648    0.285887    -0.150172    -0.172101     0.487402   -0.0741644   -0.817777     0.170767   -0.117629 
  0.4184     -0.190609     0.331581    0.170247   -0.542887    -0.132421    -0.112991     0.817103    -0.379225    0.250914    -0.369902    0.262471     0.59027     0.594769    -0.139991   -0.434591    -0.417666     0.214576    0.0384901    0.827388     0.457785     0.229368   -0.166533    -0.113506    -0.706332    0.188056 
 -0.408733   -0.0295841   -0.168519    0.0985753   0.512189     0.377322    -0.161412    -0.229083     0.0550673  -0.0287922   -0.109396   -0.191412    -0.25055    -0.0214521   -0.438985    0.187537    -0.485096     0.649349   -0.0254131    0.378485    -0.0517583    0.0792692  -0.545958    -0.382676     0.589389   -0.977118 
 -0.15938    -0.199844     0.387812    0.22786     0.156333    -1.07079     -0.194925    -0.0498368    0.018271   -0.308528    -0.314383    0.67954     -0.711688   -0.147378     0.346962   -0.0557204   -0.1904      -0.198053    0.131855     0.577268     0.300206     0.167017    0.667884    -0.844426     0.188648   -0.327246 
 -0.0499692   0.0429246   -0.418263    0.278231   -0.179783    -0.0173497    0.247446     0.394062    -0.146393   -0.367134    -0.158541   -0.150542     0.233456    0.440012     0.211172    0.0830247    0.458003    -0.658861   -0.0636804   -1.11969      0.00841167   0.0347478   0.0760238    0.0862419   -0.433907    0.874492 
 -0.251436    0.0261234   -0.116082   -0.188688   -0.250415    -0.0406005   -0.134877    -0.575531     0.18386     0.801017    -0.598947    0.13011     -0.367598   -0.954936    -0.276135   -0.152169    -0.151317     0.569635    0.238434    -0.448597    -0.0306863    0.426886    0.338273    -0.319511    -0.166246    0.669372 
 -0.268984    0.0135551   -0.252077   -0.428351   -0.00827268   0.0765569   -0.342901    -0.335984     0.117344    0.00882457  -0.528749    0.220178    -0.363087    0.0723744   -0.243891   -0.195822    -0.162367    -0.291765   -0.0299778    0.28704     -0.104149    -0.12947    -0.0344294    0.313353     0.673646   -0.0529783
 -0.0538156   0.411716     0.123063    0.0247996   0.20859     -0.253634     0.232145     0.534195     0.0339127   0.186217    -0.128346    0.100863     0.0935929  -0.0978104    0.534163   -0.302185    -0.177128     0.0199405  -0.209484    -0.103568    -0.223288    -0.193653   -0.129661    -0.240746    -0.340681   -0.0215097
 -0.162891   -0.239605     0.278019    0.629487    0.266712     0.148819    -0.122498     0.145628     0.276869   -0.379319    -0.0401887   0.454457    -0.300013    0.220451     0.410377    0.53559      0.460543     0.755066   -0.937469    -0.503491     0.0318543   -0.286159    0.491787     0.0899195   -0.302328   -0.287528 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.402200
INFO: iteration 2, average log likelihood -1.402192
INFO: iteration 3, average log likelihood -1.402184
INFO: iteration 4, average log likelihood -1.402176
INFO: iteration 5, average log likelihood -1.402169
INFO: iteration 6, average log likelihood -1.402162
INFO: iteration 7, average log likelihood -1.402155
INFO: iteration 8, average log likelihood -1.402148
INFO: iteration 9, average log likelihood -1.402142
INFO: iteration 10, average log likelihood -1.402136
INFO: EM with 100000 data points 10 iterations avll -1.402136
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
