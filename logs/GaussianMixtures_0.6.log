>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.3
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.8
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.787
Commit c71f205 (2016-09-26 16:28 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-98-generic #145-Ubuntu SMP Sat Oct 8 20:13:07 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (675.3515625 MB free)
Uptime: 24308.0 sec
Load Avg:  0.96923828125  0.998046875  1.0400390625
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz    1516066 s        109 s     142818 s     504541 s         56 s
#2  3499 MHz     634063 s       7263 s      84737 s    1610075 s          1 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.2
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.4
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.3
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.8
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:345
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:378
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:346
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1739
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:63 [inlined]
 in (::Base.##748#750{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.4960666954582257e6,[20184.1,79815.9],
[22268.5 -12376.9 11431.5; -21770.2 12422.3 -11375.3],

Array{Float64,2}[
[34539.3 -8709.98 7278.55; -8709.98 24272.9 -3915.92; 7278.55 -3915.92 24473.8],

[65104.9 8534.25 -8166.78; 8534.25 75637.1 4253.21; -8166.78 4253.21 75190.4]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.934091e+03
      1       1.157100e+03      -7.769916e+02 |        4
      2       1.133595e+03      -2.350462e+01 |        3
      3       1.090957e+03      -4.263785e+01 |        3
      4       1.017247e+03      -7.371044e+01 |        2
      5       9.918194e+02      -2.542751e+01 |        2
      6       9.619225e+02      -2.989688e+01 |        2
      7       9.555948e+02      -6.327700e+00 |        0
      8       9.555948e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 955.5947853927291)
INFO: K-means with 272 data points using 8 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.066227
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.782794
INFO: iteration 2, lowerbound -3.638199
INFO: iteration 3, lowerbound -3.487708
INFO: iteration 4, lowerbound -3.318563
INFO: iteration 5, lowerbound -3.149200
INFO: iteration 6, lowerbound -3.004547
INFO: dropping number of Gaussions to 6
INFO: iteration 7, lowerbound -2.878333
INFO: iteration 8, lowerbound -2.784245
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.731370
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.691628
INFO: dropping number of Gaussions to 3
INFO: iteration 11, lowerbound -2.645483
INFO: iteration 12, lowerbound -2.598941
INFO: iteration 13, lowerbound -2.553080
INFO: iteration 14, lowerbound -2.507193
INFO: iteration 15, lowerbound -2.463962
INFO: iteration 16, lowerbound -2.424840
INFO: iteration 17, lowerbound -2.389760
INFO: iteration 18, lowerbound -2.358120
INFO: iteration 19, lowerbound -2.330983
INFO: iteration 20, lowerbound -2.312614
INFO: iteration 21, lowerbound -2.307545
INFO: dropping number of Gaussions to 2
INFO: iteration 22, lowerbound -2.302922
INFO: iteration 23, lowerbound -2.299260
INFO: iteration 24, lowerbound -2.299256
INFO: iteration 25, lowerbound -2.299254
INFO: iteration 26, lowerbound -2.299254
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Wed 12 Oct 2016 11:14:43 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Wed 12 Oct 2016 11:14:44 AM UTC: K-means with 272 data points using 8 iterations
11.3 data points per parameter
,Wed 12 Oct 2016 11:14:46 AM UTC: EM with 272 data points 0 iterations avll -2.066227
5.8 data points per parameter
,Wed 12 Oct 2016 11:14:47 AM UTC: GMM converted to Variational GMM
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 1, lowerbound -3.782794
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 2, lowerbound -3.638199
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 3, lowerbound -3.487708
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 4, lowerbound -3.318563
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 5, lowerbound -3.149200
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 6, lowerbound -3.004547
,Wed 12 Oct 2016 11:14:49 AM UTC: dropping number of Gaussions to 6
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 7, lowerbound -2.878333
,Wed 12 Oct 2016 11:14:49 AM UTC: iteration 8, lowerbound -2.784245
,Wed 12 Oct 2016 11:14:50 AM UTC: dropping number of Gaussions to 5
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 9, lowerbound -2.731370
,Wed 12 Oct 2016 11:14:50 AM UTC: dropping number of Gaussions to 4
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 10, lowerbound -2.691628
,Wed 12 Oct 2016 11:14:50 AM UTC: dropping number of Gaussions to 3
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 11, lowerbound -2.645483
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 12, lowerbound -2.598941
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 13, lowerbound -2.553080
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 14, lowerbound -2.507193
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 15, lowerbound -2.463962
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 16, lowerbound -2.424840
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 17, lowerbound -2.389760
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 18, lowerbound -2.358120
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 19, lowerbound -2.330983
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 20, lowerbound -2.312614
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 21, lowerbound -2.307545
,Wed 12 Oct 2016 11:14:50 AM UTC: dropping number of Gaussions to 2
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 22, lowerbound -2.302922
,Wed 12 Oct 2016 11:14:50 AM UTC: iteration 23, lowerbound -2.299260
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 24, lowerbound -2.299256
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 25, lowerbound -2.299254
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 26, lowerbound -2.299254
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 27, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 28, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 29, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 30, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 31, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 32, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 33, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 34, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 35, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 36, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 37, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 38, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:51 AM UTC: iteration 39, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 40, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 41, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 42, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 43, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 44, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 45, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 46, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 47, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 48, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 49, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: iteration 50, lowerbound -2.299253
,Wed 12 Oct 2016 11:14:52 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000004
avll from stats: -1.0225145090101764
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -1.022514509010187
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -1.022514509010187
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
nx: 100000 sum(zeroth order stats): 100000.0
avll from stats: -0.9862610839566984
avll from llpg:  -0.9862610839566984
avll direct:     -0.9862610839566984
sum posterior: 100000.0
32×26 Array{Float64,2}:
  0.0564586   -0.107294    -0.0399524   -0.0362065   -0.00611936   0.0493929     0.136363     0.0305167    0.117855     0.0640695   -0.174252      0.0601716   -0.0133883   -0.015156     0.115962     -0.00264722  -0.0278597    0.00851267  -0.0572524    0.0841744    0.225107    -0.00128994  -0.134198    -0.0518551   -0.014623     0.00698503
 -0.06608     -0.01252      0.0160255   -0.125489     0.131518    -0.0290104     0.110595     0.214649    -0.122198    -0.0544123    0.244953     -0.152988     0.16561     -0.13384     -0.0127817     0.208373     0.0499291    0.102407     0.049702     0.130718     0.0700963    0.00853192   0.00102295  -0.0748419   -0.0700347    0.0171151 
 -0.135801    -0.0538463   -0.0325192   -0.0826202    0.0330677   -0.165493     -0.0659509    0.104106     0.0338463   -0.028156     0.0558848     0.0110832    0.0625481    0.122045    -0.0439128     0.00819282  -0.136118    -0.0338249    0.117051    -0.0331885    0.0540886    0.164377    -0.0845326   -0.252877     0.09626     -0.119498  
  0.32971     -0.150119     0.00889274  -0.173749     0.124706    -0.0162822    -0.0588923   -0.0132122   -0.0971131   -0.053829    -0.12754       0.0305284    0.0992248    0.00566464  -0.138219     -0.0391965   -0.0681605   -0.227517     0.129027    -0.123648     0.00441549  -0.0235197   -0.0433124   -0.0943747   -0.0866279    0.060508  
  0.206474    -0.014769    -0.0260193   -0.0958587   -0.0405739    0.0721626    -0.146896     0.151737     0.0601621    0.33678      0.0546541     0.191135    -0.0532557   -0.182089     0.0269332    -0.00224497   0.105111     0.0742503    0.0517632   -0.0212153    0.0609276   -0.0878342    0.0798001   -0.187229     0.0459317   -0.116078  
  0.138384    -0.0598413   -0.108527    -0.0220581   -0.0789949   -0.0765933     0.112175     0.120871    -0.0438873   -0.0804445   -0.0675388    -0.177316     0.0308999    0.169537    -0.0379994     0.0699901   -0.0119296   -0.0809847    0.102624     0.0631146   -0.0794989   -0.0298348    0.0605687   -0.151765     0.0986079    0.10622   
  0.108462    -0.0994527    0.140449    -0.0409939    0.105028     0.0359222    -0.100205    -0.0278775    0.0586917    0.0319892    0.0016863    -0.0290083   -0.220465     0.109028    -0.0569706     0.104031    -0.106941    -0.0398819    0.08906     -0.0351784   -0.265613     0.0745738    0.0980785    0.0214166    0.0571054   -0.0938732 
 -0.0929149    0.0750385    0.0453434    0.0620176    0.192714    -0.0364686    -0.0914918   -0.0841948    0.0979756    0.0149781   -0.101059      0.060178    -0.143751     0.0292193    0.0623948     0.0280086    0.0172905    0.122215     0.153448     0.137517    -0.0523045   -0.105642    -0.00868337   0.131944    -0.0274952    0.158481  
 -0.114217     0.080678    -0.146056    -0.125349     0.0281915   -0.166821      0.118212    -0.0203426    0.0876054    0.117421    -0.0373357    -0.0716237   -0.0112468    0.0266073   -0.0708292     0.151752    -0.133974    -0.0452383    0.135406    -0.0524854    0.0032679   -0.0221678    0.0111195    0.12393     -0.116642    -0.0272998 
  0.218307    -0.0867061   -0.0992168    0.114329     0.0353665    0.105396     -0.147244     0.103821     0.0147423   -0.154289     0.117881     -0.0103981   -0.171044    -0.053298     0.114518      0.0105036    0.0861834   -0.0434639    0.0146137   -0.112453    -0.0037158   -0.0534897    0.0484804   -0.0303535   -0.112141     0.0792413 
 -0.0743782   -0.041532    -0.00913717   0.15241     -0.0466858   -0.0703724    -0.0464145    0.024812    -0.0465671   -0.172746    -0.00602244   -0.00487364   0.134869    -0.00215006  -0.189227     -0.12388      0.286028    -0.131462    -0.0553881   -0.019258    -0.213936     0.137365    -0.133589     0.0302207   -0.0403205   -0.0391086 
  0.0179012   -0.00691143  -0.0156609   -0.145498    -0.0842936    0.13591      -0.00806513  -0.052754    -0.0220254    0.00812833   0.00998251    0.0458684   -0.183157     0.0459291   -0.0359789    -0.231632     0.195978     0.0181635   -0.0475794    0.209202    -0.008735    -0.0850349   -0.0675163   -0.158155    -0.0235465   -0.0889654 
  0.0853575   -0.0863554    0.145339    -0.129869    -0.134401    -0.000617229  -0.00106124   0.0846668    0.159269     0.005784     0.0383451     0.0488624    0.014037     0.0106295   -0.166984     -0.0965104    0.367675     0.0502975   -0.0483179   -0.129848     0.060247     0.0153524    0.0528054   -0.0863974   -0.104812     0.202254  
 -0.0332854    0.140774    -0.0901703   -0.0322382   -0.0400019   -0.148459     -0.183396    -0.0154292    0.134087    -0.0341382   -0.0268327     0.0500668    0.0589887   -0.0746852    0.109868     -0.0642788    0.0548749   -0.0486543    0.145889     0.00134656  -0.0409012    0.0214261   -0.0159579    0.0350729   -0.0539066    0.0194518 
 -0.126733     0.0195743   -0.0377058    0.020352     0.182079     0.0329739    -0.148909     0.0034763    0.119094    -0.00295947   0.000530266   0.0187829   -0.0593428   -0.0368712   -0.0492922    -0.175453    -0.142684     0.0203251   -0.0391129    0.047246     0.0476809    0.0752082   -0.121345    -0.0726995    0.0637625    0.0765353 
 -0.10851      0.0455911   -0.0655949    0.0477265   -0.123767     0.0398937    -0.151803     0.103425    -0.0804174    0.169396    -0.103599     -0.115205     0.0200415   -0.0559873    0.1126        0.0376074   -0.117293    -0.0649387    0.00805428  -0.0396963   -0.12684      0.0141126    0.113146    -0.049878     0.0466141    0.124409  
  0.0371375   -0.0891761    0.0540135   -0.0334983    0.198091     0.108257      0.2378      -0.00866383   0.0724273    0.11939      0.123713      0.151585    -0.152336    -0.147933    -0.127838     -0.0261111    0.193426    -0.0663966    0.137022     0.0207586   -0.00628323  -0.0611354    0.0635422    0.0101518   -0.0412846    0.0268202 
  0.100642    -0.0784771   -0.0357107   -0.119638     0.0260914    0.137559     -0.148536     0.0596179   -0.073373    -0.0102834    0.121538      0.0767632    0.173174     0.0852377    0.0753707    -0.0442287    0.0177813   -0.0234563    0.113624    -0.279916    -0.00969554   0.0907524    0.0450251    0.0559733   -0.00548717  -0.0275276 
 -0.0323363    0.129007     0.0152425   -0.24593      0.210259    -0.0801167     0.135273    -0.0667032   -0.0413046    0.00483646  -0.0242827    -0.00374804  -0.0158361   -0.0194777   -0.000325897   0.0680721    0.232104    -0.194294     0.129033     0.0894278   -0.0964105    0.176847     0.0207718    0.0427439    0.0126495    0.236807  
  0.096503    -0.00399216  -0.0730236   -0.0713792   -0.0627647   -0.0201843    -0.101611    -0.0839838   -0.0384414    0.0669927   -0.0176751     0.160029    -0.00394783   0.154672    -0.0333516    -0.00496673   0.0549482   -0.0803702    0.100195    -0.0858994   -0.32686      0.0656289   -0.00325408  -0.198217     0.106811     0.209033  
 -0.130526     0.0250473   -0.0971348   -0.147361    -0.0379782   -0.0530319     0.118375     0.151388    -0.0289818   -0.0671493   -0.179182     -0.00133449   0.0347459   -0.066229     0.131034     -0.0798474    0.010947     0.0353578    0.0805417   -0.0736204   -0.104758     0.0031093    0.0123911   -0.188922    -0.0112919    0.100899  
  0.110124     0.068811    -0.0125248   -0.166571     0.122392    -0.0584891    -0.0434791    0.110643    -0.0593272    0.0227421    0.0100149    -0.0337608   -0.151573     0.0537923   -0.0704391     0.109136     0.0525622   -0.0318333    0.0429199    0.117776     0.0159475    0.0816356    0.0482777    0.0797462   -0.06623      0.0445339 
  0.039962    -0.0326256   -0.0761115   -0.0706733   -0.0959695    0.00826879   -0.135159     0.110748    -0.00517262   0.0902776    0.118496     -0.0240586   -0.127143     0.0569509   -0.138903      0.0227943   -0.0250045    0.25315      0.0739097   -0.00185547   0.19858     -0.0434979   -0.16343      0.0498327   -0.0715422   -0.0355309 
  0.0532944   -0.0745844   -0.10866     -0.0774338   -0.0574042   -0.187075     -0.0423437    0.00268779  -0.130091    -0.170439     0.0156796    -0.0223873    0.0252104    0.0173607   -0.172342     -0.0998728   -0.0939136   -0.0605665    0.0725692   -0.0672684   -0.0648036    0.0836756    0.100822     0.10974     -0.125389     0.0913349 
 -0.0417306    0.0247188   -0.0156479   -0.026235     0.194032    -0.0948185    -0.0495406   -0.0493658    0.067229     0.0131148   -0.156987      0.0139424    0.0225545   -0.17424      0.0370989     0.153276     0.0251709    0.0773822   -0.0583114   -0.00389306   0.132159    -0.0502721   -0.0166125    0.00391362   0.116349     0.134366  
 -0.00958942   0.0461637   -0.050003     0.00677841  -0.0268844    0.0982482    -0.117876     0.0117498    0.04942     -0.0742517   -0.16803       0.123812    -0.0366139    0.154887     0.105766     -0.160687     0.118722    -0.106911     0.163931    -0.0133573    0.0586534    0.166559     0.0644175   -0.00738931  -0.110798    -0.101291  
 -0.0988646   -0.0357727   -0.0143072    0.0719469    0.0166463   -0.201738     -0.0160756    0.0813557    0.0920329   -0.0545185   -0.246678      0.0702155    0.0269793   -0.0773339    0.0926366     0.128893    -0.00730938   0.0862298    0.085725    -0.166731    -0.130127    -0.00324823   0.0249168    0.173755     0.00535764   0.0889804 
 -0.0936076    0.010529     0.227422    -0.0677948   -0.0275996   -0.188657     -0.172702     0.00969101  -0.0608268   -0.251515     0.0634523    -0.23833      0.12217     -0.0490538   -0.0894976     0.13459      0.0429107   -0.0274917    0.0151413   -0.173767    -0.118005    -0.180075    -0.147863     0.0135903   -0.124593     0.0462108 
 -0.0726272   -0.179254    -0.120602    -0.145616    -0.162862     0.0798788    -0.127955     0.0165274    0.0245406    0.0888373    0.156676     -0.0283836    0.0915613   -0.13218      0.0022311     0.00859766  -0.0336078   -0.0297401   -0.0364581   -0.02626     -0.0596237    0.0119041   -0.0566381   -0.0390237    0.047341     0.145524  
 -0.106816    -0.0811761   -0.209673     0.0310117   -0.111317     0.0135837     0.144695    -0.0487842    0.193833    -0.121214    -0.168488      0.0448591   -0.1058      -0.177103     0.163972      0.0431841    0.0127542   -0.0601241    0.0184975    0.142305     0.0349596   -0.0503324   -0.0593015   -0.0920039   -0.134589    -0.0160859 
 -0.0493977   -0.0449787   -0.0176766    0.0601592   -0.0603376   -0.0383714    -0.175992     0.0336821   -0.169614    -0.034859    -0.0274941    -0.131012    -0.0649643   -0.050268     0.0143264    -0.147875    -0.00876891   0.0799443    0.00980223  -0.0581427    0.0997607   -0.0724055    0.0486804   -0.00551238   0.152435    -0.110979  
 -0.155991     0.0855797   -0.103233     0.0704848   -0.0745499   -0.0890019    -0.0681959   -0.0205721   -0.214228     0.0987437    0.0488658     0.058043    -0.0397216    0.16866      0.0564612    -0.0657527    0.0998021   -0.0175966    0.0277046   -0.0981062    0.0594874   -0.260987    -0.0321605    0.0192335    0.140533    -0.193085  kind diag, method split
0: avll = -1.3966904577039212
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:268
 in _start() at ./client.jl:332
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.396753
INFO: iteration 2, average log likelihood -1.396701
INFO: iteration 3, average log likelihood -1.396411
INFO: iteration 4, average log likelihood -1.392832
INFO: iteration 5, average log likelihood -1.379840
INFO: iteration 6, average log likelihood -1.370787
INFO: iteration 7, average log likelihood -1.369221
INFO: iteration 8, average log likelihood -1.368650
INFO: iteration 9, average log likelihood -1.368204
INFO: iteration 10, average log likelihood -1.367784
INFO: iteration 11, average log likelihood -1.367349
INFO: iteration 12, average log likelihood -1.366867
INFO: iteration 13, average log likelihood -1.366339
INFO: iteration 14, average log likelihood -1.365740
INFO: iteration 15, average log likelihood -1.364911
INFO: iteration 16, average log likelihood -1.364029
INFO: iteration 17, average log likelihood -1.363497
INFO: iteration 18, average log likelihood -1.363205
INFO: iteration 19, average log likelihood -1.363025
INFO: iteration 20, average log likelihood -1.362900
INFO: iteration 21, average log likelihood -1.362806
INFO: iteration 22, average log likelihood -1.362730
INFO: iteration 23, average log likelihood -1.362666
INFO: iteration 24, average log likelihood -1.362611
INFO: iteration 25, average log likelihood -1.362564
INFO: iteration 26, average log likelihood -1.362523
INFO: iteration 27, average log likelihood -1.362489
INFO: iteration 28, average log likelihood -1.362460
INFO: iteration 29, average log likelihood -1.362437
INFO: iteration 30, average log likelihood -1.362417
INFO: iteration 31, average log likelihood -1.362402
INFO: iteration 32, average log likelihood -1.362389
INFO: iteration 33, average log likelihood -1.362378
INFO: iteration 34, average log likelihood -1.362369
INFO: iteration 35, average log likelihood -1.362361
INFO: iteration 36, average log likelihood -1.362354
INFO: iteration 37, average log likelihood -1.362348
INFO: iteration 38, average log likelihood -1.362342
INFO: iteration 39, average log likelihood -1.362336
INFO: iteration 40, average log likelihood -1.362331
INFO: iteration 41, average log likelihood -1.362325
INFO: iteration 42, average log likelihood -1.362320
INFO: iteration 43, average log likelihood -1.362315
INFO: iteration 44, average log likelihood -1.362309
INFO: iteration 45, average log likelihood -1.362303
INFO: iteration 46, average log likelihood -1.362295
INFO: iteration 47, average log likelihood -1.362285
INFO: iteration 48, average log likelihood -1.362271
INFO: iteration 49, average log likelihood -1.362251
INFO: iteration 50, average log likelihood -1.362223
INFO: EM with 100000 data points 50 iterations avll -1.362223
952.4 data points per parameter
1: avll = [-1.39675,-1.3967,-1.39641,-1.39283,-1.37984,-1.37079,-1.36922,-1.36865,-1.3682,-1.36778,-1.36735,-1.36687,-1.36634,-1.36574,-1.36491,-1.36403,-1.3635,-1.3632,-1.36303,-1.3629,-1.36281,-1.36273,-1.36267,-1.36261,-1.36256,-1.36252,-1.36249,-1.36246,-1.36244,-1.36242,-1.3624,-1.36239,-1.36238,-1.36237,-1.36236,-1.36235,-1.36235,-1.36234,-1.36234,-1.36233,-1.36233,-1.36232,-1.36231,-1.36231,-1.3623,-1.36229,-1.36228,-1.36227,-1.36225,-1.36222]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.362289
INFO: iteration 2, average log likelihood -1.362108
INFO: iteration 3, average log likelihood -1.361241
INFO: iteration 4, average log likelihood -1.354899
INFO: iteration 5, average log likelihood -1.341351
INFO: iteration 6, average log likelihood -1.333039
INFO: iteration 7, average log likelihood -1.329510
INFO: iteration 8, average log likelihood -1.327313
INFO: iteration 9, average log likelihood -1.325907
INFO: iteration 10, average log likelihood -1.325010
INFO: iteration 11, average log likelihood -1.324440
INFO: iteration 12, average log likelihood -1.324079
INFO: iteration 13, average log likelihood -1.323843
INFO: iteration 14, average log likelihood -1.323673
INFO: iteration 15, average log likelihood -1.323537
INFO: iteration 16, average log likelihood -1.323421
INFO: iteration 17, average log likelihood -1.323317
INFO: iteration 18, average log likelihood -1.323219
INFO: iteration 19, average log likelihood -1.323120
INFO: iteration 20, average log likelihood -1.323013
INFO: iteration 21, average log likelihood -1.322896
INFO: iteration 22, average log likelihood -1.322768
INFO: iteration 23, average log likelihood -1.322633
INFO: iteration 24, average log likelihood -1.322493
INFO: iteration 25, average log likelihood -1.322347
INFO: iteration 26, average log likelihood -1.322198
INFO: iteration 27, average log likelihood -1.322055
INFO: iteration 28, average log likelihood -1.321923
INFO: iteration 29, average log likelihood -1.321798
INFO: iteration 30, average log likelihood -1.321675
INFO: iteration 31, average log likelihood -1.321540
INFO: iteration 32, average log likelihood -1.321376
INFO: iteration 33, average log likelihood -1.321190
INFO: iteration 34, average log likelihood -1.321034
INFO: iteration 35, average log likelihood -1.320924
INFO: iteration 36, average log likelihood -1.320842
INFO: iteration 37, average log likelihood -1.320775
INFO: iteration 38, average log likelihood -1.320716
INFO: iteration 39, average log likelihood -1.320660
INFO: iteration 40, average log likelihood -1.320602
INFO: iteration 41, average log likelihood -1.320539
INFO: iteration 42, average log likelihood -1.320468
INFO: iteration 43, average log likelihood -1.320392
INFO: iteration 44, average log likelihood -1.320319
INFO: iteration 45, average log likelihood -1.320254
INFO: iteration 46, average log likelihood -1.320199
INFO: iteration 47, average log likelihood -1.320153
INFO: iteration 48, average log likelihood -1.320114
INFO: iteration 49, average log likelihood -1.320083
INFO: iteration 50, average log likelihood -1.320058
INFO: EM with 100000 data points 50 iterations avll -1.320058
473.9 data points per parameter
2: avll = [-1.36229,-1.36211,-1.36124,-1.3549,-1.34135,-1.33304,-1.32951,-1.32731,-1.32591,-1.32501,-1.32444,-1.32408,-1.32384,-1.32367,-1.32354,-1.32342,-1.32332,-1.32322,-1.32312,-1.32301,-1.3229,-1.32277,-1.32263,-1.32249,-1.32235,-1.3222,-1.32206,-1.32192,-1.3218,-1.32167,-1.32154,-1.32138,-1.32119,-1.32103,-1.32092,-1.32084,-1.32078,-1.32072,-1.32066,-1.3206,-1.32054,-1.32047,-1.32039,-1.32032,-1.32025,-1.3202,-1.32015,-1.32011,-1.32008,-1.32006]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.320211
INFO: iteration 2, average log likelihood -1.320022
INFO: iteration 3, average log likelihood -1.319294
INFO: iteration 4, average log likelihood -1.312103
INFO: iteration 5, average log likelihood -1.291950
INFO: iteration 6, average log likelihood -1.279581
INFO: iteration 7, average log likelihood -1.275424
INFO: iteration 8, average log likelihood -1.272797
INFO: iteration 9, average log likelihood -1.270320
WARNING: Variances had to be floored 3
INFO: iteration 10, average log likelihood -1.267061
INFO: iteration 11, average log likelihood -1.280068
INFO: iteration 12, average log likelihood -1.272935
INFO: iteration 13, average log likelihood -1.270405
INFO: iteration 14, average log likelihood -1.268022
WARNING: Variances had to be floored 3
INFO: iteration 15, average log likelihood -1.264611
INFO: iteration 16, average log likelihood -1.278420
INFO: iteration 17, average log likelihood -1.271794
INFO: iteration 18, average log likelihood -1.269421
INFO: iteration 19, average log likelihood -1.267225
INFO: iteration 20, average log likelihood -1.264001
WARNING: Variances had to be floored 3
INFO: iteration 21, average log likelihood -1.260306
INFO: iteration 22, average log likelihood -1.276537
INFO: iteration 23, average log likelihood -1.269771
INFO: iteration 24, average log likelihood -1.267163
INFO: iteration 25, average log likelihood -1.264797
INFO: iteration 26, average log likelihood -1.261667
WARNING: Variances had to be floored 3
INFO: iteration 27, average log likelihood -1.257730
INFO: iteration 28, average log likelihood -1.272749
INFO: iteration 29, average log likelihood -1.266235
INFO: iteration 30, average log likelihood -1.264369
INFO: iteration 31, average log likelihood -1.262994
INFO: iteration 32, average log likelihood -1.260968
WARNING: Variances had to be floored 3
INFO: iteration 33, average log likelihood -1.257777
INFO: iteration 34, average log likelihood -1.272165
INFO: iteration 35, average log likelihood -1.265897
INFO: iteration 36, average log likelihood -1.264168
INFO: iteration 37, average log likelihood -1.262886
INFO: iteration 38, average log likelihood -1.260947
WARNING: Variances had to be floored 3
INFO: iteration 39, average log likelihood -1.257805
INFO: iteration 40, average log likelihood -1.271989
INFO: iteration 41, average log likelihood -1.265823
INFO: iteration 42, average log likelihood -1.264151
INFO: iteration 43, average log likelihood -1.262933
INFO: iteration 44, average log likelihood -1.261098
WARNING: Variances had to be floored 3
INFO: iteration 45, average log likelihood -1.258037
INFO: iteration 46, average log likelihood -1.271882
INFO: iteration 47, average log likelihood -1.265840
INFO: iteration 48, average log likelihood -1.264235
INFO: iteration 49, average log likelihood -1.263096
INFO: iteration 50, average log likelihood -1.261428
INFO: EM with 100000 data points 50 iterations avll -1.261428
236.4 data points per parameter
3: avll = [-1.32021,-1.32002,-1.31929,-1.3121,-1.29195,-1.27958,-1.27542,-1.2728,-1.27032,-1.26706,-1.28007,-1.27293,-1.27041,-1.26802,-1.26461,-1.27842,-1.27179,-1.26942,-1.26723,-1.264,-1.26031,-1.27654,-1.26977,-1.26716,-1.2648,-1.26167,-1.25773,-1.27275,-1.26623,-1.26437,-1.26299,-1.26097,-1.25778,-1.27217,-1.2659,-1.26417,-1.26289,-1.26095,-1.2578,-1.27199,-1.26582,-1.26415,-1.26293,-1.2611,-1.25804,-1.27188,-1.26584,-1.26423,-1.2631,-1.26143]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
WARNING: Variances had to be floored 5 6
INFO: iteration 1, average log likelihood -1.258719
WARNING: Variances had to be floored 5 6
INFO: iteration 2, average log likelihood -1.258447
WARNING: Variances had to be floored 5 6
INFO: iteration 3, average log likelihood -1.257843
WARNING: Variances had to be floored 5 6
INFO: iteration 4, average log likelihood -1.250703
WARNING: Variances had to be floored 5 6 9
INFO: iteration 5, average log likelihood -1.225244
WARNING: Variances had to be floored 5 6
INFO: iteration 6, average log likelihood -1.209735
WARNING: Variances had to be floored 5 6 9
INFO: iteration 7, average log likelihood -1.194921
WARNING: Variances had to be floored 5 6
INFO: iteration 8, average log likelihood -1.194779
WARNING: Variances had to be floored 5 6 9
INFO: iteration 9, average log likelihood -1.186024
WARNING: Variances had to be floored 5 6 8
INFO: iteration 10, average log likelihood -1.186535
WARNING: Variances had to be floored 5 6 9
INFO: iteration 11, average log likelihood -1.193556
WARNING: Variances had to be floored 5 6
INFO: iteration 12, average log likelihood -1.189824
WARNING: Variances had to be floored 5 6 9
INFO: iteration 13, average log likelihood -1.181260
WARNING: Variances had to be floored 5 6
INFO: iteration 14, average log likelihood -1.183964
WARNING: Variances had to be floored 5 6 9
INFO: iteration 15, average log likelihood -1.177675
WARNING: Variances had to be floored 5 6
INFO: iteration 16, average log likelihood -1.181456
WARNING: Variances had to be floored 5 6 9
INFO: iteration 17, average log likelihood -1.176346
WARNING: Variances had to be floored 5 6
INFO: iteration 18, average log likelihood -1.181074
WARNING: Variances had to be floored 5 6 9
INFO: iteration 19, average log likelihood -1.176164
WARNING: Variances had to be floored 5 6
INFO: iteration 20, average log likelihood -1.180959
WARNING: Variances had to be floored 5 6 9
INFO: iteration 21, average log likelihood -1.176023
WARNING: Variances had to be floored 5 6
INFO: iteration 22, average log likelihood -1.180847
WARNING: Variances had to be floored 5 6 9
INFO: iteration 23, average log likelihood -1.175882
WARNING: Variances had to be floored 5 6
INFO: iteration 24, average log likelihood -1.180756
WARNING: Variances had to be floored 5 6 9
INFO: iteration 25, average log likelihood -1.175788
WARNING: Variances had to be floored 5 6
INFO: iteration 26, average log likelihood -1.180707
WARNING: Variances had to be floored 5 6 9
INFO: iteration 27, average log likelihood -1.175734
WARNING: Variances had to be floored 5 6
INFO: iteration 28, average log likelihood -1.180677
WARNING: Variances had to be floored 5 6 9
INFO: iteration 29, average log likelihood -1.175692
WARNING: Variances had to be floored 5 6
INFO: iteration 30, average log likelihood -1.180647
WARNING: Variances had to be floored 5 6 9
INFO: iteration 31, average log likelihood -1.175646
WARNING: Variances had to be floored 5 6
INFO: iteration 32, average log likelihood -1.180611
WARNING: Variances had to be floored 5 6 9
INFO: iteration 33, average log likelihood -1.175583
WARNING: Variances had to be floored 5 6
INFO: iteration 34, average log likelihood -1.180559
WARNING: Variances had to be floored 5 6 9
INFO: iteration 35, average log likelihood -1.175491
WARNING: Variances had to be floored 5 6
INFO: iteration 36, average log likelihood -1.180489
WARNING: Variances had to be floored 5 6 9
INFO: iteration 37, average log likelihood -1.175369
WARNING: Variances had to be floored 5 6 9
INFO: iteration 38, average log likelihood -1.180400
WARNING: Variances had to be floored 5 6
INFO: iteration 39, average log likelihood -1.180347
WARNING: Variances had to be floored 5 6 9
INFO: iteration 40, average log likelihood -1.175229
WARNING: Variances had to be floored 5 6 9
INFO: iteration 41, average log likelihood -1.180253
WARNING: Variances had to be floored 5 6
INFO: iteration 42, average log likelihood -1.180204
WARNING: Variances had to be floored 5 6 9
INFO: iteration 43, average log likelihood -1.175045
WARNING: Variances had to be floored 5 6 9
INFO: iteration 44, average log likelihood -1.180137
WARNING: Variances had to be floored 5 6 9
INFO: iteration 45, average log likelihood -1.180103
WARNING: Variances had to be floored 5 6 9
INFO: iteration 46, average log likelihood -1.180072
WARNING: Variances had to be floored 5 6 9
INFO: iteration 47, average log likelihood -1.180040
WARNING: Variances had to be floored 5 6
INFO: iteration 48, average log likelihood -1.180004
WARNING: Variances had to be floored 5 6 9
INFO: iteration 49, average log likelihood -1.174805
WARNING: Variances had to be floored 5 6 9
INFO: iteration 50, average log likelihood -1.179917
INFO: EM with 100000 data points 50 iterations avll -1.179917
118.1 data points per parameter
4: avll = [-1.25872,-1.25845,-1.25784,-1.2507,-1.22524,-1.20974,-1.19492,-1.19478,-1.18602,-1.18654,-1.19356,-1.18982,-1.18126,-1.18396,-1.17768,-1.18146,-1.17635,-1.18107,-1.17616,-1.18096,-1.17602,-1.18085,-1.17588,-1.18076,-1.17579,-1.18071,-1.17573,-1.18068,-1.17569,-1.18065,-1.17565,-1.18061,-1.17558,-1.18056,-1.17549,-1.18049,-1.17537,-1.1804,-1.18035,-1.17523,-1.18025,-1.1802,-1.17505,-1.18014,-1.1801,-1.18007,-1.18004,-1.18,-1.17481,-1.17992]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 9 10 11 12
INFO: iteration 1, average log likelihood -1.180101
WARNING: Variances had to be floored 9 10 11 12 17 18
INFO: iteration 2, average log likelihood -1.174584
WARNING: Variances had to be floored 9 10 11 12 17 18
INFO: iteration 3, average log likelihood -1.177923
WARNING: Variances had to be floored 9 10 11 12 17 18 26
INFO: iteration 4, average log likelihood -1.159418
WARNING: Variances had to be floored 2 9 10 11 12 15 17 18
INFO: iteration 5, average log likelihood -1.106000
WARNING: Variances had to be floored 5 9 10 11 12 14 17 18 23 26 31
INFO: iteration 6, average log likelihood -1.074660
WARNING: Variances had to be floored 9 10 11 12 15 17 18
INFO: iteration 7, average log likelihood -1.092841
WARNING: Variances had to be floored 2 9 10 11 12 16 17 18 26
INFO: iteration 8, average log likelihood -1.065355
WARNING: Variances had to be floored 5 9 10 11 12 14 15 17 18
INFO: iteration 9, average log likelihood -1.058840
WARNING: Variances had to be floored 9 10 11 12 17 18 23 26 31
INFO: iteration 10, average log likelihood -1.067078
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18
INFO: iteration 11, average log likelihood -1.056619
WARNING: Variances had to be floored 2 9 10 11 12 14 16 17 18 23 26 31
INFO: iteration 12, average log likelihood -1.061697
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18
INFO: iteration 13, average log likelihood -1.073340
WARNING: Variances had to be floored 2 9 10 11 12 17 18 23 26 31
INFO: iteration 14, average log likelihood -1.062808
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18
INFO: iteration 15, average log likelihood -1.060947
WARNING: Variances had to be floored 2 9 10 11 12 16 17 18 23 26 31
INFO: iteration 16, average log likelihood -1.068408
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18
INFO: iteration 17, average log likelihood -1.067625
WARNING: Variances had to be floored 2 9 10 11 12 14 17 18 23 26
INFO: iteration 18, average log likelihood -1.061312
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 19, average log likelihood -1.062031
WARNING: Variances had to be floored 2 9 10 11 12 16 17 18 23 26
INFO: iteration 20, average log likelihood -1.067015
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 31
INFO: iteration 21, average log likelihood -1.061178
WARNING: Variances had to be floored 2 9 10 11 12 17 18 23 26
INFO: iteration 22, average log likelihood -1.073293
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 23, average log likelihood -1.058123
WARNING: Variances had to be floored 2 9 10 11 12 14 16 17 18 23 26
INFO: iteration 24, average log likelihood -1.065312
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 25, average log likelihood -1.068715
WARNING: Variances had to be floored 2 9 10 11 12 17 18 23 26
INFO: iteration 26, average log likelihood -1.067447
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 31
INFO: iteration 27, average log likelihood -1.056582
WARNING: Variances had to be floored 2 9 10 11 12 16 17 18 23 26
INFO: iteration 28, average log likelihood -1.072908
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 29, average log likelihood -1.062884
WARNING: Variances had to be floored 2 9 10 11 12 14 17 18 23 26
INFO: iteration 30, average log likelihood -1.065722
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 31, average log likelihood -1.063965
WARNING: Variances had to be floored 2 9 10 11 12 16 17 18 23 26
INFO: iteration 32, average log likelihood -1.067048
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 31
INFO: iteration 33, average log likelihood -1.061347
WARNING: Variances had to be floored 2 9 10 11 12 17 18 23 26
INFO: iteration 34, average log likelihood -1.073276
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 35, average log likelihood -1.058155
WARNING: Variances had to be floored 2 9 10 11 12 14 16 17 18 23 26
INFO: iteration 36, average log likelihood -1.065404
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 37, average log likelihood -1.068711
WARNING: Variances had to be floored 2 9 10 11 12 17 18 23 26
INFO: iteration 38, average log likelihood -1.067463
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 31
INFO: iteration 39, average log likelihood -1.056654
WARNING: Variances had to be floored 2 9 10 11 12 16 17 18 23 26
INFO: iteration 40, average log likelihood -1.072899
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 41, average log likelihood -1.062900
WARNING: Variances had to be floored 2 9 10 11 12 14 17 18 23 26
INFO: iteration 42, average log likelihood -1.065771
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 43, average log likelihood -1.063964
WARNING: Variances had to be floored 2 9 10 11 12 16 17 18 23 26
INFO: iteration 44, average log likelihood -1.067057
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 31
INFO: iteration 45, average log likelihood -1.061385
WARNING: Variances had to be floored 2 9 10 11 12 17 18 23 26
INFO: iteration 46, average log likelihood -1.073271
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 47, average log likelihood -1.058163
WARNING: Variances had to be floored 2 9 10 11 12 14 16 17 18 23 26
INFO: iteration 48, average log likelihood -1.065429
WARNING: Variances had to be floored 2 5 9 10 11 12 15 17 18 31
INFO: iteration 49, average log likelihood -1.068710
WARNING: Variances had to be floored 2 9 10 11 12 17 18 23 26
INFO: iteration 50, average log likelihood -1.067467
INFO: EM with 100000 data points 50 iterations avll -1.067467
59.0 data points per parameter
5: avll = [-1.1801,-1.17458,-1.17792,-1.15942,-1.106,-1.07466,-1.09284,-1.06536,-1.05884,-1.06708,-1.05662,-1.0617,-1.07334,-1.06281,-1.06095,-1.06841,-1.06762,-1.06131,-1.06203,-1.06702,-1.06118,-1.07329,-1.05812,-1.06531,-1.06871,-1.06745,-1.05658,-1.07291,-1.06288,-1.06572,-1.06397,-1.06705,-1.06135,-1.07328,-1.05815,-1.0654,-1.06871,-1.06746,-1.05665,-1.0729,-1.0629,-1.06577,-1.06396,-1.06706,-1.06139,-1.07327,-1.05816,-1.06543,-1.06871,-1.06747]
[-1.39669,-1.39675,-1.3967,-1.39641,-1.39283,-1.37984,-1.37079,-1.36922,-1.36865,-1.3682,-1.36778,-1.36735,-1.36687,-1.36634,-1.36574,-1.36491,-1.36403,-1.3635,-1.3632,-1.36303,-1.3629,-1.36281,-1.36273,-1.36267,-1.36261,-1.36256,-1.36252,-1.36249,-1.36246,-1.36244,-1.36242,-1.3624,-1.36239,-1.36238,-1.36237,-1.36236,-1.36235,-1.36235,-1.36234,-1.36234,-1.36233,-1.36233,-1.36232,-1.36231,-1.36231,-1.3623,-1.36229,-1.36228,-1.36227,-1.36225,-1.36222,-1.36229,-1.36211,-1.36124,-1.3549,-1.34135,-1.33304,-1.32951,-1.32731,-1.32591,-1.32501,-1.32444,-1.32408,-1.32384,-1.32367,-1.32354,-1.32342,-1.32332,-1.32322,-1.32312,-1.32301,-1.3229,-1.32277,-1.32263,-1.32249,-1.32235,-1.3222,-1.32206,-1.32192,-1.3218,-1.32167,-1.32154,-1.32138,-1.32119,-1.32103,-1.32092,-1.32084,-1.32078,-1.32072,-1.32066,-1.3206,-1.32054,-1.32047,-1.32039,-1.32032,-1.32025,-1.3202,-1.32015,-1.32011,-1.32008,-1.32006,-1.32021,-1.32002,-1.31929,-1.3121,-1.29195,-1.27958,-1.27542,-1.2728,-1.27032,-1.26706,-1.28007,-1.27293,-1.27041,-1.26802,-1.26461,-1.27842,-1.27179,-1.26942,-1.26723,-1.264,-1.26031,-1.27654,-1.26977,-1.26716,-1.2648,-1.26167,-1.25773,-1.27275,-1.26623,-1.26437,-1.26299,-1.26097,-1.25778,-1.27217,-1.2659,-1.26417,-1.26289,-1.26095,-1.2578,-1.27199,-1.26582,-1.26415,-1.26293,-1.2611,-1.25804,-1.27188,-1.26584,-1.26423,-1.2631,-1.26143,-1.25872,-1.25845,-1.25784,-1.2507,-1.22524,-1.20974,-1.19492,-1.19478,-1.18602,-1.18654,-1.19356,-1.18982,-1.18126,-1.18396,-1.17768,-1.18146,-1.17635,-1.18107,-1.17616,-1.18096,-1.17602,-1.18085,-1.17588,-1.18076,-1.17579,-1.18071,-1.17573,-1.18068,-1.17569,-1.18065,-1.17565,-1.18061,-1.17558,-1.18056,-1.17549,-1.18049,-1.17537,-1.1804,-1.18035,-1.17523,-1.18025,-1.1802,-1.17505,-1.18014,-1.1801,-1.18007,-1.18004,-1.18,-1.17481,-1.17992,-1.1801,-1.17458,-1.17792,-1.15942,-1.106,-1.07466,-1.09284,-1.06536,-1.05884,-1.06708,-1.05662,-1.0617,-1.07334,-1.06281,-1.06095,-1.06841,-1.06762,-1.06131,-1.06203,-1.06702,-1.06118,-1.07329,-1.05812,-1.06531,-1.06871,-1.06745,-1.05658,-1.07291,-1.06288,-1.06572,-1.06397,-1.06705,-1.06135,-1.07328,-1.05815,-1.0654,-1.06871,-1.06746,-1.05665,-1.0729,-1.0629,-1.06577,-1.06396,-1.06706,-1.06139,-1.07327,-1.05816,-1.06543,-1.06871,-1.06747]
32×26 Array{Float64,2}:
  0.0504026    -0.0686588   -0.108832    -0.0426914   -0.0543279   -0.19452     -0.0248125   -0.00316385   -0.0872004   -0.164055      0.0350168    -0.031668     0.0361577    0.0171125    -0.168116     -0.119762    -0.0894468  -0.0610897     0.0733321   -0.0727875    -0.0568223    0.0741338    0.102294     0.109272    -0.134817      0.107303   
  0.100043     -0.0648905   -0.0359719   -0.115126     0.014156     0.145393    -0.133419     0.0597617    -0.0727171   -0.0124513     0.137032      0.04055      0.169982     0.084318      0.0708257    -0.0379135    0.058164   -0.0075064     0.118654    -0.270326     -0.0125965    0.0903817    0.0548445    0.0755068    0.00041761   -0.0129335  
  0.0898363     0.0174576    0.0243721   -0.132896     0.0770156   -0.0355392   -0.00769003   0.10415      -0.0138936    0.0419106    -0.0766194    -0.0229953   -0.123215     0.0438436     0.00892368    0.0538681    0.0270233  -0.0643164     0.0211999    0.103508      0.107791     0.0657791   -0.0512871    0.0420875   -0.0861225    -0.00287271 
  0.0478342     0.0378476    0.0170633   -0.0331173    0.0937442    0.00353085  -0.103042     0.000659381   0.0713877    0.157522     -0.0314867     0.128386    -0.0979314   -0.095358      0.0446924     0.00568086   0.0553226   0.0909556     0.106591     0.0642675     0.00880735  -0.0836967    0.0684723   -0.021062    -0.000845448  -0.000893165
  0.209365     -0.0834664   -0.0707128    0.101543     0.0566436    0.101546    -0.138003     0.0687236     0.00610504  -0.130934      0.108214     -0.00500411  -0.182181    -0.0599408     0.111796     -0.0177227    0.0927567  -0.0470708     0.00492544  -0.109034      0.0465721   -0.0523625    0.0532299   -0.0341401   -0.124759      0.0842901  
 -0.041448      0.147374    -0.100691    -0.0340902   -0.00593505  -0.150904    -0.170044     0.00855148    0.143328     0.0054229    -0.0643225     0.0374428    0.0716945   -0.0664727     0.10665      -0.0997837    0.0486699  -0.0388461     0.151432    -0.00382502   -0.0334672    0.0170039   -0.024204     0.0316667   -0.0481033     0.0162964  
 -0.0571031    -0.0526243   -0.0105138    0.169331    -0.0488375   -0.0788745   -0.0314244    0.018915     -0.0655735   -0.168869     -0.0219756     0.0130624    0.138857    -0.00184739   -0.180163     -0.115848     0.267737   -0.131777     -0.0665673   -0.0149287    -0.21539      0.142777    -0.131026     0.0442712   -0.047905      0.00398085 
  0.0560516    -0.0163031   -0.0871268   -0.106145    -0.0977192    0.0244865   -0.134006     0.121072      0.0441029    0.0741121     0.105591     -0.0403124   -0.124838     0.0605582    -0.132476      0.00459961  -0.0165209   0.233616      0.0757536   -0.0153851     0.175128    -0.041948    -0.200655     0.103299    -0.0678461    -0.0276005  
  0.0942549    -0.0699155    0.153979    -0.0733105   -0.42398     -0.197398    -0.00120281   0.0840856     0.152559     0.0587252     0.126469      0.0200291    0.24885     -0.0410652    -1.85214      -0.0972545    0.370859    0.0643819    -0.0115652   -0.130941     -0.501389    -0.20986      0.115917    -0.097326    -0.10045       0.410714   
  0.150963     -0.067253     0.0688282   -0.09961      0.20538     -0.162365    -0.00133537   0.0833309     0.161965     0.0111265     0.0287266     0.701379     0.0758437   -0.000996297   0.367147     -0.0969023    0.36906     0.106523     -0.050796    -0.129799      0.0393382    0.191618     0.0412174   -0.0763245   -0.0992679     0.55697    
  0.127683     -0.0718333    0.153196    -0.175077     0.165462     0.383683    -0.00117396   0.0839623     0.167579    -0.173658     -0.0251336    -0.222156    -0.11851      0.170086     -0.47456      -0.0973346    0.37025     0.0180123    -0.0983476   -0.129376      0.649752     0.195137     0.0718776   -0.0925662   -0.104066     -0.308455   
 -0.000141845  -0.136938     0.102417    -0.116752    -0.796978    -0.22739     -0.00102326   0.0844907     0.152049     0.026039     -0.0657422    -0.419805    -0.0927493   -0.225257      1.27523      -0.0985365    0.367725   -0.0404543     0.0254268   -0.133662      0.0810615   -0.265034     0.00277473  -0.0781885   -0.102516      0.438702   
  0.0958657    -0.100409     0.147454    -0.039587     0.105297     0.0307761   -0.09623     -0.051141      0.0619184    0.0324892    -0.00223133   -0.0320594   -0.225091     0.103783     -0.0777339     0.131551    -0.0707633  -0.0314527     0.0926554   -0.0427904    -0.254568     0.075149     0.0942752    0.00847593   0.0539971    -0.0549298  
 -0.0469019    -0.0360864    0.00311104  -0.104059     0.0565791   -0.042843     0.0970793    0.18856      -0.0812401   -0.042284      0.180019     -0.114207     0.136701    -0.12097      -0.000179617   0.159647     0.042512    0.0795475     0.037584     0.11814       0.0848191    0.00829488  -0.00674819  -0.0773542   -0.066301      0.0197451  
 -0.0660522    -0.0323207   -0.0321596    0.0583613    0.0209057   -0.227163    -0.0247752    0.0820904     0.107431    -0.0585328    -0.236791      0.0871       0.0240255   -0.0657326     0.118986      0.127933    -0.0160399   0.0973276     0.148496    -0.160035     -0.126879    -0.0027346    0.0178041    0.170695     0.00271628    0.0918566  
 -0.124983     -0.0530971    0.00336415  -0.086792     0.037891    -0.148278    -0.0782885    0.108337      0.0531506   -0.0236065     0.0487241     0.00600159   0.0603361    0.116701     -0.0631929     0.00135397  -0.115393   -0.0457183     0.119821    -0.0314379     0.0550549    0.114955    -0.0975708   -0.249483     0.0965332    -0.113114   
 -0.0430527     0.00403091  -0.0456518   -0.141805    -0.486352     0.0347297   -0.00824295  -0.0397492    -0.0270522   -0.0569156     0.00547149    0.3179      -0.165725     0.0629539    -0.034729     -0.214015     0.18144    -0.00768618   -0.0788177    0.348931     -0.00561968  -0.882281    -0.0610604   -0.401894    -0.0536971    -0.0733206  
  0.0153761    -0.0417591   -0.0224919   -0.142492     0.194024     0.226013    -0.00832601  -0.0515609    -0.00426024   0.0245681     0.00944977   -0.182643    -0.233298     0.0507825    -0.033249     -0.241757     0.176483   -0.0660854     0.0210528    0.112578      0.0160379    0.489068    -0.0585979    0.0372462   -0.00564948   -0.097782   
 -0.0958756    -0.0836013    0.0231134   -0.0180788    0.239461     0.0793439    0.292073     0.00150416    0.0474092    0.172036      0.110166      0.154974    -0.180465    -0.135306     -0.128935     -0.0421406   -0.648486   -0.149039      0.140136    -0.000854026  -0.0891737   -0.0673883    0.0699012    0.0131611   -0.0912155     0.0318837  
  0.12558      -0.0881312    0.0779629   -0.00805539   0.18695      0.121492     0.23971      0.00255552    0.0988393   -0.029456      0.0802342     0.149442    -0.180092    -0.177933     -0.126513     -0.00524853   1.28571    -0.0369794     0.0892006    0.0347342     0.0363321   -0.0606975    0.0612769    0.00480815   0.0363903     0.0232438  
 -0.0406473     0.0336464   -0.147125    -0.125406     0.0220963   -0.167512     0.158107    -0.0392907     0.0843262    0.11668       0.00094937   -0.0688234   -0.0118891    0.0290371    -0.0666389     0.148969    -0.122308   -0.0752434     0.132338    -0.0524917     0.023925    -0.0429129    0.0151038    0.123851    -0.114552     -0.0430419  
 -0.0226752     0.0294256   -0.0270297   -0.0081711    0.102484    -0.00699512  -0.0686817   -0.0300149     0.0685853   -0.0391335    -0.192881      0.0639471    0.00852933  -0.0326224     0.0677386     0.0229716    0.0623173   0.000466914   0.0465938   -0.00295637    0.117049     0.0444805    0.0317877   -0.00666425   0.0146506     0.029979   
 -0.135212      0.104971    -0.0900827    0.0668819   -0.0686624   -0.0852705   -0.068377    -0.0215102    -0.179171     0.0959085     0.0246103     0.0539667    0.0398499    0.158957      0.0570541    -0.0643186    0.0998007  -0.0262396     0.00624449  -0.0951131     0.00987515  -0.253491    -0.013359     0.0389014    0.156857     -0.191815   
 -0.0530129    -0.177758    -0.118782    -0.14664     -0.167289     0.0945103   -0.127406     0.00918463    0.0221871    0.108658      0.150704     -0.0211835    0.0604714   -0.126899      0.00347223   -0.00112698  -0.0342348  -0.034043     -0.0348485    0.00370986   -0.0585629    0.0192964   -0.0583999   -0.0551912    0.0566755     0.124216   
  0.102518      0.0202011   -0.0625909   -0.0667763   -0.0887307   -0.0213208   -0.103398    -0.0748447    -0.0381465    0.0815102    -0.0205145     0.160216    -0.0207043    0.150977     -0.0502331    -0.029017     0.0541729  -0.0875141     0.102049    -0.0825943    -0.317088     0.0618703    0.00101302  -0.200264     0.111461      0.21693    
 -0.129641     -0.0653856   -0.205986     0.0242885   -0.107882     0.0139515    0.14508     -0.038011      0.215599    -0.111723     -0.168585      0.0429529   -0.103971    -0.177562      0.161643      0.0462575    0.0313421  -0.0696258     0.00995171   0.164977      0.0815505   -0.0956768   -0.0625958   -0.0951611   -0.1433        0.00329203 
 -0.0831118     0.0492402    0.230853    -0.101674    -0.00249088  -0.188185    -0.175496     0.0141008    -0.0661681   -0.256075      0.0290865    -0.236604     0.0808844   -0.0468844    -0.0842712     0.121278     0.023635   -0.0328992     0.00473003  -0.174395     -0.118974    -0.181954    -0.141026     0.0142935   -0.0949674     0.0331052  
 -0.103437      0.0722389   -0.0615958    0.0579133   -0.104506     0.0414682   -0.14015      0.106908     -0.065718     0.165839     -0.0870108    -0.114942     0.0721196   -0.045993      0.146768      0.0182171   -0.10325    -0.0671219     0.0124604   -0.0577208    -0.147275     0.0208158    0.106519    -0.0557893    0.053379      0.0998425  
  0.140722     -0.0677497    0.00368456  -0.0160355    0.0271672   -0.0309955   -0.118126     0.0630889    -0.117242    -0.0465847    -0.0736105    -0.0401663    0.0219935   -0.0866335    -0.0649541    -0.088542    -0.0180728  -0.0836663     0.0696565   -0.0911225     0.0503074   -0.0586288    0.0016193   -0.0330384    0.029842     -0.0272645  
 -0.106625      0.0730231   -0.0705638   -0.179349     0.067276    -0.0641107    0.12123      0.0423169    -0.0397335   -0.0360232    -0.11364      -0.0145386   -0.00334119  -0.0249952     0.0443409     0.0132465    0.127962   -0.081038      0.10941      0.00568448   -0.103584     0.0862781    0.0171059   -0.0888886    0.00525438    0.141279   
  0.158422     -0.0827523   -0.114989    -0.00249068  -0.0789061   -0.0840826    0.131436     0.116135     -0.0356023   -0.0397651    -0.0686173    -0.162605     0.0266376    0.227635     -0.0251701     0.0674924   -0.0206048  -0.0788032     0.0907049    0.0744255    -0.0721418   -0.0292813    0.0529068   -0.140843     0.0828285     0.0859319  
 -0.145028      0.0232618   -0.0268439    0.0266783    0.175673     0.0286734   -0.151108    -0.0014166     0.13299     -0.000949248  -0.000298244   0.0345992   -0.0614052   -0.0278738    -0.0946843    -0.176093    -0.140854   -0.0201392    -0.0147976    0.00919821    0.0759993    0.0677547   -0.14474     -0.07386      0.028426      0.0654339  INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 31
INFO: iteration 1, average log likelihood -1.056674
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 23 26 31
INFO: iteration 2, average log likelihood -1.045865
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 16 17 18 31
INFO: iteration 3, average log likelihood -1.056411
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 23 26 31
INFO: iteration 4, average log likelihood -1.045639
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 16 17 18 31
INFO: iteration 5, average log likelihood -1.056400
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 23 26 31
INFO: iteration 6, average log likelihood -1.045596
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 16 17 18 31
INFO: iteration 7, average log likelihood -1.056399
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 23 26 31
INFO: iteration 8, average log likelihood -1.045585
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 16 17 18 31
INFO: iteration 9, average log likelihood -1.056399
WARNING: Variances had to be floored 2 5 9 10 11 12 14 15 17 18 23 26 31
INFO: iteration 10, average log likelihood -1.045582
INFO: EM with 100000 data points 10 iterations avll -1.045582
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.920789e+05
      1       6.638458e+05      -2.282331e+05 |       32
      2       6.341528e+05      -2.969301e+04 |       32
      3       6.172217e+05      -1.693105e+04 |       32
      4       6.066342e+05      -1.058751e+04 |       32
      5       6.002258e+05      -6.408376e+03 |       32
      6       5.964796e+05      -3.746266e+03 |       32
      7       5.934568e+05      -3.022718e+03 |       32
      8       5.909535e+05      -2.503306e+03 |       32
      9       5.889094e+05      -2.044103e+03 |       32
     10       5.876944e+05      -1.214984e+03 |       32
     11       5.871046e+05      -5.898439e+02 |       32
     12       5.867924e+05      -3.122304e+02 |       32
     13       5.865995e+05      -1.928310e+02 |       32
     14       5.864486e+05      -1.509381e+02 |       31
     15       5.863027e+05      -1.459106e+02 |       32
     16       5.861102e+05      -1.924822e+02 |       32
     17       5.857791e+05      -3.311149e+02 |       32
     18       5.852877e+05      -4.914203e+02 |       32
     19       5.848350e+05      -4.526995e+02 |       32
     20       5.845581e+05      -2.768832e+02 |       32
     21       5.844113e+05      -1.467919e+02 |       32
     22       5.843316e+05      -7.970802e+01 |       32
     23       5.842867e+05      -4.492410e+01 |       31
     24       5.842583e+05      -2.835590e+01 |       32
     25       5.842312e+05      -2.707637e+01 |       30
     26       5.842049e+05      -2.634527e+01 |       31
     27       5.841831e+05      -2.176122e+01 |       32
     28       5.841685e+05      -1.465640e+01 |       28
     29       5.841589e+05      -9.597437e+00 |       26
     30       5.841515e+05      -7.340002e+00 |       21
     31       5.841481e+05      -3.463447e+00 |       20
     32       5.841463e+05      -1.733540e+00 |       16
     33       5.841456e+05      -6.904305e-01 |        8
     34       5.841454e+05      -2.334642e-01 |        6
     35       5.841453e+05      -8.613871e-02 |        4
     36       5.841452e+05      -1.100219e-01 |        4
     37       5.841451e+05      -1.375373e-01 |        5
     38       5.841449e+05      -2.120141e-01 |        6
     39       5.841447e+05      -1.361212e-01 |        3
     40       5.841446e+05      -1.151854e-01 |        4
     41       5.841445e+05      -1.485096e-01 |        2
     42       5.841444e+05      -7.229357e-02 |        0
     43       5.841444e+05       0.000000e+00 |        0
K-means converged with 43 iterations (objv = 584144.3980620811)
INFO: K-means with 32000 data points using 43 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.307113
INFO: iteration 2, average log likelihood -1.278101
INFO: iteration 3, average log likelihood -1.251024
INFO: iteration 4, average log likelihood -1.218805
INFO: iteration 5, average log likelihood -1.165709
WARNING: Variances had to be floored 1 4 13 17
INFO: iteration 6, average log likelihood -1.094866
WARNING: Variances had to be floored 2 12 31
INFO: iteration 7, average log likelihood -1.095580
WARNING: Variances had to be floored 8 9 11 23
INFO: iteration 8, average log likelihood -1.055176
WARNING: Variances had to be floored 1 4 17
INFO: iteration 9, average log likelihood -1.064848
WARNING: Variances had to be floored 7 13
INFO: iteration 10, average log likelihood -1.074308
WARNING: Variances had to be floored 2 8 12 23 31
INFO: iteration 11, average log likelihood -1.030738
WARNING: Variances had to be floored 1 4 9 11 17
INFO: iteration 12, average log likelihood -1.054230
INFO: iteration 13, average log likelihood -1.105543
WARNING: Variances had to be floored 13 23
INFO: iteration 14, average log likelihood -1.046705
WARNING: Variances had to be floored 1 2 4 7 8 12 17 31
INFO: iteration 15, average log likelihood -1.019344
WARNING: Variances had to be floored 9 11
INFO: iteration 16, average log likelihood -1.099728
INFO: iteration 17, average log likelihood -1.083695
WARNING: Variances had to be floored 1 13 17 23
INFO: iteration 18, average log likelihood -1.030772
WARNING: Variances had to be floored 2 4 8 12 31
INFO: iteration 19, average log likelihood -1.054472
WARNING: Variances had to be floored 9 11
INFO: iteration 20, average log likelihood -1.064788
WARNING: Variances had to be floored 1 13 17
INFO: iteration 21, average log likelihood -1.055801
WARNING: Variances had to be floored 4 7 23
INFO: iteration 22, average log likelihood -1.074696
WARNING: Variances had to be floored 2 31
INFO: iteration 23, average log likelihood -1.046295
WARNING: Variances had to be floored 1 8 9 11 12 13 17
INFO: iteration 24, average log likelihood -1.019238
WARNING: Variances had to be floored 4
INFO: iteration 25, average log likelihood -1.104018
WARNING: Variances had to be floored 23
INFO: iteration 26, average log likelihood -1.056883
WARNING: Variances had to be floored 1 2 7 12 13 17 31
INFO: iteration 27, average log likelihood -1.013933
WARNING: Variances had to be floored 4 8 9 11
INFO: iteration 28, average log likelihood -1.079691
INFO: iteration 29, average log likelihood -1.092896
WARNING: Variances had to be floored 1 17 23
INFO: iteration 30, average log likelihood -1.032795
WARNING: Variances had to be floored 2 4 8 9 12 13 24
INFO: iteration 31, average log likelihood -1.038115
WARNING: Variances had to be floored 11
INFO: iteration 32, average log likelihood -1.089984
WARNING: Variances had to be floored 1 7 17
INFO: iteration 33, average log likelihood -1.058771
WARNING: Variances had to be floored 4 23
INFO: iteration 34, average log likelihood -1.063854
WARNING: Variances had to be floored 2 8 9 12 13 24
INFO: iteration 35, average log likelihood -1.025744
WARNING: Variances had to be floored 1 11 17
INFO: iteration 36, average log likelihood -1.059072
WARNING: Variances had to be floored 4 23
INFO: iteration 37, average log likelihood -1.070124
WARNING: Variances had to be floored 7
INFO: iteration 38, average log likelihood -1.037188
WARNING: Variances had to be floored 1 2 8 9 12 13 17 24
INFO: iteration 39, average log likelihood -0.986287
WARNING: Variances had to be floored 4 11 23
INFO: iteration 40, average log likelihood -1.081461
INFO: iteration 41, average log likelihood -1.079842
WARNING: Variances had to be floored 1 17
INFO: iteration 42, average log likelihood -1.017967
WARNING: Variances had to be floored 2 4 7 8 9 11 12 13 23 24
INFO: iteration 43, average log likelihood -0.998262
INFO: iteration 44, average log likelihood -1.108143
WARNING: Variances had to be floored 1 17
INFO: iteration 45, average log likelihood -1.055037
WARNING: Variances had to be floored 13 23
INFO: iteration 46, average log likelihood -1.043245
WARNING: Variances had to be floored 2 4 8 9 11 12 17 24
INFO: iteration 47, average log likelihood -0.999855
WARNING: Variances had to be floored 1
INFO: iteration 48, average log likelihood -1.086367
WARNING: Variances had to be floored 7 23
INFO: iteration 49, average log likelihood -1.059376
WARNING: Variances had to be floored 4 13 17
INFO: iteration 50, average log likelihood -1.023893
INFO: EM with 100000 data points 50 iterations avll -1.023893
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.100162    -0.0643733   -0.0356302   -0.115824     0.0166967    0.14831     -0.138937     0.0605795    -0.0709617   -0.00718945    0.142443     0.040441     0.171194    0.0850839    0.0737508   -0.0381203    0.0560896   -0.00608103   0.118527    -0.275954    -0.0125685    0.0899731     0.0532444    0.0737519    0.000388955  -0.0167018 
 -0.0125311   -0.0202005   -0.028366    -0.14261     -0.0951998    0.139594    -0.00975674  -0.0523099    -0.0194766   -0.0106117     0.010677     0.0305307   -0.198728    0.0563955   -0.0330975   -0.233263     0.174835    -0.0398386   -0.0160696    0.212861     0.00952783  -0.0958711    -0.0605939   -0.152004    -0.0175857    -0.0886075 
 -0.0613455   -0.0575813   -0.0145223    0.173154    -0.0519207   -0.0767398   -0.0360082    0.0221165    -0.0737333   -0.171786     -0.00804758   0.00998386   0.144629   -0.00200426  -0.179857    -0.116325     0.269343    -0.130431    -0.0651824   -0.0181916   -0.221058     0.136547     -0.127389     0.0391929   -0.0467558     0.00328158
 -0.0661373   -0.0186817    0.0172587   -0.123712     0.0960135   -0.0461141    0.0929257    0.219449     -0.121549    -0.0528853     0.277179    -0.15712      0.168748   -0.147596    -0.0242771    0.203256     0.0575409    0.0876988    0.0506144    0.118413     0.0651399    0.00864082    0.00607007  -0.0806458   -0.0689811     0.0225413 
 -0.0435504   -0.031115    -0.0130427    0.0521391   -0.0605481   -0.0446184   -0.18385      0.134339     -0.137129    -0.036111     -0.0210222   -0.103105    -0.0536447  -0.0999298    0.0139669   -0.153796     0.0202335    0.0737943    0.00774238  -0.0593912    0.100558    -0.0767136     0.0488302    0.0222524    0.155445     -0.113657  
 -0.152501     0.0278159   -0.0269907    0.0303942    0.175724     0.0314949   -0.156632    -0.00110338    0.130001    -0.00270804    0.00540027   0.031355    -0.0612855  -0.0298519   -0.10662     -0.177692    -0.142087    -0.0166477   -0.00991362   0.00579028   0.0706096    0.0671095    -0.140777    -0.0748271    0.0345468     0.0642489 
 -0.134015    -0.0540493    0.0226068   -0.100713     0.02536     -0.185765    -0.0792594    0.107121      0.0660845   -0.0314752     0.056532    -0.0016936    0.0635686   0.130793    -0.0748392    0.00611567  -0.124683    -0.0479885    0.125135    -0.035344     0.0584165    0.186832     -0.106882    -0.273671     0.100836     -0.122092  
  0.230045    -0.087272    -0.0830264    0.0950941    0.0677127    0.104452    -0.133215     0.0731794     0.00600109  -0.184993      0.115171    -0.0200191   -0.229564   -0.0808501    0.0935413   -0.00536218   0.105805    -0.0659873   -0.0203911   -0.111912     0.0445326   -0.0593751     0.094045    -0.0586274   -0.112705      0.0738553 
  0.00477105   0.114473     0.0155926   -0.242873     0.188046    -0.0773034    0.131018    -0.0876872    -0.0349193    0.000433588  -0.0272663   -0.0172785   -0.0366187  -0.0220155   -0.0119438    0.0792067    0.228033    -0.190247     0.112632     0.0870157   -0.0958035    0.180673      0.0216948    0.0357032    0.0205419     0.235393  
 -0.029392     0.134445    -0.0996004   -0.0311741    0.00405604  -0.137694    -0.175808     0.0202589     0.1378      -0.0101616    -0.0472213    0.0378074    0.0626495  -0.0712898    0.1033      -0.117129     0.0427868   -0.0413579    0.14576     -0.0221731   -0.0400387    0.0117745    -0.0167907    0.0281212   -0.0558765     0.0162984 
 -0.0877994   -0.0345523   -0.0373888    0.0667471    0.0163637   -0.219738    -0.0333352    0.0830408     0.102937    -0.0617251    -0.239872     0.0803393    0.0275498  -0.061573     0.115555     0.126225    -0.0194856    0.0964062    0.141064    -0.160365    -0.124406    -0.00319795    0.0169557    0.166586     0.00162521    0.0845301 
  0.168239    -0.0852051   -0.115428     0.0144089   -0.0737119   -0.0841301    0.121663     0.117037     -0.046537    -0.0455739    -0.0628543   -0.175634     0.0275131   0.24731     -0.0300325    0.0645665   -0.0171193   -0.0811654    0.110144     0.0734803   -0.103545    -0.0330935     0.0583562   -0.146728     0.0900317     0.0784105 
  0.322745    -0.103565     0.0147062   -0.0857085    0.114441    -0.0214404   -0.0489963   -0.00966694   -0.108036    -0.0605846    -0.128364     0.029284     0.093666   -0.0649119   -0.138703    -0.00151627  -0.0672047   -0.242992     0.126464    -0.124352     0.0044998   -0.0394115    -0.0473863   -0.0841645   -0.0851811     0.0597103 
  0.101933     0.0179187   -0.0636804   -0.0644412   -0.0854186   -0.0211507   -0.103275    -0.0794267    -0.0383105    0.0815827    -0.020708     0.159978    -0.0216302   0.151473    -0.0512307   -0.028221     0.055776    -0.0875378    0.101144    -0.0849358   -0.320754     0.0624187     0.00199214  -0.200066     0.112458      0.218794  
 -0.0295067    0.0387325   -0.0255859   -0.0194715    0.170957    -0.0876108   -0.049916    -0.0470082     0.0428899   -0.0114099    -0.119429     0.0244915    0.0476834  -0.158048     0.0369105    0.140665     0.0389776    0.0698739   -0.0419178   -0.00987106   0.145017    -0.0688856    -0.00704359  -0.00092668   0.0976824     0.126927  
  0.0919813   -0.0310541   -0.0856234   -0.0660142   -0.0812089    0.0403769   -0.137936     0.105525      0.0271948    0.0653677     0.113833    -0.0311262   -0.125298    0.0468276   -0.0777863    0.00282105  -0.00188461   0.18776      0.0655066   -0.0380829    0.162197    -0.0438615    -0.164023     0.0807227   -0.0838014    -0.0018787 
  0.0969269   -0.0824896    0.11805     -0.117532    -0.164283    -0.0303736   -0.00135238   0.0843283     0.158491    -0.0288401     0.0194351    0.0405828    0.0259397  -0.00559774  -0.226275    -0.0965537    0.367911     0.0400228   -0.0387335   -0.131094     0.0742233   -0.00128185    0.0588344   -0.0865283   -0.10081       0.242016  
 -0.0593608   -0.178858    -0.120889    -0.14637     -0.166966     0.0948725   -0.127079     0.00855972    0.0221909    0.105858      0.150824    -0.0189036    0.0542394  -0.12301      0.00338557  -0.00368189  -0.0321014   -0.035076    -0.0353924    0.00170459  -0.0587593    0.014792     -0.0593971   -0.0575712    0.0579272     0.121325  
 -0.0954553    0.0716807    0.0553754    0.0570222    0.195143    -0.0800198   -0.0921524   -0.100721      0.0739466   -0.00119434   -0.10371      0.0640235   -0.138724   -0.00516374   0.060403     0.0143863    0.00531353   0.112125     0.155844     0.133267    -0.0428749   -0.0887464     0.00517021   0.12768     -0.0433487     0.145419  
  0.051105    -0.068455    -0.108718    -0.0444576   -0.0568123   -0.194443    -0.0249528   -0.0019676    -0.0889277   -0.164531      0.0378205   -0.0349067    0.037465    0.0190811   -0.167307    -0.118043    -0.0904583   -0.0611511    0.0733565   -0.0742344   -0.0585295    0.0721426     0.103554     0.109379    -0.133328      0.104962  
  0.198629     0.00480219  -0.0120845   -0.110087    -0.0217196    0.0660092   -0.126492     0.122079      0.0701616    0.320994      0.0565407    0.194683    -0.0547151  -0.191607     0.0275436    0.00110766   0.0952603    0.0702852    0.0507969   -0.0193934    0.0615818   -0.0892609     0.118353    -0.180117     0.0425924    -0.154051  
 -0.0882187    0.0531185    0.229934    -0.0999454   -0.00452428  -0.187936    -0.174828     0.0132973    -0.0701881   -0.256427      0.0356461   -0.23374      0.0697466  -0.0449667   -0.0823586    0.120601     0.0236933   -0.0335347    0.00375497  -0.175018    -0.118915    -0.183204     -0.140195     0.0142915   -0.0921473     0.0284148 
 -0.167963     0.116249    -0.102728     0.0723865   -0.0581371   -0.0944334   -0.0684701   -0.0195213    -0.210278     0.125278      0.00675563   0.0544975    0.0284332   0.177333     0.0558129   -0.0590333    0.0875595   -0.0247288    0.0106342   -0.0900678    0.00540015  -0.289862     -0.0177915    0.0551001    0.167522     -0.23032   
 -0.136084    -0.0613912   -0.20812      0.0236977   -0.107793     0.0139106    0.141279    -0.0343999     0.216336    -0.117003     -0.168239     0.0427779   -0.101892   -0.177939     0.15833      0.046319     0.0271608   -0.0697209    0.0105026    0.162988     0.0948238   -0.0998168    -0.0633852   -0.0953232   -0.142029      0.00314475
 -0.0400068    0.033498    -0.147174    -0.125343     0.0220288   -0.167502     0.157786    -0.0394274     0.0850277    0.116731      0.0010892   -0.0689037   -0.0137544   0.0286447   -0.0671226    0.149376    -0.122101    -0.0748228    0.132808    -0.0530431    0.0224613   -0.0430277     0.0154254    0.123327    -0.114323     -0.0432659 
  0.0773256   -0.104885     0.00175038  -0.0298288   -0.0362548    0.00721803   0.135785     0.0531194     0.11971      0.061952     -0.17302      0.0623242   -0.0093745  -0.00678976   0.0985079   -0.023801    -0.0285437   -0.0253119   -0.0419048    0.0726982    0.228106     0.00267328   -0.133183    -0.0571299   -0.0445408    -0.0107167 
 -0.0103653    0.0372566   -0.0331547    0.00140955  -0.0215789    0.104431    -0.120947     0.0135049     0.0565531   -0.0678739    -0.244843     0.100714    -0.0330624   0.158366     0.104255    -0.155647     0.113337    -0.090099     0.163816    -0.00858631   0.0687759    0.156289      0.0867772   -0.00558083  -0.0750134    -0.116005  
  0.0925235    0.0661642   -0.00565312  -0.18147      0.125138    -0.0581019   -0.061376     0.122911     -0.0570652    0.0299583    -0.00512635  -0.0713817   -0.146811    0.0627318   -0.050812     0.107977     0.0425041   -0.0524453    0.0438618    0.134022     0.046019     0.0822193    -0.0082779    0.0759057   -0.101288      0.0220329 
 -0.20362      0.0286049   -0.140477    -0.111792    -0.040816    -0.0539845    0.111525     0.176324     -0.0303414   -0.0640424    -0.195721    -0.0110541    0.0290561  -0.0274061    0.106609    -0.0596349    0.048086     0.0262407    0.109581    -0.0646081   -0.106005    -0.000128551   0.0130921   -0.189662    -0.0074094     0.0593703 
  0.093988    -0.100368     0.146999    -0.0399406    0.105527     0.0310489   -0.0995528   -0.0521571     0.0607202    0.0322573    -0.00193101  -0.0333289   -0.22566     0.103182    -0.0809709    0.133538    -0.0710409   -0.0343141    0.0925262   -0.043438    -0.256156     0.075945      0.0941228    0.00857887   0.0515002    -0.0534017 
  0.0138051   -0.0857617    0.0513431   -0.0150015    0.213742     0.099459     0.265428     0.000995937   0.0722486    0.0776705     0.0964903    0.152383    -0.181226   -0.154986    -0.126964    -0.0247415    0.308396    -0.0953245    0.113745     0.0172038   -0.0249158   -0.0637852     0.0656391    0.0088075   -0.028855      0.0273699 
 -0.106226     0.0707366   -0.0632703    0.0640935   -0.10573      0.0415039   -0.144672     0.109969     -0.0724931    0.165804     -0.0874027   -0.114679     0.0893421  -0.0518019    0.14699      0.0237347   -0.113416    -0.0625166    0.00414088  -0.0601352   -0.16478      0.0162477     0.108207    -0.0612667    0.0533298     0.0986786 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 1 2 8 9 11 12 24
INFO: iteration 1, average log likelihood -1.019788
WARNING: Variances had to be floored 1 2 8 9 11 12 17 23 24
INFO: iteration 2, average log likelihood -0.977504
WARNING: Variances had to be floored 1 2 4 7 8 9 11 12 13 24
INFO: iteration 3, average log likelihood -0.980319
WARNING: Variances had to be floored 1 2 8 9 11 12 17 23 24
INFO: iteration 4, average log likelihood -0.998700
WARNING: Variances had to be floored 1 2 8 9 11 12 24
INFO: iteration 5, average log likelihood -0.995073
WARNING: Variances had to be floored 1 2 4 7 8 9 11 12 13 17 23 24
INFO: iteration 6, average log likelihood -0.959882
WARNING: Variances had to be floored 1 2 8 9 11 12 24
INFO: iteration 7, average log likelihood -1.018715
WARNING: Variances had to be floored 1 2 8 9 11 12 17 23 24
INFO: iteration 8, average log likelihood -0.977761
WARNING: Variances had to be floored 1 2 4 7 8 9 11 12 13 24
INFO: iteration 9, average log likelihood -0.980254
WARNING: Variances had to be floored 1 2 8 9 11 12 17 23 24
INFO: iteration 10, average log likelihood -0.998700
INFO: EM with 100000 data points 10 iterations avll -0.998700
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.0202999     0.110284     0.0392105   -0.0223643   -0.177039     -0.0309395   -0.196102     0.0749147    0.117915     0.189052     0.0958165   -0.0987919    0.0369739     0.0011077    0.0731341    0.0886382    -0.06889       0.041305    -0.0856217   -0.13669      0.050636     0.0488088    0.11039     -0.0538537    -0.0560175   -0.00511883
  0.0181672    -0.0150012    0.148738    -0.157597    -0.0623488    -0.166142     0.0974845    0.133933    -0.0805828   -0.055413    -0.0919275    0.14282      0.0193398    -0.157676     0.0545368   -0.132545      0.0122359    -0.0890317   -0.138703    -0.0956512    0.0580036    0.0242978   -0.0517464    0.0840643     0.00875074  -0.193705  
 -0.0899136    -0.0162022   -0.0789016   -0.0498642    0.0856       -0.036395     0.0922507   -0.0379578    0.0140408   -0.0304605   -0.0107213   -0.0914075    0.00777227    0.0654151    0.0135615    0.0941649    -0.00103928    0.0128861    0.0141215   -0.109623     0.115421     0.012064     0.00911329   0.12383       0.242035     0.0571183 
  0.0211865     0.0187273    0.0754309    0.0786384    0.137775      0.143892    -0.00603311  -0.0680058    0.00272587  -0.0616729    0.0336767    0.171397     0.0413188    -0.0599918   -0.0463238   -0.0440501    -0.0114785    -0.0417595   -0.113183    -0.0244165    0.138353    -0.190825     0.0116878    0.148366      0.136434    -0.0419053 
 -0.11848      -0.0344601    0.0624987   -0.0415373   -0.0723974    -0.0039323    0.0396713   -0.0141876    0.124035     0.0190726    0.111369     0.045574    -0.153258      0.0693571    0.182736    -0.00466929   -0.0851052     0.0100782    0.145757    -0.117532     0.00482583  -0.079912     0.019875    -0.120903      0.194713     0.0327587 
  0.154943      0.119713     0.04945     -0.0409318    0.110902      0.0148788    0.0563672    0.20707      0.0501399   -0.118055     0.141216    -0.00303299  -0.0501661     0.0236958   -0.170106    -0.109629      0.0811993     0.0815923   -0.0262087    0.0470279    0.0467117   -0.0993328   -0.0414724    0.0648413    -0.106755     0.0306153 
 -0.0510971     0.0775576   -0.116971    -0.00637817   0.0101923    -0.13364      0.00292077  -0.0533465   -0.0594409   -0.0119973   -0.154441     0.105461    -0.347656     -0.0450655   -0.024007    -0.0118437     0.0113749     0.0786172    0.0554584   -0.178891    -0.0307301   -0.0447345    0.0597914   -0.0826808    -0.0482263    0.119709  
  0.0324107    -0.0302381    0.0578477    0.106155    -0.0583367    -0.134648    -0.122365     0.100169    -0.0790514   -0.155939    -0.0459653   -0.0590945    0.125985      0.115344     0.143436    -0.241246     -0.0373956     0.032229    -0.100963     0.0152077    0.0608146    0.0460965   -0.0844143    0.0535232    -0.272246    -0.0563305 
 -0.0992081    -0.0111678    0.0149225    0.0240761   -0.0260163    -0.0668352   -0.0817317    0.0325317   -0.0282226   -0.0115246   -0.057345    -0.0644332   -0.00462523   -0.0535217   -0.135454     0.117282      0.0543988    -0.135954    -0.0674101    0.0218288   -0.00256244   0.0986147   -0.0188068    0.0422419    -0.014297    -0.0761254 
 -0.0398196     0.105549     0.112185    -0.207982     0.0213271     0.0990953    0.145748    -0.113817     0.0752633   -0.0567877   -0.0466079   -0.0936693   -0.108986     -0.127219    -0.0311008    0.0883824    -0.0932986     0.0192342    0.225798    -0.0231388    0.0752306    0.128604    -0.0415724   -0.0615248     0.111772     0.055232  
 -0.0322336    -0.0246313   -0.0913003    0.0464635    0.0431057     0.025827     0.0934892   -0.0068281   -0.0783666    0.0166342    0.155156    -0.101937     0.0909242     0.0227678   -0.191067    -0.0180269     0.118011     -0.0290899   -0.118469    -0.0873191   -0.128336     0.130938     0.0216475   -0.0410643    -0.0399731    0.104868  
  0.000242597  -0.00444412  -0.0190259    0.0852479    0.0379432    -0.100623    -0.112471     0.137203    -0.0174404    0.0243931    0.0799295   -0.00687447   0.179698      0.0250431    0.0509269    0.195948      0.202519     -0.074675     0.0886545   -0.0367292    0.190974    -0.0650858    0.182169     0.00564933   -0.181268    -0.0403916 
 -0.0762227     0.0589128   -0.119587    -0.0672489    0.0142567     0.0102637    0.162008     0.0502032   -0.0217746   -0.102753     0.0678774    0.008417     0.155012     -0.117649     0.0708011    0.000693668   0.229549     -0.0811587   -0.0807291    0.204198     0.0985436   -0.0200126   -0.0582018    0.102727     -0.167398    -0.0517579 
  0.296005      0.0585666   -0.0691256    0.162467    -0.3271       -0.100555     0.0798254   -0.0119394   -0.0737047    0.0905149   -0.129629     0.0973645    0.0794233    -0.159793     0.124419     0.1115       -0.115677     -0.0138188   -0.0185879   -0.019686    -0.12852     -0.0118816    0.0259679   -0.130042      0.0880729    0.0865155 
  0.0432736    -0.104564    -0.0170997    0.0782547    0.0149995     0.075229     0.0887004   -0.0660106    0.0142734    0.0633275    0.024603    -0.108812     0.00897297   -0.171059    -0.0721245   -0.00707162    0.168222      0.110499    -0.0901185    0.07206     -0.0478217   -0.0932413   -0.0504513    0.000496968  -0.0312653    0.0422329 
 -0.0594395     0.143091     0.0100633   -0.118985     0.125652      0.015569     0.087357    -0.0910718    0.0775504   -0.117944    -0.0295745   -0.00748974  -0.0791007     0.0335768    0.0166475    0.0385332     0.00758709   -0.0355778    0.0207512   -0.11949     -0.204473    -0.0170798   -0.0393718    0.0525042    -0.233721    -0.0832491 
 -0.0917367     0.0625584   -0.0149363   -0.0362865    0.0640724     0.00949437   0.0529805    0.0266545   -0.232236     0.00605343  -0.194613    -0.120042     0.000857553  -0.0156144    0.0365576   -0.115604      0.0315376     0.053112    -0.0228371   -0.136689    -0.0834472   -0.0452735   -0.195823     0.116652      0.0185119    0.107065  
 -0.0110189    -0.038337    -0.135664    -0.0409829   -0.082956      0.159945     0.0879518    0.122368    -0.13539     -0.104934    -0.0183616   -0.226273    -0.0166885    -0.148503     0.0452621    0.1711        0.0506543    -0.0224327   -0.088184     0.143503     0.0380374    0.0436841   -0.00208007  -0.0528984    -0.085024     0.174607  
  0.112085     -0.0373545   -0.0117672   -0.0988983   -0.233551     -0.0837057    0.0188458   -0.0977602   -0.0577125   -0.0387785    0.0827928   -0.209692    -0.0478388     0.0150775   -0.0999005   -0.0781048    -0.124622     -0.118943     0.0703284   -0.0163115   -0.208227     0.00403212   0.123018    -0.187449     -0.0550058    0.0208343 
 -0.0101486     0.0346217   -0.0581424    0.0173333    0.0344267     0.00702437  -0.071845    -0.102698     0.0789277   -0.122044     0.0113767   -0.00366635  -0.0104463    -0.0740985   -0.0143283   -0.160351      0.0163431     0.0145293    0.0515971    0.0498706   -0.0410799   -0.296106    -0.0742094   -0.12073      -0.0234762    0.0186767 
  0.0245964    -0.0612704   -0.10838     -0.121746    -0.0932508    -0.061509    -0.0790551   -0.103666     0.0308966    0.0423874   -0.0192502   -0.124837     0.073246      0.11702      0.123713    -0.112037     -0.0628938     0.0604475    0.103363    -0.026203     0.0296096   -0.00179966  -0.0611754   -0.0646769     0.148121     0.0904978 
 -0.0762926    -0.244855    -0.106881     0.0944902   -0.111529     -0.194017    -0.0155182   -0.0150358   -0.14229      0.12929     -0.109905    -0.0318527   -0.10732       0.0465209   -0.132141    -0.028065      0.096063     -0.10986     -0.041575    -0.155941    -0.101086    -0.168849     0.138798     0.0937527    -0.0318811   -0.118891  
  0.0186598     0.0284112    0.0748528    0.16219      0.00910434   -0.0076784   -0.140633     0.119478    -0.372795    -0.180955     0.00718403   0.142286    -0.11658      -0.0167628   -0.0405249    0.117927      0.0139351     0.127175     0.195384    -0.0880814    0.0535634   -0.0888148   -0.010658     0.0770412    -0.0977014   -0.0498241 
 -0.100717     -0.184969    -0.18841     -0.147686     0.120487     -0.180309     0.00795106  -0.166945     0.0755679    0.0616254    0.126946    -0.104015     0.0692052    -0.00216661  -0.0959369    0.121235      0.0751291     0.00856134   0.0634756    0.0955939    0.0289324    0.0548641    0.0303182   -0.130672     -0.139022    -0.0695261 
 -0.0748621    -0.0565744   -0.00640833  -0.138736     0.140673     -0.0402413   -0.075426     0.0302748   -0.0988129    0.0863481    0.0245461   -0.0546531    0.0107007     0.0098872   -0.108053     0.0615577    -0.293898     -0.0874781    0.0328322    0.00571614  -0.186982    -0.0564381    0.148016    -0.138357     -0.134133    -0.0299604 
 -0.176644     -0.115867    -0.0515506   -0.0309028    0.0662736     0.0351537   -0.0459498    0.0632458    0.0751749    0.0042733    0.224022    -0.0209941   -0.189835     -0.122683    -0.0219653    0.00484236    0.124762     -0.0884955   -0.0372501   -0.1686       0.0166033   -0.076493     0.124121    -0.0164142     0.0952481   -0.101523  
  0.0219411     0.00559734   0.0732918    0.137535    -0.0317804     0.112342     0.0330388    0.0896438    0.0785973    0.171352    -0.0809201    0.137913    -0.0131284    -0.0350615    0.072976    -0.0985066     0.0589273     0.0427622   -0.0626722    0.00032394   0.015693     0.0123194    0.0535299    0.0692198     0.0222618    0.0990341 
 -0.105648      0.102943     0.205457     0.0401884    0.0106302    -0.00320765  -0.0915571    0.0400339    0.104315    -0.124547    -0.0502425   -0.126959     0.053362      0.1047      -0.0449288    0.016297     -0.0557626     0.180054    -0.00222874   0.0749692    0.0317527   -0.10356      0.0888734   -0.00801317    0.0070465    0.0316129 
 -0.257654     -0.0632505    0.0643782    0.0726284    0.183455     -0.0181611    0.0366871   -0.0724604   -0.043553     0.0543677   -0.00726748   0.124774    -0.0954983     0.00957722  -0.0392116    0.0915795     0.0188669     0.0389167   -0.0972381    0.141192     0.0496661   -0.0669104    0.00773021  -0.0556493    -0.0445695   -0.013386  
  0.0756673     0.0917741   -0.115554     0.0806862   -0.00671293    0.171739     0.151406     0.0993442    0.00255992   0.108479     0.027129     0.00153076   0.0728565    -0.0318233   -0.00428501  -0.169705      0.11414      -0.0869901   -0.00469718   0.250329     0.253841     0.0624957   -0.0295674   -0.0664764     0.0445955   -0.00925392
  0.0135742     0.189983    -0.143361     0.113213    -0.0500699    -0.172551     0.00899064  -0.0915274    0.029057     0.0121456    0.0522993    0.138934     0.178036     -0.00580093  -0.0636452   -0.01491       0.000157956   0.0170962    0.141984     0.0516914    0.0522521   -0.125614     0.0253339   -0.0121413    -0.0602285   -0.012986  
 -0.134705     -0.0319926   -0.152862    -0.107182     0.000598164   0.0335005    0.13776      0.00271415   0.129686     0.0859685    0.0268146    0.0502734    0.0063044    -0.117198     0.0172288    0.0351802    -0.156541      0.161183     0.168035    -0.0341767   -0.00235961  -0.012942     0.0125137   -0.0312944     0.0616113   -0.0358894 kind full, method split
0: avll = -1.4215881333335465
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.421608
INFO: iteration 2, average log likelihood -1.421550
INFO: iteration 3, average log likelihood -1.421513
INFO: iteration 4, average log likelihood -1.421473
INFO: iteration 5, average log likelihood -1.421425
INFO: iteration 6, average log likelihood -1.421365
INFO: iteration 7, average log likelihood -1.421280
INFO: iteration 8, average log likelihood -1.421135
INFO: iteration 9, average log likelihood -1.420843
INFO: iteration 10, average log likelihood -1.420245
INFO: iteration 11, average log likelihood -1.419198
INFO: iteration 12, average log likelihood -1.417892
INFO: iteration 13, average log likelihood -1.416868
INFO: iteration 14, average log likelihood -1.416352
INFO: iteration 15, average log likelihood -1.416153
INFO: iteration 16, average log likelihood -1.416082
INFO: iteration 17, average log likelihood -1.416056
INFO: iteration 18, average log likelihood -1.416046
INFO: iteration 19, average log likelihood -1.416042
INFO: iteration 20, average log likelihood -1.416041
INFO: iteration 21, average log likelihood -1.416040
INFO: iteration 22, average log likelihood -1.416039
INFO: iteration 23, average log likelihood -1.416039
INFO: iteration 24, average log likelihood -1.416039
INFO: iteration 25, average log likelihood -1.416038
INFO: iteration 26, average log likelihood -1.416038
INFO: iteration 27, average log likelihood -1.416038
INFO: iteration 28, average log likelihood -1.416038
INFO: iteration 29, average log likelihood -1.416038
INFO: iteration 30, average log likelihood -1.416038
INFO: iteration 31, average log likelihood -1.416037
INFO: iteration 32, average log likelihood -1.416037
INFO: iteration 33, average log likelihood -1.416037
INFO: iteration 34, average log likelihood -1.416037
INFO: iteration 35, average log likelihood -1.416037
INFO: iteration 36, average log likelihood -1.416037
INFO: iteration 37, average log likelihood -1.416037
INFO: iteration 38, average log likelihood -1.416037
INFO: iteration 39, average log likelihood -1.416037
INFO: iteration 40, average log likelihood -1.416037
INFO: iteration 41, average log likelihood -1.416037
INFO: iteration 42, average log likelihood -1.416037
INFO: iteration 43, average log likelihood -1.416037
INFO: iteration 44, average log likelihood -1.416037
INFO: iteration 45, average log likelihood -1.416037
INFO: iteration 46, average log likelihood -1.416037
INFO: iteration 47, average log likelihood -1.416037
INFO: iteration 48, average log likelihood -1.416037
INFO: iteration 49, average log likelihood -1.416037
INFO: iteration 50, average log likelihood -1.416037
INFO: EM with 100000 data points 50 iterations avll -1.416037
952.4 data points per parameter
1: avll = [-1.42161,-1.42155,-1.42151,-1.42147,-1.42143,-1.42137,-1.42128,-1.42113,-1.42084,-1.42024,-1.4192,-1.41789,-1.41687,-1.41635,-1.41615,-1.41608,-1.41606,-1.41605,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.416057
INFO: iteration 2, average log likelihood -1.415995
INFO: iteration 3, average log likelihood -1.415956
INFO: iteration 4, average log likelihood -1.415912
INFO: iteration 5, average log likelihood -1.415861
INFO: iteration 6, average log likelihood -1.415802
INFO: iteration 7, average log likelihood -1.415737
INFO: iteration 8, average log likelihood -1.415669
INFO: iteration 9, average log likelihood -1.415602
INFO: iteration 10, average log likelihood -1.415540
INFO: iteration 11, average log likelihood -1.415483
INFO: iteration 12, average log likelihood -1.415429
INFO: iteration 13, average log likelihood -1.415377
INFO: iteration 14, average log likelihood -1.415324
INFO: iteration 15, average log likelihood -1.415270
INFO: iteration 16, average log likelihood -1.415216
INFO: iteration 17, average log likelihood -1.415163
INFO: iteration 18, average log likelihood -1.415114
INFO: iteration 19, average log likelihood -1.415069
INFO: iteration 20, average log likelihood -1.415031
INFO: iteration 21, average log likelihood -1.414999
INFO: iteration 22, average log likelihood -1.414973
INFO: iteration 23, average log likelihood -1.414951
INFO: iteration 24, average log likelihood -1.414934
INFO: iteration 25, average log likelihood -1.414920
INFO: iteration 26, average log likelihood -1.414908
INFO: iteration 27, average log likelihood -1.414897
INFO: iteration 28, average log likelihood -1.414888
INFO: iteration 29, average log likelihood -1.414880
INFO: iteration 30, average log likelihood -1.414872
INFO: iteration 31, average log likelihood -1.414865
INFO: iteration 32, average log likelihood -1.414858
INFO: iteration 33, average log likelihood -1.414852
INFO: iteration 34, average log likelihood -1.414846
INFO: iteration 35, average log likelihood -1.414841
INFO: iteration 36, average log likelihood -1.414835
INFO: iteration 37, average log likelihood -1.414831
INFO: iteration 38, average log likelihood -1.414826
INFO: iteration 39, average log likelihood -1.414822
INFO: iteration 40, average log likelihood -1.414819
INFO: iteration 41, average log likelihood -1.414816
INFO: iteration 42, average log likelihood -1.414813
INFO: iteration 43, average log likelihood -1.414810
INFO: iteration 44, average log likelihood -1.414807
INFO: iteration 45, average log likelihood -1.414805
INFO: iteration 46, average log likelihood -1.414803
INFO: iteration 47, average log likelihood -1.414802
INFO: iteration 48, average log likelihood -1.414800
INFO: iteration 49, average log likelihood -1.414798
INFO: iteration 50, average log likelihood -1.414797
INFO: EM with 100000 data points 50 iterations avll -1.414797
473.9 data points per parameter
2: avll = [-1.41606,-1.416,-1.41596,-1.41591,-1.41586,-1.4158,-1.41574,-1.41567,-1.4156,-1.41554,-1.41548,-1.41543,-1.41538,-1.41532,-1.41527,-1.41522,-1.41516,-1.41511,-1.41507,-1.41503,-1.415,-1.41497,-1.41495,-1.41493,-1.41492,-1.41491,-1.4149,-1.41489,-1.41488,-1.41487,-1.41486,-1.41486,-1.41485,-1.41485,-1.41484,-1.41484,-1.41483,-1.41483,-1.41482,-1.41482,-1.41482,-1.41481,-1.41481,-1.41481,-1.41481,-1.4148,-1.4148,-1.4148,-1.4148,-1.4148]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414814
INFO: iteration 2, average log likelihood -1.414760
INFO: iteration 3, average log likelihood -1.414728
INFO: iteration 4, average log likelihood -1.414695
INFO: iteration 5, average log likelihood -1.414660
INFO: iteration 6, average log likelihood -1.414622
INFO: iteration 7, average log likelihood -1.414581
INFO: iteration 8, average log likelihood -1.414537
INFO: iteration 9, average log likelihood -1.414491
INFO: iteration 10, average log likelihood -1.414444
INFO: iteration 11, average log likelihood -1.414396
INFO: iteration 12, average log likelihood -1.414347
INFO: iteration 13, average log likelihood -1.414295
INFO: iteration 14, average log likelihood -1.414240
INFO: iteration 15, average log likelihood -1.414181
INFO: iteration 16, average log likelihood -1.414117
INFO: iteration 17, average log likelihood -1.414048
INFO: iteration 18, average log likelihood -1.413979
INFO: iteration 19, average log likelihood -1.413910
INFO: iteration 20, average log likelihood -1.413847
INFO: iteration 21, average log likelihood -1.413792
INFO: iteration 22, average log likelihood -1.413745
INFO: iteration 23, average log likelihood -1.413707
INFO: iteration 24, average log likelihood -1.413677
INFO: iteration 25, average log likelihood -1.413652
INFO: iteration 26, average log likelihood -1.413631
INFO: iteration 27, average log likelihood -1.413614
INFO: iteration 28, average log likelihood -1.413599
INFO: iteration 29, average log likelihood -1.413586
INFO: iteration 30, average log likelihood -1.413574
INFO: iteration 31, average log likelihood -1.413563
INFO: iteration 32, average log likelihood -1.413553
INFO: iteration 33, average log likelihood -1.413544
INFO: iteration 34, average log likelihood -1.413536
INFO: iteration 35, average log likelihood -1.413528
INFO: iteration 36, average log likelihood -1.413520
INFO: iteration 37, average log likelihood -1.413513
INFO: iteration 38, average log likelihood -1.413507
INFO: iteration 39, average log likelihood -1.413500
INFO: iteration 40, average log likelihood -1.413494
INFO: iteration 41, average log likelihood -1.413489
INFO: iteration 42, average log likelihood -1.413483
INFO: iteration 43, average log likelihood -1.413478
INFO: iteration 44, average log likelihood -1.413474
INFO: iteration 45, average log likelihood -1.413469
INFO: iteration 46, average log likelihood -1.413464
INFO: iteration 47, average log likelihood -1.413460
INFO: iteration 48, average log likelihood -1.413456
INFO: iteration 49, average log likelihood -1.413452
INFO: iteration 50, average log likelihood -1.413448
INFO: EM with 100000 data points 50 iterations avll -1.413448
236.4 data points per parameter
3: avll = [-1.41481,-1.41476,-1.41473,-1.4147,-1.41466,-1.41462,-1.41458,-1.41454,-1.41449,-1.41444,-1.4144,-1.41435,-1.4143,-1.41424,-1.41418,-1.41412,-1.41405,-1.41398,-1.41391,-1.41385,-1.41379,-1.41375,-1.41371,-1.41368,-1.41365,-1.41363,-1.41361,-1.4136,-1.41359,-1.41357,-1.41356,-1.41355,-1.41354,-1.41354,-1.41353,-1.41352,-1.41351,-1.41351,-1.4135,-1.41349,-1.41349,-1.41348,-1.41348,-1.41347,-1.41347,-1.41346,-1.41346,-1.41346,-1.41345,-1.41345]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.413456
INFO: iteration 2, average log likelihood -1.413406
INFO: iteration 3, average log likelihood -1.413368
INFO: iteration 4, average log likelihood -1.413328
INFO: iteration 5, average log likelihood -1.413281
INFO: iteration 6, average log likelihood -1.413227
INFO: iteration 7, average log likelihood -1.413164
INFO: iteration 8, average log likelihood -1.413093
INFO: iteration 9, average log likelihood -1.413016
INFO: iteration 10, average log likelihood -1.412934
INFO: iteration 11, average log likelihood -1.412851
INFO: iteration 12, average log likelihood -1.412769
INFO: iteration 13, average log likelihood -1.412690
INFO: iteration 14, average log likelihood -1.412616
INFO: iteration 15, average log likelihood -1.412547
INFO: iteration 16, average log likelihood -1.412484
INFO: iteration 17, average log likelihood -1.412426
INFO: iteration 18, average log likelihood -1.412374
INFO: iteration 19, average log likelihood -1.412327
INFO: iteration 20, average log likelihood -1.412284
INFO: iteration 21, average log likelihood -1.412245
INFO: iteration 22, average log likelihood -1.412210
INFO: iteration 23, average log likelihood -1.412177
INFO: iteration 24, average log likelihood -1.412147
INFO: iteration 25, average log likelihood -1.412119
INFO: iteration 26, average log likelihood -1.412093
INFO: iteration 27, average log likelihood -1.412067
INFO: iteration 28, average log likelihood -1.412044
INFO: iteration 29, average log likelihood -1.412021
INFO: iteration 30, average log likelihood -1.411999
INFO: iteration 31, average log likelihood -1.411978
INFO: iteration 32, average log likelihood -1.411958
INFO: iteration 33, average log likelihood -1.411938
INFO: iteration 34, average log likelihood -1.411920
INFO: iteration 35, average log likelihood -1.411901
INFO: iteration 36, average log likelihood -1.411884
INFO: iteration 37, average log likelihood -1.411867
INFO: iteration 38, average log likelihood -1.411851
INFO: iteration 39, average log likelihood -1.411835
INFO: iteration 40, average log likelihood -1.411820
INFO: iteration 41, average log likelihood -1.411805
INFO: iteration 42, average log likelihood -1.411790
INFO: iteration 43, average log likelihood -1.411777
INFO: iteration 44, average log likelihood -1.411763
INFO: iteration 45, average log likelihood -1.411750
INFO: iteration 46, average log likelihood -1.411738
INFO: iteration 47, average log likelihood -1.411725
INFO: iteration 48, average log likelihood -1.411714
INFO: iteration 49, average log likelihood -1.411702
INFO: iteration 50, average log likelihood -1.411691
INFO: EM with 100000 data points 50 iterations avll -1.411691
118.1 data points per parameter
4: avll = [-1.41346,-1.41341,-1.41337,-1.41333,-1.41328,-1.41323,-1.41316,-1.41309,-1.41302,-1.41293,-1.41285,-1.41277,-1.41269,-1.41262,-1.41255,-1.41248,-1.41243,-1.41237,-1.41233,-1.41228,-1.41225,-1.41221,-1.41218,-1.41215,-1.41212,-1.41209,-1.41207,-1.41204,-1.41202,-1.412,-1.41198,-1.41196,-1.41194,-1.41192,-1.4119,-1.41188,-1.41187,-1.41185,-1.41183,-1.41182,-1.4118,-1.41179,-1.41178,-1.41176,-1.41175,-1.41174,-1.41173,-1.41171,-1.4117,-1.41169]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.411689
INFO: iteration 2, average log likelihood -1.411619
INFO: iteration 3, average log likelihood -1.411552
INFO: iteration 4, average log likelihood -1.411474
INFO: iteration 5, average log likelihood -1.411377
INFO: iteration 6, average log likelihood -1.411257
INFO: iteration 7, average log likelihood -1.411112
INFO: iteration 8, average log likelihood -1.410949
INFO: iteration 9, average log likelihood -1.410777
INFO: iteration 10, average log likelihood -1.410606
INFO: iteration 11, average log likelihood -1.410447
INFO: iteration 12, average log likelihood -1.410302
INFO: iteration 13, average log likelihood -1.410176
INFO: iteration 14, average log likelihood -1.410065
INFO: iteration 15, average log likelihood -1.409969
INFO: iteration 16, average log likelihood -1.409885
INFO: iteration 17, average log likelihood -1.409812
INFO: iteration 18, average log likelihood -1.409746
INFO: iteration 19, average log likelihood -1.409687
INFO: iteration 20, average log likelihood -1.409634
INFO: iteration 21, average log likelihood -1.409586
INFO: iteration 22, average log likelihood -1.409542
INFO: iteration 23, average log likelihood -1.409500
INFO: iteration 24, average log likelihood -1.409462
INFO: iteration 25, average log likelihood -1.409426
INFO: iteration 26, average log likelihood -1.409392
INFO: iteration 27, average log likelihood -1.409359
INFO: iteration 28, average log likelihood -1.409328
INFO: iteration 29, average log likelihood -1.409298
INFO: iteration 30, average log likelihood -1.409269
INFO: iteration 31, average log likelihood -1.409241
INFO: iteration 32, average log likelihood -1.409214
INFO: iteration 33, average log likelihood -1.409188
INFO: iteration 34, average log likelihood -1.409163
INFO: iteration 35, average log likelihood -1.409138
INFO: iteration 36, average log likelihood -1.409114
INFO: iteration 37, average log likelihood -1.409092
INFO: iteration 38, average log likelihood -1.409070
INFO: iteration 39, average log likelihood -1.409048
INFO: iteration 40, average log likelihood -1.409028
INFO: iteration 41, average log likelihood -1.409008
INFO: iteration 42, average log likelihood -1.408990
INFO: iteration 43, average log likelihood -1.408972
INFO: iteration 44, average log likelihood -1.408955
INFO: iteration 45, average log likelihood -1.408938
INFO: iteration 46, average log likelihood -1.408923
INFO: iteration 47, average log likelihood -1.408908
INFO: iteration 48, average log likelihood -1.408893
INFO: iteration 49, average log likelihood -1.408880
INFO: iteration 50, average log likelihood -1.408867
INFO: EM with 100000 data points 50 iterations avll -1.408867
59.0 data points per parameter
5: avll = [-1.41169,-1.41162,-1.41155,-1.41147,-1.41138,-1.41126,-1.41111,-1.41095,-1.41078,-1.41061,-1.41045,-1.4103,-1.41018,-1.41007,-1.40997,-1.40989,-1.40981,-1.40975,-1.40969,-1.40963,-1.40959,-1.40954,-1.4095,-1.40946,-1.40943,-1.40939,-1.40936,-1.40933,-1.4093,-1.40927,-1.40924,-1.40921,-1.40919,-1.40916,-1.40914,-1.40911,-1.40909,-1.40907,-1.40905,-1.40903,-1.40901,-1.40899,-1.40897,-1.40895,-1.40894,-1.40892,-1.40891,-1.40889,-1.40888,-1.40887]
[-1.42159,-1.42161,-1.42155,-1.42151,-1.42147,-1.42143,-1.42137,-1.42128,-1.42113,-1.42084,-1.42024,-1.4192,-1.41789,-1.41687,-1.41635,-1.41615,-1.41608,-1.41606,-1.41605,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41604,-1.41606,-1.416,-1.41596,-1.41591,-1.41586,-1.4158,-1.41574,-1.41567,-1.4156,-1.41554,-1.41548,-1.41543,-1.41538,-1.41532,-1.41527,-1.41522,-1.41516,-1.41511,-1.41507,-1.41503,-1.415,-1.41497,-1.41495,-1.41493,-1.41492,-1.41491,-1.4149,-1.41489,-1.41488,-1.41487,-1.41486,-1.41486,-1.41485,-1.41485,-1.41484,-1.41484,-1.41483,-1.41483,-1.41482,-1.41482,-1.41482,-1.41481,-1.41481,-1.41481,-1.41481,-1.4148,-1.4148,-1.4148,-1.4148,-1.4148,-1.41481,-1.41476,-1.41473,-1.4147,-1.41466,-1.41462,-1.41458,-1.41454,-1.41449,-1.41444,-1.4144,-1.41435,-1.4143,-1.41424,-1.41418,-1.41412,-1.41405,-1.41398,-1.41391,-1.41385,-1.41379,-1.41375,-1.41371,-1.41368,-1.41365,-1.41363,-1.41361,-1.4136,-1.41359,-1.41357,-1.41356,-1.41355,-1.41354,-1.41354,-1.41353,-1.41352,-1.41351,-1.41351,-1.4135,-1.41349,-1.41349,-1.41348,-1.41348,-1.41347,-1.41347,-1.41346,-1.41346,-1.41346,-1.41345,-1.41345,-1.41346,-1.41341,-1.41337,-1.41333,-1.41328,-1.41323,-1.41316,-1.41309,-1.41302,-1.41293,-1.41285,-1.41277,-1.41269,-1.41262,-1.41255,-1.41248,-1.41243,-1.41237,-1.41233,-1.41228,-1.41225,-1.41221,-1.41218,-1.41215,-1.41212,-1.41209,-1.41207,-1.41204,-1.41202,-1.412,-1.41198,-1.41196,-1.41194,-1.41192,-1.4119,-1.41188,-1.41187,-1.41185,-1.41183,-1.41182,-1.4118,-1.41179,-1.41178,-1.41176,-1.41175,-1.41174,-1.41173,-1.41171,-1.4117,-1.41169,-1.41169,-1.41162,-1.41155,-1.41147,-1.41138,-1.41126,-1.41111,-1.41095,-1.41078,-1.41061,-1.41045,-1.4103,-1.41018,-1.41007,-1.40997,-1.40989,-1.40981,-1.40975,-1.40969,-1.40963,-1.40959,-1.40954,-1.4095,-1.40946,-1.40943,-1.40939,-1.40936,-1.40933,-1.4093,-1.40927,-1.40924,-1.40921,-1.40919,-1.40916,-1.40914,-1.40911,-1.40909,-1.40907,-1.40905,-1.40903,-1.40901,-1.40899,-1.40897,-1.40895,-1.40894,-1.40892,-1.40891,-1.40889,-1.40888,-1.40887]
32×26 Array{Float64,2}:
 -0.0607719    0.0415434    0.165285    0.00412418  -0.00917012  -0.082003   -0.254955    -0.0822511  -0.229637    -0.0040238    0.190098    -0.0803173  -0.14102    -0.399401    0.0850066   0.0452686  -0.120505    0.192753    0.0866591   -0.123715   -0.102647   -0.3058       0.109724    0.118074    -0.0339068   -0.204782  
 -0.0891084    0.231058     0.0993946  -0.344772    -0.170123     0.0470057  -0.097806    -0.268755   -0.0690246   -0.00630881  -0.0584466   -0.355747    0.0301662  -0.0532023   0.27749    -0.129785   -0.0239181  -0.0506068  -0.0524845    0.048206    0.0591792   0.294691    -0.308476    0.261644    -0.232891     0.48313   
 -0.161164     0.326223     0.616651   -0.750193    -0.057126     0.0339004   0.319418     0.110601   -0.32062     -0.295328    -0.0923241    0.130376    0.183827   -0.209298    0.0471556  -0.354977   -0.428571   -0.0659718   0.0609074    0.28834    -0.0425034   0.652313     0.0444027  -0.0255495    0.474754    -0.340327  
 -0.402128    -0.288919     0.0540243  -0.677536     0.143993    -0.0894075   0.0540206   -0.38983    -0.042343     0.566616    -0.226074     0.052326   -0.16767     0.0921649  -0.0995926  -0.170054   -0.0116942  -0.0390845   0.539543     0.40495    -0.297384    0.304855     0.28256    -0.331461     0.0152257   -0.28855   
 -0.335487     0.200431    -0.032855    0.561944     0.465231     0.0982094   0.178569     0.624098    0.134808     0.462667    -0.253063     0.428632   -0.116171   -0.237554    0.107585   -0.149173    0.154547    0.0396914  -0.0836929    0.172077   -0.0101582  -0.209759     0.0390183   0.334143    -0.413307     0.0389025 
 -0.749641    -0.32616      0.207277    0.453985    -0.0363108   -0.219109   -0.0812051   -0.503873   -0.199118    -0.195236    -0.141076     0.546476   -0.275564   -0.502105    0.121067    0.0547993   0.149879   -0.163394   -0.270174     0.0451449  -0.197885   -0.323603     0.185255    0.473902     0.816033    -0.137435  
  0.403864    -0.0134369   -0.0273106   0.0751748   -0.0900792   -0.267379    0.335669     0.196656    0.167446    -0.2017       0.00254515  -0.149261    0.229867    0.104716   -0.282674   -0.08608    -0.389494   -0.0656009  -0.166067     0.182005   -0.0281118   0.110875    -0.0646523  -0.288571    -0.0618039   -0.143846  
 -0.167747    -0.0544746   -0.164536    0.115704     0.0941886    0.210475    0.112002     0.0634314   0.177682    -0.0110701   -0.0154526    0.173193   -0.0541774   0.278595   -0.116466    0.039844    0.224291   -0.130244   -0.0818509   -0.114327    0.11742    -0.0689681    0.0546888  -0.0558726    0.148791     0.0352998 
  0.0798476   -0.10047     -0.619355   -0.0148463   -0.154035    -0.0638997  -0.433205    -0.223211   -0.00305144  -0.49043      0.707641    -0.0991363  -0.588354    0.3054     -0.155462   -0.154002   -0.725353   -0.991551   -0.229031    -0.0662853   0.346274    0.0247403   -0.229203    0.36292      0.321172    -0.305505  
 -0.00384479   0.275386    -0.405379    0.48386     -0.107371    -0.204777   -0.258178     0.532086    0.268119    -0.339402     0.0846494   -0.195549    0.141978   -0.521492   -0.0142402   0.225608   -0.337122    0.359839   -0.16137     -0.504005    0.181225   -0.487963     0.286657   -0.215267     0.080112    -0.196869  
 -0.27679     -0.624881    -0.576995   -0.0708164   -0.0382054    0.397634    0.0279288   -0.811081    0.156435     0.519463     0.25675     -0.269804    0.489772    0.0137135  -0.0783725   0.678256    0.457546    0.0454232  -0.491738    -0.6381      0.217379    0.145964     0.361099   -0.175027     0.207627    -0.357801  
 -0.383581    -0.415058    -0.596507    0.0201888    0.363727    -0.323149   -0.400244     0.305445   -0.157921     0.126035    -0.263443    -0.233781   -0.398903   -0.297743   -0.371329    0.363976    0.452431   -0.44612    -0.210025    -0.776534    0.0979229   0.0575447   -0.0762616  -0.234569    -0.0705381   -0.386847  
  0.153988     0.485284    -0.286292   -0.105209    -0.520015    -0.0130559   0.0638706    0.0557478   0.00665167  -0.345868     0.0684076   -0.0590282  -0.138705   -0.166562    0.112938    0.287353    0.838597   -0.447745    0.258178     0.450801   -0.16502     0.127673     0.149444   -0.636001    -0.0403141   -0.0143361 
 -0.131492     0.199613     0.369218   -0.157153    -0.700169    -0.385337    0.373703     0.0302955   0.0498966    0.585273    -0.413351     0.348454    0.109631   -0.414341   -0.0519384   0.442871    0.203138    1.03302     0.290323     0.272019   -0.366161   -0.0176089    0.180269   -0.583089    -0.11532      0.439205  
  0.0131201    0.145618     0.0904697   0.311493     0.141926    -0.551066   -0.366718    -0.193459   -0.346796    -0.325971    -0.0868278    0.279644   -0.224867    0.0182064  -0.161319   -0.435668    0.532385    0.695729    0.71399     -0.502744    0.247799    0.27108      0.270099    0.440786    -0.00464916   0.350919  
  0.0654722    0.0178363   -0.414432   -0.0121752    0.394451    -0.41711    -0.175188     0.378755   -0.461861    -0.546599     0.0550794   -0.120007    0.284981   -0.165455   -0.173959    0.798523    0.689142    0.28944    -0.0533974   -0.122357   -0.258997    0.00787637  -0.0834918  -0.00587293  -0.342396     0.467456  
  0.0353879   -0.389683    -0.1919      0.023547    -0.131053    -0.349399    0.50985      0.326616    0.412603    -0.0155086    0.145005    -0.237408    0.322141    0.239061   -0.366351    0.304327   -0.688678   -0.447827   -0.71027      0.545467   -0.867198   -0.342199    -0.576037   -0.205389    -0.556048    -0.387313  
  1.06629      0.142059    -0.334486   -0.120025    -0.34775     -0.434004    0.204568     0.110434    0.104589     0.0467537    0.728495    -0.4193      0.828429    0.379814   -0.288689    0.334072   -0.341561    0.390988    0.164056     0.268648   -0.0553557   0.217644     0.146175   -0.696067    -0.652149     0.0281907 
  0.554324    -0.195908     0.0986063   0.588372    -0.477336     0.0795616  -0.389       -0.691966   -0.864294    -0.0735384    0.256063    -0.0830192   0.144537   -0.342746    0.332291    0.158932    0.17695     0.0207222  -0.19343     -0.0244757  -0.504124   -0.357775     0.1942      0.430958    -0.491577     0.0873314 
  0.550845    -0.495602     0.361276    0.0713929    0.100987     0.547862    0.514669    -0.0755745   0.28137     -0.127018     0.134423     0.0564652   0.0144976   0.582745   -0.400311    0.443022   -0.0588982   0.646956    0.306492    -0.167232   -0.242006   -0.377355     0.0994208   0.437539    -0.356974     0.0425753 
 -0.253147    -0.122215    -0.194987    0.176273     0.3573       0.214763    0.621099     0.257281    0.685403     0.218863     0.0944615    0.498595    0.276659    0.599062   -0.259668   -0.52695    -0.115744    0.0126612  -0.0656046    0.104369    0.356762   -0.106071     0.265459   -0.262414     0.713461     0.0097848 
  0.0216291    0.210811    -0.127615   -0.586932    -0.0631062    0.563102   -0.350675     0.204864   -0.224356    -0.0686912   -0.149043     0.210577    0.688869    0.358005   -0.343719    0.850834    0.298466   -0.303587   -0.228014    -0.161451    0.5294     -0.458393     0.554673   -0.559182    -0.188958     0.0811147 
  0.232663     0.00459941   0.270575   -0.83887      0.0865491    0.592005    0.0798663   -0.28205    -0.289019    -0.123311    -0.0741291   -0.319357   -0.396216    0.512161   -0.0846977  -0.179079    0.482667   -0.139257    0.639201    -0.040792    0.206955    0.320974    -0.317342    0.0264373   -0.117606     0.608027  
  0.3525      -0.253866    -0.274217    0.570572     0.371131     0.159954    0.186041     0.12728     0.292824    -0.0698583   -0.0289185   -0.0250322   0.0546717   1.18545    -0.632293   -0.129472    0.763594   -0.473876   -0.484762     0.118775   -0.263744   -0.0339188   -0.390974   -0.207551    -0.113825     0.111807  
 -0.0145138    0.453021     0.490427    0.0724402   -0.118989     0.552334    0.0242844   -0.0579428   0.0813977    0.090156     0.106018    -0.140737   -0.146377   -0.115457    0.359822   -0.641594   -0.469307   -0.224026    0.193363     0.516748    0.252539   -0.779849    -0.243009    0.0411657   -0.0952497    0.0143634 
  0.0984353   -0.0524307    0.340334    0.393998     0.188631    -0.616418    0.488108    -0.0185542   0.366186     0.275173     0.0749258   -0.502846   -0.535779   -0.0856483   0.425515   -0.816898   -0.35562     0.111534    0.377744     0.287305   -0.625321    0.517133    -0.527622    0.502595    -0.121148     0.00516655
 -0.776531     0.58268      0.0211602  -0.331769    -0.132807     0.205137   -0.508887     0.106567   -0.363948     0.726618    -0.313186     0.196042   -0.344671   -0.795458    0.848616   -0.740648    0.268651   -0.484524    0.0482629   -0.0739441   0.7421      0.300704     0.203692   -0.345095     0.749888     0.0702221 
 -0.294243    -0.501254    -0.0627608   0.474329     0.979253     0.315012   -0.303179     0.0230784   0.0160142   -0.109371    -0.114835    -0.315086   -0.16926     0.0901783   0.674851   -0.69708     0.0822111  -0.401944   -0.30299     -0.313986    0.577986   -0.181305    -0.0860689   0.367919     0.348733    -0.0214409 
  0.0328556   -0.16242     -0.332767   -0.500559    -0.595515    -0.261859   -0.55556     -0.456843   -0.0900002    0.163161     0.140759    -0.118362    0.0435671   0.209183   -0.355827   -0.123958   -0.879284    0.517544   -0.177689    -0.329972    0.602218   -0.228481    -0.480428    0.178779    -0.210914     0.107842  
 -0.190325     0.308842    -0.0330568  -0.603023    -0.343203    -0.549404   -0.258058    -0.184984   -0.48463      0.264839    -0.144387    -0.354411   -0.422638   -1.18914     0.52632     0.640919   -0.274034    0.177741    0.486037    -0.267936    0.154366    0.211331    -0.0391586   0.0265458   -0.697317    -0.0856067 
 -0.16641      0.152608     0.148533    0.0146475    0.102424     0.0861033  -0.00542498   0.152198   -0.0533739   -0.119472    -0.100375    -0.112677   -0.103737   -0.0387091  -0.101624    0.178847    0.0202492  -0.0776153   0.134872    -0.0407917  -0.169706   -0.431743    -0.219037    0.0824913   -0.0883943    0.0156208 
 -0.0662051   -0.0538905   -0.0406885  -0.00674047  -0.0421346   -0.301916   -0.031159    -0.0290942   0.0169867    0.0689032   -0.0467619    0.224992    0.0766324  -0.274284    0.304065   -0.149147    0.340422    0.217755   -0.00160048  -0.212985    0.221246    0.783341     0.564015   -0.0442557    0.23509      0.0473171 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408854
INFO: iteration 2, average log likelihood -1.408842
INFO: iteration 3, average log likelihood -1.408830
INFO: iteration 4, average log likelihood -1.408819
INFO: iteration 5, average log likelihood -1.408808
INFO: iteration 6, average log likelihood -1.408797
INFO: iteration 7, average log likelihood -1.408787
INFO: iteration 8, average log likelihood -1.408776
INFO: iteration 9, average log likelihood -1.408767
INFO: iteration 10, average log likelihood -1.408757
INFO: EM with 100000 data points 10 iterations avll -1.408757
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       9.150696e+05
      1       7.030665e+05      -2.120031e+05 |       32
      2       6.909690e+05      -1.209747e+04 |       32
      3       6.857034e+05      -5.265644e+03 |       32
      4       6.825461e+05      -3.157280e+03 |       32
      5       6.804669e+05      -2.079187e+03 |       32
      6       6.789914e+05      -1.475525e+03 |       32
      7       6.778354e+05      -1.156009e+03 |       32
      8       6.769118e+05      -9.235938e+02 |       32
      9       6.761559e+05      -7.559185e+02 |       32
     10       6.755467e+05      -6.091599e+02 |       32
     11       6.750587e+05      -4.879879e+02 |       32
     12       6.746363e+05      -4.224179e+02 |       32
     13       6.742972e+05      -3.391114e+02 |       32
     14       6.740010e+05      -2.962314e+02 |       32
     15       6.737405e+05      -2.604210e+02 |       32
     16       6.735211e+05      -2.194887e+02 |       32
     17       6.733382e+05      -1.828187e+02 |       32
     18       6.731867e+05      -1.515725e+02 |       32
     19       6.730492e+05      -1.374497e+02 |       32
     20       6.729293e+05      -1.199554e+02 |       32
     21       6.728185e+05      -1.107916e+02 |       32
     22       6.727244e+05      -9.403452e+01 |       32
     23       6.726442e+05      -8.020030e+01 |       32
     24       6.725748e+05      -6.942700e+01 |       32
     25       6.724990e+05      -7.581484e+01 |       32
     26       6.724227e+05      -7.630660e+01 |       32
     27       6.723464e+05      -7.624292e+01 |       32
     28       6.722684e+05      -7.805881e+01 |       32
     29       6.721886e+05      -7.980247e+01 |       32
     30       6.721119e+05      -7.667095e+01 |       32
     31       6.720411e+05      -7.085268e+01 |       32
     32       6.719767e+05      -6.438626e+01 |       32
     33       6.719130e+05      -6.371911e+01 |       32
     34       6.718565e+05      -5.648947e+01 |       32
     35       6.717969e+05      -5.958241e+01 |       32
     36       6.717381e+05      -5.880072e+01 |       32
     37       6.716783e+05      -5.978575e+01 |       32
     38       6.716188e+05      -5.954304e+01 |       32
     39       6.715670e+05      -5.171519e+01 |       32
     40       6.715304e+05      -3.660395e+01 |       32
     41       6.714960e+05      -3.447348e+01 |       32
     42       6.714631e+05      -3.283349e+01 |       32
     43       6.714309e+05      -3.217592e+01 |       32
     44       6.714054e+05      -2.550080e+01 |       32
     45       6.713779e+05      -2.757005e+01 |       32
     46       6.713545e+05      -2.336419e+01 |       32
     47       6.713329e+05      -2.160191e+01 |       32
     48       6.713150e+05      -1.788398e+01 |       32
     49       6.712983e+05      -1.677133e+01 |       32
     50       6.712825e+05      -1.576134e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 671282.4955984489)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420799
INFO: iteration 2, average log likelihood -1.415712
INFO: iteration 3, average log likelihood -1.414278
INFO: iteration 4, average log likelihood -1.413142
INFO: iteration 5, average log likelihood -1.411972
INFO: iteration 6, average log likelihood -1.410994
INFO: iteration 7, average log likelihood -1.410388
INFO: iteration 8, average log likelihood -1.410063
INFO: iteration 9, average log likelihood -1.409878
INFO: iteration 10, average log likelihood -1.409756
INFO: iteration 11, average log likelihood -1.409664
INFO: iteration 12, average log likelihood -1.409590
INFO: iteration 13, average log likelihood -1.409527
INFO: iteration 14, average log likelihood -1.409471
INFO: iteration 15, average log likelihood -1.409422
INFO: iteration 16, average log likelihood -1.409377
INFO: iteration 17, average log likelihood -1.409336
INFO: iteration 18, average log likelihood -1.409298
INFO: iteration 19, average log likelihood -1.409263
INFO: iteration 20, average log likelihood -1.409230
INFO: iteration 21, average log likelihood -1.409199
INFO: iteration 22, average log likelihood -1.409170
INFO: iteration 23, average log likelihood -1.409143
INFO: iteration 24, average log likelihood -1.409118
INFO: iteration 25, average log likelihood -1.409094
INFO: iteration 26, average log likelihood -1.409071
INFO: iteration 27, average log likelihood -1.409051
INFO: iteration 28, average log likelihood -1.409031
INFO: iteration 29, average log likelihood -1.409013
INFO: iteration 30, average log likelihood -1.408995
INFO: iteration 31, average log likelihood -1.408979
INFO: iteration 32, average log likelihood -1.408964
INFO: iteration 33, average log likelihood -1.408949
INFO: iteration 34, average log likelihood -1.408935
INFO: iteration 35, average log likelihood -1.408922
INFO: iteration 36, average log likelihood -1.408910
INFO: iteration 37, average log likelihood -1.408898
INFO: iteration 38, average log likelihood -1.408886
INFO: iteration 39, average log likelihood -1.408875
INFO: iteration 40, average log likelihood -1.408864
INFO: iteration 41, average log likelihood -1.408854
INFO: iteration 42, average log likelihood -1.408844
INFO: iteration 43, average log likelihood -1.408834
INFO: iteration 44, average log likelihood -1.408824
INFO: iteration 45, average log likelihood -1.408815
INFO: iteration 46, average log likelihood -1.408805
INFO: iteration 47, average log likelihood -1.408796
INFO: iteration 48, average log likelihood -1.408787
INFO: iteration 49, average log likelihood -1.408778
INFO: iteration 50, average log likelihood -1.408769
INFO: EM with 100000 data points 50 iterations avll -1.408769
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.187766     -0.177466   -0.00820605   0.40006     0.0892902  -0.232754     0.639035    0.645107     0.423503     0.616236    0.0186046    0.239712   -0.0345214   0.0862246  -0.0289236  -0.0746635  -0.899109   -0.0810631  -0.831794    0.661921   -0.315683   -0.266906   -0.236392     -0.258243   -0.643607   -0.615513  
 -0.312443     -0.367522   -0.413248    -0.0884243  -0.311165   -0.218614    -0.169542   -0.442861     0.247673    -0.209961    0.583599     0.0585722  -0.522148    0.253942   -0.294747   -0.164266   -0.840579   -0.583056   -0.296598   -0.124564    0.385523    0.228125   -0.0992196     0.393325    0.652458   -0.519903  
 -0.545902      0.205269   -0.140257    -0.204626   -0.341524   -0.667434    -0.61141     0.0631106   -0.230546     0.527114   -0.279917     0.257735   -0.0443273  -1.24949     0.430856    0.099778   -0.120634   -0.172793   -0.146863   -0.347054    0.470025    0.214163    0.150468     -0.26393    -0.137555    0.321566  
 -0.652125      0.408189    0.355629    -0.252984   -0.201002    0.622738    -0.164272    0.0561834    0.0399519    0.506925    0.00871467   0.362563   -0.261029   -0.419788    0.557539   -0.571897    0.410656   -0.133834    0.162495   -0.23116     0.596277    0.155277    0.461187     -0.364965    0.720793   -0.158517  
  0.331378      0.214752    0.597394     0.196648   -0.70075     0.151834     0.882176   -0.303622     0.568781     0.371102   -0.522078    -0.121512   -0.135554   -0.364441    0.492291   -0.171065   -0.0926347   0.631267    0.125988    0.192775   -0.12773     0.125309    0.00531046   -0.403515   -0.240864    0.588446  
  0.166613      0.234189   -0.0611547    0.565132   -0.247539    0.103229    -0.511203   -0.226065    -0.343648    -0.131718    0.424184     0.0813105  -0.201589   -0.35924     0.306182    0.0800284   0.0330305  -0.265736   -0.090983    0.425095   -0.388784   -0.65866     0.0828894     0.515864   -0.233652   -0.00539691
  0.486821      0.0216268  -0.164553    -0.581807   -0.45544     0.199692    -0.400912   -0.279688    -0.444133    -0.370627    0.174586    -0.0673736   0.698688    0.233327   -0.403033    0.718721    0.0993266   0.186066   -0.0684115  -0.324937    0.248928   -0.117901    0.270786     -0.536877   -0.240938   -0.0338822 
 -0.212439     -0.524788   -0.144525    -0.0710407   0.295969    0.407706     0.169836    0.060001     0.0594395    0.105139   -0.192807    -0.0443412   0.682054    0.0085342   0.167573    0.0914113  -0.182666   -0.150868   -0.419185    0.0116831   0.381014    0.329014    0.57312       0.286985    0.109834    0.07152   
  0.198719      0.129359   -0.269238    -0.444559   -0.540111   -0.00773881  -0.178813   -0.237298     0.157162     0.286543    0.224178    -0.171865    0.0217789   0.23945    -0.201273   -0.469041   -1.10558     0.179669   -0.0755211   0.200364    0.696831   -0.666544   -0.729165     -0.0817207  -0.296852    0.192613  
  0.0529897    -0.200201    0.0549586    0.310964    0.692161    0.449874    -0.25088     0.0209952   -0.23558     -0.156087   -0.0460737   -0.325101   -0.331744    0.116832    0.702495   -0.990462   -0.137189   -0.678522   -0.100096   -0.115812    0.738966   -0.226626   -0.166002      0.102867    0.235712    0.0225021 
  0.130984      0.115706    0.00848024  -0.117214   -0.0784128   0.0108688    0.118058    0.00874841   0.00544767  -0.161844    0.0243486   -0.17285     0.0478372   0.0823201  -0.0539444  -0.0414624   0.0105811  -0.139181    0.0115922   0.128269   -0.0588557   0.116463   -0.14413      -0.109878   -0.0894913   0.0790418 
  0.000799718  -0.221669   -0.447533    -0.196062    0.0940899   0.372245    -0.157529   -0.212627     0.394098     0.506425    0.633185    -0.422173    0.0140592   0.425868    0.161553    0.56672     1.2154     -0.402235   -0.0799523  -0.501683    0.215851    0.0500839   0.286748     -0.441394   -0.332232    0.239238  
 -0.265967      0.317584    0.793216    -0.643659   -0.157292   -0.118149     0.122698   -0.11838     -0.369525    -0.121771   -0.086277     0.121731    0.0717728  -0.286266    0.106845   -0.515988   -0.560307    0.128703    0.222643    0.467245   -0.146892    0.375463    0.00768166    0.0323543   0.370367   -0.370088  
 -0.0176845     0.115047    0.10743     -0.39132    -0.15828     0.0171107   -0.432075   -0.557626    -0.392671     0.0777535  -0.195462    -0.435042   -0.261348    0.0268311   0.18381    -0.141356    0.0679321   0.089833   -0.0224091  -0.30218     0.123057    0.239006   -0.462604      0.684466   -0.342997    0.661005  
  0.136604      0.136704    0.0176364    0.260305    0.0892132  -0.426717    -0.407222   -0.191142    -0.305694    -0.28092     0.0311832    0.424236   -0.19293     0.0989485  -0.0428752  -0.0344143   0.545617    0.926282    0.403719   -0.493493    0.220764    0.227324    0.492263      0.315455   -0.132731    0.327919  
  0.220816     -0.127886    0.13385      0.0797349   0.647766    0.120408     0.216412    0.683055     0.187842    -0.221512    0.0489755   -0.161177    0.293264    0.0215978  -0.407007    0.126004   -0.122731    0.804795    0.330561   -0.413082   -0.0326334  -0.530204   -0.105458      0.496946   -0.123927    0.354576  
 -0.180415      0.074903    0.0192799    0.0836579  -0.0863117  -0.036728    -0.130118   -0.00808149  -0.146917    -0.0109984  -0.110719     0.104131   -0.0471145  -0.21698     0.0794362   0.0393621   0.116571    0.0231044   0.0375394  -0.13013     0.0645926  -0.061257    0.06636       0.161642    0.0326578   0.0630234 
  0.138474      0.184053    0.224349     0.394903    0.744994   -0.201935     0.316736    0.586776    -0.350533    -0.43158    -0.444918     0.165371    0.143104   -0.612949    0.237177    0.471729    1.08711    -0.209262   -0.178525    0.018907   -0.398487    0.253551    0.369358     -0.267389   -0.0538278   0.0192019 
 -1.14596       0.180896   -0.0354863    0.310836    0.441115   -0.0766966    0.0754985   0.137876    -0.0181242   -0.0877158  -0.571966     0.284267   -0.366778   -0.356699    0.379193   -0.375093    0.418392   -0.448172    0.0577321   0.224184   -0.0449046   0.0705415  -0.452884      0.495707    0.308719    0.213759  
 -0.3415        0.388419    0.242505    -0.519035   -0.480949   -0.125747     0.17127     0.290244    -0.0588794    0.44706    -0.431242     0.367531    0.165397   -0.349079   -0.452782    0.921011    0.300877    0.521871    0.171055    0.122106   -0.422245   -0.138254    0.070868     -0.403963   -0.396168    0.177553  
  0.355244     -0.215662   -0.30519      0.191183   -0.0790004  -0.469753     0.411938    0.10876      0.220897    -0.559182    0.197704    -0.523272    0.478003    0.434686   -0.597272    0.360199   -0.244123   -0.149827   -0.362508    0.456178   -0.719688   -0.112544   -0.628609     -0.12501    -0.559428    0.142303  
  0.25836      -0.17839    -0.0639498    0.0589383   0.309819   -0.883765     0.488477   -0.0566542    0.266534     0.288389    0.146233    -0.250234   -0.0670934  -0.0061471   0.224775   -0.655429   -0.142146    0.318592    0.117711   -0.0474713  -0.251152    1.08887    -0.163234      0.0407721   0.108945   -0.0499897 
 -0.144112     -0.18966    -0.352686     0.321676    0.309167    0.39752      0.318418    0.300862     0.447008    -0.020529   -0.0130027    0.7022      0.399171    0.902257   -0.589659   -0.240908    0.402013   -0.376461   -0.44524     0.114153    0.269419   -0.249425    0.047137     -0.207822    0.346376    0.0717008 
 -0.015458      0.34168     0.0450118   -0.565215   -0.322201   -0.322876    -0.190385   -0.129434    -0.621308    -0.104753    0.108474    -0.860024   -0.371644   -1.00832     0.489999    0.714245   -0.0940382   0.223386    0.658454   -0.0110307  -0.128068    0.137173   -0.181896     -0.0176822  -0.538158   -0.286118  
 -0.274624     -0.104341    0.0867434    0.2776      0.137442   -0.0170195    0.0729165  -0.0119904    0.136262     0.199848    0.0339391    0.307811   -0.216203   -0.0593684   0.0207346  -0.104275    0.0172393   0.182621    0.0067361  -0.122543   -0.0029581  -0.24459     0.188119      0.192487    0.14614    -0.141981  
 -0.151387      0.0860004  -0.507936    -0.02726     0.0284121  -0.105787    -0.336067    0.333412    -0.0560077   -0.269239   -0.00861146  -0.152182    0.0328325  -0.17761    -0.344238    0.29844     0.0335209  -0.112518   -0.139307   -0.442053    0.281627   -0.239119    0.0542777    -0.369595    0.168787   -0.173471  
  0.54         -0.71698     0.507777    -0.246154   -0.0810088   0.673139     0.617822   -0.6698       0.0207706    0.241987    0.096103     0.353373   -0.153575    0.518171   -0.336728    0.281966    0.0495395   0.327254    0.507289    0.232364   -0.481795   -0.190252   -0.0303166     0.264143   -0.285047    0.0403048 
 -0.140529     -0.498091   -0.129413     0.580538    0.270286    0.0305523   -0.102126   -0.325787    -0.0787635   -0.064091   -0.0306879   -0.379061   -0.107686   -0.410803    0.180901    0.251799   -0.118267    0.0302519  -0.36859    -0.900573   -0.397635   -0.57187     0.000260644   0.152869   -0.144732   -0.663002  
  0.992754      0.166171   -0.218384     0.327118   -0.291266   -0.132117     0.332217    0.460872     0.365466     0.234523    0.883102    -0.318664    0.887741    0.113249   -0.119579    0.285747   -0.42895     0.19679     0.0480899   0.0488223   0.159172   -0.13602     0.549482     -0.584282   -0.251575   -0.333238  
 -0.0656681    -0.610709   -0.378417    -0.157695    0.406057   -0.367131    -0.010158    0.00310233  -0.0491162    0.366732   -0.693808    -0.342721   -0.957       0.529413   -0.669432    0.184021    0.497311   -0.542135    0.162637   -0.289408   -0.13369     0.397892   -0.193758     -0.339411   -0.113174   -0.344524  
 -0.0133448     0.115908   -0.534283    -0.15737    -0.503666   -0.518953    -0.0983378  -0.108211    -0.118931     0.208158    0.075756     0.161438    0.280642   -0.120124    0.237374   -0.120332    0.397163    0.316177    0.496991    0.885692   -0.179101    0.327262    0.624969     -0.684672    0.108137    0.188044  
  0.0481495     0.53279     0.563131    -0.263022    0.355965    0.254431     0.476963    0.410019     0.424427    -0.138039    0.16403     -0.383261   -0.552006    0.809938   -0.101663   -0.375164   -0.271467   -0.28983     0.840639    0.604336   -0.235375   -0.307873   -0.249357     -0.111868    0.181109    0.0943264 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.408761
INFO: iteration 2, average log likelihood -1.408752
INFO: iteration 3, average log likelihood -1.408743
INFO: iteration 4, average log likelihood -1.408735
INFO: iteration 5, average log likelihood -1.408727
INFO: iteration 6, average log likelihood -1.408718
INFO: iteration 7, average log likelihood -1.408710
INFO: iteration 8, average log likelihood -1.408702
INFO: iteration 9, average log likelihood -1.408693
INFO: iteration 10, average log likelihood -1.408685
INFO: EM with 100000 data points 10 iterations avll -1.408685
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
