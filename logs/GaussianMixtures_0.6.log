>>> 'Pkg.add("GaussianMixtures")' log
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing FileIO v0.2.0
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing HDF5 v0.6.6
INFO: Installing JLD v0.6.5
INFO: Installing LegacyStrings v0.1.1
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing PDMats v0.5.0
INFO: Installing Rmath v0.1.4
INFO: Installing SHA v0.2.1
INFO: Installing ScikitLearnBase v0.2.0
INFO: Installing StaticArrays v0.0.11
INFO: Installing StatsBase v0.11.1
INFO: Installing StatsFuns v0.3.1
INFO: Installing URIParser v0.1.6
INFO: Building Blosc
INFO: Building Rmath
INFO: Building HDF5
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of GaussianMixtures
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("GaussianMixtures")' log
Julia Version 0.6.0-dev.1146
Commit c55037a (2016-10-28 19:30 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-100-generic #147-Ubuntu SMP Tue Oct 18 16:48:51 UTC 2016 x86_64 x86_64
Memory: 2.939289093017578 GB (600.875 MB free)
Uptime: 19871.0 sec
Load Avg:  0.9970703125  0.99462890625  1.03662109375
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3497 MHz    1295284 s       6414 s     112308 s     337833 s         13 s
#2  3497 MHz     480562 s        328 s      62671 s    1377074 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - GaussianMixtures              0.1.0
 - JSON                          0.8.0
20 additional packages:
 - BinDeps                       0.4.5
 - Blosc                         0.1.7
 - Calculus                      0.1.15
 - Clustering                    0.7.0
 - Compat                        0.9.3
 - Distances                     0.3.2
 - Distributions                 0.11.0
 - FileIO                        0.2.0
 - HDF5                          0.6.6
 - JLD                           0.6.5
 - LegacyStrings                 0.1.1
 - NearestNeighbors              0.1.0
 - PDMats                        0.5.0
 - Rmath                         0.1.4
 - SHA                           0.2.1
 - ScikitLearnBase               0.2.0
 - StaticArrays                  0.0.11
 - StatsBase                     0.11.1
 - StatsFuns                     0.3.1
 - URIParser                     0.1.6
INFO: Testing GaussianMixtures
INFO: Testing Data
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1722
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::##9#10{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}})(::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl:23
 in macro expansion at ./asyncmap.jl:64 [inlined]
 in (::Base.##763#765{Base.AsyncCollector,Base.AsyncCollectorState})() at ./task.jl:363
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/data.jl, in expression starting on line 26
(100000,-1.127628902611667e7,[62919.9,37080.1],
[-5489.89 26292.9 26384.7; 6052.77 -26307.3 -26449.7],

Array{Float64,2}[
[62008.9 1191.33 2228.4; 1191.33 55942.5 -5518.97; 2228.4 -5518.97 56509.4],

[37784.8 -1081.88 -1458.71; -1081.88 43233.8 6100.44; -1458.71 6100.44 42932.4]])
WARNING: rmprocs: process 1 not removed
INFO: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.517387e+03
      1       1.011753e+03      -5.056337e+02 |        7
      2       9.304452e+02      -8.130829e+01 |        2
      3       9.277076e+02      -2.737565e+00 |        0
      4       9.277076e+02       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 927.707624300363)
INFO: K-means with 272 data points using 4 iterations
11.3 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: Running 0 iterations EM on full cov GMM with 8 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:270
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 4
INFO: EM with 272 data points 0 iterations avll -2.082536
5.8 data points per parameter
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:90
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: digamma{T <: Number}(x::AbstractArray{T}) is deprecated, use digamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in digamma(::Array{Float64,1}) at ./deprecated.jl:50
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in expectations(::GaussianMixtures.VGMM{Float64}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:93
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:243
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::Array{Float64,2}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:217
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:225
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in mylogdet at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:44 [inlined]
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
WARNING: lgamma{T <: Number}(x::AbstractArray{T}) is deprecated, use lgamma.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in lgamma(::Array{Float64,1}) at ./deprecated.jl:50
 in (::GaussianMixtures.#logB#24{Int64})(::UpperTriangular{Float64,Array{Float64,2}}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:206
 in lowerbound(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,1}, ::Array{Float64,2}, ::Array{Array{Float64,2},1}, ::Array{Float64,1}, ::Array{Float64,1}, ::Float64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:229
 in emstep!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:261
 in #em!#29(::Int64, ::Function, ::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:272
 in em!(::GaussianMixtures.VGMM{Float64}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/bayes.jl:269
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/bayes.jl, in expression starting on line 7
INFO: iteration 1, lowerbound -3.861606
INFO: iteration 2, lowerbound -3.751812
INFO: iteration 3, lowerbound -3.617307
INFO: iteration 4, lowerbound -3.435680
INFO: iteration 5, lowerbound -3.222619
INFO: iteration 6, lowerbound -3.012931
INFO: dropping number of Gaussions to 7
INFO: iteration 7, lowerbound -2.829244
INFO: iteration 8, lowerbound -2.697070
INFO: dropping number of Gaussions to 5
INFO: iteration 9, lowerbound -2.605401
INFO: dropping number of Gaussions to 4
INFO: iteration 10, lowerbound -2.525890
INFO: iteration 11, lowerbound -2.473383
INFO: dropping number of Gaussions to 3
INFO: iteration 12, lowerbound -2.426685
INFO: iteration 13, lowerbound -2.386601
INFO: iteration 14, lowerbound -2.354769
INFO: iteration 15, lowerbound -2.328292
INFO: iteration 16, lowerbound -2.311298
INFO: iteration 17, lowerbound -2.307832
INFO: dropping number of Gaussions to 2
INFO: iteration 18, lowerbound -2.302917
INFO: iteration 19, lowerbound -2.299259
INFO: iteration 20, lowerbound -2.299256
INFO: iteration 21, lowerbound -2.299254
INFO: iteration 22, lowerbound -2.299254
INFO: iteration 23, lowerbound -2.299253
INFO: iteration 24, lowerbound -2.299253
INFO: iteration 25, lowerbound -2.299253
INFO: iteration 26, lowerbound -2.299253
INFO: iteration 27, lowerbound -2.299253
INFO: iteration 28, lowerbound -2.299253
INFO: iteration 29, lowerbound -2.299253
INFO: iteration 30, lowerbound -2.299253
INFO: iteration 31, lowerbound -2.299253
INFO: iteration 32, lowerbound -2.299253
INFO: iteration 33, lowerbound -2.299253
INFO: iteration 34, lowerbound -2.299253
INFO: iteration 35, lowerbound -2.299253
INFO: iteration 36, lowerbound -2.299253
INFO: iteration 37, lowerbound -2.299253
INFO: iteration 38, lowerbound -2.299253
INFO: iteration 39, lowerbound -2.299253
INFO: iteration 40, lowerbound -2.299253
INFO: iteration 41, lowerbound -2.299253
INFO: iteration 42, lowerbound -2.299253
INFO: iteration 43, lowerbound -2.299253
INFO: iteration 44, lowerbound -2.299253
INFO: iteration 45, lowerbound -2.299253
INFO: iteration 46, lowerbound -2.299253
INFO: iteration 47, lowerbound -2.299253
INFO: iteration 48, lowerbound -2.299253
INFO: iteration 49, lowerbound -2.299253
INFO: iteration 50, lowerbound -2.299253
INFO: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
GaussianMixtures.History[Sat 29 Oct 2016 09:59:46 AM UTC: Initializing GMM, 8 Gaussians diag covariance 2 dimensions using 272 data points
,Sat 29 Oct 2016 09:59:47 AM UTC: K-means with 272 data points using 4 iterations
11.3 data points per parameter
,Sat 29 Oct 2016 09:59:48 AM UTC: EM with 272 data points 0 iterations avll -2.082536
5.8 data points per parameter
,Sat 29 Oct 2016 09:59:48 AM UTC: GMM converted to Variational GMM
,Sat 29 Oct 2016 09:59:50 AM UTC: iteration 1, lowerbound -3.861606
,Sat 29 Oct 2016 09:59:50 AM UTC: iteration 2, lowerbound -3.751812
,Sat 29 Oct 2016 09:59:50 AM UTC: iteration 3, lowerbound -3.617307
,Sat 29 Oct 2016 09:59:50 AM UTC: iteration 4, lowerbound -3.435680
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 5, lowerbound -3.222619
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 6, lowerbound -3.012931
,Sat 29 Oct 2016 09:59:51 AM UTC: dropping number of Gaussions to 7
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 7, lowerbound -2.829244
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 8, lowerbound -2.697070
,Sat 29 Oct 2016 09:59:51 AM UTC: dropping number of Gaussions to 5
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 9, lowerbound -2.605401
,Sat 29 Oct 2016 09:59:51 AM UTC: dropping number of Gaussions to 4
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 10, lowerbound -2.525890
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 11, lowerbound -2.473383
,Sat 29 Oct 2016 09:59:51 AM UTC: dropping number of Gaussions to 3
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 12, lowerbound -2.426685
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 13, lowerbound -2.386601
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 14, lowerbound -2.354769
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 15, lowerbound -2.328292
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 16, lowerbound -2.311298
,Sat 29 Oct 2016 09:59:51 AM UTC: iteration 17, lowerbound -2.307832
,Sat 29 Oct 2016 09:59:52 AM UTC: dropping number of Gaussions to 2
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 18, lowerbound -2.302917
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 19, lowerbound -2.299259
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 20, lowerbound -2.299256
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 21, lowerbound -2.299254
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 22, lowerbound -2.299254
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 23, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 24, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 25, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 26, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 27, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 28, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 29, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 30, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 31, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 32, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 33, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 34, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 35, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 36, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 37, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:52 AM UTC: iteration 38, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 39, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 40, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 41, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 42, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 43, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 44, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 45, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 46, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 47, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 48, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 49, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: iteration 50, lowerbound -2.299253
,Sat 29 Oct 2016 09:59:53 AM UTC: 50 variational Bayes EM-like iterations using 272 data points, final lowerbound -2.299253
]
α = [178.045,95.9549]
β = [178.045,95.9549]
m = [4.2503 79.2869; 2.00023 53.852]
ν = [180.045,97.9549]
W = UpperTriangular{Float64,Array{Float64,2}}[
[0.184042 -0.00764405; 0.0 0.00858171],

[0.375876 -0.00895312; 0.0 0.0127487]]
Kind: diag, size256
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,1}) at ./deprecated.jl:50
 in rand(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:58
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: sqrt{T <: Number}(x::AbstractArray{T}) is deprecated, use sqrt.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sqrt(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:48
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:67
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in stats(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:123
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:10 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 99999.99999999996
avll from stats: -0.9927201657056816
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in llpg(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:290
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:13 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:14 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll from llpg:  -0.9927201657056823
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:15 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
avll direct:     -0.9927201657056823
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:17 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
sum posterior: 100000.0
Kind: full, size16
WARNING: is is deprecated, use === instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in is(::Array{Float64,2}, ::Vararg{Array{Float64,2},N}) at ./deprecated.jl:30
 in _rcopy!(::Array{Float64,2}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/utils.jl:10
 in unwhiten!(::Array{Float64,2}, ::PDMats.PDMat{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/PDMats/src/pdmat.jl:60
 in rand(::Distributions.MvNormal{Float64,PDMats.PDMat{Float64,Array{Float64,2}},Array{Float64,1}}, ::Int64) at /home/vagrant/.julia/v0.6/Distributions/src/multivariates.jl:18
 in rand(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/rand.jl:60
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:7 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 2
nx: 100000 sum(zeroth order stats): 100000.00000000001
avll from stats: -0.9893916543677451
avll from llpg:  -0.9893916543677452
avll direct:     -0.9893916543677452
sum posterior: 100000.0
32×26 Array{Float64,2}:
 -0.0839376    0.169154     0.0744389    0.171923    -0.124864    -0.14402     0.118157     0.00755459   0.00550017  -0.136808    -0.239507     0.0238866   -0.194683     0.0171726    0.0548571    0.0128619    0.121645     -0.0301133    0.0756106   -0.024497    0.0424383    0.1626     -0.144649     0.113111   -0.0211433     0.0975671 
 -0.129588     0.105417    -0.033824     0.0418887   -0.0439338   -0.0999309   0.210389    -0.046897    -0.0544057    0.0019172    0.160863     0.0170599   -0.136641    -0.0271405   -0.117392     0.121885    -0.0542806     0.181058     0.0917232   -0.279599    0.0161634    0.0868981  -0.0836744    0.0455116   0.0966862     0.124178  
 -0.00642394  -0.134012    -0.219247     0.061041     0.0266937   -0.130478   -0.0645903   -0.0130833   -0.029759    -0.110578    -0.0801354   -0.0362586   -0.0190254   -0.0566981    0.123736    -0.0715317    0.0538385     0.0918474    0.157356     0.0300935  -0.0512839    0.0156242   0.173011     0.11638    -0.143347      0.0943064 
  0.0156525   -0.00682863  -0.00185729   0.0978197   -0.0851131   -0.0615774   0.0672956   -0.111437     0.0355081   -0.01679     -0.0351813   -0.0217785    0.0505839    0.0033275    0.00837488  -0.0567706   -0.11999       0.202446     0.029533    -0.311762    0.173209    -0.0437968  -0.0663963    0.0435499   0.0573578     0.0377695 
  0.0526449    0.0036221   -0.109086     0.0263095    0.171174    -0.172358   -0.11147     -0.0275744   -0.0400681   -0.0272273    0.0494763   -0.0125487    0.0251523    0.0415424   -0.0785427   -0.127511     0.110171      0.0755891    0.0650609    0.0212692  -0.112705    -0.0137653  -0.0199622   -0.222288    0.0267612     0.0405466 
 -0.0186641    0.0058028    0.12937      0.132564     0.0229833   -0.0677279   0.0917231   -0.11945      0.0876464   -0.110661    -0.0539756   -0.0561444   -0.00106744   0.18341     -0.0775476   -0.0104972    0.0225524    -0.0220856   -0.0104455    0.10089     0.0613507    0.021816    0.230277     0.148678   -0.163838     -0.0216803 
  0.204086     0.124956     0.153878    -0.0723037    0.111338     0.104462    0.207589    -0.0941127   -0.0305254    0.0511138    0.137843    -0.0312831   -0.0465731   -0.0342327   -0.0599634    0.00422387  -0.158855     -0.0143816    0.0204039   -0.0420087  -0.127459     0.127958    0.086622     0.0782568   0.0133512    -0.0733822 
  0.0487789    0.080446    -0.0808472   -0.0496592    0.141559    -0.0506666   0.0197906   -0.0493625    0.0544344    0.121038     0.0246993    0.0979844    0.0243512   -0.10157      0.132123    -0.0984647   -0.124759     -0.0304975   -0.146637     0.0936516  -0.112388     0.0141435   0.0121824   -0.0999071  -0.0588637     0.0700838 
 -0.0345026    0.0627456    0.0331673    0.0713619    0.0957787   -0.0701211   0.0225061   -0.183134    -0.180106     0.0374296    0.00658875   0.0286906   -0.105429    -0.0262161    0.182763    -0.0778921   -0.17228      -0.122024    -0.239698     0.161382    0.131118     0.169613   -0.0664307   -0.0401959  -0.101777     -0.0134568 
  0.0774342   -0.122374     0.202161     0.0402297   -0.0891611    0.111231    0.0642261   -0.124104     0.0255719   -0.0680239    0.272219     0.0548752    0.0902443   -0.0988752   -0.0641538    0.23102     -0.0196135     0.114738    -0.00258021  -0.0655088  -0.166007     0.155047    0.103281    -0.0706375   0.198947     -0.0397618 
  0.00465478  -0.166563    -0.00119028  -0.0311643   -0.0202383   -0.0248805   0.0336964    0.0243141   -0.00431769   0.131471    -0.0739255   -0.122977     0.301398    -0.21665      0.0465569    0.120628    -0.0808196    -0.0317669   -0.0330589   -0.0884692  -0.0669745    0.0963377   0.0577231    0.0745302  -0.026439      0.163102  
 -0.030316     0.0120818   -0.0363576    0.177165     0.189218    -0.0508729   0.125985    -0.170238    -0.104834    -0.0105294    0.106626    -0.0244661   -0.0274548    0.0640011   -0.00659853  -0.104539    -0.000182094   0.0629967   -0.0518886    0.0743648  -0.00783264  -0.0260356   0.0119125    0.0656274  -0.00801055    0.00910185
 -0.108515    -0.0610549   -0.0218525   -0.00810867  -0.00467644  -0.0532587  -0.0298144    0.187458    -0.214525     0.0377247   -0.0261191   -0.116233     0.00107873   0.116328    -0.127272    -0.10698     -0.140565     -0.0933422    0.0824845    0.111485    0.101653     0.131789   -0.165663    -0.0986001   0.0711214     0.0150804 
 -0.00362362   0.0834362   -0.0209492   -0.0757707    0.0418269    0.0641536  -0.00255272   0.0516224   -0.037359    -0.0452871    0.0217532    0.0112043   -0.056098     0.0649134   -0.11285      0.0728031   -0.0302177    -0.0669396    0.132322    -0.149821    0.0301663    0.0492965   0.0773426    0.0524329  -0.0180018    -0.0938842 
  0.119763    -0.0235644    0.186022    -0.181227    -0.17628      0.0870223  -0.14414     -0.0384894   -0.105211     0.073057    -0.13461      0.0684712    0.0187831    0.0685772    0.213981    -0.0244818   -0.0240292     0.0276081    0.145772     0.0892582   0.0982044    0.0660799   0.0877866    0.0477456  -0.0582341     0.0178232 
 -0.0429588    0.0168767    0.106132    -0.0544489   -0.00449214  -0.0299646  -0.0996719    0.0373104   -0.0440408    0.104976     0.0509693   -0.101427     0.0613853   -0.0743831    0.0555463   -0.019069     0.092211     -0.0168469    0.0767256   -0.0466649  -0.0250525    0.111833    0.0219071    0.105275    0.198517     -0.278446  
 -0.0257935   -0.0267996    0.021951     0.0393819   -0.115087    -0.0682898   0.00454712  -0.0692359   -0.0171957    0.097734     0.0906144    0.181902     0.138866    -0.112992     0.296928     0.0210078   -0.0461831     0.00420923  -0.0644734   -0.0489779   0.0207107   -0.042296   -0.0858854    0.124404   -0.273761     -0.187505  
 -0.0762229   -0.0592406    0.0013488   -0.00941806  -0.106126     0.154082    0.114316     0.135377    -0.0321577    0.136259    -0.0420728    0.0906981   -0.00690726   0.0717025   -0.00408006   0.00162575   0.0452536    -0.0203427   -0.110873    -0.102632   -0.0980137   -0.0786487  -0.189569     0.0545208  -0.0508156     0.0300741 
  0.0664985    0.0307446    0.117403     0.0561766   -0.0717932   -0.0437778   0.02148      0.0478168    0.0254157    0.00607558   0.144641    -0.110495     0.0380798   -0.236317     0.0230544   -0.018992    -0.0844975    -0.175989    -0.0104736    0.0918595  -0.104269    -0.0387355   0.00545991   0.0396587   0.160144      0.0112171 
  0.0835838   -0.00966221   0.00152607  -0.118268    -0.176476     0.0335706   0.0785365   -0.230835     0.137443    -0.127706     0.0359793    0.118065    -0.0267321   -0.182881     0.0211903   -0.0850855   -0.00689663    0.0201567    0.177774    -0.0496656   0.223584     0.142478   -0.169788     0.0250379  -0.0677183     0.124711  
  0.021148     0.0299224    0.0632489    0.134196     0.147589     0.205953    0.163777    -0.0161176   -0.127473    -0.113848    -0.12964      0.00483776  -0.0452747   -0.00306231   0.0126406    0.0367084    0.0838132    -0.026808    -0.0332382   -0.0173861  -0.0596043   -0.20202    -0.144283    -0.105443   -0.0276182     0.214307  
 -0.165883     0.0846156    0.10783      0.155444    -0.0120745   -0.033462    0.0243023   -0.0374865    0.0409637    0.124946    -0.0161891   -0.082436    -0.13932      0.0262325   -0.0920302   -0.0139105    0.0984825    -0.063763    -0.0628351    0.0491112  -0.112977    -0.178971    0.0601755    0.0193968   0.0185317    -0.0520861 
 -0.0425698    0.0383879    0.158991    -0.00971227   0.0190531   -0.231701    0.0490189    0.0146903   -0.0057966   -0.0883556   -0.240156     0.0236705   -0.0371165   -0.0302817    0.0378705   -0.218742     0.137063      0.0155672    0.00678836   0.138305    0.0252176   -0.064165    0.0263168   -0.119776    0.0505118    -0.0115953 
 -0.0154982   -0.107699     0.13307      0.0591595    0.0333952    0.248838   -0.105244     0.0480411    0.0188806   -0.0441515    0.0685916   -0.0414393   -0.0421153    0.0175696   -0.0516811    0.0455853   -0.0685645     0.0354206    0.068025    -0.128162    0.0557173    0.113994    0.0481376   -0.119478   -0.000190016  -0.186536  
  0.0175407   -0.148802    -0.0791821   -0.060923     0.0807077   -0.0325082   0.0408704   -0.0209386    0.0527891    0.0516779   -0.00121729  -0.0476067    0.174739    -0.101501     0.0683597   -0.0343025   -0.0685429    -0.0103021    0.116118     0.304758   -0.0211271    0.174536    0.0843785   -0.0771193   0.163288     -0.255993  
  0.0107079    0.226815     0.00941039   0.164305     0.0551599   -0.0214342   0.123909     0.297179     0.0326437    0.0545198   -0.0747526    0.0317828    0.0760019    0.151859    -0.00672015  -0.00130043  -0.0764146    -0.0187736    0.123321     0.126888    0.0542749    0.130413    0.123504     0.0911594  -0.0271025     0.0741505 
 -0.129417     0.01488     -0.077692     0.0722731   -0.0308692   -0.0192168  -0.056233    -9.34865e-5   0.00602052  -0.117372    -0.0882992   -0.0459923    0.0178719    0.0422466   -0.309312     0.0170916    0.215243     -0.0728378    0.0150589   -0.0138193   0.0060465   -0.0651529   0.0362092   -0.0344294   0.0957339    -0.0218149 
  0.0168925    0.0251109   -0.0964263    0.1451       0.0300194   -0.0281125   0.0997319    0.101098     0.00555573   0.0106598    0.0200121    0.0932543    0.164138     0.178526     0.21273      0.266829     0.0273223    -0.0167141   -0.170446    -0.142363    0.0274865   -0.11456    -0.0105482    0.157631    0.0461077     0.0223401 
  0.143377    -0.150379     0.126632    -0.0731856    0.0592462   -0.19256     0.0346711   -0.0202481    0.0810431   -0.00423011   0.136378     0.00153137  -0.0506086    0.0728459   -0.0687288    0.0784865   -0.00981839   -0.0128403   -0.167894     0.14586     0.00992931   0.0406794  -0.0588472   -0.0244176   0.104859     -0.183139  
  0.100955     0.132826     0.0143771    0.16776     -0.0468423    0.0841151  -0.0260077    0.00323496  -0.116168     0.0377779   -0.0460034   -0.094646    -0.0468113    0.075497     0.0394076   -0.0847179    0.177088     -0.00785226  -0.199318     0.10215    -0.117084    -0.0399428   0.164684    -0.0867747  -0.0681351     0.00539424
 -0.02023     -0.101898    -0.0786551   -0.0121076   -0.0296042   -0.145142    0.138924     0.106848    -0.0762038    0.0861535    0.241451     0.00893172  -0.114061     0.0307671   -0.0997901   -0.0284133    0.0876028     0.118422    -0.0312988    0.0737202  -0.00885817   0.0856532   0.0144855    0.0379036  -0.0157628    -0.061316  
 -0.140919     0.146628     0.0346899    0.0813282    0.0724187   -0.0650786  -0.0318314    0.0632195    0.144396    -0.119756     0.138563    -0.139171     0.00371432   0.0715261   -0.0174206   -0.0891106    0.126624     -0.204176     0.068825     0.232844   -0.056317     0.21762     0.0502981    0.0143341   0.0443639    -0.0491633 kind diag, method split
0: avll = -1.3975990908529474
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{Float64,2}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:50
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:261
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{Float64,2}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMM2#12(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:147
 in (::GaussianMixtures.#kw##GMM2)(::Array{Any,1}, ::GaussianMixtures.#GMM2, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:32
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in macro expansion; at /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl:29 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianMixtures/test/train.jl, in expression starting on line 22
INFO: iteration 1, average log likelihood -1.397683
INFO: iteration 2, average log likelihood -1.397595
INFO: iteration 3, average log likelihood -1.396944
INFO: iteration 4, average log likelihood -1.390733
INFO: iteration 5, average log likelihood -1.374214
INFO: iteration 6, average log likelihood -1.366638
INFO: iteration 7, average log likelihood -1.365168
INFO: iteration 8, average log likelihood -1.364253
INFO: iteration 9, average log likelihood -1.363378
INFO: iteration 10, average log likelihood -1.362532
INFO: iteration 11, average log likelihood -1.361696
INFO: iteration 12, average log likelihood -1.360849
INFO: iteration 13, average log likelihood -1.360115
INFO: iteration 14, average log likelihood -1.359562
INFO: iteration 15, average log likelihood -1.359168
INFO: iteration 16, average log likelihood -1.358899
INFO: iteration 17, average log likelihood -1.358718
INFO: iteration 18, average log likelihood -1.358592
INFO: iteration 19, average log likelihood -1.358500
INFO: iteration 20, average log likelihood -1.358430
INFO: iteration 21, average log likelihood -1.358370
INFO: iteration 22, average log likelihood -1.358312
INFO: iteration 23, average log likelihood -1.358250
INFO: iteration 24, average log likelihood -1.358183
INFO: iteration 25, average log likelihood -1.358115
INFO: iteration 26, average log likelihood -1.358049
INFO: iteration 27, average log likelihood -1.357990
INFO: iteration 28, average log likelihood -1.357940
INFO: iteration 29, average log likelihood -1.357899
INFO: iteration 30, average log likelihood -1.357867
INFO: iteration 31, average log likelihood -1.357842
INFO: iteration 32, average log likelihood -1.357823
INFO: iteration 33, average log likelihood -1.357808
INFO: iteration 34, average log likelihood -1.357798
INFO: iteration 35, average log likelihood -1.357790
INFO: iteration 36, average log likelihood -1.357785
INFO: iteration 37, average log likelihood -1.357781
INFO: iteration 38, average log likelihood -1.357778
INFO: iteration 39, average log likelihood -1.357776
INFO: iteration 40, average log likelihood -1.357775
INFO: iteration 41, average log likelihood -1.357774
INFO: iteration 42, average log likelihood -1.357773
INFO: iteration 43, average log likelihood -1.357773
INFO: iteration 44, average log likelihood -1.357772
INFO: iteration 45, average log likelihood -1.357772
INFO: iteration 46, average log likelihood -1.357772
INFO: iteration 47, average log likelihood -1.357772
INFO: iteration 48, average log likelihood -1.357772
INFO: iteration 49, average log likelihood -1.357772
INFO: iteration 50, average log likelihood -1.357772
INFO: EM with 100000 data points 50 iterations avll -1.357772
952.4 data points per parameter
1: avll = [-1.39768,-1.39759,-1.39694,-1.39073,-1.37421,-1.36664,-1.36517,-1.36425,-1.36338,-1.36253,-1.3617,-1.36085,-1.36011,-1.35956,-1.35917,-1.3589,-1.35872,-1.35859,-1.3585,-1.35843,-1.35837,-1.35831,-1.35825,-1.35818,-1.35812,-1.35805,-1.35799,-1.35794,-1.3579,-1.35787,-1.35784,-1.35782,-1.35781,-1.3578,-1.35779,-1.35778,-1.35778,-1.35778,-1.35778,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.357880
INFO: iteration 2, average log likelihood -1.357762
INFO: iteration 3, average log likelihood -1.357181
INFO: iteration 4, average log likelihood -1.351762
INFO: iteration 5, average log likelihood -1.335549
INFO: iteration 6, average log likelihood -1.325139
INFO: iteration 7, average log likelihood -1.322078
INFO: iteration 8, average log likelihood -1.320664
INFO: iteration 9, average log likelihood -1.319660
INFO: iteration 10, average log likelihood -1.318808
INFO: iteration 11, average log likelihood -1.317997
INFO: iteration 12, average log likelihood -1.317222
INFO: iteration 13, average log likelihood -1.316527
INFO: iteration 14, average log likelihood -1.315919
INFO: iteration 15, average log likelihood -1.315389
INFO: iteration 16, average log likelihood -1.314900
INFO: iteration 17, average log likelihood -1.314434
INFO: iteration 18, average log likelihood -1.313987
INFO: iteration 19, average log likelihood -1.313551
INFO: iteration 20, average log likelihood -1.313147
INFO: iteration 21, average log likelihood -1.312782
INFO: iteration 22, average log likelihood -1.312465
INFO: iteration 23, average log likelihood -1.312199
INFO: iteration 24, average log likelihood -1.311979
INFO: iteration 25, average log likelihood -1.311801
INFO: iteration 26, average log likelihood -1.311656
INFO: iteration 27, average log likelihood -1.311542
INFO: iteration 28, average log likelihood -1.311453
INFO: iteration 29, average log likelihood -1.311383
INFO: iteration 30, average log likelihood -1.311327
INFO: iteration 31, average log likelihood -1.311280
INFO: iteration 32, average log likelihood -1.311240
INFO: iteration 33, average log likelihood -1.311206
INFO: iteration 34, average log likelihood -1.311176
INFO: iteration 35, average log likelihood -1.311149
INFO: iteration 36, average log likelihood -1.311125
INFO: iteration 37, average log likelihood -1.311103
INFO: iteration 38, average log likelihood -1.311083
INFO: iteration 39, average log likelihood -1.311065
INFO: iteration 40, average log likelihood -1.311048
INFO: iteration 41, average log likelihood -1.311032
INFO: iteration 42, average log likelihood -1.311017
INFO: iteration 43, average log likelihood -1.311002
INFO: iteration 44, average log likelihood -1.310986
INFO: iteration 45, average log likelihood -1.310970
INFO: iteration 46, average log likelihood -1.310952
INFO: iteration 47, average log likelihood -1.310932
INFO: iteration 48, average log likelihood -1.310911
INFO: iteration 49, average log likelihood -1.310890
INFO: iteration 50, average log likelihood -1.310868
INFO: EM with 100000 data points 50 iterations avll -1.310868
473.9 data points per parameter
2: avll = [-1.35788,-1.35776,-1.35718,-1.35176,-1.33555,-1.32514,-1.32208,-1.32066,-1.31966,-1.31881,-1.318,-1.31722,-1.31653,-1.31592,-1.31539,-1.3149,-1.31443,-1.31399,-1.31355,-1.31315,-1.31278,-1.31246,-1.3122,-1.31198,-1.3118,-1.31166,-1.31154,-1.31145,-1.31138,-1.31133,-1.31128,-1.31124,-1.31121,-1.31118,-1.31115,-1.31112,-1.3111,-1.31108,-1.31107,-1.31105,-1.31103,-1.31102,-1.311,-1.31099,-1.31097,-1.31095,-1.31093,-1.31091,-1.31089,-1.31087]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.311008
INFO: iteration 2, average log likelihood -1.310838
INFO: iteration 3, average log likelihood -1.310249
INFO: iteration 4, average log likelihood -1.304730
INFO: iteration 5, average log likelihood -1.286989
INFO: iteration 6, average log likelihood -1.272547
INFO: iteration 7, average log likelihood -1.265461
INFO: iteration 8, average log likelihood -1.260667
INFO: iteration 9, average log likelihood -1.257423
INFO: iteration 10, average log likelihood -1.255417
INFO: iteration 11, average log likelihood -1.254228
INFO: iteration 12, average log likelihood -1.253451
INFO: iteration 13, average log likelihood -1.252834
INFO: iteration 14, average log likelihood -1.252234
INFO: iteration 15, average log likelihood -1.251574
INFO: iteration 16, average log likelihood -1.250833
INFO: iteration 17, average log likelihood -1.250067
INFO: iteration 18, average log likelihood -1.249370
INFO: iteration 19, average log likelihood -1.248795
INFO: iteration 20, average log likelihood -1.248355
INFO: iteration 21, average log likelihood -1.248043
INFO: iteration 22, average log likelihood -1.247835
INFO: iteration 23, average log likelihood -1.247703
INFO: iteration 24, average log likelihood -1.247618
INFO: iteration 25, average log likelihood -1.247562
INFO: iteration 26, average log likelihood -1.247525
INFO: iteration 27, average log likelihood -1.247499
INFO: iteration 28, average log likelihood -1.247479
INFO: iteration 29, average log likelihood -1.247464
INFO: iteration 30, average log likelihood -1.247453
INFO: iteration 31, average log likelihood -1.247444
INFO: iteration 32, average log likelihood -1.247438
INFO: iteration 33, average log likelihood -1.247433
INFO: iteration 34, average log likelihood -1.247430
INFO: iteration 35, average log likelihood -1.247428
INFO: iteration 36, average log likelihood -1.247427
INFO: iteration 37, average log likelihood -1.247426
INFO: iteration 38, average log likelihood -1.247425
INFO: iteration 39, average log likelihood -1.247425
INFO: iteration 40, average log likelihood -1.247424
INFO: iteration 41, average log likelihood -1.247424
INFO: iteration 42, average log likelihood -1.247424
INFO: iteration 43, average log likelihood -1.247424
INFO: iteration 44, average log likelihood -1.247424
INFO: iteration 45, average log likelihood -1.247424
INFO: iteration 46, average log likelihood -1.247424
INFO: iteration 47, average log likelihood -1.247424
INFO: iteration 48, average log likelihood -1.247424
INFO: iteration 49, average log likelihood -1.247424
INFO: iteration 50, average log likelihood -1.247424
INFO: EM with 100000 data points 50 iterations avll -1.247424
236.4 data points per parameter
3: avll = [-1.31101,-1.31084,-1.31025,-1.30473,-1.28699,-1.27255,-1.26546,-1.26067,-1.25742,-1.25542,-1.25423,-1.25345,-1.25283,-1.25223,-1.25157,-1.25083,-1.25007,-1.24937,-1.24879,-1.24836,-1.24804,-1.24783,-1.2477,-1.24762,-1.24756,-1.24752,-1.2475,-1.24748,-1.24746,-1.24745,-1.24744,-1.24744,-1.24743,-1.24743,-1.24743,-1.24743,-1.24743,-1.24743,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.247601
INFO: iteration 2, average log likelihood -1.247334
INFO: iteration 3, average log likelihood -1.244939
INFO: iteration 4, average log likelihood -1.222857
WARNING: Variances had to be floored 5 11
INFO: iteration 5, average log likelihood -1.181052
WARNING: Variances had to be floored 12
INFO: iteration 6, average log likelihood -1.181031
WARNING: Variances had to be floored 5 11
INFO: iteration 7, average log likelihood -1.168216
INFO: iteration 8, average log likelihood -1.176446
WARNING: Variances had to be floored 5 11 12
INFO: iteration 9, average log likelihood -1.161911
INFO: iteration 10, average log likelihood -1.178530
WARNING: Variances had to be floored 5
INFO: iteration 11, average log likelihood -1.168519
WARNING: Variances had to be floored 11
INFO: iteration 12, average log likelihood -1.168312
WARNING: Variances had to be floored 5 12
INFO: iteration 13, average log likelihood -1.167376
WARNING: Variances had to be floored 11
INFO: iteration 14, average log likelihood -1.172934
WARNING: Variances had to be floored 5
INFO: iteration 15, average log likelihood -1.167589
WARNING: Variances had to be floored 11 12
INFO: iteration 16, average log likelihood -1.167686
WARNING: Variances had to be floored 5
INFO: iteration 17, average log likelihood -1.169437
INFO: iteration 18, average log likelihood -1.173036
WARNING: Variances had to be floored 5 11
INFO: iteration 19, average log likelihood -1.157551
WARNING: Variances had to be floored 12
INFO: iteration 20, average log likelihood -1.170496
WARNING: Variances had to be floored 5 11
INFO: iteration 21, average log likelihood -1.160811
INFO: iteration 22, average log likelihood -1.169255
WARNING: Variances had to be floored 5 11 12
INFO: iteration 23, average log likelihood -1.154661
INFO: iteration 24, average log likelihood -1.171384
WARNING: Variances had to be floored 5
INFO: iteration 25, average log likelihood -1.161352
WARNING: Variances had to be floored 11
INFO: iteration 26, average log likelihood -1.161056
WARNING: Variances had to be floored 5 12
INFO: iteration 27, average log likelihood -1.160227
WARNING: Variances had to be floored 11
INFO: iteration 28, average log likelihood -1.165669
WARNING: Variances had to be floored 5
INFO: iteration 29, average log likelihood -1.160550
WARNING: Variances had to be floored 11 12
INFO: iteration 30, average log likelihood -1.160838
WARNING: Variances had to be floored 5
INFO: iteration 31, average log likelihood -1.163291
INFO: iteration 32, average log likelihood -1.167758
WARNING: Variances had to be floored 5 11
INFO: iteration 33, average log likelihood -1.153021
WARNING: Variances had to be floored 12
INFO: iteration 34, average log likelihood -1.166774
WARNING: Variances had to be floored 5 11
INFO: iteration 35, average log likelihood -1.157780
INFO: iteration 36, average log likelihood -1.167230
WARNING: Variances had to be floored 5 11 12
INFO: iteration 37, average log likelihood -1.153012
INFO: iteration 38, average log likelihood -1.170038
WARNING: Variances had to be floored 5
INFO: iteration 39, average log likelihood -1.159902
WARNING: Variances had to be floored 11
INFO: iteration 40, average log likelihood -1.159714
WARNING: Variances had to be floored 5 12
INFO: iteration 41, average log likelihood -1.158859
WARNING: Variances had to be floored 11
INFO: iteration 42, average log likelihood -1.164551
WARNING: Variances had to be floored 5
INFO: iteration 43, average log likelihood -1.159338
WARNING: Variances had to be floored 11 12
INFO: iteration 44, average log likelihood -1.159943
WARNING: Variances had to be floored 5
INFO: iteration 45, average log likelihood -1.162306
INFO: iteration 46, average log likelihood -1.167074
WARNING: Variances had to be floored 5 11
INFO: iteration 47, average log likelihood -1.152133
WARNING: Variances had to be floored 12
INFO: iteration 48, average log likelihood -1.166185
WARNING: Variances had to be floored 5 11
INFO: iteration 49, average log likelihood -1.157057
INFO: iteration 50, average log likelihood -1.166757
INFO: EM with 100000 data points 50 iterations avll -1.166757
118.1 data points per parameter
4: avll = [-1.2476,-1.24733,-1.24494,-1.22286,-1.18105,-1.18103,-1.16822,-1.17645,-1.16191,-1.17853,-1.16852,-1.16831,-1.16738,-1.17293,-1.16759,-1.16769,-1.16944,-1.17304,-1.15755,-1.1705,-1.16081,-1.16925,-1.15466,-1.17138,-1.16135,-1.16106,-1.16023,-1.16567,-1.16055,-1.16084,-1.16329,-1.16776,-1.15302,-1.16677,-1.15778,-1.16723,-1.15301,-1.17004,-1.1599,-1.15971,-1.15886,-1.16455,-1.15934,-1.15994,-1.16231,-1.16707,-1.15213,-1.16619,-1.15706,-1.16676]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 9 10 21 22 24
INFO: iteration 1, average log likelihood -1.152721
WARNING: Variances had to be floored 9 10 21 22 23
INFO: iteration 2, average log likelihood -1.152369
WARNING: Variances had to be floored 9 10 21 22 24
INFO: iteration 3, average log likelihood -1.150949
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 4, average log likelihood -1.133320
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 5, average log likelihood -1.072944
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 6, average log likelihood -1.066500
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 27
INFO: iteration 7, average log likelihood -1.049357
WARNING: Variances had to be floored 9 10 21 22 23 24 31
INFO: iteration 8, average log likelihood -1.061003
WARNING: Variances had to be floored 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 9, average log likelihood -1.026354
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 10, average log likelihood -1.070172
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 11, average log likelihood -1.028988
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 12, average log likelihood -1.055986
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 27
INFO: iteration 13, average log likelihood -1.046417
WARNING: Variances had to be floored 9 10 21 22 23 24 31
INFO: iteration 14, average log likelihood -1.054618
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 15, average log likelihood -1.023858
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 16, average log likelihood -1.075965
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 17, average log likelihood -1.031147
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 18, average log likelihood -1.049878
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 27
INFO: iteration 19, average log likelihood -1.044109
WARNING: Variances had to be floored 9 10 21 22 23 24 31
INFO: iteration 20, average log likelihood -1.060290
WARNING: Variances had to be floored 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 21, average log likelihood -1.025878
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 22, average log likelihood -1.070190
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 23, average log likelihood -1.028819
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 24, average log likelihood -1.055619
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 27
INFO: iteration 25, average log likelihood -1.046369
WARNING: Variances had to be floored 9 10 21 22 23 24 31
INFO: iteration 26, average log likelihood -1.054500
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 27, average log likelihood -1.023660
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 28, average log likelihood -1.075928
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 29, average log likelihood -1.031064
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 30, average log likelihood -1.049751
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 27
INFO: iteration 31, average log likelihood -1.044034
WARNING: Variances had to be floored 9 10 21 22 23 24 31
INFO: iteration 32, average log likelihood -1.060162
WARNING: Variances had to be floored 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 33, average log likelihood -1.025630
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 34, average log likelihood -1.069611
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 35, average log likelihood -1.026500
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 36, average log likelihood -1.048346
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 26 27
INFO: iteration 37, average log likelihood -1.035950
WARNING: Variances had to be floored 9 10 21 22 23 24 31
INFO: iteration 38, average log likelihood -1.056441
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 39, average log likelihood -1.024960
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 40, average log likelihood -1.075917
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 41, average log likelihood -1.030540
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 42, average log likelihood -1.046102
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 26 27
INFO: iteration 43, average log likelihood -1.034885
WARNING: Variances had to be floored 9 10 21 22 23 24 31
INFO: iteration 44, average log likelihood -1.062342
WARNING: Variances had to be floored 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 45, average log likelihood -1.027424
WARNING: Variances had to be floored 9 10 21 22 23 24
INFO: iteration 46, average log likelihood -1.070189
WARNING: Variances had to be floored 3 9 10 11 19 20 21 22 23 24 27 31
INFO: iteration 47, average log likelihood -1.028396
WARNING: Variances had to be floored 9 10 17 21 22 23 24 29 30
INFO: iteration 48, average log likelihood -1.052186
WARNING: Variances had to be floored 9 10 11 19 20 21 22 23 24 27
INFO: iteration 49, average log likelihood -1.037211
WARNING: Variances had to be floored 9 10 21 22 23 24 26 31
INFO: iteration 50, average log likelihood -1.044258
INFO: EM with 100000 data points 50 iterations avll -1.044258
59.0 data points per parameter
5: avll = [-1.15272,-1.15237,-1.15095,-1.13332,-1.07294,-1.0665,-1.04936,-1.061,-1.02635,-1.07017,-1.02899,-1.05599,-1.04642,-1.05462,-1.02386,-1.07596,-1.03115,-1.04988,-1.04411,-1.06029,-1.02588,-1.07019,-1.02882,-1.05562,-1.04637,-1.0545,-1.02366,-1.07593,-1.03106,-1.04975,-1.04403,-1.06016,-1.02563,-1.06961,-1.0265,-1.04835,-1.03595,-1.05644,-1.02496,-1.07592,-1.03054,-1.0461,-1.03489,-1.06234,-1.02742,-1.07019,-1.0284,-1.05219,-1.03721,-1.04426]
[-1.3976,-1.39768,-1.39759,-1.39694,-1.39073,-1.37421,-1.36664,-1.36517,-1.36425,-1.36338,-1.36253,-1.3617,-1.36085,-1.36011,-1.35956,-1.35917,-1.3589,-1.35872,-1.35859,-1.3585,-1.35843,-1.35837,-1.35831,-1.35825,-1.35818,-1.35812,-1.35805,-1.35799,-1.35794,-1.3579,-1.35787,-1.35784,-1.35782,-1.35781,-1.3578,-1.35779,-1.35778,-1.35778,-1.35778,-1.35778,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35777,-1.35788,-1.35776,-1.35718,-1.35176,-1.33555,-1.32514,-1.32208,-1.32066,-1.31966,-1.31881,-1.318,-1.31722,-1.31653,-1.31592,-1.31539,-1.3149,-1.31443,-1.31399,-1.31355,-1.31315,-1.31278,-1.31246,-1.3122,-1.31198,-1.3118,-1.31166,-1.31154,-1.31145,-1.31138,-1.31133,-1.31128,-1.31124,-1.31121,-1.31118,-1.31115,-1.31112,-1.3111,-1.31108,-1.31107,-1.31105,-1.31103,-1.31102,-1.311,-1.31099,-1.31097,-1.31095,-1.31093,-1.31091,-1.31089,-1.31087,-1.31101,-1.31084,-1.31025,-1.30473,-1.28699,-1.27255,-1.26546,-1.26067,-1.25742,-1.25542,-1.25423,-1.25345,-1.25283,-1.25223,-1.25157,-1.25083,-1.25007,-1.24937,-1.24879,-1.24836,-1.24804,-1.24783,-1.2477,-1.24762,-1.24756,-1.24752,-1.2475,-1.24748,-1.24746,-1.24745,-1.24744,-1.24744,-1.24743,-1.24743,-1.24743,-1.24743,-1.24743,-1.24743,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.24742,-1.2476,-1.24733,-1.24494,-1.22286,-1.18105,-1.18103,-1.16822,-1.17645,-1.16191,-1.17853,-1.16852,-1.16831,-1.16738,-1.17293,-1.16759,-1.16769,-1.16944,-1.17304,-1.15755,-1.1705,-1.16081,-1.16925,-1.15466,-1.17138,-1.16135,-1.16106,-1.16023,-1.16567,-1.16055,-1.16084,-1.16329,-1.16776,-1.15302,-1.16677,-1.15778,-1.16723,-1.15301,-1.17004,-1.1599,-1.15971,-1.15886,-1.16455,-1.15934,-1.15994,-1.16231,-1.16707,-1.15213,-1.16619,-1.15706,-1.16676,-1.15272,-1.15237,-1.15095,-1.13332,-1.07294,-1.0665,-1.04936,-1.061,-1.02635,-1.07017,-1.02899,-1.05599,-1.04642,-1.05462,-1.02386,-1.07596,-1.03115,-1.04988,-1.04411,-1.06029,-1.02588,-1.07019,-1.02882,-1.05562,-1.04637,-1.0545,-1.02366,-1.07593,-1.03106,-1.04975,-1.04403,-1.06016,-1.02563,-1.06961,-1.0265,-1.04835,-1.03595,-1.05644,-1.02496,-1.07592,-1.03054,-1.0461,-1.03489,-1.06234,-1.02742,-1.07019,-1.0284,-1.05219,-1.03721,-1.04426]
32×26 Array{Float64,2}:
  0.0456421   -0.213825     -0.0748309    -0.0835802   0.0256495    0.000915707  -0.111509    -0.0192914    0.0420761    0.0262518    -0.0287055   -0.00693075   0.157908    -0.0832723    0.0853622   -0.0725872    -0.104213    -0.091363     0.0611779    0.376894    -0.0220196     0.156107    -0.534569    -0.103713    0.140041    -0.330792  
 -0.0165468   -0.0904482    -0.0821734    -0.0435622   0.117553    -0.0535198     0.175207    -0.0272366    0.0685636    0.0789036     0.0342147   -0.157472     0.209219    -0.0836628    0.0482792   -0.149709     -0.0461064   -0.00907925   0.143964     0.263537     0.0462146     0.186917     0.89726     -0.0836538   0.182939    -0.222757  
  0.204571     0.119987      0.145179     -0.0676884   0.123964     0.108362      0.21134     -0.0923235   -0.0347886    0.0516493     0.128786    -0.0443194   -0.0717386   -0.0206726   -0.0637477    0.00363535   -0.13774     -0.0130726    0.0212157   -0.0284796   -0.126684      0.109469     0.0723148    0.0779837   0.0141395   -0.0510705 
 -0.128333     0.0941882    -0.0477726     0.0380447  -0.0145819   -0.126695      0.189827    -0.0471293   -0.0541228    0.00292598    0.136312     0.0108765   -0.122297     0.00476542  -0.11136      0.104654     -0.052846     0.173939     0.0705435   -0.324224     0.0240416     0.0922505   -0.0645728    0.0596847   0.101119     0.15558   
  0.0565601   -0.0303982     0.13993       0.0816891  -0.0824915    0.0869426     0.0521302   -0.0800469   -0.039593    -0.0389784     0.173341     0.015132     0.0697779   -0.0556213   -0.0534245    0.135477      0.0729521    0.074226    -0.0695297   -0.0231441   -0.14656       0.0960775    0.123519    -0.095529    0.123979    -0.0394248 
 -0.126246     0.0159059    -0.0927905     0.0737106  -0.0267647   -0.0151091    -0.0768439    0.0194633    0.0375958   -0.116352     -0.0668423   -0.0417554    0.051802     0.0443241   -0.311881     0.0171534     0.23493     -0.0649371    0.0162313   -0.0220408   -0.0240848    -0.0559149    0.0271602   -0.0483753   0.0885481   -0.0182759 
  0.143983    -0.146404      0.0702952    -0.0747746   0.0415927   -0.196752      0.0478659   -0.0110827    0.0815534    0.00256815    0.171098    -0.00411675  -0.0501254    0.089343    -0.0438974    0.0609678    -0.036469    -0.0290055   -0.201314     0.183827     0.00455439    0.0658313   -0.051664    -0.0216644   0.100084    -0.167565  
  0.00222547   0.0571338    -0.0756937     0.117902    0.0471162   -0.0602233     0.0250175    0.129257    -0.0183796   -0.0167581    -0.0761865   -0.0111226    0.031245     0.0550447    0.0723328   -0.0190141     0.0157486    0.0301936    0.118386     0.087164     0.000369759   0.0700229    0.139243     0.0708995  -0.0851428    0.0834094 
 -0.0390898    0.0194289     0.123098      0.398667   -0.0483427   -0.112702      0.00804829   0.289857     0.0266192   -0.0447559     0.48113     -0.202114     0.110548    -0.236072    -0.469035    -0.0197787    -0.0838641   -0.169155     0.0770763    0.0813133   -0.27669      -0.0373937    0.00495272   0.0326203   0.0785495    0.00601415
  0.125413     0.0183952     0.116147     -0.257508   -0.0926803    0.00831354    0.0212552   -0.130021     0.0263542    0.123709     -0.142008    -0.0667642   -0.0257346   -0.236366     0.399882    -0.0165807    -0.0818775   -0.178678    -0.0347889    0.118604     0.0494547    -0.0560059    0.00538295   0.0392102   0.202085     0.00690453
 -0.0192985   -0.0212443     0.0200912     0.0544838  -0.118186    -0.0453259     0.00704296  -0.0616871   -0.0239475    0.0885484     0.087997     0.182235     0.132825    -0.113636     0.290182     0.012176     -0.0192289    0.0028801   -0.106915    -0.0463382    0.072476     -0.0346539   -0.0815578    0.108329   -0.275176    -0.187103  
 -0.0179237   -0.00620973   -0.0388462     0.191264    0.201848    -0.0673492     0.129065    -0.169054    -0.104605    -0.000564051   0.101327    -0.0236832   -0.0273232    0.076168     0.0217883   -0.105477     -0.00565512   0.0479771   -0.00325982   0.0749529   -0.00014388    0.0072931    0.026776     0.0716929  -0.00486434   0.0221087 
  0.0463916    0.0958572    -0.0694337    -0.041018    0.146018    -0.0529358     0.0350915    0.0102748    0.0558669    0.117743      0.0254379    0.0989197    0.0271179   -0.0629181    0.123497    -0.0993529    -0.129424    -0.00420719  -0.169961     0.0937771   -0.0999762     0.0162574   -0.0389211   -0.108192   -0.0690155    0.0896524 
 -0.0834982    0.166564      0.0544512     0.154114   -0.127625    -0.142512      0.131821     0.00675253   0.050371    -0.106741     -0.220857     0.032867    -0.182876     0.0257819    0.0399119    0.000541736   0.121584    -0.0232889    0.0258087   -0.0234153    0.0343281     0.164589    -0.114623     0.115531   -0.0144244    0.130981  
  0.0196619   -0.0367982    -0.00248463    0.0779698  -0.0866157   -0.0526627     0.0620389   -0.107491     0.0329737   -0.0396089    -0.00774825  -0.0152237    0.056845     0.011396    -0.0174078   -0.0549626    -0.0815394    0.201431     0.00278016  -0.268266     0.172719     -0.0598953   -0.0619195    0.0154193   0.00634814   0.0130304 
 -0.14872      0.0998943     0.0902485     0.12663     0.0240942   -0.0629068     0.00420061   0.0181229    0.0847206    0.0191665     0.0615587   -0.127096    -0.0692867    0.0581961   -0.0535744   -0.0534392     0.113019    -0.134835     0.00831906   0.146596    -0.116263      0.0201018    0.0536291    0.0169406   0.0189708   -0.0415595 
 -0.0545467   -0.11803       0.11054       0.0612731   0.0301869    0.237591     -0.101126     0.0451396    0.00735145  -0.0366876     0.0640979   -0.0517097   -0.0459466    0.015465    -0.0478335    0.0177165    -0.0822527    0.0334575    0.0580851   -0.10583      0.0500163     0.106749     0.0343601   -0.118695   -0.00215338  -0.16208   
 -0.0137242   -0.122146     -0.0595918     0.0153245  -0.045523    -0.113337      0.143194     0.115773    -0.0904117    0.0880573     0.24165      0.0163967   -0.0923041    0.127907    -0.107444    -0.0285362     0.100709     0.112511    -0.0348669    0.07005     -0.0131358     0.0874411    0.00290913   0.0374602  -0.0304626   -0.0653113 
  0.0858978    0.0208074    -0.0046921    -0.123511   -0.196559     0.0403804     0.0835779   -0.205914     0.147509    -0.117485      0.0362634    0.117166    -0.034936    -0.1802       0.0239631   -0.0784956     0.00775196   0.0155074    0.164915    -0.0575883    0.2141        0.139633    -0.161558     0.0221772  -0.068857     0.141313  
 -0.0368743    0.0315503     0.147594     -0.0104509   0.015563    -0.233725      0.00906665   0.105194    -0.0256647   -0.073471     -0.246029     0.0245092   -0.0239573   -0.0231373    0.057036    -0.239438      0.145206    -0.00453252   0.0141142    0.126175     0.018028     -0.0729696    0.0315444   -0.154641    0.0520897   -0.0366004 
  0.0159005    0.0448036    -0.0831277     0.168768   -0.026756    -0.0119257     0.111479     0.0330684    0.0172627   -0.147068      0.0167751    0.231669     0.14936      0.178429     0.302509     0.257753      0.0264927   -0.0164089   -0.156977    -0.239509     0.0148022    -0.108651    -0.104713     0.158923    0.0605677    0.0834146 
  0.0162414   -0.000947728  -0.101365      0.129735    0.0954004   -0.0257172     0.110343     0.136758    -0.0447034    0.167063      0.0208529   -0.0646849    0.147781     0.181016     0.0249052    0.256806      0.109458    -0.016294    -0.196637    -0.0504138    0.0180147    -0.100451     0.0824044    0.149531    0.0336621   -0.0431773 
 -0.313431     0.0840192     0.158747     -0.0855243  -0.00464542  -0.164228     -0.156995     0.017442    -0.0562262   -0.0333893     0.051949    -0.098288     0.104516    -0.107363     0.193336     0.0399219     0.137707    -0.0212358    0.130713    -0.226336    -0.0572722     0.176486     0.00577867   0.228544    0.268823    -0.327882  
  0.217071    -0.0318647     0.0684023    -0.0449137  -0.00497984   0.142071     -0.072956     0.0586235   -0.0378635    0.118294      0.0508157   -0.102018     0.0654176   -0.0561234   -0.0272172   -0.0910176     0.0432018   -0.00586569   0.0561267    0.140338    -0.0130252     0.0494012    0.0501628    0.0640886   0.117872    -0.283038  
  0.0265512    0.0402605     0.0287227     0.0669651   0.108624    -0.0469203     0.0216076   -0.177224    -0.190813     0.0433414     0.0103742    0.0397451   -0.107971    -0.0251996    0.191695    -0.0796813    -0.17109     -0.201613    -0.272099     0.173776     0.136111      0.17442     -0.0963099    0.0195128  -0.1082      -0.0079013 
  0.00131394   0.0795918    -0.0403677    -0.0664731   0.0697501    0.0523829    -0.0101757    0.0428171   -0.0532568   -0.0446309     0.0136641    0.014229    -0.0559328    0.064976    -0.112944     0.0704799    -0.0268109   -0.0764628    0.122128    -0.142782     0.0230969     0.0483806    0.0707072    0.0669813  -0.00127244  -0.0873422 
  0.0133187    0.0258149     0.0679895     0.132462    0.154396     0.201574      0.176314    -0.0172236   -0.157542    -0.113528     -0.144494    -0.00500232  -0.0340261    0.022867     0.00280287   0.0404871     0.0873518   -0.0751021   -0.0339244   -0.0175504   -0.0509502    -0.189076    -0.127447    -0.104341   -0.0289209    0.204548  
 -0.010156     0.00614285    0.13635       0.122078    0.0447519   -0.0695351     0.105738    -0.119142     0.0601048   -0.111246     -0.0441933   -0.0644201    0.00109672   0.186831    -0.0738493   -0.00936345    0.0220334   -0.0116991   -0.0115798    0.137852     0.0577144     0.0788555    0.235085     0.177347   -0.180039    -0.00425833
  0.0237787    0.00470408   -0.109357      0.033686    0.167486    -0.151283     -0.0972947   -0.0135083   -0.081206    -0.0299916     0.0567667   -0.00871745   0.0471671    0.0682764   -0.0804115   -0.131854      0.115024     0.0744648    0.112056    -0.00764748  -0.143318     -0.0456719   -0.012051    -0.193328    0.0295672    0.0292857 
 -0.0136519   -0.16398      -0.000970125  -0.0375251  -0.0326133   -0.0333785     0.0101314    0.0229459   -0.0550958    0.13993      -0.0643287   -0.107126     0.298238    -0.152741     0.0450213    0.118343     -0.0803905   -0.0346833   -0.0342281   -0.096994    -0.0712374     0.112626     0.0577162    0.0759916  -0.0457744    0.159374  
 -0.104883    -0.0504273     0.0117647    -0.0156125  -0.0128815   -0.0537028    -0.0196992    0.177667    -0.212261     0.0361496    -0.0206683   -0.113175    -0.0309088    0.0978895   -0.128998    -0.106788     -0.148668    -0.105021     0.0785758    0.107087     0.10209       0.131842    -0.163499    -0.0805228   0.0707453    0.0170486 
  0.022165    -0.0345587     0.107993     -0.0919752  -0.138304     0.123914     -0.0138525    0.0323705   -0.0746775    0.0936554    -0.0577308    0.0718854    0.00296447   0.0636585    0.100552    -0.0152065    -0.0329494   -0.0164957    0.0116416   -0.0161315   -0.0113363    -0.00812712  -0.0536185    0.0453125  -0.0644878    0.0335637 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 1, average log likelihood -1.026106
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 2, average log likelihood -1.019569
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30 31
INFO: iteration 3, average log likelihood -1.017018
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 4, average log likelihood -1.022826
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 5, average log likelihood -1.014346
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 26 27 29 30 31
INFO: iteration 6, average log likelihood -1.007296
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 7, average log likelihood -1.025870
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 8, average log likelihood -1.019464
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30 31
INFO: iteration 9, average log likelihood -1.016915
WARNING: Variances had to be floored 3 9 10 11 17 19 20 21 22 23 24 27 29 30
INFO: iteration 10, average log likelihood -1.022907
INFO: EM with 100000 data points 10 iterations avll -1.022907
59.0 data points per parameter
kind diag, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.171739e+05
      1       6.637387e+05      -1.534353e+05 |       32
      2       6.372690e+05      -2.646966e+04 |       32
      3       6.211815e+05      -1.608751e+04 |       32
      4       6.105534e+05      -1.062805e+04 |       32
      5       6.045193e+05      -6.034107e+03 |       32
      6       6.007248e+05      -3.794566e+03 |       32
      7       5.980814e+05      -2.643341e+03 |       32
      8       5.962484e+05      -1.833001e+03 |       32
      9       5.950367e+05      -1.211729e+03 |       32
     10       5.939625e+05      -1.074159e+03 |       32
     11       5.927354e+05      -1.227100e+03 |       32
     12       5.914724e+05      -1.263019e+03 |       32
     13       5.904563e+05      -1.016166e+03 |       32
     14       5.896511e+05      -8.051885e+02 |       32
     15       5.890718e+05      -5.792965e+02 |       32
     16       5.886345e+05      -4.372551e+02 |       32
     17       5.882029e+05      -4.316572e+02 |       32
     18       5.877743e+05      -4.285480e+02 |       32
     19       5.874368e+05      -3.374914e+02 |       32
     20       5.872596e+05      -1.771909e+02 |       32
     21       5.871371e+05      -1.224924e+02 |       32
     22       5.870366e+05      -1.005673e+02 |       32
     23       5.869695e+05      -6.702259e+01 |       32
     24       5.869239e+05      -4.569018e+01 |       32
     25       5.868944e+05      -2.946334e+01 |       31
     26       5.868798e+05      -1.454894e+01 |       32
     27       5.868702e+05      -9.640292e+00 |       29
     28       5.868618e+05      -8.410484e+00 |       30
     29       5.868485e+05      -1.330146e+01 |       27
     30       5.868280e+05      -2.044459e+01 |       31
     31       5.867893e+05      -3.873136e+01 |       32
     32       5.867361e+05      -5.321902e+01 |       31
     33       5.866301e+05      -1.059935e+02 |       31
     34       5.864747e+05      -1.554229e+02 |       31
     35       5.862649e+05      -2.097753e+02 |       32
     36       5.860652e+05      -1.996958e+02 |       32
     37       5.859897e+05      -7.547591e+01 |       31
     38       5.859719e+05      -1.784395e+01 |       31
     39       5.859657e+05      -6.156764e+00 |       23
     40       5.859637e+05      -2.029082e+00 |       20
     41       5.859617e+05      -2.043706e+00 |       21
     42       5.859599e+05      -1.781138e+00 |       22
     43       5.859586e+05      -1.235866e+00 |       15
     44       5.859574e+05      -1.249663e+00 |       15
     45       5.859567e+05      -7.319262e-01 |        8
     46       5.859564e+05      -2.190838e-01 |        2
     47       5.859564e+05      -3.084207e-02 |        0
     48       5.859564e+05       0.000000e+00 |        0
K-means converged with 48 iterations (objv = 585956.4134946784)
INFO: K-means with 32000 data points using 48 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.314054
INFO: iteration 2, average log likelihood -1.281369
INFO: iteration 3, average log likelihood -1.247916
INFO: iteration 4, average log likelihood -1.206484
INFO: iteration 5, average log likelihood -1.162171
INFO: iteration 6, average log likelihood -1.122223
WARNING: Variances had to be floored 22 30
INFO: iteration 7, average log likelihood -1.075500
WARNING: Variances had to be floored 13 14 21 24
INFO: iteration 8, average log likelihood -1.069281
WARNING: Variances had to be floored 7 15 29
INFO: iteration 9, average log likelihood -1.079214
WARNING: Variances had to be floored 4 12 19 23
INFO: iteration 10, average log likelihood -1.058648
WARNING: Variances had to be floored 10 16 22 30
INFO: iteration 11, average log likelihood -1.069230
WARNING: Variances had to be floored 21
INFO: iteration 12, average log likelihood -1.090206
WARNING: Variances had to be floored 4 13 14 15 24 28
INFO: iteration 13, average log likelihood -1.042664
WARNING: Variances had to be floored 12 16 19 22 29 30 32
INFO: iteration 14, average log likelihood -1.059522
WARNING: Variances had to be floored 7 10 23
INFO: iteration 15, average log likelihood -1.117724
WARNING: Variances had to be floored 21
INFO: iteration 16, average log likelihood -1.090823
WARNING: Variances had to be floored 14 15 24 30
INFO: iteration 17, average log likelihood -1.045063
WARNING: Variances had to be floored 4 13 19 22
INFO: iteration 18, average log likelihood -1.049865
WARNING: Variances had to be floored 7 12 16 29 32
INFO: iteration 19, average log likelihood -1.060341
WARNING: Variances had to be floored 14 21 23
INFO: iteration 20, average log likelihood -1.077542
WARNING: Variances had to be floored 4 15 30
INFO: iteration 21, average log likelihood -1.064369
WARNING: Variances had to be floored 13 19 22 24
INFO: iteration 22, average log likelihood -1.066609
WARNING: Variances had to be floored 12 14 16
INFO: iteration 23, average log likelihood -1.072170
WARNING: Variances had to be floored 7 10 15 21 28 32
INFO: iteration 24, average log likelihood -1.049531
WARNING: Variances had to be floored 4 13 22 23 30
INFO: iteration 25, average log likelihood -1.072724
WARNING: Variances had to be floored 16
INFO: iteration 26, average log likelihood -1.098457
WARNING: Variances had to be floored 14 19 24
INFO: iteration 27, average log likelihood -1.058887
WARNING: Variances had to be floored 4 12 15 21 30
INFO: iteration 28, average log likelihood -1.051283
WARNING: Variances had to be floored 13 16 22 23 28 32
INFO: iteration 29, average log likelihood -1.075952
INFO: iteration 30, average log likelihood -1.109138
WARNING: Variances had to be floored 4 10 14 19
INFO: iteration 31, average log likelihood -1.035751
WARNING: Variances had to be floored 7 12 15 16 21 22 24 30
INFO: iteration 32, average log likelihood -1.041073
WARNING: Variances had to be floored 13 23 29
INFO: iteration 33, average log likelihood -1.112407
INFO: iteration 34, average log likelihood -1.102104
WARNING: Variances had to be floored 4 10 16 19 28
INFO: iteration 35, average log likelihood -1.026911
WARNING: Variances had to be floored 12 14 15 22 23 24 30 32
INFO: iteration 36, average log likelihood -1.052786
WARNING: Variances had to be floored 13 21
INFO: iteration 37, average log likelihood -1.108758
WARNING: Variances had to be floored 7 10 29
INFO: iteration 38, average log likelihood -1.074105
WARNING: Variances had to be floored 16
INFO: iteration 39, average log likelihood -1.050686
WARNING: Variances had to be floored 4 13 14 15 19 22 24 28 30
INFO: iteration 40, average log likelihood -1.011602
WARNING: Variances had to be floored 10 12 21
INFO: iteration 41, average log likelihood -1.113078
WARNING: Variances had to be floored 7 16 32
INFO: iteration 42, average log likelihood -1.090815
WARNING: Variances had to be floored 29
INFO: iteration 43, average log likelihood -1.069466
WARNING: Variances had to be floored 10 13 19 22 23 24 30
INFO: iteration 44, average log likelihood -1.013159
WARNING: Variances had to be floored 4 12 15 16 21
INFO: iteration 45, average log likelihood -1.088758
WARNING: Variances had to be floored 14 28
INFO: iteration 46, average log likelihood -1.096358
WARNING: Variances had to be floored 7 10 22 32
INFO: iteration 47, average log likelihood -1.051320
WARNING: Variances had to be floored 4 16 29 30
INFO: iteration 48, average log likelihood -1.042245
WARNING: Variances had to be floored 12 13 19 21 23 24
INFO: iteration 49, average log likelihood -1.067952
WARNING: Variances had to be floored 10 14 28
INFO: iteration 50, average log likelihood -1.087913
INFO: EM with 100000 data points 50 iterations avll -1.087913
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.107936     0.0940186   -0.0307626    0.0315472  -0.00856558  -0.111507     0.192954   -0.0506934   -0.0553413    0.00500244   0.145036     0.0114594   -0.133948     0.00489706  -0.112124     0.104445     -0.0529005    0.17542      0.0763911   -0.299026     0.0144478     0.0882825   -0.0636247    0.055837     0.091899     0.167856  
  0.0463986    0.0934798   -0.0692089   -0.0402007   0.145844    -0.0510661    0.0345602   0.00971787   0.0536718    0.116434     0.0260896    0.0956122    0.0253417   -0.0589955    0.119308    -0.0965684    -0.131596     0.00335209  -0.164851     0.0938266   -0.0991523     0.016037    -0.0361048   -0.105543    -0.0690416    0.0849438 
 -0.0177153   -0.0053489   -0.0384825    0.191603    0.199144    -0.0677272    0.129851   -0.169019    -0.104532     0.00116191   0.101028    -0.0215008   -0.0264889    0.0768459    0.0225657   -0.105249     -0.00584926   0.0447086   -0.00217446   0.0745153    0.000773039   0.00679267   0.0263641    0.0721995   -0.00523406   0.0205472 
 -0.0155967   -0.149319    -0.00496101  -0.0359644  -0.0122395   -0.0566059   -0.010152    0.0236464   -0.0405999    0.129842    -0.0536072   -0.102781     0.291128    -0.141585     0.0288996    0.0937574    -0.0617911   -0.02224     -0.023308    -0.111632    -0.079093      0.113166     0.0512834    0.0722578   -0.0428185    0.155476  
  0.006852    -0.118193    -0.187028     0.0569429   0.0295965   -0.113007    -0.0401777  -0.0304795   -0.0509482   -0.0760561   -0.063413    -0.0374671   -0.033764    -0.0559839    0.12043     -0.0700489     0.0636655    0.0888774    0.177095     0.0140606   -0.0820563     0.0520711    0.152003     0.107813    -0.13905      0.107324  
 -0.125913     0.0162192   -0.0927829    0.0704704  -0.0269211   -0.0149823   -0.0730207   0.0237565    0.0424667   -0.115254    -0.072364    -0.041036     0.0508295    0.0518062   -0.312479     0.0153024     0.231761    -0.0661363    0.0163833   -0.0224486   -0.0226397    -0.051684     0.0287957   -0.0470101    0.0886036   -0.0169375 
  0.0919079    0.114767     0.0395012    0.104556   -0.0787247    0.088012    -0.0367887  -0.00507169  -0.126886     0.027945    -0.0677108   -0.0566403   -0.0179222    0.0808632    0.0680521   -0.077492      0.148852    -0.0116717   -0.121422     0.0690423   -0.06615      -0.0309027    0.153999    -0.0619792   -0.0510334    0.0111931 
 -0.012656    -0.125057    -0.0578777    0.0127182  -0.0454044   -0.109188     0.142993    0.108007    -0.0842535    0.0851058    0.237505     0.0200888   -0.0919419    0.121191    -0.10788     -0.0296737     0.0998854    0.112262    -0.0268466    0.0690272   -0.00638254    0.0877813   -0.00138632   0.0378659   -0.0261904   -0.0676746 
  0.0153067   -0.0371612   -0.00343619   0.0646352  -0.0823634   -0.0579571    0.0619759  -0.0970028    0.0219801   -0.0438595   -0.00087084  -0.0171789    0.0587599    0.0167786   -0.0311845   -0.0589427    -0.112185     0.198838     0.0169271   -0.248242     0.165001     -0.0440185   -0.0761564    0.0162398    0.0160772    0.0133738 
 -0.0191806   -0.02621      0.0204042    0.0457643  -0.118365    -0.0516334    0.0060084  -0.0645361   -0.0212695    0.0812441    0.0891473    0.185491     0.130541    -0.118309     0.294561     0.0148223    -0.0204104    0.00246436  -0.0969988   -0.0469577    0.0721185    -0.0349109   -0.0864056    0.108557    -0.271181    -0.186648  
 -0.085036     0.160087     0.0561979    0.150011   -0.126405    -0.136144     0.131198    0.012497     0.0462975   -0.0975093   -0.222333     0.0392627   -0.177439     0.0298769    0.039149     0.000722369   0.122188    -0.0339304    0.0277799   -0.0255252    0.0310806     0.159973    -0.115425     0.115155    -0.0158942    0.131327  
 -0.031981     0.0602743   -0.0814125   -0.0410783   0.0246242    0.074617    -0.0100341   0.0853158   -0.0652408   -0.0830051    0.00208067   0.0345715   -0.0351814    0.0766349   -0.0804639    0.0410835     0.00553057  -0.133999     0.0633785   -0.143743    -0.010581      0.0110873    0.0149409    0.0883351   -0.0207822   -0.0735854 
 -0.10984     -0.0677965   -0.00835768  -0.0237124  -0.0211259    6.88949e-5  -0.0183095   0.184873    -0.176555     0.0269068   -0.0308503   -0.118601    -0.0485045    0.10084     -0.0883178   -0.0749215    -0.256273    -0.152365     0.0510835    0.0988069    0.0499574     0.0815012   -0.163903    -0.059136     0.0497051    0.0296611 
  0.0173653    0.0185313   -0.0942515    0.148637    0.0355192   -0.020184     0.122649    0.0902247   -0.00979456   0.00181673   0.0186682    0.094156     0.15351      0.181642     0.173889     0.266318      0.0519467   -0.0164894   -0.177782    -0.158704     0.019436     -0.105515    -0.0191592    0.158373     0.0495305    0.0240611 
  0.0419049    0.0382698    0.120756    -0.0187235  -0.0878524    0.0698936    0.0281979  -0.0872312    0.126235    -0.0401242   -0.118561     0.0494658    0.019488    -0.105765    -0.0803162   -0.0359368     0.0114206   -0.0290588    0.0451572    0.00423666   0.0295507    -0.0215005   -0.0117325    0.0443915   -0.014082     0.102947  
  0.0453513    0.0201711    0.120541     0.0558006  -0.0732859   -0.0483987    0.0189518   0.0707159    0.0241527    0.0520601    0.174126    -0.12806      0.0425248   -0.234382    -0.00959019  -0.0191082    -0.0821834   -0.176936     0.0176017    0.100575    -0.103499     -0.043789     0.00427485   0.0389653    0.148403     0.0024577 
  0.0557337   -0.0770561    0.182329     0.0346142  -0.061406     0.0845849    0.064857   -0.0760748    0.0204025   -0.0615311    0.257657     0.0471543    0.127137    -0.0906535   -0.0948182    0.2392        0.0127838    0.119141    -0.0111937   -0.0530589   -0.124171      0.145712     0.106494    -0.0606131    0.172661    -0.0526144 
  0.0252763   -0.136764    -0.0775999   -0.0659041   0.0702582   -0.0197195    0.0412712  -0.0255811    0.0521498    0.0518097    0.00969918  -0.0783797    0.161226    -0.0862941    0.0621498   -0.0978689    -0.0755224   -0.0577148    0.10217      0.318827     0.00318292    0.16508      0.157154    -0.0862256    0.158074    -0.30294   
 -0.126422     0.167863     0.0563461    0.115364    0.10429     -0.0360017    0.0967537   0.28         0.0223289    0.0381703   -0.0495659    0.0288753    0.0648892    0.157576     0.0178162    0.0169901    -0.00628628   0.0175765    0.141966     0.163254     0.148109      0.0855269    0.0943863    0.0136924    0.00226029   0.0710693 
  0.138301    -0.119917     0.0741957   -0.0730345   0.0162747   -0.152918     0.0492161  -0.0106481    0.0598388    0.0154507    0.126731     0.00353315  -0.0437263    0.0989952   -0.025304     0.0504899    -0.0296378   -0.00515209  -0.149102     0.171388     0.0188458     0.0687073   -0.0329528   -0.00826534   0.0863742   -0.152075  
  0.242038     0.150059     0.137641    -0.0204679   0.143826     0.0657244    0.193825   -0.0063057   -0.03587      0.0506107    0.0809167   -0.0330297   -0.040306     0.0135367   -0.0819054   -0.00458898   -0.192074    -0.0338539    0.0376736   -0.00334523  -0.129498      0.119879     0.077347     0.068835     0.00429174  -0.0475371 
 -0.0360995   -0.122465     0.114845     0.0344911   0.0148449    0.23426     -0.0749039   0.0244414    0.0165161   -0.0450802    0.0529588   -0.0221727   -0.0363618   -0.0140278   -0.0313274    0.0100041    -0.103588     0.0330238    0.0842472   -0.112411     0.0757312     0.118884     0.00487261  -0.0942002   -0.00792203  -0.129931  
 -0.0737786   -0.0444859    0.0208108   -0.0154322  -0.123847     0.104124     0.0867709   0.102609    -0.0707122    0.252385     0.0274187    0.124466    -0.0100584    0.0776085   -0.0320011   -0.00863602    0.12089      0.00290561  -0.0731286   -0.129486    -0.0626594    -0.0342279   -0.159489     0.049776    -0.041885     0.0490036 
 -0.0147344   -0.00587115  -0.163636     0.0378424   0.20316     -0.174199    -0.106326   -0.0251437   -0.14832     -0.0486981    0.0702889   -0.0235722    0.113623     0.0822743   -0.0811337   -0.11995       0.143627     0.0723761    0.158792    -0.0201396   -0.163347     -0.0489423   -0.0290864   -0.326769     0.0291893    0.0308522 
  0.00229589   0.0148289    0.100669     0.122039    0.107763     0.0761819    0.146897   -0.0624862   -0.0577854   -0.112618    -0.0966603   -0.0317209   -0.0155126    0.0987729   -0.0320477    0.0204432     0.0543443   -0.0460703   -0.0181391    0.051891    -0.0005236    -0.0611511    0.0405576    0.0336856   -0.0984578    0.108355  
 -0.101693     0.123969     0.0696411    0.0342815   0.0433951   -0.0560637   -0.0352428   0.0388703    0.102435    -0.0808941    0.111895    -0.139402     0.00304798   0.0626968    0.00202696  -0.0768154     0.127662    -0.200241     0.0763785    0.23386     -0.0229531     0.178213     0.0375385    0.0188931   -0.0112328   -0.0383223 
  0.027541     0.0387371    0.0246971    0.0677814   0.111241    -0.0474098    0.0203313  -0.176122    -0.187714     0.0411646    0.00794495   0.042392    -0.107169    -0.022553     0.179899    -0.0732293    -0.168726    -0.204669    -0.259404     0.164053     0.131209      0.167855    -0.0907499    0.0108632   -0.110358    -0.00992048
 -0.042496     0.0239534    0.109744    -0.0640636  -0.00491133  -0.00386986  -0.113328    0.0387129   -0.0465653    0.0440877    0.0511421   -0.0995285    0.0845109   -0.0812005    0.0767249   -0.0264056     0.0912674   -0.0128943    0.0941327   -0.0413333   -0.0354613     0.113256     0.0279879    0.14467      0.192351    -0.301448  
  0.108288    -0.0118016    0.19725     -0.131136   -0.170195     0.116154    -0.176355   -0.0375633   -0.0902556    0.174916    -0.120489     0.087644     0.0149532    0.0656375    0.130286    -0.0253373    -0.0775438    0.1058       0.0782525   -0.0446983    0.0343425     0.0603118    0.00125387   0.0414277   -0.0763669    0.051218  
  0.0795963    0.0592956   -0.0267829   -0.148818   -0.225491     0.0200978    0.0863534  -0.210432     0.153931    -0.128605     0.0290386    0.118189    -0.0277442   -0.173933     0.0242215   -0.0793065     0.0103704    0.0111883    0.174421    -0.0620175    0.249351      0.139436    -0.168015     0.0292312   -0.0692465    0.147767  
 -0.166037     0.054117     0.126643     0.173708   -0.012577    -0.0485209    0.0399238  -0.0107028    0.0428175    0.124271    -0.0135741   -0.0830079   -0.144289     0.0428836   -0.0946448   -0.0170257     0.0883506   -0.0629446   -0.0456019    0.0348557   -0.181362     -0.168406     0.0571181    0.0160071    0.0398377   -0.0458267 
 -0.0329345    0.0273577    0.154093    -0.0189747   0.00748118  -0.207299     0.0227298   0.0778831   -0.0197597   -0.0784377   -0.230143     0.0372405   -0.0321577   -0.0237173    0.0590104   -0.22174       0.134935    -0.00380398   0.0118708    0.114298     0.0259953    -0.0555273    0.0234      -0.142185     0.0404503   -0.0116264 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
WARNING: Variances had to be floored 16 22 32
INFO: iteration 1, average log likelihood -1.066834
WARNING: Variances had to be floored 4 7 16 22 24 30 32
INFO: iteration 2, average log likelihood -1.016175
WARNING: Variances had to be floored 10 12 13 14 16 19 22 23 29 30 32
INFO: iteration 3, average log likelihood -0.983882
WARNING: Variances had to be floored 4 7 16 21 22 24 32
INFO: iteration 4, average log likelihood -1.030296
WARNING: Variances had to be floored 15 16 22 28 30 32
INFO: iteration 5, average log likelihood -1.010736
WARNING: Variances had to be floored 4 7 10 12 13 16 19 22 23 29 30 32
INFO: iteration 6, average log likelihood -0.988672
WARNING: Variances had to be floored 4 14 16 21 22 24 32
INFO: iteration 7, average log likelihood -1.026801
WARNING: Variances had to be floored 7 16 22 29 30 32
INFO: iteration 8, average log likelihood -1.015898
WARNING: Variances had to be floored 4 10 12 13 16 19 22 23 24 28 30 32
INFO: iteration 9, average log likelihood -0.981429
WARNING: Variances had to be floored 7 14 16 21 22 29 32
INFO: iteration 10, average log likelihood -1.030828
INFO: EM with 100000 data points 10 iterations avll -1.030828
59.0 data points per parameter
32×26 Array{Float64,2}:
 -0.114284    -0.11466      -0.0141743    -0.00929718   0.224327    -0.0292782   -0.10667      0.120733    -0.00844382   0.0860412   -0.100253     0.0306567    0.0521714   -0.00505515  -0.0200629    0.0250293   0.0467753    0.108307    0.0982644   -0.264759    0.192078     0.0475087    -0.0864376   0.0844961    0.062041   -0.0643712 
  0.224563    -0.0219367     0.018411      0.0325646    0.104597    -0.0630417    0.146583    -0.0461965    0.170176     0.149196     0.081463    -0.0931935    0.251076    -0.0709622    0.0539744    0.239797    0.203998     0.0468966  -0.0209115    0.153527    0.055713     0.1988        0.0443775   0.0981826   -0.0442035   0.219733  
  0.0317016   -0.17623       0.03319       0.0015473    0.0777942   -0.0656922    0.0506554    0.0343463   -0.0493703   -0.00122943  -0.0139171    0.00938803  -0.181806     0.0138827   -0.0396548    0.149284   -0.0144441    0.123596    0.109324    -0.103606    0.14726      0.0883992    -0.0871019   0.252483    -0.0487073   0.0285651 
 -0.0567034   -0.0147695    -0.0884467     0.0508323    0.0145621    0.110171     0.105545    -0.262171    -0.0485042    0.128034     0.0331112   -0.0455258   -0.0203056    0.10849     -0.0581075   -0.136575   -0.047249     0.273308    0.113053    -0.0853621  -0.0378971   -0.0183227    -0.108      -0.00159266   0.116534   -0.0095156 
 -0.0134181    0.0393934     0.0579259     0.0167284    0.0347046   -0.0301234    0.164433     0.200971     0.0617948    0.0763959   -0.109296    -0.0675528   -0.0011041   -0.0676923    0.0869222    0.0364053  -0.0506605    0.052826   -0.0585767    0.107057   -0.129628    -0.0677203     0.0974894   0.0384924    0.194446   -0.154889  
 -0.0887249   -0.0503722     0.0416609    -0.0620949    0.0446169    0.112022    -0.0969668    0.0942983   -0.0734563    0.0726589   -0.0340811    0.0683067   -0.064604    -0.0492614    0.132811     0.0578412   0.0603537    0.0527273   0.0838718   -0.0984167  -0.0766065   -0.0523249    -0.0136729   0.00481085   0.0380904  -0.0299275 
 -0.0783528   -0.0295971     0.0913759     0.0377796    0.00166353  -0.104602     0.0267627   -0.0389926    0.0641208    0.0227292   -0.0132009   -0.00654726   0.0116544    0.0735176   -0.0307275    0.0477186  -0.135992     0.109975    0.00370315  -0.0399728  -0.0158468   -3.54758e-5    0.0523926   0.0710185   -0.0879992   0.135624  
 -0.033901    -0.0551305     0.139622     -0.112726     0.112271     0.120664     0.117639     0.0115253   -0.0840458   -0.0475869   -0.0671407   -0.104244     0.0583334    0.00479071   0.00869715   0.119289    0.0360995   -0.105752    0.0263831    0.102114   -0.0365329    0.104771     -0.0573305   0.131204     0.010642   -0.0820005 
 -0.231352    -0.018554     -0.121132      0.0073949    0.0593684   -0.139147    -0.24441     -0.0451254    0.0831805   -0.0125488   -0.177939    -0.0542058   -0.0338014    0.0577004    0.133747     0.0029318  -0.0656648    0.119297    0.0136368    0.0982375  -0.0185717   -0.104491      0.206962    0.102973    -0.0243297   0.195268  
  0.0357396   -0.0564036     0.103995     -0.053595    -0.00977688   0.0577592   -0.0832358    0.139387    -0.119668    -0.0157608   -0.229092     0.0151281   -0.105665     0.0530372   -0.15283      0.12579    -0.11036      0.0501882  -0.129443    -0.0472046   0.0429067   -0.000517484   0.0320146  -0.0833183   -0.0714341   0.0908667 
 -0.100913    -0.255073     -0.025034      0.0706402   -0.109463    -0.0858123   -0.0798852   -0.102538    -0.33281     -0.0158802   -0.0204692    0.0271573   -0.0990017   -0.135815    -0.0956781   -0.065898   -0.0435064   -0.0256964   0.0202209   -0.0214689  -0.0311811    0.200626      0.177416   -0.0337945    0.0174129  -0.251302  
 -0.253072     0.098187      0.1036       -0.0101752    0.0488928    0.0509225   -0.0760037   -0.0584702   -0.0691921    0.0794726   -0.0248564    0.0374258   -0.112326     0.0614111   -0.0775513    0.0654607  -0.172781     0.139542   -0.0687034   -0.131765   -0.0691827   -0.0484651     0.140502   -0.108164     0.116232   -0.0274226 
 -0.127302     0.0502755    -0.124177     -0.129019     0.0113629    0.0300173   -0.102298    -0.0115293    0.0757539   -0.126184    -0.149156     0.0905071   -0.114512     0.00719517   0.226566    -0.171081   -0.071796     0.0267851   0.212272     0.0895912   0.0170475   -0.166137      0.0448549   0.0136168    0.0191203  -0.0910546 
 -0.0508662   -0.149787      0.0549271    -0.24268     -0.129746     0.0782844   -0.0863851    0.0619437    0.0361715    0.0343628    0.0488829   -0.123617    -0.0246789   -0.0470093   -0.0901482    0.0881331   0.0773765   -0.0796009   0.0265271    0.225987   -0.00806418  -0.0610124     0.0683377   0.0934647   -0.0893868  -0.0171799 
  0.164629     0.158825      0.0709883     0.112859     0.0436999   -0.246202     0.136004    -0.0780074   -0.0215403   -0.142788    -0.0646558   -0.114476     0.0354849   -0.00920255  -0.376703     0.0156737  -0.0856858   -0.0748644   0.0115391    0.0962223  -0.0777503    0.126409     -0.0966029  -0.0581072    0.0945569  -0.0677982 
 -0.0872398   -0.116982      0.0401705    -0.135689    -0.0785681    0.0309445    0.0574115   -0.00474189   0.1224      -0.055492    -0.0711282    0.143065     0.0732664    0.0693126   -0.0307191   -0.064109   -0.0499888    0.0322454   0.0971276   -0.0296504   0.0185447   -0.00272091   -0.0390176  -0.0105792    0.0767726  -0.0559508 
 -0.188529    -0.109523     -0.0659266    -0.0542337   -0.172827     0.0106941   -0.00158314   0.0327955   -0.0758447   -0.0455414   -0.124443     0.00640147   0.0326171   -0.0819543   -0.0739199    0.109601   -0.274679     0.0110916   0.0340188   -0.157168    0.00505703  -0.0688198     0.0510963   0.0489828    0.125174    0.156697  
  0.0684073   -0.140985      0.0866589    -0.0511594    0.120393    -0.00777582   0.0166085   -0.151734     0.159039     0.112463    -0.12944      0.0304304    0.117796     0.0413346   -0.0789392    0.104836   -0.0998456   -0.201751    0.0184194    0.0970304   0.126909    -0.0557966    -0.138942    0.135749    -0.0722544  -0.00371541
  0.277773    -0.0117599     0.0446281    -0.249822    -0.257156    -0.0493462    0.041763     0.0709135   -0.192179    -0.00592069  -0.0215903   -0.00148423  -0.0826679   -0.100337    -0.0196384   -0.003502    0.0220109    0.154119    0.0176385    0.0505706  -0.0634102   -0.15791      -0.0420047  -0.0526446    0.152414   -0.0595652 
  0.0656007    0.126744      0.000792229  -0.0558808    0.0700274   -0.0954191   -0.0435224    0.0174828   -0.09547     -0.11724      0.0893245   -0.309162    -0.0339203   -0.133876     0.0341626   -0.115023    0.0119811   -0.0667676   0.0156507    0.115066    0.0912675   -0.133643      0.110075    0.0518674   -0.0927811   0.0216508 
 -0.0565211    0.058802     -0.0413157     0.0497633   -0.0668081   -0.175801    -0.0729275    0.0995715    0.0170072   -0.0992366    0.213778     0.0361913   -0.112949     0.0497496    0.0554368    0.0420984  -0.00103615  -0.148349   -0.337681    -0.144812    0.0505615   -0.0351463    -0.137594    0.0584908   -0.0127416  -0.069698  
 -0.114153     0.0856309    -0.196327     -0.0869741    0.0503582   -0.10101      0.0335987    0.0964431    0.0851927    0.0807822    0.105123     0.0354619   -0.147232    -0.00823353   0.0340098   -0.0124615  -0.127676     0.0389631  -0.0150388    0.0500619  -0.132319     0.140276      0.0191127   0.0696923   -0.0261772   0.235626  
  0.0547593    0.0758362     0.0338953     0.0787791    0.114516    -0.00483914   0.0675583    0.0202434    0.0813981   -0.0436749   -0.0166334    0.111024    -0.0291507    0.126268    -0.00588177  -0.0531793  -0.100997    -0.198121   -0.00279941   0.0920845   0.0915617   -0.217473      0.0878193  -0.233665    -0.137979    0.018302  
  0.0592099   -0.0261586     0.0902899    -0.0902543   -0.0512152   -0.0727872   -0.149665    -0.110102    -0.198913     0.112757     0.0220815    0.15992      0.275762    -0.052013     0.17265     -0.117819   -0.00783117  -0.0586168  -0.0522826   -0.148277    0.0663533    0.00849431   -0.0370023  -0.0179528   -0.118689    0.00826876
 -0.0284238   -0.0368809     0.06789      -0.0181643    0.0343077    0.0791543   -0.0515166    0.0746847   -0.0998263   -0.128926    -0.141102    -0.0518231   -0.070077     0.0536054   -0.00197795  -0.188141    0.121123    -0.0101727  -0.059137    -0.0362063  -0.109256     0.0375769    -0.0646281  -0.108586     0.112732    0.0395213 
  0.0374788   -0.120065      0.141038      0.10334      0.0960342   -0.0816253   -0.138347    -0.181428     0.0229722    0.0420258   -0.0333758    0.0736284   -0.0648997   -0.0755101   -0.0740144   -0.0580201   0.0888669   -0.101033    0.0263139   -0.0384524   0.00577468   0.108516     -0.143212    0.0262245   -0.0664065  -0.0933677 
 -0.00946445  -0.000315839  -0.0408051    -0.0280021    0.108786    -0.0681335    0.0414896   -0.0385302   -0.088017    -0.104516     0.0355131    0.0533389    0.0976384    0.0792792    0.148345     0.0769507   0.125069    -0.0504386  -0.10631     -0.0286974   0.196892     0.12477      -0.272424    0.00908088  -0.0858462   0.0749154 
  0.0325901    0.128414     -0.0492913     0.103809    -0.0552877   -0.0827356   -0.0558128    0.153893    -0.0762201    0.0450363    0.00350822  -0.0546183   -0.01961      0.0182203   -0.018761     0.0534339   0.0222581   -0.112142    0.165106    -0.104398   -0.0097276   -0.0099984    -0.0794094  -0.0532758    0.131767   -0.052714  
 -0.0810012    0.027579      0.0864015     0.0572872   -0.0826281    0.0171863    0.0334991   -0.0859755    0.00635942   0.287338    -0.0780177   -0.222294    -0.0471269    0.00248121   0.0648272   -0.241891    0.0130827    0.0736703   0.066415     0.0944641   0.187231     0.0513753     0.0637796   0.203333    -0.0983107   0.105418  
 -0.0193122   -0.185486      0.166536      0.260957    -0.115289    -0.10263     -0.0537125   -0.265176    -0.00804862   0.145848    -0.212329     0.0981024   -0.014303     0.0134603    0.127663     0.0405484   0.0304644   -0.230353    0.0101238    0.0184645   0.0371575    0.158401      0.0892121   0.0839196    0.0079982  -0.0142615 
 -0.15823      0.0760157     0.0306719     0.0940445   -0.0304774    0.0905735   -0.0208958   -0.0211782   -0.0577571   -0.0647518   -0.15931      0.22028      0.0290214   -0.0645574    0.102097     0.0623967  -0.00506028  -0.131077    0.00252379   0.156384   -0.0377902   -0.0288812    -0.0818087  -0.105173     0.0405152   0.220891  
  0.00715537  -0.00162575    0.0446626     0.0746289    0.0840138   -0.0226606   -0.108079    -0.185235     0.0781303   -0.0993867    0.0336258    0.0202075    0.00888318   0.236668     0.117378     0.0749494  -0.110336    -0.0450941  -0.0692482    0.017881   -0.00355018  -0.0157077     0.130192   -0.163118    -0.225592   -0.0803822 kind full, method split
0: avll = -1.4206317770801553
INFO: Running 50 iterations EM on diag cov GMM with 2 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.420650
INFO: iteration 2, average log likelihood -1.420596
INFO: iteration 3, average log likelihood -1.420557
INFO: iteration 4, average log likelihood -1.420510
INFO: iteration 5, average log likelihood -1.420450
INFO: iteration 6, average log likelihood -1.420365
INFO: iteration 7, average log likelihood -1.420229
INFO: iteration 8, average log likelihood -1.419972
INFO: iteration 9, average log likelihood -1.419445
INFO: iteration 10, average log likelihood -1.418477
INFO: iteration 11, average log likelihood -1.417175
INFO: iteration 12, average log likelihood -1.416064
INFO: iteration 13, average log likelihood -1.415461
INFO: iteration 14, average log likelihood -1.415218
INFO: iteration 15, average log likelihood -1.415128
INFO: iteration 16, average log likelihood -1.415095
INFO: iteration 17, average log likelihood -1.415082
INFO: iteration 18, average log likelihood -1.415077
INFO: iteration 19, average log likelihood -1.415075
INFO: iteration 20, average log likelihood -1.415074
INFO: iteration 21, average log likelihood -1.415073
INFO: iteration 22, average log likelihood -1.415073
INFO: iteration 23, average log likelihood -1.415073
INFO: iteration 24, average log likelihood -1.415073
INFO: iteration 25, average log likelihood -1.415072
INFO: iteration 26, average log likelihood -1.415072
INFO: iteration 27, average log likelihood -1.415072
INFO: iteration 28, average log likelihood -1.415072
INFO: iteration 29, average log likelihood -1.415072
INFO: iteration 30, average log likelihood -1.415072
INFO: iteration 31, average log likelihood -1.415072
INFO: iteration 32, average log likelihood -1.415072
INFO: iteration 33, average log likelihood -1.415072
INFO: iteration 34, average log likelihood -1.415072
INFO: iteration 35, average log likelihood -1.415071
INFO: iteration 36, average log likelihood -1.415071
INFO: iteration 37, average log likelihood -1.415071
INFO: iteration 38, average log likelihood -1.415071
INFO: iteration 39, average log likelihood -1.415071
INFO: iteration 40, average log likelihood -1.415071
INFO: iteration 41, average log likelihood -1.415071
INFO: iteration 42, average log likelihood -1.415071
INFO: iteration 43, average log likelihood -1.415071
INFO: iteration 44, average log likelihood -1.415071
INFO: iteration 45, average log likelihood -1.415071
INFO: iteration 46, average log likelihood -1.415071
INFO: iteration 47, average log likelihood -1.415071
INFO: iteration 48, average log likelihood -1.415071
INFO: iteration 49, average log likelihood -1.415071
INFO: iteration 50, average log likelihood -1.415071
INFO: EM with 100000 data points 50 iterations avll -1.415071
952.4 data points per parameter
1: avll = [-1.42065,-1.4206,-1.42056,-1.42051,-1.42045,-1.42036,-1.42023,-1.41997,-1.41944,-1.41848,-1.41718,-1.41606,-1.41546,-1.41522,-1.41513,-1.41509,-1.41508,-1.41508,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507]
INFO: Running 50 iterations EM on diag cov GMM with 4 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.415086
INFO: iteration 2, average log likelihood -1.415022
INFO: iteration 3, average log likelihood -1.414967
INFO: iteration 4, average log likelihood -1.414899
INFO: iteration 5, average log likelihood -1.414813
INFO: iteration 6, average log likelihood -1.414707
INFO: iteration 7, average log likelihood -1.414587
INFO: iteration 8, average log likelihood -1.414463
INFO: iteration 9, average log likelihood -1.414348
INFO: iteration 10, average log likelihood -1.414254
INFO: iteration 11, average log likelihood -1.414185
INFO: iteration 12, average log likelihood -1.414137
INFO: iteration 13, average log likelihood -1.414107
INFO: iteration 14, average log likelihood -1.414088
INFO: iteration 15, average log likelihood -1.414076
INFO: iteration 16, average log likelihood -1.414067
INFO: iteration 17, average log likelihood -1.414061
INFO: iteration 18, average log likelihood -1.414056
INFO: iteration 19, average log likelihood -1.414051
INFO: iteration 20, average log likelihood -1.414047
INFO: iteration 21, average log likelihood -1.414044
INFO: iteration 22, average log likelihood -1.414041
INFO: iteration 23, average log likelihood -1.414038
INFO: iteration 24, average log likelihood -1.414035
INFO: iteration 25, average log likelihood -1.414032
INFO: iteration 26, average log likelihood -1.414029
INFO: iteration 27, average log likelihood -1.414027
INFO: iteration 28, average log likelihood -1.414025
INFO: iteration 29, average log likelihood -1.414022
INFO: iteration 30, average log likelihood -1.414020
INFO: iteration 31, average log likelihood -1.414018
INFO: iteration 32, average log likelihood -1.414016
INFO: iteration 33, average log likelihood -1.414015
INFO: iteration 34, average log likelihood -1.414013
INFO: iteration 35, average log likelihood -1.414011
INFO: iteration 36, average log likelihood -1.414010
INFO: iteration 37, average log likelihood -1.414008
INFO: iteration 38, average log likelihood -1.414007
INFO: iteration 39, average log likelihood -1.414006
INFO: iteration 40, average log likelihood -1.414004
INFO: iteration 41, average log likelihood -1.414003
INFO: iteration 42, average log likelihood -1.414002
INFO: iteration 43, average log likelihood -1.414001
INFO: iteration 44, average log likelihood -1.414000
INFO: iteration 45, average log likelihood -1.413998
INFO: iteration 46, average log likelihood -1.413997
INFO: iteration 47, average log likelihood -1.413996
INFO: iteration 48, average log likelihood -1.413995
INFO: iteration 49, average log likelihood -1.413994
INFO: iteration 50, average log likelihood -1.413993
INFO: EM with 100000 data points 50 iterations avll -1.413993
473.9 data points per parameter
2: avll = [-1.41509,-1.41502,-1.41497,-1.4149,-1.41481,-1.41471,-1.41459,-1.41446,-1.41435,-1.41425,-1.41418,-1.41414,-1.41411,-1.41409,-1.41408,-1.41407,-1.41406,-1.41406,-1.41405,-1.41405,-1.41404,-1.41404,-1.41404,-1.41403,-1.41403,-1.41403,-1.41403,-1.41402,-1.41402,-1.41402,-1.41402,-1.41402,-1.41401,-1.41401,-1.41401,-1.41401,-1.41401,-1.41401,-1.41401,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.41399,-1.41399]
INFO: Running 50 iterations EM on diag cov GMM with 8 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.414002
INFO: iteration 2, average log likelihood -1.413949
INFO: iteration 3, average log likelihood -1.413903
INFO: iteration 4, average log likelihood -1.413850
INFO: iteration 5, average log likelihood -1.413785
INFO: iteration 6, average log likelihood -1.413706
INFO: iteration 7, average log likelihood -1.413614
INFO: iteration 8, average log likelihood -1.413515
INFO: iteration 9, average log likelihood -1.413417
INFO: iteration 10, average log likelihood -1.413324
INFO: iteration 11, average log likelihood -1.413241
INFO: iteration 12, average log likelihood -1.413169
INFO: iteration 13, average log likelihood -1.413107
INFO: iteration 14, average log likelihood -1.413054
INFO: iteration 15, average log likelihood -1.413009
INFO: iteration 16, average log likelihood -1.412970
INFO: iteration 17, average log likelihood -1.412936
INFO: iteration 18, average log likelihood -1.412907
INFO: iteration 19, average log likelihood -1.412882
INFO: iteration 20, average log likelihood -1.412859
INFO: iteration 21, average log likelihood -1.412840
INFO: iteration 22, average log likelihood -1.412822
INFO: iteration 23, average log likelihood -1.412806
INFO: iteration 24, average log likelihood -1.412791
INFO: iteration 25, average log likelihood -1.412777
INFO: iteration 26, average log likelihood -1.412765
INFO: iteration 27, average log likelihood -1.412753
INFO: iteration 28, average log likelihood -1.412741
INFO: iteration 29, average log likelihood -1.412730
INFO: iteration 30, average log likelihood -1.412720
INFO: iteration 31, average log likelihood -1.412710
INFO: iteration 32, average log likelihood -1.412701
INFO: iteration 33, average log likelihood -1.412692
INFO: iteration 34, average log likelihood -1.412683
INFO: iteration 35, average log likelihood -1.412675
INFO: iteration 36, average log likelihood -1.412667
INFO: iteration 37, average log likelihood -1.412659
INFO: iteration 38, average log likelihood -1.412652
INFO: iteration 39, average log likelihood -1.412645
INFO: iteration 40, average log likelihood -1.412638
INFO: iteration 41, average log likelihood -1.412631
INFO: iteration 42, average log likelihood -1.412625
INFO: iteration 43, average log likelihood -1.412618
INFO: iteration 44, average log likelihood -1.412612
INFO: iteration 45, average log likelihood -1.412607
INFO: iteration 46, average log likelihood -1.412601
INFO: iteration 47, average log likelihood -1.412595
INFO: iteration 48, average log likelihood -1.412590
INFO: iteration 49, average log likelihood -1.412585
INFO: iteration 50, average log likelihood -1.412580
INFO: EM with 100000 data points 50 iterations avll -1.412580
236.4 data points per parameter
3: avll = [-1.414,-1.41395,-1.4139,-1.41385,-1.41378,-1.41371,-1.41361,-1.41352,-1.41342,-1.41332,-1.41324,-1.41317,-1.41311,-1.41305,-1.41301,-1.41297,-1.41294,-1.41291,-1.41288,-1.41286,-1.41284,-1.41282,-1.41281,-1.41279,-1.41278,-1.41276,-1.41275,-1.41274,-1.41273,-1.41272,-1.41271,-1.4127,-1.41269,-1.41268,-1.41267,-1.41267,-1.41266,-1.41265,-1.41264,-1.41264,-1.41263,-1.41262,-1.41262,-1.41261,-1.41261,-1.4126,-1.4126,-1.41259,-1.41258,-1.41258]
INFO: Running 50 iterations EM on diag cov GMM with 16 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.412583
INFO: iteration 2, average log likelihood -1.412534
INFO: iteration 3, average log likelihood -1.412491
INFO: iteration 4, average log likelihood -1.412443
INFO: iteration 5, average log likelihood -1.412385
INFO: iteration 6, average log likelihood -1.412314
INFO: iteration 7, average log likelihood -1.412230
INFO: iteration 8, average log likelihood -1.412135
INFO: iteration 9, average log likelihood -1.412032
INFO: iteration 10, average log likelihood -1.411927
INFO: iteration 11, average log likelihood -1.411825
INFO: iteration 12, average log likelihood -1.411729
INFO: iteration 13, average log likelihood -1.411641
INFO: iteration 14, average log likelihood -1.411561
INFO: iteration 15, average log likelihood -1.411489
INFO: iteration 16, average log likelihood -1.411426
INFO: iteration 17, average log likelihood -1.411372
INFO: iteration 18, average log likelihood -1.411324
INFO: iteration 19, average log likelihood -1.411283
INFO: iteration 20, average log likelihood -1.411248
INFO: iteration 21, average log likelihood -1.411217
INFO: iteration 22, average log likelihood -1.411189
INFO: iteration 23, average log likelihood -1.411164
INFO: iteration 24, average log likelihood -1.411141
INFO: iteration 25, average log likelihood -1.411120
INFO: iteration 26, average log likelihood -1.411100
INFO: iteration 27, average log likelihood -1.411081
INFO: iteration 28, average log likelihood -1.411063
INFO: iteration 29, average log likelihood -1.411046
INFO: iteration 30, average log likelihood -1.411029
INFO: iteration 31, average log likelihood -1.411013
INFO: iteration 32, average log likelihood -1.410997
INFO: iteration 33, average log likelihood -1.410982
INFO: iteration 34, average log likelihood -1.410967
INFO: iteration 35, average log likelihood -1.410952
INFO: iteration 36, average log likelihood -1.410938
INFO: iteration 37, average log likelihood -1.410923
INFO: iteration 38, average log likelihood -1.410910
INFO: iteration 39, average log likelihood -1.410896
INFO: iteration 40, average log likelihood -1.410883
INFO: iteration 41, average log likelihood -1.410870
INFO: iteration 42, average log likelihood -1.410857
INFO: iteration 43, average log likelihood -1.410845
INFO: iteration 44, average log likelihood -1.410833
INFO: iteration 45, average log likelihood -1.410822
INFO: iteration 46, average log likelihood -1.410810
INFO: iteration 47, average log likelihood -1.410799
INFO: iteration 48, average log likelihood -1.410789
INFO: iteration 49, average log likelihood -1.410779
INFO: iteration 50, average log likelihood -1.410769
INFO: EM with 100000 data points 50 iterations avll -1.410769
118.1 data points per parameter
4: avll = [-1.41258,-1.41253,-1.41249,-1.41244,-1.41238,-1.41231,-1.41223,-1.41213,-1.41203,-1.41193,-1.41183,-1.41173,-1.41164,-1.41156,-1.41149,-1.41143,-1.41137,-1.41132,-1.41128,-1.41125,-1.41122,-1.41119,-1.41116,-1.41114,-1.41112,-1.4111,-1.41108,-1.41106,-1.41105,-1.41103,-1.41101,-1.411,-1.41098,-1.41097,-1.41095,-1.41094,-1.41092,-1.41091,-1.4109,-1.41088,-1.41087,-1.41086,-1.41085,-1.41083,-1.41082,-1.41081,-1.4108,-1.41079,-1.41078,-1.41077]
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.410768
INFO: iteration 2, average log likelihood -1.410698
INFO: iteration 3, average log likelihood -1.410628
INFO: iteration 4, average log likelihood -1.410541
INFO: iteration 5, average log likelihood -1.410429
INFO: iteration 6, average log likelihood -1.410289
INFO: iteration 7, average log likelihood -1.410127
INFO: iteration 8, average log likelihood -1.409953
INFO: iteration 9, average log likelihood -1.409775
INFO: iteration 10, average log likelihood -1.409604
INFO: iteration 11, average log likelihood -1.409443
INFO: iteration 12, average log likelihood -1.409297
INFO: iteration 13, average log likelihood -1.409166
INFO: iteration 14, average log likelihood -1.409050
INFO: iteration 15, average log likelihood -1.408947
INFO: iteration 16, average log likelihood -1.408856
INFO: iteration 17, average log likelihood -1.408775
INFO: iteration 18, average log likelihood -1.408703
INFO: iteration 19, average log likelihood -1.408639
INFO: iteration 20, average log likelihood -1.408580
INFO: iteration 21, average log likelihood -1.408528
INFO: iteration 22, average log likelihood -1.408480
INFO: iteration 23, average log likelihood -1.408435
INFO: iteration 24, average log likelihood -1.408394
INFO: iteration 25, average log likelihood -1.408356
INFO: iteration 26, average log likelihood -1.408320
INFO: iteration 27, average log likelihood -1.408286
INFO: iteration 28, average log likelihood -1.408255
INFO: iteration 29, average log likelihood -1.408225
INFO: iteration 30, average log likelihood -1.408196
INFO: iteration 31, average log likelihood -1.408169
INFO: iteration 32, average log likelihood -1.408144
INFO: iteration 33, average log likelihood -1.408119
INFO: iteration 34, average log likelihood -1.408096
INFO: iteration 35, average log likelihood -1.408073
INFO: iteration 36, average log likelihood -1.408051
INFO: iteration 37, average log likelihood -1.408030
INFO: iteration 38, average log likelihood -1.408010
INFO: iteration 39, average log likelihood -1.407990
INFO: iteration 40, average log likelihood -1.407971
INFO: iteration 41, average log likelihood -1.407952
INFO: iteration 42, average log likelihood -1.407934
INFO: iteration 43, average log likelihood -1.407917
INFO: iteration 44, average log likelihood -1.407900
INFO: iteration 45, average log likelihood -1.407884
INFO: iteration 46, average log likelihood -1.407868
INFO: iteration 47, average log likelihood -1.407853
INFO: iteration 48, average log likelihood -1.407838
INFO: iteration 49, average log likelihood -1.407824
INFO: iteration 50, average log likelihood -1.407811
INFO: EM with 100000 data points 50 iterations avll -1.407811
59.0 data points per parameter
5: avll = [-1.41077,-1.4107,-1.41063,-1.41054,-1.41043,-1.41029,-1.41013,-1.40995,-1.40978,-1.4096,-1.40944,-1.4093,-1.40917,-1.40905,-1.40895,-1.40886,-1.40878,-1.4087,-1.40864,-1.40858,-1.40853,-1.40848,-1.40844,-1.40839,-1.40836,-1.40832,-1.40829,-1.40825,-1.40822,-1.4082,-1.40817,-1.40814,-1.40812,-1.4081,-1.40807,-1.40805,-1.40803,-1.40801,-1.40799,-1.40797,-1.40795,-1.40793,-1.40792,-1.4079,-1.40788,-1.40787,-1.40785,-1.40784,-1.40782,-1.40781]
[-1.42063,-1.42065,-1.4206,-1.42056,-1.42051,-1.42045,-1.42036,-1.42023,-1.41997,-1.41944,-1.41848,-1.41718,-1.41606,-1.41546,-1.41522,-1.41513,-1.41509,-1.41508,-1.41508,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41507,-1.41509,-1.41502,-1.41497,-1.4149,-1.41481,-1.41471,-1.41459,-1.41446,-1.41435,-1.41425,-1.41418,-1.41414,-1.41411,-1.41409,-1.41408,-1.41407,-1.41406,-1.41406,-1.41405,-1.41405,-1.41404,-1.41404,-1.41404,-1.41403,-1.41403,-1.41403,-1.41403,-1.41402,-1.41402,-1.41402,-1.41402,-1.41402,-1.41401,-1.41401,-1.41401,-1.41401,-1.41401,-1.41401,-1.41401,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.414,-1.41399,-1.41399,-1.414,-1.41395,-1.4139,-1.41385,-1.41378,-1.41371,-1.41361,-1.41352,-1.41342,-1.41332,-1.41324,-1.41317,-1.41311,-1.41305,-1.41301,-1.41297,-1.41294,-1.41291,-1.41288,-1.41286,-1.41284,-1.41282,-1.41281,-1.41279,-1.41278,-1.41276,-1.41275,-1.41274,-1.41273,-1.41272,-1.41271,-1.4127,-1.41269,-1.41268,-1.41267,-1.41267,-1.41266,-1.41265,-1.41264,-1.41264,-1.41263,-1.41262,-1.41262,-1.41261,-1.41261,-1.4126,-1.4126,-1.41259,-1.41258,-1.41258,-1.41258,-1.41253,-1.41249,-1.41244,-1.41238,-1.41231,-1.41223,-1.41213,-1.41203,-1.41193,-1.41183,-1.41173,-1.41164,-1.41156,-1.41149,-1.41143,-1.41137,-1.41132,-1.41128,-1.41125,-1.41122,-1.41119,-1.41116,-1.41114,-1.41112,-1.4111,-1.41108,-1.41106,-1.41105,-1.41103,-1.41101,-1.411,-1.41098,-1.41097,-1.41095,-1.41094,-1.41092,-1.41091,-1.4109,-1.41088,-1.41087,-1.41086,-1.41085,-1.41083,-1.41082,-1.41081,-1.4108,-1.41079,-1.41078,-1.41077,-1.41077,-1.4107,-1.41063,-1.41054,-1.41043,-1.41029,-1.41013,-1.40995,-1.40978,-1.4096,-1.40944,-1.4093,-1.40917,-1.40905,-1.40895,-1.40886,-1.40878,-1.4087,-1.40864,-1.40858,-1.40853,-1.40848,-1.40844,-1.40839,-1.40836,-1.40832,-1.40829,-1.40825,-1.40822,-1.4082,-1.40817,-1.40814,-1.40812,-1.4081,-1.40807,-1.40805,-1.40803,-1.40801,-1.40799,-1.40797,-1.40795,-1.40793,-1.40792,-1.4079,-1.40788,-1.40787,-1.40785,-1.40784,-1.40782,-1.40781]
32×26 Array{Float64,2}:
 -0.172261    0.345445      0.0969033   -0.866522    -0.0864853   -0.140301     -0.580261      0.357203    -0.07257     -0.100343    -0.45114     -0.0333166   -0.0893269   0.140288    -0.286998     0.279606   -0.405788   -0.404794    -0.045003     0.209135   -0.165752    0.056699    -0.00193747   0.101221   -0.267868    -0.955485 
  0.298642    0.0998457     0.178252    -0.0591803   -0.379459    -0.0562452    -0.494069     -0.388691    -1.05627     -0.00671097   0.409482    -0.205785    -0.174747   -0.0587983    0.14312     -0.34575    -0.415426    0.0285243    0.081963    -0.014633   -0.401049   -0.538157    -0.432314     0.319917    0.160028    -0.751723 
 -0.16517    -0.000524309   0.257994     0.246845     0.63143      0.303601     -0.660087      0.100965     0.0546449    0.118292    -0.957267     0.877223    -0.440252    0.934814    -0.208234    -1.42007    -0.410075   -0.312502    -0.594078     0.608572    0.549019   -0.365637     0.22658     -0.516773   -0.692502     0.17231  
  0.147971   -0.616846      0.0728004   -0.191704     0.1108       0.0546861    -0.430453      0.205372    -0.0736265   -0.0826829   -0.379546     0.396061    -0.262953   -0.261132     0.291506    -0.0756195   0.727152   -0.987021     0.0748276   -0.0503065   0.268438   -0.349112     0.250338     0.0386401   0.257807    -0.179238 
 -0.0163735  -0.252651     -0.985025    -0.0698625    0.00199518  -0.134313     -0.798558      0.0386668   -0.0980591   -0.0211087   -0.272174    -0.525529    -0.367814    0.816877     0.195429     0.251136    0.622629   -0.136047    -0.288141    -0.228421   -0.496655    0.576219     0.856959    -0.948129   -0.289127     0.326604 
 -0.811364   -0.0953944    -1.17164      0.192136     0.276808    -0.295745     -1.03718      -0.329755     1.0989       0.0142798   -0.570197    -1.05298      0.365341    0.373449     0.134379     0.271839   -0.256092    1.14875      0.145201    -0.879937    0.890155    0.024879     0.22572     -0.140222   -0.709063     0.735183 
 -0.181315    0.0103056     0.0216649   -0.221297     0.189596     0.00733189   -0.154978      0.141053     0.00284207   0.0376143   -0.0740775   -0.0504994   -0.294208    0.212389     0.0462526   -0.0713553  -0.119598   -0.0887839   -0.117651    -0.280749    0.113164   -0.227628    -0.177441    -0.0378126   0.00416163   0.2232   
 -0.246727    0.497329      0.257516    -0.0518756    0.0807361   -0.095136     -0.300511     -0.636046    -0.472205    -0.19644     -0.33543     -0.405633    -0.316998    0.669052     0.103611    -0.133676   -0.0705892   0.0108155   -0.231471    -0.371382   -0.305903    0.111556     0.859616     0.100166    0.157959     0.534696 
  0.351755    0.231779      0.0953348   -0.414612    -0.319679     0.374052      0.647758      0.423319     0.471378     0.0528628   -0.0675176    0.650158     0.321715   -0.433573     0.114528    -0.160311    0.286821    0.00695481  -0.0474828    0.0593399   0.0882368   0.0188323   -0.598875     0.508796    0.0565738   -0.371587 
 -0.140646    0.125504      0.293741     0.290755    -0.356973    -0.497416      0.33999      -0.00767479  -0.331972     0.373306    -0.207032     0.243228     0.181736   -0.170327    -0.200909     0.187925   -0.0958897   0.722974     0.162525     0.140589   -0.491194    0.325715    -0.48036      0.228688   -0.210609    -0.162014 
  0.25009    -0.263223     -0.0225534    0.457464    -0.567732     0.404712      0.410259     -0.281556     0.243485     0.0928613    0.528601    -0.256247     0.302272   -0.431637     0.376709     0.341835    0.0394716   0.458387     0.293687     0.327218   -0.105922    0.523252     0.533806    -0.440869    0.260234     0.619723 
 -0.219478   -0.72507      -0.102915     0.617207     0.818384    -0.422809      0.368305      0.284635     0.426368     0.262431    -0.0132974    0.0879832    0.206469   -0.410077    -0.518919    -0.0690012   0.3347     -0.17147     -0.203848    -0.0228568   0.21131     0.0778605    0.225201    -0.254508   -0.110298     0.168959 
 -0.343286    0.615823     -0.0308286   -0.446264     0.341732    -0.42948      -0.160689      0.259809    -0.0918987   -0.0890762    0.234253    -0.261762    -0.447798    0.120448    -0.198137     0.0657443  -0.277192   -0.34287     -0.315338    -0.0567566   0.793631   -0.693758    -0.406946     0.157536   -0.312768     0.652528 
 -0.176715    0.0609456     0.02108     -0.403409     0.635819     0.452051     -0.0182565    -0.180375     0.176988    -0.146674    -0.00035511  -0.175772     0.0865054  -0.115718    -0.43226     -0.312383   -0.734081    0.197912     0.0155015   -0.0346476   0.619887   -0.502383    -0.335881     0.199566    0.772934     0.554589 
  0.0571957   0.275563     -0.104294     0.33106      0.212809     0.15504      -0.0669125    -0.417665    -0.120841    -0.789217     0.280999    -0.35498      0.208207   -0.0171426   -0.0879808   -0.475236   -0.353191   -0.326305     0.265392     0.298695    0.847089    0.547062     0.0237536    0.285594   -0.192749    -0.541687 
  0.810233    0.277895     -0.216703     0.0786893    0.249213    -0.551258      0.135192     -0.351442    -0.729372    -0.80566      0.48131     -0.491908    -0.0619881  -0.0503233    0.340296     1.20453    -0.264975   -0.0157652   -0.00311175   0.115707    0.701187    0.37504     -0.624488     0.682143   -0.384491    -0.19482  
 -0.303099    0.213287      0.384124    -0.253703     0.0258484   -0.339923     -0.0632421     0.39509      0.0774507   -0.155904    -0.359114     0.152463    -0.277569    0.451701    -0.0567829   -0.17616    -0.0239018  -0.0692422   -0.303346    -0.598731   -0.410665   -0.377196    -0.246218     0.0301024  -0.206383    -0.0608207
 -0.0203045  -0.504373      0.572013     0.408301     0.0453269   -0.48792      -0.16008      -0.332994    -0.167276     0.0202721    0.0521457    0.408873     0.116028    0.0478279   -0.638608     0.0303388   0.17688    -0.00561231  -0.206774    -0.485588   -0.61381     0.00993475  -0.42048     -0.299629    0.340337    -0.103873 
  0.193946   -0.17741      -0.446804    -0.287604     0.196473     0.249292     -0.00607972    0.258917     0.15246      0.059339    -0.0853619   -0.112454    -0.122326    0.0349137    0.0333647    0.412062   -0.269848    0.0635948   -0.213114    -0.165386   -0.364104   -0.330708    -0.57426     -0.377707   -0.446733     0.0247207
 -0.535448    0.0253374    -0.0329339   -0.414456    -0.128427     0.371451      0.253385      0.262227     0.356984     0.445467    -0.322345     0.206512     0.183516    0.361965     0.0598537   -0.320259    0.0829507  -0.045738    -0.155242    -0.376761   -0.451225   -0.547906     0.0563474   -0.629992    0.398392     0.447487 
  0.14436     0.0666337    -0.152039    -0.0348629    0.157718     0.000444925  -0.000542577   0.135625     0.0692698   -0.205179    -0.186514    -0.00332233   0.186189   -0.148706    -0.00540457  -0.188601    0.0666044  -0.0348034   -0.172822     0.0687872   0.281336   -0.165286    -0.0945123    0.219776    0.0862912   -0.151237 
 -0.0700756  -0.0479328     0.0986877    0.00297076   0.00470764   0.0111612    -0.041256     -0.170536    -0.0682574    0.076584     0.0469982   -0.050451    -0.0933417   0.104043    -0.007818     0.118994   -0.104043   -0.0210032   -0.00493628  -0.0192353  -0.0841327   0.0590003    0.05412     -0.0630349  -0.0163379    0.049541 
  0.217882    0.679703     -0.37531     -0.220427     0.388997     0.0838952     0.141492      0.0915746    0.365162     0.198949     0.529329     0.200433    -0.496927   -0.0434652    0.100027    -0.221014   -0.280476    0.129666    -0.237454    -0.653388    0.301939    0.343054     0.0231332    0.407451   -0.0749674    0.160112 
  0.0870717   0.324521     -0.383959    -0.224689     0.376889    -0.114657      0.154175      0.170183    -0.0121422    0.386041     0.578049    -0.306496     0.606099   -0.525424    -0.372406    -0.257114   -0.166741    0.189113     0.2765       0.78101     0.223754   -0.0659637    0.282707    -0.0995987   0.102958    -0.279835 
 -0.211906   -0.0997889     0.354044     0.047806    -0.489034    -0.0778813    -0.214521      0.0477443   -0.143822     0.102021    -0.270181     0.269398     0.0565354  -0.015978    -0.178489     0.153572    0.107887    0.112962     0.597511     0.0212725  -0.0924823   0.538209    -0.0161294    0.0257685  -0.0316358   -0.23101  
 -0.169091   -0.238045     -0.192636     0.311066    -0.197082     0.211414      0.22858      -0.309877     0.070617     0.0815015    0.041947     0.138791     0.186284    0.0746931   -0.0194906    0.0580655   0.256533    0.606265     0.393065    -0.290261    0.418512    0.335694    -0.244715     0.113482    0.201219     0.768518 
 -0.536543   -0.759078      0.51036      0.286195     0.06169     -0.0983831    -0.247928     -0.451133    -0.156763    -0.209032     0.00542935  -0.37834     -0.676859   -0.00707256   0.148505     0.450186   -0.405223   -0.0752824    0.316823     0.276541    0.241755    0.246639     0.0622394   -0.148467   -0.100988     0.213736 
 -0.301478   -0.803427      0.320117     0.616906    -0.186884     0.111153     -0.0391094    -0.363316     0.0334574   -0.427989    -0.461272     0.0169862    0.474613   -0.339456    -0.189698     0.149082   -0.18577    -0.364323     0.247378     1.20204     0.273418   -0.254351    -0.112702     0.200259    0.289115    -0.252104 
 -0.102332    0.212291      0.0965749    0.231266    -0.402127     0.056843      0.215493      0.0247657   -0.339317    -0.0786096   -0.464801     0.0796019   -0.148184    0.067182     0.952092     0.0692617   0.253144   -0.172613     0.0121766    0.643544   -0.140368    0.0147511    0.836884     0.0263496  -0.277447    -0.0832533
  0.185707   -0.207327      0.00609217   0.302509    -0.267969    -0.261824      0.697804      0.155651    -0.0993934    0.494076     0.427167    -0.0160301   -0.181271   -0.00571901   0.550966     0.104817    0.26257    -0.126544    -9.29109e-5   0.057794   -0.491277   -0.325682     0.412807     0.173504   -0.213062     0.286028 
  0.800038    0.20513       0.0912978    0.572229    -0.0622834   -0.388341     -0.0405755    -0.388211    -0.0735347    0.137171     0.327638    -0.122636     0.164545   -0.30248      0.273501     0.226947    0.412558   -0.26386     -0.517209     0.246636    0.389494    0.37725      0.759823     0.254637    0.112051     0.0258177
  0.524039   -0.184435      0.0530397   -0.119593    -0.236614     0.289107     -0.154698     -0.313737    -0.0869219   -0.108327    -0.439946    -0.310331     0.540298   -0.0160786    0.858596    -0.0856368   0.392389    0.247294     0.133937     0.0689256  -0.579324    0.530768     0.38306      0.0116333   0.422647    -0.782787 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407797
INFO: iteration 2, average log likelihood -1.407784
INFO: iteration 3, average log likelihood -1.407772
INFO: iteration 4, average log likelihood -1.407760
INFO: iteration 5, average log likelihood -1.407748
INFO: iteration 6, average log likelihood -1.407737
INFO: iteration 7, average log likelihood -1.407725
INFO: iteration 8, average log likelihood -1.407714
INFO: iteration 9, average log likelihood -1.407704
INFO: iteration 10, average log likelihood -1.407693
INFO: EM with 100000 data points 10 iterations avll -1.407693
59.0 data points per parameter
kind full, method kmeans
INFO: Initializing GMM, 32 Gaussians diag covariance 26 dimensions using 100000 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       8.960172e+05
      1       7.001907e+05      -1.958265e+05 |       32
      2       6.879654e+05      -1.222526e+04 |       32
      3       6.832864e+05      -4.679009e+03 |       32
      4       6.807975e+05      -2.488876e+03 |       32
      5       6.790549e+05      -1.742656e+03 |       32
      6       6.777120e+05      -1.342911e+03 |       32
      7       6.766482e+05      -1.063787e+03 |       32
      8       6.757822e+05      -8.660084e+02 |       32
      9       6.750018e+05      -7.803701e+02 |       32
     10       6.743408e+05      -6.609838e+02 |       32
     11       6.738054e+05      -5.353891e+02 |       32
     12       6.733871e+05      -4.183190e+02 |       32
     13       6.730470e+05      -3.400816e+02 |       32
     14       6.727515e+05      -2.955582e+02 |       32
     15       6.724916e+05      -2.598744e+02 |       32
     16       6.722949e+05      -1.967010e+02 |       32
     17       6.721269e+05      -1.680482e+02 |       32
     18       6.719753e+05      -1.515677e+02 |       32
     19       6.718330e+05      -1.423138e+02 |       32
     20       6.716925e+05      -1.404773e+02 |       32
     21       6.715684e+05      -1.240799e+02 |       32
     22       6.714577e+05      -1.107046e+02 |       32
     23       6.713634e+05      -9.428666e+01 |       32
     24       6.712651e+05      -9.834624e+01 |       32
     25       6.711560e+05      -1.090825e+02 |       32
     26       6.710564e+05      -9.962866e+01 |       32
     27       6.709650e+05      -9.133691e+01 |       32
     28       6.708826e+05      -8.241220e+01 |       32
     29       6.708001e+05      -8.249666e+01 |       32
     30       6.707217e+05      -7.846587e+01 |       32
     31       6.706499e+05      -7.177601e+01 |       32
     32       6.705869e+05      -6.297853e+01 |       32
     33       6.705295e+05      -5.738266e+01 |       32
     34       6.704765e+05      -5.300980e+01 |       32
     35       6.704248e+05      -5.173937e+01 |       32
     36       6.703717e+05      -5.306828e+01 |       32
     37       6.703290e+05      -4.271871e+01 |       32
     38       6.702833e+05      -4.570478e+01 |       32
     39       6.702282e+05      -5.508273e+01 |       32
     40       6.701657e+05      -6.246905e+01 |       32
     41       6.700914e+05      -7.430567e+01 |       32
     42       6.700059e+05      -8.553495e+01 |       32
     43       6.699224e+05      -8.344919e+01 |       32
     44       6.698459e+05      -7.655477e+01 |       32
     45       6.697688e+05      -7.704946e+01 |       32
     46       6.696854e+05      -8.341041e+01 |       32
     47       6.696056e+05      -7.987150e+01 |       32
     48       6.695441e+05      -6.143559e+01 |       32
     49       6.694872e+05      -5.688492e+01 |       32
     50       6.694401e+05      -4.710131e+01 |       32
K-means terminated without convergence after 50 iterations (objv = 669440.1330045857)
INFO: K-means with 32000 data points using 50 iterations
37.0 data points per parameter
INFO: Running 50 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.419572
INFO: iteration 2, average log likelihood -1.414564
INFO: iteration 3, average log likelihood -1.413124
INFO: iteration 4, average log likelihood -1.412013
INFO: iteration 5, average log likelihood -1.410906
INFO: iteration 6, average log likelihood -1.409971
INFO: iteration 7, average log likelihood -1.409364
INFO: iteration 8, average log likelihood -1.409023
INFO: iteration 9, average log likelihood -1.408822
INFO: iteration 10, average log likelihood -1.408685
INFO: iteration 11, average log likelihood -1.408580
INFO: iteration 12, average log likelihood -1.408493
INFO: iteration 13, average log likelihood -1.408419
INFO: iteration 14, average log likelihood -1.408353
INFO: iteration 15, average log likelihood -1.408293
INFO: iteration 16, average log likelihood -1.408239
INFO: iteration 17, average log likelihood -1.408190
INFO: iteration 18, average log likelihood -1.408144
INFO: iteration 19, average log likelihood -1.408102
INFO: iteration 20, average log likelihood -1.408063
INFO: iteration 21, average log likelihood -1.408027
INFO: iteration 22, average log likelihood -1.407993
INFO: iteration 23, average log likelihood -1.407962
INFO: iteration 24, average log likelihood -1.407933
INFO: iteration 25, average log likelihood -1.407906
INFO: iteration 26, average log likelihood -1.407881
INFO: iteration 27, average log likelihood -1.407857
INFO: iteration 28, average log likelihood -1.407835
INFO: iteration 29, average log likelihood -1.407814
INFO: iteration 30, average log likelihood -1.407795
INFO: iteration 31, average log likelihood -1.407776
INFO: iteration 32, average log likelihood -1.407759
INFO: iteration 33, average log likelihood -1.407742
INFO: iteration 34, average log likelihood -1.407726
INFO: iteration 35, average log likelihood -1.407711
INFO: iteration 36, average log likelihood -1.407697
INFO: iteration 37, average log likelihood -1.407683
INFO: iteration 38, average log likelihood -1.407670
INFO: iteration 39, average log likelihood -1.407657
INFO: iteration 40, average log likelihood -1.407645
INFO: iteration 41, average log likelihood -1.407633
INFO: iteration 42, average log likelihood -1.407621
INFO: iteration 43, average log likelihood -1.407610
INFO: iteration 44, average log likelihood -1.407599
INFO: iteration 45, average log likelihood -1.407588
INFO: iteration 46, average log likelihood -1.407578
INFO: iteration 47, average log likelihood -1.407568
INFO: iteration 48, average log likelihood -1.407558
INFO: iteration 49, average log likelihood -1.407549
INFO: iteration 50, average log likelihood -1.407539
INFO: EM with 100000 data points 50 iterations avll -1.407539
59.0 data points per parameter
32×26 Array{Float64,2}:
  0.00434793  -0.0351038  -0.101174   -3.07944e-5  -0.0330766   0.0690094    0.0717618  -0.0193158    0.0219064   -0.000402516  -0.060558    0.0211674   0.0863482   0.0165064    0.00800605   0.0104866    -0.000691312   0.107645     0.0808624    0.0285772   0.123458    0.0247494  -0.0600884    0.0773147   0.000164473   0.0644164 
 -0.0297527    0.318792   -0.492003   -0.158635     0.161886    0.0225176   -0.43569    -0.0163216   -0.232649     0.0436009    -0.0820522  -0.44692    -0.616874    0.735129     0.241529     0.0978083     0.315135     -0.0902051   -0.475746    -0.773412   -0.315377    0.174991    0.568909    -0.45652    -0.0590873     0.608237  
  0.728372    -0.0727136  -0.474912   -0.245936    -0.142013   -0.183659     0.431088   -0.0700466   -0.434594     0.0412781     0.299161   -0.042163   -0.175108   -0.0701045    0.244955     1.22093      -0.270073      0.22714      0.0796634    0.042202   -0.267043    0.361185   -0.658971     0.185268   -0.313249      0.109728  
 -0.415147     0.158938    0.548573    0.0389155   -0.0484243  -0.508119    -0.0390381  -1.04077     -0.592914    -0.355973     -0.267471    0.30469     0.0266311   0.35939      0.145355    -0.0875671     0.208619     -0.00958427   0.0138031   -0.239301   -0.0605295   0.432025    0.550954     0.339755   -0.0972618    -0.0281022 
 -0.615137    -0.809431    0.430019    0.454024    -0.151744   -0.0729563   -0.212274   -0.525765    -0.25004     -0.38365      -0.425383   -0.272182   -0.170262   -0.0925114    0.0034197    0.323635     -0.106573     -0.215144     0.288973     0.890727    0.260015   -0.130214    0.283226    -0.162798    0.118243      0.335899  
 -0.397795     0.116831    0.491194   -0.759925     0.0540991  -0.276598    -0.0659285   0.845504    -0.00241633  -0.196088     -0.497884    0.0896349  -0.456448    0.50463     -0.193663    -0.0345255    -0.0358926    -0.86204     -0.23783     -0.275591   -0.356918   -0.581457   -0.0265609   -0.177489   -0.0864625    -0.253556  
  0.199968    -0.330413    0.0250149   0.546523    -0.521106    0.249048     0.314362   -0.391487     0.102222    -0.00255094    0.477322   -0.102947    0.41796    -0.431542     0.255235     0.184         0.106502      0.500343     0.545653     0.269039    0.120206    0.712153    0.352828    -0.150922    0.33484       0.28227   
 -0.102377    -0.169848    0.449492    0.0812982   -0.0373069  -0.143156    -0.211399   -0.216256    -0.217821     0.194569      0.0084666   0.0938965  -0.274533    0.0892756   -0.193339     0.147831     -0.0600454    -0.150478    -0.040179    -0.207869   -0.416189    0.0242473  -0.0380447   -0.20157     0.0898638    -0.0137575 
 -0.373597    -0.303167   -0.10077    -0.195659     0.595414    0.136723    -0.16914    -0.113218    -0.0632728    0.0187616    -0.0562814   0.233391    0.194844    0.0240293   -0.307508    -0.11622      -0.0742629     0.230806    -0.184754    -0.567242   -0.0811318  -0.338382   -0.970823    -0.749882    0.140295      0.335528  
  0.412416    -0.500163    0.13208     0.165455    -0.0156756  -0.266769     0.289694    0.331902    -0.438806     0.309777      0.0256417   0.349409   -0.262084   -0.382875     0.607377    -0.121744      0.476667     -0.425009    -0.033563     0.099018   -0.219308   -0.304358    0.0201752    0.401848    0.0843967    -0.474732  
  0.443649    -0.116128    0.182346   -0.302108    -0.395653    0.302171    -0.321924   -0.446178    -0.319756    -0.237064     -0.77307    -0.317159    0.558735    0.143759     0.637629    -0.000232536   0.259125      0.211231    -0.0395164    0.259257   -0.612677   -0.11866     0.0487052   -0.142803    0.394951     -0.495629  
 -0.204938    -0.175028    0.198687   -0.13147     -0.574291    0.232976    -0.316331    0.35881      0.0164464    0.0243094    -0.74169     0.333741   -0.214126    0.10662      0.0444947    0.462039      0.214125     -0.00847352   0.532736    -0.160099   -0.0996086   0.651389    0.06773     -0.046776   -0.136777     -0.197707  
 -0.755042    -0.0309127  -0.786658    0.195986     0.159789   -0.113949    -0.672624   -0.397625     0.687045     0.147882     -0.298706   -0.563982    0.146192    0.350833     0.0777287    0.0980132    -0.282519      1.17837      0.213776    -0.86244     0.591546    0.186754    0.0920206   -0.0742027  -0.26404       0.826719  
 -0.186856    -0.634401    0.300703    0.125673    -0.0833696   0.00730656  -0.0397046   0.177608     0.125502    -0.0355339    -0.473462    0.59693     0.576482   -0.734739    -0.334345    -0.0396567    -0.0628052    -0.279768     0.305239     0.750973    0.339099   -0.14921    -0.624949     0.298317    0.139906     -0.840238  
 -0.269594    -0.213115   -0.115952    0.348573     0.458995   -0.506002     0.448795    0.165458     0.207804     0.709522      0.744716    0.133771   -0.357125   -0.345464    -0.707727     0.085264     -0.0865302    -0.149773     0.231224    -0.241051    0.525217    0.0928025  -0.038348     0.236978   -0.269826      0.690103  
 -0.357873     0.0995387   0.509377   -0.32857     -0.161732    0.532942     0.887805   -0.300112     0.411922    -0.443315     -0.0345983   0.236525   -0.193847   -0.320143     0.269422    -0.0903287    -0.0314219     0.311807     0.00845162  -0.296028    0.755418    0.168559   -0.906486     0.940246    0.330561      0.315051  
  0.485014     0.347704    0.202587    0.429534    -0.373968   -0.393019    -0.0474521  -0.0745499   -0.154783     0.0774728     0.0426501  -0.272527    0.108344   -0.0430394    0.512069     0.0642858     0.320467     -0.130378    -0.201166     0.335416    0.0841664   0.380131    0.780237     0.248367   -0.129735      0.0477252 
 -0.198849     0.0684606   0.307961    0.266762     0.595326    0.287361    -0.68881     0.130095     0.0263926    0.121343     -1.01086     0.934256   -0.462092    0.981482    -0.253977    -1.40563      -0.412355     -0.319031    -0.615853     0.619946    0.549434   -0.365269    0.169502    -0.524986   -0.700205      0.185069  
 -0.450981     0.888201   -0.242224   -0.363365    -0.0364115   0.58728      0.395484    0.123147    -0.527391     0.536027      0.138922    0.0461635  -0.288923    0.110864     1.00769     -0.161308     -0.236257      0.0022331   -0.102637     0.433693   -0.301554   -0.219155    0.525985    -0.132617   -0.0692373    -0.29687   
  0.195875    -0.143169   -0.414701   -0.159745     0.481894    0.266021    -0.334018    0.00562625   0.130023    -0.418474      0.0217434   0.182136   -0.199458   -0.336665     0.366732    -0.0545401     0.266388     -0.677464    -0.314351     0.0910604   0.909819   -0.239931    0.467503     0.0602606   0.117195      0.00183544
  0.103161    -0.153083   -0.678024   -0.417115    -0.140042    0.0181142   -1.20531     0.483952    -0.465449     0.780712      0.0771222  -0.23087     0.249411    0.155056    -0.239399    -0.0385346    -0.349345     -0.163617     0.290544     0.265708   -0.400932   -0.322103    0.164968    -0.312028   -0.258058     -0.457698  
  0.739822     0.737596   -0.452372   -0.318604    -0.136051    0.21558      0.364327    0.349979     0.663653     0.35326       0.284688    0.505561    0.183716   -0.101172    -0.0788014   -0.45111       0.190493      0.102443    -0.213814    -0.444494   -0.202924    0.199505   -0.180927     0.493789   -0.235062     -0.157607  
  0.112098    -0.123506    0.130212    0.35867      0.331392    0.12117     -0.445312   -0.485337    -0.143993    -0.518544      0.312806   -0.360701   -0.145816    0.00262812  -0.20819      0.0755697    -0.548478     -0.327034     0.208042     0.253626    0.261497    0.576401   -0.284105     0.115244   -0.228319     -0.639091  
  0.280371    -0.0785394  -0.140896    0.584424     0.246864   -0.0737944    0.694353   -0.455488     0.0786049   -0.227839      0.357051   -0.226934   -0.076855    0.10796      0.200963    -0.105132     -0.302806      0.261908    -0.710355     0.368387   -0.0719387  -0.901223   -0.0785661    0.334446   -0.216625      0.277856  
  0.0861185   -0.211319   -0.0203782   0.294265     0.0893365   0.0325774    0.114418   -0.0042325    0.0909083    0.103961     -0.081245   -0.0753132  -0.033691   -0.0239946    0.238831     0.0922026     0.163527      0.0592788   -0.0899006    0.0160546  -0.128265    0.204045    0.32119     -0.267696   -0.00285729    0.226782  
 -0.436331    -0.405488   -0.147789   -0.253889    -0.179523    0.408837     0.628705    0.199224     0.640992     0.739469     -0.151359    0.269617    0.120099    0.245179     0.14199     -0.102415      0.0537753    -0.190991     9.27908e-5  -0.0671472  -0.380336   -0.527434    0.37611     -0.504279    0.41094       0.787034  
  0.0833558    0.398801   -0.302273   -0.319762     0.544938    0.429515     0.107163    0.249378     0.210993     0.00593562    0.185143   -0.592415   -0.307289    0.0192327   -0.149054    -0.279332     -0.808847      0.22721      0.175796     0.206017    0.698399   -0.372076   -0.00754853   0.33326     0.311484      0.492267  
 -0.528401     0.069698    0.539978    0.140295    -0.538532   -0.519772     0.272648    0.332202    -0.108853     0.33918      -0.0392605   0.171997    0.172695    0.0756872   -0.284269    -0.118521     -0.0480939     0.639587     0.169411    -0.0792533  -0.543954    0.0666441  -0.480921    -0.13705    -0.104339     -0.187294  
 -0.108767     0.457268    0.0809747  -0.576522     0.304956   -0.172079    -0.216646    0.390231     0.255357    -0.166843     -0.280175   -0.0686853  -0.181893    0.166599    -0.0595838    0.0630554    -0.293801     -0.123204    -0.494412    -0.388333   -0.130625   -0.298891   -0.309695     0.0390997  -0.209769     -0.124742  
  0.127977     0.351332   -0.329778   -0.183183     0.369927   -0.127704     0.290912   -0.00217752   0.0822797   -0.0252219     0.251253   -0.168203    0.859113   -0.509194    -0.444469    -0.279458      0.0306898     0.0627141    0.103505     0.659846    0.311275    0.0391378   0.192169    -0.0326109   0.170111     -0.34074   
 -0.0452697    0.65152     0.211733   -0.355292    -0.0875695  -0.448977    -0.483368   -0.260075    -0.594847    -0.289272      0.202266   -0.248244   -0.242411   -0.0307174   -0.198992    -0.0166361    -0.34447      -0.0887312   -0.0151732   -0.0169081   0.296548   -0.332764   -0.30446      0.740247   -0.0331424    -0.260953  
 -0.0812889   -0.523609    0.333545    0.620582     0.217798   -0.356421    -0.163648    0.17108      0.55117     -0.619141     -0.178448    0.0114078   0.0825071   0.241355    -0.510752    -0.42478       0.478569     -0.184714    -0.108148    -0.658864    0.270647   -0.0482496   0.0732795    0.0884      0.278419     -0.0887004 INFO: Running 10 iterations EM on diag cov GMM with 32 Gaussians in 26 dimensions
INFO: iteration 1, average log likelihood -1.407530
INFO: iteration 2, average log likelihood -1.407522
INFO: iteration 3, average log likelihood -1.407513
INFO: iteration 4, average log likelihood -1.407505
INFO: iteration 5, average log likelihood -1.407496
INFO: iteration 6, average log likelihood -1.407488
INFO: iteration 7, average log likelihood -1.407481
INFO: iteration 8, average log likelihood -1.407473
INFO: iteration 9, average log likelihood -1.407466
INFO: iteration 10, average log likelihood -1.407459
INFO: EM with 100000 data points 10 iterations avll -1.407459
59.0 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 900 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.408999e+04
      1       7.823675e+03      -6.266314e+03 |        0
      2       7.823675e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 7823.675494229463)
INFO: K-means with 900 data points using 2 iterations
150.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.043155
INFO: iteration 2, average log likelihood -2.043154
INFO: iteration 3, average log likelihood -2.043154
INFO: iteration 4, average log likelihood -2.043154
INFO: iteration 5, average log likelihood -2.043154
INFO: iteration 6, average log likelihood -2.043154
INFO: iteration 7, average log likelihood -2.043154
INFO: iteration 8, average log likelihood -2.043154
INFO: iteration 9, average log likelihood -2.043154
INFO: iteration 10, average log likelihood -2.043154
INFO: EM with 900 data points 10 iterations avll -2.043154
81.8 data points per parameter
INFO: GaussianMixtures tests passed

>>> End of log
