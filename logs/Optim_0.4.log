>>> 'Pkg.add("Optim")' log
INFO: Installing Calculus v0.1.15
INFO: Installing DiffBase v0.0.2
INFO: Installing ForwardDiff v0.3.3
INFO: Installing LineSearches v0.1.1
INFO: Installing NaNMath v0.2.2
INFO: Installing Optim v0.7.0
INFO: Installing PositiveFactorizations v0.0.3
INFO: Package database updated
INFO: METADATA is out-of-date — you may not have the latest version of Optim
INFO: Use `Pkg.update()` to get the latest versions of your packages

>>> 'Pkg.test("Optim")' log
Julia Version 0.4.7
Commit ae26b25 (2016-09-18 16:17 UTC)
Platform Info:
  System: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-105-generic #152-Ubuntu SMP Fri Dec 2 15:37:11 UTC 2016 x86_64 x86_64
Memory: 2.9392738342285156 GB (1081.65234375 MB free)
Uptime: 11877.0 sec
Load Avg:  0.8828125  0.9638671875  0.95068359375
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3499 MHz     544887 s       4766 s      47785 s     477470 s         18 s
#2  3499 MHz     331932 s       1702 s      43602 s     770798 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.3
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.4
2 required packages:
 - JSON                          0.8.0
 - Optim                         0.7.0
7 additional packages:
 - Calculus                      0.1.15
 - Compat                        0.9.5
 - DiffBase                      0.0.2
 - ForwardDiff                   0.3.3
 - LineSearches                  0.1.1
 - NaNMath                       0.2.2
 - PositiveFactorizations        0.0.3
INFO: Testing Optim
WARNING: New definition 
    write(Base.IO, ForwardDiff.Partials) at /home/vagrant/.julia/v0.4/ForwardDiff/src/partials.jl:57
is ambiguous with: 
    write(Base.Base64.Base64EncodePipe, AbstractArray{UInt8, 1}) at base64.jl:89.
To fix, define 
    write(Base.Base64.Base64EncodePipe, ForwardDiff.Partials{N<:Any, UInt8})
before the new definition.
Running tests:
 * types.jl
 * bfgs.jl
 * gradient_descent.jl
 * accelerated_gradient_descent.jl
Iter     Function value   Gradient norm 
     0     1.000000e+00     4.000000e+00
     1     5.397751e-01     2.518950e+00
 * momentum_gradient_descent.jl
 * grid_search.jl
 * l_bfgs.jl
 * levenberg_marquardt.jl
WARNING: Problem solving for delta_x: predicted residual increase.
18.95337979630715 (predicted_residual) >
3.6729087897253905 (residual) + 3.552713678800501e-15 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.83860477635018 (predicted_residual) >
2.9812894464055533 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
2.5347148551516008 (predicted_residual) >
2.1895906529240503 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
2.4662071702097417 (predicted_residual) >
2.0497617658979004 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
2.402223943602616 (predicted_residual) >
1.9102621894841052 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
1.9548243717543228 (predicted_residual) >
1.8272637677633554 (residual) + 2.220446049250313e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.289097004965162 (predicted_residual) >
1.7676814916153436 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.5282850316144354 (predicted_residual) >
0.43569220875204495 (residual) + 1.1102230246251565e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.38479244475307006 (predicted_residual) >
0.3191560404065722 (residual) + 5.551115123125783e-17 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.0867292689898465 (predicted_residual) >
0.07825577278289107 (residual) + 1.3877787807814457e-17 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
0.05060004314422784 (predicted_residual) >
0.04864792336713843 (residual) + 6.938893903907228e-18 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.367145765447734 (predicted_residual) >
3.358226500859248 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.3680470964852383 (predicted_residual) >
3.367145765447712 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.368137102125979 (predicted_residual) >
3.36804709648524 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.3681461013241285 (predicted_residual) >
3.3681371021259796 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.368147001219089 (predicted_residual) >
3.3681461013241263 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.6257200159753995 (predicted_residual) >
3.6218983224136934 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.634856357243633 (predicted_residual) >
3.6257200159766194 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.6357709238952762 (predicted_residual) >
3.634856357243618 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.635862247103403 (predicted_residual) >
3.6357709238952736 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.6358713782290524 (predicted_residual) >
3.6358622471034012 (residual) + 4.440892098500626e-16 (eps)
WARNING: Problem solving for delta_x: predicted residual increase.
3.6358722911766694 (predicted_residual) >
3.63587137822905 (residual) + 4.440892098500626e-16 (eps)
     0     4.093196e+03              NaN
 * lambda: 10.0

     1     2.798224e+03     5.199044e+02
 * g(x): 519.9043679445366
 * lambda: 1.0
 * dx: [-1.3753719483817706,-2.268489926535481,-0.5169191140258013]

     2     4.992133e+02     2.193666e+02
 * g(x): 219.3665686869151
 * lambda: 0.1
 * dx: [-3.3345282071987015,-5.791990282603159,-2.002503382825614]

     3     9.266003e+00     2.104323e+01
 * g(x): 21.04322827390338
 * lambda: 0.010000000000000002
 * dx: [-0.15734860970728481,1.5303269456439366,-2.1279209211925756]

     4     9.815963e-01     3.012647e-01
 * g(x): 0.30126472561688666
 * lambda: 0.0010000000000000002
 * dx: [-0.08533847526335515,1.4220761005726923,-0.344096768037179]

     5     9.379170e-01     7.194056e-03
 * g(x): 0.007194056040641439
 * lambda: 0.00010000000000000003
 * dx: [0.05773010936977138,0.09515204854810133,-0.01719065976553666]

     6     9.379163e-01     7.827368e-07
 * g(x): 7.827368227186771e-7
 * lambda: 1.0000000000000004e-5
 * dx: [0.00025873849595333304,-0.0001437613365856425,-8.199125241275593e-5]

     7     9.379163e-01     7.398526e-12
 * g(x): 7.398526236102043e-12
 * lambda: 1.0000000000000004e-6
 * dx: [9.769458136003215e-8,-9.595379397390548e-8,-7.419398427094509e-9]

 * newton.jl
 * newton_trust_region.jl
 * cg.jl
 * nelder_mead.jl
WARNING: NelderMead(a::Real,g::Real,b::Real) is deprecated, use NelderMead(initial_simplex=AffineSimplexer(),parameters=FixedParameters(a,g,b,0.5)) instead.
 in depwarn at deprecated.jl:73
 in call at deprecated.jl:50
 in NelderMead at /home/vagrant/.julia/v0.4/Optim/src/nelder_mead.jl:53
 in include at ./boot.jl:261
 in include_from_node1 at ./loading.jl:333
 [inlined code] from /home/vagrant/.julia/v0.4/Optim/test/runtests.jl:41
 in anonymous at no file:0
 in include at ./boot.jl:261
 in include_from_node1 at ./loading.jl:333
 in process_options at ./client.jl:280
 in _start at ./client.jl:378
while loading /home/vagrant/.julia/v0.4/Optim/test/nelder_mead.jl, in expression starting on line 20
 * optimize.jl
 * simulated_annealing.jl
 * particle_swarm.jl
 * api.jl
Iter     Function value   Gradient norm 
     0     1.000000e+00     2.000000e+00
 * Current step size: 1.0
 * g(x): [-2.0,0.0]
 * ~inv(H): [1.0 0.0
 0.0 1.0]
 * x: [0.0,0.0]
     1     8.415971e-01     1.578600e+00
 * Current step size: 0.042768077654337065
 * g(x): [-1.5785998400300938,-1.4632867729979242]
 * ~inv(H): [12.260820411071911 3.4724400035880936
 3.4724400035880936 1.0]
 * x: [0.08553615530867413,0.0]
     2     6.928766e-01     1.076526e+01
 * Current step size: 0.01143894820261991
 * g(x): [6.590040993876368,-10.765261839227247]
 * ~inv(H): [0.09755432555822985 0.05561860874038116
 0.05561860874038116 0.04030179753623098]
 * x: [0.36505949086640005,0.079442122675499]
     3     4.336347e-01     1.082559e+00
 * Current step size: 0.5289383775238637
 * g(x): [-1.0825588837187847,-0.34241413678581245]
 * ~inv(H): [0.08642895329759773 0.06138326588451566
 0.06138326588451566 0.04860312015279905]
 * x: [0.34171314616735027,0.11505580357965983]
     4     3.268654e-01     1.203708e+00
 * Current step size: 0.7832069592846927
 * g(x): [-0.09839677627716781,-1.2037082999506954]
 * ~inv(H): [0.27842751754766193 0.21395206510478715
 0.21395206510478715 0.16891347473888252]
 * x: [0.4314552937349688,0.18013512899217476]
     5     2.989460e-01     1.820631e+00
 * Current step size: 0.10324029716686389
 * g(x): [0.5998987628106676,-1.8206313074099922]
 * ~inv(H): [0.1772728271406797 0.1529726293715945
 0.15297262937159456 0.13560134462583273]
 * x: [0.46087178355791397,0.20329964434280273]
     6     2.380606e-01     3.824597e+00
 * Current step size: 0.5242180516622564
 * g(x): [3.317879999148621,-3.824596886801146]
 * ~inv(H): [0.11749909347998533 0.11432852203558802
 0.11432852203558808 0.11448813761617876]
 * x: [0.5511216920212516,0.2846121349823616]
     7     1.377832e-01     4.305388e+00
 * Current step size: 2.654333836482214
 * g(x): [4.305387780883387,-3.657061711344489]
 * ~inv(H): [0.10559052300284952 0.1287935569735938
 0.1287935569735939 0.1683653505547278]
 * x: [0.6769706063292672,0.4400038932770932]
     8     9.932907e-02     1.240226e+00
 * Current step size: 0.5277472177097148
 * g(x): [-1.2402258640724222,0.44592549283627436]
 * ~inv(H): [0.12047899026168724 0.16494905107389302
 0.16494905107389313 0.23081973637893397]
 * x: [0.6856245199014988,0.472310609754342]
     9     6.543213e-02     6.765640e-01
 * Current step size: 0.8017424505942204
 * g(x): [0.502941174668424,-0.6765639654057409]
 * ~inv(H): [0.36626219446146857 0.5145981989382932
 0.5145981989382932 0.7265430960391137]
 * x: [0.7464496512850596,0.5538042620765583]
    10     5.511958e-02     1.419213e+00
 * Current step size: 0.16297281487330761
 * g(x): [1.4192134417115665,-1.211167971472693]
 * ~inv(H): [0.21147280127724322 0.31246909080110485
 0.31246909080110485 0.46459888969678076]
 * x: [0.7731690866149542,0.5917345966396391]
Iter     Function value   Gradient norm 
     0     1.000000e+00     2.000000e+00
 * Current step size: 1.0
 * g(x): [-2.0,0.0]
 * ~inv(H): [1.0 0.0
 0.0 1.0]
 * x: [0.0,0.0]
     1     8.415971e-01     1.578600e+00
 * Current step size: 0.042768077654337065
 * g(x): [-1.5785998400300938,-1.4632867729979242]
 * ~inv(H): [12.260820411071911 3.4724400035880936
 3.4724400035880936 1.0]
 * x: [0.08553615530867413,0.0]
     2     6.928766e-01     1.076526e+01
 * Current step size: 0.01143894820261991
 * g(x): [6.590040993876368,-10.765261839227247]
 * ~inv(H): [0.09755432555822985 0.05561860874038116
 0.05561860874038116 0.04030179753623098]
 * x: [0.36505949086640005,0.079442122675499]
     3     4.336347e-01     1.082559e+00
 * Current step size: 0.5289383775238637
 * g(x): [-1.0825588837187847,-0.34241413678581245]
 * ~inv(H): [0.08642895329759773 0.06138326588451566
 0.06138326588451566 0.04860312015279905]
 * x: [0.34171314616735027,0.11505580357965983]
     4     3.268654e-01     1.203708e+00
 * Current step size: 0.7832069592846927
 * g(x): [-0.09839677627716781,-1.2037082999506954]
 * ~inv(H): [0.27842751754766193 0.21395206510478715
 0.21395206510478715 0.16891347473888252]
 * x: [0.4314552937349688,0.18013512899217476]
     5     2.989460e-01     1.820631e+00
 * Current step size: 0.10324029716686389
 * g(x): [0.5998987628106676,-1.8206313074099922]
 * ~inv(H): [0.1772728271406797 0.1529726293715945
 0.15297262937159456 0.13560134462583273]
 * x: [0.46087178355791397,0.20329964434280273]
     6     2.380606e-01     3.824597e+00
 * Current step size: 0.5242180516622564
 * g(x): [3.317879999148621,-3.824596886801146]
 * ~inv(H): [0.11749909347998533 0.11432852203558802
 0.11432852203558808 0.11448813761617876]
 * x: [0.5511216920212516,0.2846121349823616]
     7     1.377832e-01     4.305388e+00
 * Current step size: 2.654333836482214
 * g(x): [4.305387780883387,-3.657061711344489]
 * ~inv(H): [0.10559052300284952 0.1287935569735938
 0.1287935569735939 0.1683653505547278]
 * x: [0.6769706063292672,0.4400038932770932]
     8     9.932907e-02     1.240226e+00
 * Current step size: 0.5277472177097148
 * g(x): [-1.2402258640724222,0.44592549283627436]
 * ~inv(H): [0.12047899026168724 0.16494905107389302
 0.16494905107389313 0.23081973637893397]
 * x: [0.6856245199014988,0.472310609754342]
     9     6.543213e-02     6.765640e-01
 * Current step size: 0.8017424505942204
 * g(x): [0.502941174668424,-0.6765639654057409]
 * ~inv(H): [0.36626219446146857 0.5145981989382932
 0.5145981989382932 0.7265430960391137]
 * x: [0.7464496512850596,0.5538042620765583]
    10     5.511958e-02     1.419213e+00
 * Current step size: 0.16297281487330761
 * g(x): [1.4192134417115665,-1.211167971472693]
 * ~inv(H): [0.21147280127724322 0.31246909080110485
 0.31246909080110485 0.46459888969678076]
 * x: [0.7731690866149542,0.5917345966396391]
    11     4.013082e-02     3.376464e+00
 * Current step size: 0.7692406779958673
 * g(x): [3.3764638861612575,-2.225535925799549]
 * ~inv(H): [0.16596031736290695 0.26082566468818047
 0.26082566468818036 0.4128398706025068]
 * x: [0.8334217956088235,0.6834642097668377]
    12     6.980795e-03     5.135785e-01
 * Current step size: 4.153824060425805
 * g(x): [-0.5135784925739654,0.18950864380857801]
 * ~inv(H): [0.37279387466211067 0.6350814716463098
 0.6350814716463098 1.088528130945603]
 * x: [0.9169878839984639,0.8418143226190231]
    13     4.822164e-03     1.639696e+00
 * Current step size: 0.43568592755685387
 * g(x): [1.6396956706416248,-0.9197364361997584]
 * ~inv(H): [0.21277966223893663 0.38512074243449224
 0.38512074243449224 0.7005134374449606]
 * x: [0.9479675444006876,0.8940437830560709]
    14     1.768810e-03     4.107685e-01
 * Current step size: 2.042291166259496
 * g(x): [-0.4107685286199115,0.17125983330164019]
 * ~inv(H): [0.3033758724195759 0.5801280591151603
 0.5801280591151603 1.1142909527249176]
 * x: [0.9588237274123387,0.9201992394153989]
    15     4.218795e-04     1.186910e-01
 * Current step size: 0.8328229696007655
 * g(x): [0.1186910070851086,-0.08111407238122759]
 * ~inv(H): [0.4903744908513556 0.9453928635051384
 0.9453928635051384 1.8267232015823822]
 * x: [0.9798646882925809,0.9597292370008106]
    16     9.522312e-05     3.374010e-01
 * Current step size: 0.8498104659755683
 * g(x): [0.33740103123189163,-0.173900230455315]
 * ~inv(H): [0.436788904106256 0.8603032690972453
 0.8603032690972453 1.6984746476183599]
 * x: [0.9955705649385959,0.9902912486198784]
    17     1.824025e-06     2.888770e-02
 * Current step size: 1.456401071694573
 * g(x): [-0.028887703530359368,0.013283506692918223]
 * ~inv(H): [0.49455770183552655 0.9851517376363241
 0.9851517376363241 1.9674497843583114]
 * x: [0.9988240339751772,0.9977158683799104]
    18     3.847551e-09     1.733584e-03
 * Current step size: 0.9445083346822327
 * g(x): [0.0017335836947083379,-0.0009090400136191334]
 * ~inv(H): [0.495163457689567 0.9884615159206567
 0.9884615159206565 1.977995631119267]
 * x: [0.9999577902041284,0.9999110369898556]
    19     6.629175e-13     2.322282e-05
 * Current step size: 1.0664072842242067
 * g(x): [2.322282039644862e-5,-1.1011587064402306e-5]
 * ~inv(H): [0.5032494702161785 1.0061461191632277
 1.0061461191632275 2.0166182808176325]
 * x: [1.000000599816529,1.0000011445754822]
    20     1.745741e-19     1.175798e-08
 * Current step size: 0.986714750685407
 * g(x): [-1.1757975485618804e-8,6.161249288538784e-9]
 * ~inv(H): [0.49985452220886173 0.9996938112870364
 0.9996938112870362 2.0043554658026466]
 * x: [1.0000000002822615,1.0000000005953293]
    21     5.374115e-30     9.015011e-14
 * Current step size: 1.0006216777739572
 * g(x): [9.015010959956277e-14,-4.440892098500626e-14]
 * ~inv(H): [0.49985452220886173 0.9996938112870364
 0.9996938112870362 2.0043554658026466]
 * x: [1.0000000000000007,1.000000000000001]
Iter     Function value    √(Σ(yᵢ-ȳ)²)/n 
------   --------------    --------------
     0     9.506641e-01     4.576214e-02
 * step_type: initial
 * centroid: [0.0125,0.0]
     1     9.506641e-01     2.023096e-02
 * step_type: outside contraction
 * centroid: [0.0125,0.0]
     2     9.506641e-01     2.172172e-02
 * step_type: expansion
 * centroid: [0.021875000000000002,-0.00625]
     3     9.506641e-01     5.243757e-02
 * step_type: expansion
 * centroid: [0.045312500000000006,-0.009375000000000001]
     4     9.506641e-01     4.259749e-02
 * step_type: reflection
 * centroid: [0.08203125,-0.010937500000000003]
     5     9.506641e-01     4.265507e-02
 * step_type: reflection
 * centroid: [0.11875000000000001,-0.012500000000000004]
     6     9.506641e-01     3.109209e-02
 * step_type: reflection
 * centroid: [0.13515625,-0.004687500000000004]
     7     9.506641e-01     3.215435e-02
 * step_type: reflection
 * centroid: [0.1515625,0.003124999999999996]
     8     9.506641e-01     2.418419e-02
 * step_type: reflection
 * centroid: [0.16796875,0.010937499999999996]
     9     9.506641e-01     2.426367e-02
 * step_type: reflection
 * centroid: [0.18437499999999998,0.018749999999999996]
    10     9.506641e-01     2.124416e-02
 * step_type: reflection
 * centroid: [0.20078124999999997,0.026562499999999996]
    11     9.506641e-01     1.809652e-02
 * step_type: reflection
 * centroid: [0.21718749999999998,0.034374999999999996]
    12     9.506641e-01     2.151280e-02
 * step_type: reflection
 * centroid: [0.23359374999999996,0.042187499999999996]
    13     9.506641e-01     1.648361e-02
 * step_type: reflection
 * centroid: [0.24999999999999994,0.049999999999999996]
    14     9.506641e-01     2.780850e-02
 * step_type: reflection
 * centroid: [0.26640624999999996,0.057812499999999996]
    15     9.506641e-01     2.340189e-02
 * step_type: inside contraction
 * centroid: [0.24609374999999994,0.0671875]
    16     9.506641e-01     1.487609e-02
 * step_type: reflection
 * centroid: [0.27075654044445774,0.06793710183256974]
    17     9.506641e-01     2.056105e-02
 * step_type: reflection
 * centroid: [0.28716279044445775,0.07574960183256974]
    18     9.506641e-01     8.300336e-03
 * step_type: reflection
 * centroid: [0.30356904044445776,0.08356210183256974]
    19     9.506641e-01     2.460350e-02
 * step_type: expansion
 * centroid: [0.3199752904444578,0.09137460183256974]
    20     9.506641e-01     2.134752e-02
 * step_type: inside contraction
 * centroid: [0.3157936047777712,0.10587519908371512]
    21     9.506641e-01     2.758615e-02
 * step_type: expansion
 * centroid: [0.3343588988952721,0.10953165364377715]
    22     9.506641e-01     6.347518e-02
 * step_type: expansion
 * centroid: [0.3743632031206793,0.13423517954938086]
    23     9.506641e-01     4.615543e-02
 * step_type: inside contraction
 * centroid: [0.4353275647889252,0.17708734066961299]
    24     9.506641e-01     3.809765e-02
 * step_type: outside contraction
 * centroid: [0.41817806899351356,0.17580747561346435]
    25     9.506641e-01     6.120849e-02
 * step_type: expansion
 * centroid: [0.43962234401969824,0.19843422668121263]
    26     9.506641e-01     3.814177e-02
 * step_type: outside contraction
 * centroid: [0.5103825273805715,0.25628096940673195]
    27     9.506641e-01     2.033749e-02
 * step_type: inside contraction
 * centroid: [0.5588073926831426,0.297567153019201]
    28     9.506641e-01     1.224326e-02
 * step_type: outside contraction
 * centroid: [0.5588073926831426,0.297567153019201]
    29     9.506641e-01     2.171812e-02
 * step_type: expansion
 * centroid: [0.5752020526617637,0.31474345258643355]
    30     9.506641e-01     1.698053e-02
 * step_type: reflection
 * centroid: [0.5890307264300239,0.33557920044960254]
    31     9.506641e-01     2.736366e-02
 * step_type: expansion
 * centroid: [0.6028594001982839,0.3564149483127715]
    32     9.506641e-01     2.469437e-02
 * step_type: reflection
 * centroid: [0.6494773939237861,0.4116032953104055]
    33     9.506641e-01     1.937227e-02
 * step_type: inside contraction
 * centroid: [0.6960953876492884,0.4667916423080396]
    34     9.506641e-01     1.528363e-02
 * step_type: reflection
 * centroid: [0.6665146633406318,0.4330311595751971]
    35     9.506641e-01     1.330356e-02
 * step_type: reflection
 * centroid: [0.6538742325479772,0.4240199253943331]
    36     9.506641e-01     1.491784e-02
 * step_type: reflection
 * centroid: [0.6708145260639791,0.44876917394631144]
    37     9.506641e-01     1.037548e-02
 * step_type: inside contraction
 * centroid: [0.7003952503726356,0.48252965667915393]
    38     9.506641e-01     9.321927e-03
 * step_type: reflection
 * centroid: [0.6966800410732795,0.4825600284464805]
    39     9.506641e-01     8.326755e-03
 * step_type: reflection
 * centroid: [0.7136203345892814,0.5073092769984588]
    40     9.506641e-01     7.254579e-03
 * step_type: inside contraction
 * centroid: [0.7342758374046394,0.5320281537831106]
    41     9.506641e-01     1.090885e-02
 * step_type: expansion
 * centroid: [0.735712405763931,0.5372774821395998]
    42     9.506641e-01     8.711934e-03
 * step_type: reflection
 * centroid: [0.7618411302175797,0.5770260191458119]
    43     9.506641e-01     1.307535e-02
 * step_type: expansion
 * centroid: [0.7879698546712284,0.616774556152024]
    44     9.506641e-01     1.072049e-02
 * step_type: inside contraction
 * centroid: [0.8324754410001705,0.6865216700667036]
    45     9.506641e-01     9.769055e-03
 * step_type: expansion
 * centroid: [0.8296572142715495,0.6858878738591034]
    46     9.506641e-01     5.845739e-03
 * step_type: reflection
 * centroid: [0.8717508926655869,0.7609417663340889]
    47     9.506641e-01     7.291009e-03
 * step_type: expansion
 * centroid: [0.9138445710596244,0.8359956588090744]
    48     9.506641e-01     3.308060e-03
 * step_type: inside contraction
 * centroid: [0.9821218838972727,0.9606629487956502]
    49     9.506641e-01     2.966401e-03
 * step_type: inside contraction
 * centroid: [0.9714616025045758,0.9468914902692855]
    50     9.506641e-01     8.392380e-04
 * step_type: inside contraction
 * centroid: [0.987575155709205,0.9733496162325236]
    51     9.506641e-01     6.520787e-04
 * step_type: inside contraction
 * centroid: [0.987575155709205,0.9733496162325236]
    52     9.506641e-01     2.138461e-04
 * step_type: reflection
 * centroid: [0.9925435357037661,0.9849056446963466]
    53     9.506641e-01     4.377739e-04
 * step_type: inside contraction
 * centroid: [0.9975119156983272,0.9964616731601695]
    54     9.506641e-01     2.795646e-04
 * step_type: inside contraction
 * centroid: [1.0190940492074865,1.0379750013530953]
    55     9.506641e-01     2.601891e-05
 * step_type: inside contraction
 * centroid: [0.9992063829191212,0.9985669449356882]
    56     9.506641e-01     7.183578e-05
 * step_type: inside contraction
 * centroid: [0.9992063829191212,0.9985669449356882]
    57     9.506641e-01     6.903285e-05
 * step_type: inside contraction
 * centroid: [1.0093219430861469,1.0183056302611477]
    58     9.506641e-01     1.812548e-05
 * step_type: inside contraction
 * centroid: [1.0032068446165392,1.006846644866492]
    59     9.506641e-01     1.793986e-05
 * step_type: reflection
 * centroid: [1.0021903993782184,1.0044898121262391]
    60     9.506641e-01     1.342990e-05
 * step_type: inside contraction
 * centroid: [0.9964255874142469,0.993113893393545]
    61     9.506641e-01     5.930672e-06
 * step_type: inside contraction
 * centroid: [0.9989515505088659,0.9978216351299631]
    62     9.506641e-01     2.776279e-06
 * step_type: inside contraction
 * centroid: [1.0002870959987837,1.0006754316186264]
    63     9.506641e-01     1.589405e-06
 * step_type: inside contraction
 * centroid: [0.9999827117406848,0.9999183010668825]
    64     9.506641e-01     2.403931e-06
 * step_type: inside contraction
 * centroid: [0.9981780179694459,0.9964375970963775]
    65     9.506641e-01     1.282629e-06
 * step_type: outside contraction
 * centroid: [0.9997769089745442,0.9996195351248023]
    66     9.506641e-01     5.065110e-07
 * step_type: inside contraction
 * centroid: [1.0007994388958976,1.0016341875038093]
    67     9.506641e-01     3.665532e-07
 * step_type: reflection
 * centroid: [1.0001816969863528,1.0004036702374126]
    68     9.506641e-01     9.090210e-08
 * step_type: inside contraction
 * centroid: [1.0000515047402159,1.000083751317584]
    69     9.506641e-01     1.245959e-07
 * step_type: inside contraction
 * centroid: [0.9996394976358349,0.9992980179753612]
    70     9.506641e-01     1.049977e-07
 * step_type: inside contraction
 * centroid: [1.000039558669294,1.0001023156552793]
    71     9.506641e-01     1.805558e-08
 * step_type: inside contraction
 * centroid: [0.9999618843595492,0.9999215777988404]
    72     9.506641e-01     2.664886e-08
 * step_type: inside contraction
 * centroid: [0.9999618843595492,0.9999215777988404]
    73     9.506641e-01     1.532154e-08
 * step_type: inside contraction
 * centroid: [1.0000784517065542,1.0001619215134534]
    74     9.506641e-01     9.739938e-09
 * step_type: inside contraction
 * centroid: [0.9998885913759734,0.9997866816393803]
Iter     Function value   Gradient norm 
     0    -1.033256e-01              NaN
 * x_lower: -2.0
 * x_upper: 1.0
 * minimizer: -0.8541019662496847
     1    -1.033256e-01              NaN
 * x_lower: -2.0
 * x_upper: -0.14589803375031563
 * minimizer: -0.8541019662496847
     2    -1.033256e-01              NaN
 * x_lower: -1.291796067500631
 * x_upper: -0.14589803375031563
 * minimizer: -0.8541019662496847
     3    -1.033256e-01              NaN
 * x_lower: -1.291796067500631
 * x_upper: -0.5835921350012621
 * minimizer: -0.8541019662496847
     4    -1.033256e-01              NaN
 * x_lower: -1.0212862362522084
 * x_upper: -0.5835921350012621
 * minimizer: -0.8541019662496847
     5    -1.249988e-01              NaN
 * x_lower: -0.8541019662496847
 * x_upper: -0.5835921350012621
 * minimizer: -0.7507764050037857
     6    -1.249988e-01              NaN
 * x_lower: -0.8541019662496847
 * x_upper: -0.6869176962471611
 * minimizer: -0.7507764050037857
     7    -1.249988e-01              NaN
 * x_lower: -0.7902432574930602
 * x_upper: -0.6869176962471611
 * minimizer: -0.7507764050037857
     8    -1.249988e-01              NaN
 * x_lower: -0.7902432574930602
 * x_upper: -0.7263845487364357
 * minimizer: -0.7507764050037857
     9    -1.249988e-01              NaN
 * x_lower: -0.7658514012257102
 * x_upper: -0.7263845487364357
 * minimizer: -0.7507764050037857
    10    -1.249988e-01              NaN
 * x_lower: -0.7658514012257102
 * x_upper: -0.7414595449583602
 * minimizer: -0.7507764050037857
    11    -1.249988e-01              NaN
 * x_lower: -0.7565345411802846
 * x_upper: -0.7414595449583602
 * minimizer: -0.7507764050037857
    12    -1.249988e-01              NaN
 * x_lower: -0.7565345411802846
 * x_upper: -0.7472176811348591
 * minimizer: -0.7507764050037857
    13    -1.249988e-01              NaN
 * x_lower: -0.752975817311358
 * x_upper: -0.7472176811348591
 * minimizer: -0.7507764050037857
    14    -1.249993e-01              NaN
 * x_lower: -0.7507764050037857
 * x_upper: -0.7472176811348591
 * minimizer: -0.7494170934424312
    15    -1.249993e-01              NaN
 * x_lower: -0.7507764050037857
 * x_upper: -0.7485769926962135
 * minimizer: -0.7494170934424312
    16    -1.250000e-01              NaN
 * x_lower: -0.7507764050037857
 * x_upper: -0.7494170934424312
 * minimizer: -0.749936304257568
    17    -1.250000e-01              NaN
 * x_lower: -0.750257194188649
 * x_upper: -0.7494170934424312
 * minimizer: -0.749936304257568
    18    -1.250000e-01              NaN
 * x_lower: -0.750257194188649
 * x_upper: -0.7497379833735123
 * minimizer: -0.749936304257568
    19    -1.250000e-01              NaN
 * x_lower: -0.750257194188649
 * x_upper: -0.749936304257568
 * minimizer: -0.7500588733045933
    20    -1.250000e-01              NaN
 * x_lower: -0.7501346251416237
 * x_upper: -0.749936304257568
 * minimizer: -0.7500588733045933
    21    -1.250000e-01              NaN
 * x_lower: -0.7500588733045933
 * x_upper: -0.749936304257568
 * minimizer: -0.7500120560945983
    22    -1.250000e-01              NaN
 * x_lower: -0.7500588733045933
 * x_upper: -0.749983121467563
 * minimizer: -0.7500120560945983
    23    -1.250000e-01              NaN
 * x_lower: -0.750029938677558
 * x_upper: -0.749983121467563
 * minimizer: -0.7500120560945983
    24    -1.250000e-01              NaN
 * x_lower: -0.7500120560945983
 * x_upper: -0.749983121467563
 * minimizer: -0.7500010040505226
    25    -1.250000e-01              NaN
 * x_lower: -0.7500120560945983
 * x_upper: -0.7499941735116387
 * minimizer: -0.7500010040505226
    26    -1.250000e-01              NaN
 * x_lower: -0.7500052255557144
 * x_upper: -0.7499941735116387
 * minimizer: -0.7500010040505226
    27    -1.250000e-01              NaN
 * x_lower: -0.7500052255557144
 * x_upper: -0.7499983950168304
 * minimizer: -0.7500010040505226
    28    -1.250000e-01              NaN
 * x_lower: -0.7500026165220222
 * x_upper: -0.7499983950168304
 * minimizer: -0.7500010040505226
    29    -1.250000e-01              NaN
 * x_lower: -0.7500010040505226
 * x_upper: -0.7499983950168304
 * minimizer: -0.75000000748833
    30    -1.250000e-01              NaN
 * x_lower: -0.7500010040505226
 * x_upper: -0.7499993915790231
 * minimizer: -0.75000000748833
    31    -1.250000e-01              NaN
 * x_lower: -0.7500003881412156
 * x_upper: -0.7499993915790231
 * minimizer: -0.75000000748833
    32    -1.250000e-01              NaN
 * x_lower: -0.7500003881412156
 * x_upper: -0.7499997722319087
 * minimizer: -0.75000000748833
    33    -1.250000e-01              NaN
 * x_lower: -0.7500001528847944
 * x_upper: -0.7499997722319087
 * minimizer: -0.75000000748833
    34    -1.250000e-01              NaN
 * x_lower: -0.7500001528847944
 * x_upper: -0.7499999176283731
 * minimizer: -0.75000000748833
    35    -1.250000e-01              NaN
 * x_lower: -0.7500000630248376
 * x_upper: -0.7499999176283731
 * minimizer: -0.75000000748833
    36    -1.250000e-01              NaN
 * x_lower: -0.7500000630248376
 * x_upper: -0.7499999731648807
 * minimizer: -0.75000000748833
    37    -1.250000e-01              NaN
 * x_lower: -0.7500000287013883
 * x_upper: -0.7499999731648807
 * minimizer: -0.75000000748833
    38    -1.250000e-01              NaN
 * x_lower: -0.75000000748833
 * x_upper: -0.7499999731648807
 * minimizer: -0.749999994377939
 * golden_section.jl
 * brent.jl
 * type_stability.jl
 * array.jl
 * constrained.jl
WARNING: could not attach metadata for @simd loop.
 * callbacks.jl
 * precon.jl
Test a basic preconditioning example
N = 10
Optim.GradientDescent{T} WITHOUT preconditioning : g_calls = 890, f_calls = 890
Optim.GradientDescent{T} WITH preconditioning : g_calls = 23, f_calls = 23
Optim.ConjugateGradient{T} WITHOUT preconditioning : g_calls = 49, f_calls = 73
Optim.ConjugateGradient{T} WITH preconditioning : g_calls = 11, f_calls = 16
Optim.LBFGS{T} WITHOUT preconditioning : g_calls = 33, f_calls = 33
Optim.LBFGS{T} WITH preconditioning : g_calls = 25, f_calls = 25
N = 50
Optim.GradientDescent{T} WITHOUT preconditioning : g_calls = 3501, f_calls = 3501
    (gradient descent is not expected to converge)
Optim.GradientDescent{T} WITH preconditioning : g_calls = 12, f_calls = 12
Optim.ConjugateGradient{T} WITHOUT preconditioning : g_calls = 185, f_calls = 276
Optim.ConjugateGradient{T} WITH preconditioning : g_calls = 7, f_calls = 10
Optim.LBFGS{T} WITHOUT preconditioning : g_calls = 386, f_calls = 386
Optim.LBFGS{T} WITH preconditioning : g_calls = 15, f_calls = 15
N = 250
Optim.GradientDescent{T} WITHOUT preconditioning : g_calls = 3502, f_calls = 3502
    (gradient descent is not expected to converge)
Optim.GradientDescent{T} WITH preconditioning : g_calls = 9, f_calls = 9
Optim.ConjugateGradient{T} WITHOUT preconditioning : g_calls = 1371, f_calls = 1993
Optim.ConjugateGradient{T} WITH preconditioning : g_calls = 5, f_calls = 7
Optim.LBFGS{T} WITHOUT preconditioning : g_calls = 1283, f_calls = 1283
Optim.LBFGS{T} WITH preconditioning : g_calls = 12, f_calls = 12
 * initial_convergence.jl
 * extrapolate.jl
--------------------
Rosenbrock Example: 
--------------------
LBFGS Default Options: g_calls = 157, f_calls = 157
CG Default Options: g_calls = 65, f_calls = 86
LBFGS + Backtracking + Extrapolation: g_calls = 23, f_calls = 52
--------------------------------------
p-Laplacian Example (preconditioned): 
--------------------------------------
LBFGS Default Options: g_calls = 15, f_calls = 15
CG Default Options: g_calls = 10, f_calls = 13
LBFGS + Backtracking + Extrapolation: g_calls = 9, f_calls = 18
INFO: Optim tests passed

>>> End of log
