>>> 'Pkg.add("ScikitLearn")' log
INFO: Cloning cache of ScikitLearn from https://github.com/cstjean/ScikitLearn.jl.git
INFO: Installing BinDeps v0.4.5
INFO: Installing Conda v0.4.0
INFO: Installing DataStructures v0.4.6
INFO: Installing Iterators v0.2.0
INFO: Installing MacroTools v0.3.2
INFO: Installing Parameters v0.5.0
INFO: Installing PyCall v1.7.2
INFO: Installing Requires v0.3.0
INFO: Installing SHA v0.3.0
INFO: Installing ScikitLearn v0.2.3
INFO: Installing ScikitLearnBase v0.2.1
INFO: Installing StatsBase v0.11.1
INFO: Installing URIParser v0.1.6
INFO: Building Conda
INFO: Building PyCall
INFO: No system-wide Python was found; got the following error:
could not spawn `'' -c "import distutils.sysconfig; print(distutils.sysconfig.get_config_var('VERSION'))"`: no such file or directory (ENOENT)
using the Python distribution in the Conda package
Fetching package metadata .......
Solving package specifications: ..........

# All requested packages already installed.
# packages in environment at /home/vagrant/.julia/v0.6/Conda/deps/usr:
#
numpy                     1.10.4                   py27_2  
INFO: PyCall is using /home/vagrant/.julia/v0.6/Conda/deps/usr/bin/python (Python 2.7.12) at /home/vagrant/.julia/v0.6/Conda/deps/usr/bin/python, libpython = /home/vagrant/.julia/v0.6/Conda/deps/usr/lib/libpython2.7
INFO: Package database updated

>>> 'Pkg.test("ScikitLearn")' log
Julia Version 0.6.0-dev.1367
Commit 3ae5636 (2016-12-03 01:50 UTC)
Platform Info:
  OS: Linux (x86_64-pc-linux-gnu)
  CPU: Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz
  WORD_SIZE: 64
           Ubuntu 14.04.5 LTS
  uname: Linux 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64
Memory: 2.9392738342285156 GB (743.0703125 MB free)
Uptime: 22011.0 sec
Load Avg:  0.673828125  0.5458984375  0.55517578125
Intel(R) Xeon(R) CPU E3-1241 v3 @ 3.50GHz: 
       speed         user         nice          sys         idle          irq
#1  3504 MHz    1025666 s       2745 s     120141 s     862266 s         53 s
#2  3504 MHz     303858 s       4307 s      58518 s    1790301 s          0 s

  BLAS: libopenblas (USE64BITINT DYNAMIC_ARCH NO_AFFINITY Nehalem)
  LAPACK: libopenblas64_
  LIBM: libopenlibm
  LLVM: libLLVM-3.7.1 (ORCJIT, haswell)
Environment:
  TERM = vt100
  LD_LIBRARY_PATH = :/usr/local/lib/
  PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/home/vagrant/julia/bin/
  JAVA_HOME = /usr/lib/jvm/java-7-oracle
  HOME = /home/vagrant

Package Directory: /home/vagrant/.julia/v0.6
2 required packages:
 - JSON                          0.8.0
 - ScikitLearn                   0.2.3
13 additional packages:
 - BinDeps                       0.4.5
 - Compat                        0.9.5
 - Conda                         0.4.0
 - DataStructures                0.4.6
 - Iterators                     0.2.0
 - MacroTools                    0.3.2
 - Parameters                    0.5.0
 - PyCall                        1.7.2
 - Requires                      0.3.0
 - SHA                           0.3.0
 - ScikitLearnBase               0.2.1
 - StatsBase                     0.11.1
 - URIParser                     0.1.6
INFO: Computing test dependencies for ScikitLearn...
INFO: Cloning cache of GaussianMixtures from https://github.com/davidavdav/GaussianMixtures.jl.git
INFO: Cloning cache of GaussianProcesses from https://github.com/STOR-i/GaussianProcesses.jl.git
INFO: Cloning cache of LowRankModels from https://github.com/madeleineudell/LowRankModels.jl.git
INFO: Installing ArrayViews v0.6.4
INFO: Installing Blosc v0.1.7
INFO: Installing Calculus v0.1.15
INFO: Installing Clustering v0.7.0
INFO: Installing ColorTypes v0.2.12
INFO: Installing Colors v0.6.9
INFO: Installing DataArrays v0.3.10
INFO: Installing DataFrames v0.8.5
INFO: Installing DecisionTree v0.5.0
INFO: Installing DiffBase v0.0.1
INFO: Installing Distances v0.3.2
INFO: Installing Distributions v0.11.0
INFO: Installing DualNumbers v0.2.3
INFO: Installing FileIO v0.2.0
INFO: Installing FixedPointNumbers v0.2.1
INFO: Installing ForwardDiff v0.3.0
INFO: Installing GZip v0.2.20
INFO: Installing GaussianMixtures v0.1.0
INFO: Installing GaussianProcesses v0.2.1
INFO: Installing HDF5 v0.7.0
INFO: Installing Hiccup v0.0.3
INFO: Installing JLD v0.6.6
INFO: Installing Juno v0.2.5
INFO: Installing LaTeXStrings v0.2.0
INFO: Installing Lazy v0.11.4
INFO: Installing LegacyStrings v0.1.1
INFO: Installing LowRankModels v0.1.2
INFO: Installing Media v0.2.4
INFO: Installing NBInclude v1.0.1
INFO: Installing NMF v0.2.5
INFO: Installing NaNMath v0.2.2
INFO: Installing NearestNeighbors v0.1.0
INFO: Installing Optim v0.4.5
INFO: Installing PDMats v0.5.1
INFO: Installing Polynomials v0.1.1
INFO: Installing PositiveFactorizations v0.0.3
INFO: Installing Primes v0.1.1
INFO: Installing PyPlot v2.2.4
INFO: Installing RData v0.0.4
INFO: Installing RDatasets v0.2.0
INFO: Installing Reexport v0.0.3
INFO: Installing Rmath v0.1.4
INFO: Installing Roots v0.2.1
INFO: Installing SortingAlgorithms v0.1.0
INFO: Installing StaticArrays v0.1.0
INFO: Installing StatsFuns v0.3.1
INFO: Building Blosc
INFO: Building Rmath
WARNING: takebuf_string(b) is deprecated, use String(take!(b)) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in takebuf_string(::Base.AbstractIOBuffer{Array{UInt8,1}}) at ./deprecated.jl:50
 in macro expansion; at /home/vagrant/.julia/v0.6/BinDeps/src/dependencies.jl:887 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::SubString{String}) at ./sysimg.jl:14
 in evalfile(::SubString{String}, ::Array{String,1}) at ./loading.jl:572 (repeats 2 times)
 in cd(::##2#4, ::String) at ./file.jl:69
 in (::##1#3)(::IOStream) at ./none:13
 in open(::##1#3, ::String, ::String) at ./iostream.jl:152
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/Rmath/deps/build.jl, in expression starting on line 39
INFO: Building HDF5
INFO: Building Conda
INFO: Building PyCall
INFO: No system-wide Python was found; got the following error:
could not spawn `'' -c "import distutils.sysconfig; print(distutils.sysconfig.get_config_var('VERSION'))"`: no such file or directory (ENOENT)
using the Python distribution in the Conda package
Fetching package metadata .......
Solving package specifications: ..........

# All requested packages already installed.
# packages in environment at /home/vagrant/.julia/v0.6/Conda/deps/usr:
#
numpy                     1.10.4                   py27_2  
INFO: PyCall is using /home/vagrant/.julia/v0.6/Conda/deps/usr/bin/python (Python 2.7.12) at /home/vagrant/.julia/v0.6/Conda/deps/usr/bin/python, libpython = /home/vagrant/.julia/v0.6/Conda/deps/usr/lib/libpython2.7
INFO: Testing ScikitLearn
INFO: Installing sklearn via the Conda scikit-learn package...
Fetching package metadata .......
Solving package specifications: ..........

Package plan for installation in environment /home/vagrant/.julia/v0.6/Conda/deps/usr:

The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    scikit-learn-0.17.1        |      np110py27_2         8.6 MB

The following NEW packages will be INSTALLED:

    scikit-learn: 0.17.1-np110py27_2

Fetching packages ...
scikit-learn-0   0% |                              | ETA:  --:--:--   0.00  B/sscikit-learn-0   1% |                               | ETA:  0:00:00  15.77 MB/sscikit-learn-0   2% |                               | ETA:  0:00:00  17.03 MB/sscikit-learn-0   3% |#                              | ETA:  0:00:00  18.83 MB/sscikit-learn-0   4% |#                              | ETA:  0:00:00  20.40 MB/sscikit-learn-0   5% |#                              | ETA:  0:00:00  21.38 MB/sscikit-learn-0   6% |##                             | ETA:  0:00:00  21.54 MB/sscikit-learn-0   7% |##                             | ETA:  0:00:00  21.74 MB/sscikit-learn-0   8% |##                             | ETA:  0:00:00  22.65 MB/sscikit-learn-0   9% |###                            | ETA:  0:00:00  23.50 MB/sscikit-learn-0  10% |###                            | ETA:  0:00:00  24.13 MB/sscikit-learn-0  11% |###                            | ETA:  0:00:00  24.64 MB/sscikit-learn-0  13% |####                           | ETA:  0:00:00  25.04 MB/sscikit-learn-0  14% |####                           | ETA:  0:00:00  25.45 MB/sscikit-learn-0  15% |####                           | ETA:  0:00:00  25.95 MB/sscikit-learn-0  16% |#####                          | ETA:  0:00:00  26.25 MB/sscikit-learn-0  17% |#####                          | ETA:  0:00:00  26.46 MB/sscikit-learn-0  18% |#####                          | ETA:  0:00:00  26.77 MB/sscikit-learn-0  19% |######                         | ETA:  0:00:00  27.13 MB/sscikit-learn-0  20% |######                         | ETA:  0:00:00  27.40 MB/sscikit-learn-0  21% |######                         | ETA:  0:00:00  27.55 MB/sscikit-learn-0  22% |#######                        | ETA:  0:00:00  27.77 MB/sscikit-learn-0  23% |#######                        | ETA:  0:00:00  27.72 MB/sscikit-learn-0  24% |#######                        | ETA:  0:00:00  27.94 MB/sscikit-learn-0  26% |########                       | ETA:  0:00:00  28.10 MB/sscikit-learn-0  27% |########                       | ETA:  0:00:00  28.25 MB/sscikit-learn-0  28% |########                       | ETA:  0:00:00  28.40 MB/sscikit-learn-0  29% |#########                      | ETA:  0:00:00  28.55 MB/sscikit-learn-0  30% |#########                      | ETA:  0:00:00  28.65 MB/sscikit-learn-0  31% |#########                      | ETA:  0:00:00  28.83 MB/sscikit-learn-0  32% |##########                     | ETA:  0:00:00  28.87 MB/sscikit-learn-0  33% |##########                     | ETA:  0:00:00  28.99 MB/sscikit-learn-0  34% |##########                     | ETA:  0:00:00  29.07 MB/sscikit-learn-0  35% |###########                    | ETA:  0:00:00  29.20 MB/sscikit-learn-0  36% |###########                    | ETA:  0:00:00  29.31 MB/sscikit-learn-0  37% |###########                    | ETA:  0:00:00  29.39 MB/sscikit-learn-0  39% |############                   | ETA:  0:00:00  29.46 MB/sscikit-learn-0  40% |############                   | ETA:  0:00:00  29.59 MB/sscikit-learn-0  41% |############                   | ETA:  0:00:00  29.66 MB/sscikit-learn-0  42% |#############                  | ETA:  0:00:00  29.75 MB/sscikit-learn-0  43% |#############                  | ETA:  0:00:00  29.79 MB/sscikit-learn-0  44% |#############                  | ETA:  0:00:00  29.84 MB/sscikit-learn-0  45% |##############                 | ETA:  0:00:00  29.91 MB/sscikit-learn-0  46% |##############                 | ETA:  0:00:00  29.96 MB/sscikit-learn-0  47% |##############                 | ETA:  0:00:00  29.99 MB/sscikit-learn-0  48% |###############                | ETA:  0:00:00  30.04 MB/sscikit-learn-0  49% |###############                | ETA:  0:00:00  30.08 MB/sscikit-learn-0  50% |###############                | ETA:  0:00:00  30.02 MB/sscikit-learn-0  52% |################               | ETA:  0:00:00  30.07 MB/sscikit-learn-0  53% |################               | ETA:  0:00:00  30.08 MB/sscikit-learn-0  54% |################               | ETA:  0:00:00  29.93 MB/sscikit-learn-0  55% |#################              | ETA:  0:00:00  29.91 MB/sscikit-learn-0  56% |#################              | ETA:  0:00:00  29.95 MB/sscikit-learn-0  57% |#################              | ETA:  0:00:00  29.99 MB/sscikit-learn-0  58% |##################             | ETA:  0:00:00  30.05 MB/sscikit-learn-0  59% |##################             | ETA:  0:00:00  30.06 MB/sscikit-learn-0  60% |##################             | ETA:  0:00:00  30.09 MB/sscikit-learn-0  61% |###################            | ETA:  0:00:00  30.12 MB/sscikit-learn-0  62% |###################            | ETA:  0:00:00  30.15 MB/sscikit-learn-0  64% |###################            | ETA:  0:00:00  30.19 MB/sscikit-learn-0  65% |####################           | ETA:  0:00:00  30.26 MB/sscikit-learn-0  66% |####################           | ETA:  0:00:00  30.31 MB/sscikit-learn-0  67% |####################           | ETA:  0:00:00  30.34 MB/sscikit-learn-0  68% |#####################          | ETA:  0:00:00  30.37 MB/sscikit-learn-0  69% |#####################          | ETA:  0:00:00  30.39 MB/sscikit-learn-0  70% |#####################          | ETA:  0:00:00  30.42 MB/sscikit-learn-0  71% |######################         | ETA:  0:00:00  30.45 MB/sscikit-learn-0  72% |######################         | ETA:  0:00:00  30.46 MB/sscikit-learn-0  73% |######################         | ETA:  0:00:00  30.51 MB/sscikit-learn-0  74% |#######################        | ETA:  0:00:00  30.53 MB/sscikit-learn-0  75% |#######################        | ETA:  0:00:00  30.59 MB/sscikit-learn-0  77% |#######################        | ETA:  0:00:00  30.60 MB/sscikit-learn-0  78% |########################       | ETA:  0:00:00  30.53 MB/sscikit-learn-0  79% |########################       | ETA:  0:00:00  30.56 MB/sscikit-learn-0  80% |########################       | ETA:  0:00:00  30.55 MB/sscikit-learn-0  81% |#########################      | ETA:  0:00:00  30.57 MB/sscikit-learn-0  82% |#########################      | ETA:  0:00:00  30.59 MB/sscikit-learn-0  83% |#########################      | ETA:  0:00:00  30.61 MB/sscikit-learn-0  84% |##########################     | ETA:  0:00:00  30.63 MB/sscikit-learn-0  85% |##########################     | ETA:  0:00:00  30.63 MB/sscikit-learn-0  86% |##########################     | ETA:  0:00:00  30.65 MB/sscikit-learn-0  87% |###########################    | ETA:  0:00:00  30.66 MB/sscikit-learn-0  88% |###########################    | ETA:  0:00:00  30.66 MB/sscikit-learn-0  90% |###########################    | ETA:  0:00:00  30.66 MB/sscikit-learn-0  91% |############################   | ETA:  0:00:00  30.68 MB/sscikit-learn-0  92% |############################   | ETA:  0:00:00  30.67 MB/sscikit-learn-0  93% |############################   | ETA:  0:00:00  30.65 MB/sscikit-learn-0  94% |#############################  | ETA:  0:00:00  30.73 MB/sscikit-learn-0  95% |#############################  | ETA:  0:00:00  30.89 MB/sscikit-learn-0  96% |#############################  | ETA:  0:00:00  30.98 MB/sscikit-learn-0  97% |############################## | ETA:  0:00:00  31.13 MB/sscikit-learn-0  98% |############################## | ETA:  0:00:00  31.26 MB/sscikit-learn-0  99% |############################## | ETA:  0:00:00  31.41 MB/sscikit-learn-0 100% |###############################| Time: 0:00:00  31.40 MB/s
Extracting packages ...
[                    ]|                                                  |   0%[scikit-learn        ]|                                                  |   0%[      COMPLETE      ]|##################################################| 100%
Linking packages ...
[      COMPLETE      ]|                                                  |   0%[scikit-learn        ]|                                                  |   0%[scikit-learn        ]|                                                  |   0%[      COMPLETE      ]|##################################################| 100%
WARNING: Method definition require(Symbol) in module Base at loading.jl:376 overwritten in module Main at /home/vagrant/.julia/v0.6/Requires/src/require.jl:12.
WARNING: Method definition #sample(Array{Any, 1}, ScikitLearnBase.#sample, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition predict_log_proba(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition transform(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition get_feature_names(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #get_params(Array{Any, 1}, ScikitLearnBase.#get_params, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition fit_predict!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #fit_transform!(Array{Any, 1}, ScikitLearnBase.#fit_transform!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition get_params(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition inverse_transform(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #fit!(Array{Any, 1}, ScikitLearnBase.#fit!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #get_feature_names(Array{Any, 1}, ScikitLearnBase.#get_feature_names, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #predict_proba(Array{Any, 1}, ScikitLearnBase.#predict_proba, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition score(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition is_classifier(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:49 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:49.
WARNING: Method definition is_pairwise(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:52 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:52.
WARNING: Method definition is_pairwise(Any) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:51 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:51.
WARNING: Method definition set_params!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #transform(Array{Any, 1}, ScikitLearnBase.#transform, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition clone(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:48 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:48.
WARNING: Method definition #inverse_transform(Array{Any, 1}, ScikitLearnBase.#inverse_transform, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #score_samples(Array{Any, 1}, ScikitLearnBase.#score_samples, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #decision_function(Array{Any, 1}, ScikitLearnBase.#decision_function, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #partial_fit!(Array{Any, 1}, ScikitLearnBase.#partial_fit!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition predict(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition fit!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition fit_transform!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition score_samples(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #predict(Array{Any, 1}, ScikitLearnBase.#predict, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition decision_function(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #score(Array{Any, 1}, ScikitLearnBase.#score, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition sample(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition #fit_predict!(Array{Any, 1}, ScikitLearnBase.#fit_predict!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition #set_params!(Array{Any, 1}, ScikitLearnBase.#set_params!, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
WARNING: Method definition get_classes(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:55 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:55.
WARNING: Method definition partial_fit!(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition predict_proba(PyCall.PyObject, Any...) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:95 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:95.
WARNING: Method definition get_components(PyCall.PyObject) in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/Skcore.jl:56 overwritten in module Skcore at /home/vagrant/.julia/v0.6/ScikitLearn/src/Skcore.jl:56.
WARNING: Method definition #predict_log_proba(Array{Any, 1}, ScikitLearnBase.#predict_log_proba, PyCall.PyObject, Any...) in module Skcore overwritten in module Skcore.
/home/vagrant/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/sklearn/svm/base.py:547: ChangedBehaviorWarning: The decision_function_shape default value will change from 'ovo' to 'ovr' in 0.18. This will change the shape of the decision function returned by SVC.
  "SVC.", ChangedBehaviorWarning)
WARNING: filter(flt,itr) is deprecated, use Iterators.filter(flt,itr) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in filter(::Function, ::Tuple{Array{Float64,2},Array{Int64,1}}) at ./deprecated.jl:50
 in check_consistent_length(::Array{Float64,2}, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/sk_utils.jl:90
 in #cross_val_score#79(::Void, ::Void, ::Int64, ::Int64, ::Void, ::Function, ::MockClassifier, ::Array{Float64,2}, ::Array{Int64,1}) at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/cross_validation.jl:272
 in test_cross_val_score() at /home/vagrant/.julia/v0.6/ScikitLearn/test/test_crossvalidation.jl:120
 in all_test_crossvalidation() at /home/vagrant/.julia/v0.6/ScikitLearn/test/test_crossvalidation.jl:223
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/runtests.jl, in expression starting on line 19
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: The least populated class in y has only 2 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
WARNING: ignoring conflicting import of CrossValidation.cross_val_score into Main
WARNING: filter(flt,itr) is deprecated, use Iterators.filter(flt,itr) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in filter(::Function, ::Tuple{Array{Float64,2},Array{String,1}}) at ./deprecated.jl:50
 in check_consistent_length(::Array{Float64,2}, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/sk_utils.jl:90
 in #cross_val_score#79(::Void, ::Int64, ::Int64, ::Int64, ::Void, ::Function, ::PyCall.PyObject, ::Array{Float64,2}, ::Array{String,1}) at /home/vagrant/.julia/v0.6/ScikitLearn/test/../src/cross_validation.jl:272
 in (::Skcore.#kw##cross_val_score)(::Array{Any,1}, ::Skcore.#cross_val_score, ::PyCall.PyObject, ::Array{Float64,2}, ::Array{String,1}) at ./<missing>:0
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/test_quickstart.jl, in expression starting on line 24
WARNING: round{T <: Real}(x::AbstractArray{T}) is deprecated, use round.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in round(::Array{Float64,2}) at ./deprecated.jl:50
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/test_dataframes.jl, in expression starting on line 38
WARNING: filter(flt,itr) is deprecated, use Iterators.filter(flt,itr) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in filter(::Function, ::Tuple{DataFrames.DataFrame,DataArrays.DataArray{Int64,1}}) at ./deprecated.jl:50
 in check_consistent_length(::DataFrames.DataFrame, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/sk_utils.jl:90
 in #cross_val_score#79(::Void, ::Void, ::Int64, ::Int64, ::Void, ::Function, ::ScikitLearn.Skcore.Pipeline, ::DataFrames.DataFrame, ::DataArrays.DataArray{Int64,1}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/cross_validation.jl:272
 in cross_val_score(::ScikitLearn.Skcore.Pipeline, ::DataFrames.DataFrame, ::DataArrays.DataArray{Int64,1}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/cross_validation.jl:270
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/test_dataframes.jl, in expression starting on line 47
Testing ../examples/Classifier_Comparison.ipynb
WARNING: No working GUI backend found for matplotlib.
Testing ../examples/Classifier_Comparison_Julia.ipynb
WARNING: replacing module Testing
Testing ../examples/Clustering_Comparison.ipynb
WARNING: replacing module Testing
/home/vagrant/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/sklearn/manifold/spectral_embedding_.py:217: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.
  warnings.warn("Graph is not fully connected, spectral embedding"
/home/vagrant/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:207: UserWarning: the number of connected components of the connectivity matrix is 2 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
/home/vagrant/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:443: UserWarning: the number of connected components of the connectivity matrix is 2 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
/home/vagrant/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:207: UserWarning: the number of connected components of the connectivity matrix is 3 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
/home/vagrant/.julia/v0.6/Conda/deps/usr/lib/python2.7/site-packages/sklearn/cluster/hierarchical.py:443: UserWarning: the number of connected components of the connectivity matrix is 3 > 1. Completing it to avoid stopping the tree early.
  connectivity, n_components = _fix_connectivity(X, connectivity)
Testing ../examples/Cross_Validated_Predictions.ipynb
WARNING: replacing module Testing
WARNING: filter(flt,itr) is deprecated, use Iterators.filter(flt,itr) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in filter(::Function, ::Tuple{Array{Float64,2},Array{Float64,1}}) at ./deprecated.jl:50
 in check_consistent_length(::Array{Float64,2}, ::Vararg{Any,N}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/sk_utils.jl:90
 in #cross_val_predict#84(::Int64, ::Int64, ::Int64, ::Void, ::Function, ::PyCall.PyObject, ::Array{Float64,2}, ::Array{Float64,1}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/cross_validation.jl:382
 in (::ScikitLearn.Skcore.#kw##cross_val_predict)(::Array{Any,1}, ::ScikitLearn.Skcore.#cross_val_predict, ::PyCall.PyObject, ::Array{Float64,2}, ::Array{Float64,1}) at ./<missing>:0
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Cross_Validated_Predictions.ipynb:In[2], in expression starting on line 7
Testing ../examples/Decision_Tree_Regression.ipynb
WARNING: replacing module Testing
WARNING: sin{T <: Number}(x::AbstractArray{T}) is deprecated, use sin.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in sin(::Array{Float64,1}) at ./deprecated.jl:50
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Decision_Tree_Regression.ipynb:In[1], in expression starting on line 11
Testing ../examples/Decision_Tree_Regression_Julia.ipynb
WARNING: replacing module Testing
Testing ../examples/Density_Estimation.ipynb
WARNING: replacing module Testing
Testing ../examples/Density_Estimation_Julia.ipynb
WARNING: replacing module Testing
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 600 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       7.850874e+03
      1       5.165666e+03      -2.685208e+03 |        0
      2       5.165666e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 5165.6661634557)
INFO: K-means with 600 data points using 2 iterations
100.0 data points per parameter
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::Array{Float64,2}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:45
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: isnan{T <: Number}(x::AbstractArray{T}) is deprecated, use isnan.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isnan(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: isinf{T <: Number}(x::AbstractArray{T}) is deprecated, use isinf.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in isinf(::UpperTriangular{Float64,Array{Float64,2}}) at ./deprecated.jl:50
 in sanitycheck!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:56
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:131
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:366
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::GaussianMixtures.#kw##stats)(::Array{Any,1}, ::GaussianMixtures.#stats, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:236
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in #15 at ./<missing>:0 [inlined]
 in next at ./generator.jl:26 [inlined]
 in collect_to!(::Array{Float64,1}, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64, ::Int64) at ./array.jl:399
 in collect_to_with_first!(::Array{Float64,1}, ::Float64, ::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}, ::Int64) at ./array.jl:386
 in collect(::Base.Generator{UnitRange{Int64},GaussianMixtures.##15#16{GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}}}) at ./array.jl:367
 in llpg(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:324
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:354
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::GaussianMixtures.#kw##stats)(::Array{Any,1}, ::GaussianMixtures.#stats, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:236
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:355
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::GaussianMixtures.#kw##stats)(::Array{Any,1}, ::GaussianMixtures.#stats, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:236
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexp(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/compat.jl:21
 in mapslices(::GaussianMixtures.#logsumexp, ::Array{Float64,2}, ::Array{Int64,1}) at ./abstractarray.jl:1741
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:356
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::GaussianMixtures.#kw##stats)(::Array{Any,1}, ::GaussianMixtures.#stats, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:236
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,2}) at ./deprecated.jl:50
 in gmmposterior(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:358
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:85
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::GaussianMixtures.#kw##stats)(::Array{Any,1}, ::GaussianMixtures.#stats, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:236
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in stats(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Int64) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:86
 in #stats#35(::Int64, ::Bool, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/stats.jl:141
 in (::GaussianMixtures.#kw##stats)(::Array{Any,1}, ::GaussianMixtures.#stats, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #em!#14(::Int64, ::Float64, ::Int64, ::Int64, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:236
 in (::GaussianMixtures.#kw##em!)(::Array{Any,1}, ::GaussianMixtures.#em!, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at ./<missing>:0
 in #GMMk#9(::Symbol, ::Int64, ::Int64, ::Int64, ::Function, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:132
 in (::GaussianMixtures.#kw##GMMk)(::Array{Any,1}, ::GaussianMixtures.#GMMk, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in #GMM#7(::Symbol, ::Symbol, ::Int64, ::Int64, ::Int64, ::Int64, ::Type{T}, ::Int64, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:34
 in (::Core.#kw#Type)(::Array{Any,1}, ::Type{GaussianMixtures.GMM}, ::Int64, ::Array{Float64,2}) at ./<missing>:0
 in fit! at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:43 [inlined]
 in fit!(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:39
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[1], in expression starting on line 27
INFO: iteration 1, average log likelihood -2.026541
INFO: iteration 2, average log likelihood -2.026538
INFO: iteration 3, average log likelihood -2.026538
INFO: iteration 4, average log likelihood -2.026538
INFO: iteration 5, average log likelihood -2.026538
INFO: iteration 6, average log likelihood -2.026538
INFO: iteration 7, average log likelihood -2.026538
INFO: iteration 8, average log likelihood -2.026538
INFO: iteration 9, average log likelihood -2.026538
INFO: iteration 10, average log likelihood -2.026538
INFO: EM with 600 data points 10 iterations avll -2.026538
54.5 data points per parameter
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in density(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:59
 in score_samples(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/scikitlearn.jl:70
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[2], in expression starting on line 6
WARNING: log{T <: Number}(x::AbstractArray{T}) is deprecated, use log.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in log(::Array{Float64,1}) at ./deprecated.jl:50
 in logsumexpw at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:157 [inlined]
 in avll(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/GaussianMixtures/src/train.jl:336
 in _score(::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Void, ::ScikitLearnBase.#score) at /home/vagrant/.julia/v0.6/ScikitLearn/src/cross_validation.jl:649
 in #_fit_and_score#92(::Bool, ::Bool, ::String, ::Function, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Void, ::Function, ::Array{Int64,1}, ::Array{Int64,1}, ::Int64, ::Dict{Symbol,Any}, ::Dict{Symbol,Any}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/cross_validation.jl:574
 in (::ScikitLearn.Skcore.#kw##_fit_and_score)(::Array{Any,1}, ::ScikitLearn.Skcore.#_fit_and_score, ::GaussianMixtures.GMM{Float64,Array{UpperTriangular{Float64,Array{Float64,2}},1}}, ::Array{Float64,2}, ::Void, ::Function, ::Array{Int64,1}, ::Array{Int64,1}, ::Int64, ::Dict{Symbol,Any}, ::Dict{Symbol,Any}) at ./<missing>:0
 in (::ScikitLearn.Skcore.##106#109{Dict{Any,Any},ScikitLearn.Skcore.GridSearchCV,Array{Float64,2},Void})(::Tuple{Array{Int64,1},Array{Int64,1}}) at ./<missing>:0
 in collect(::Base.Generator{Array{Tuple{Array{Int64,1},Array{Int64,1}},1},ScikitLearn.Skcore.##106#109{Dict{Any,Any},ScikitLearn.Skcore.GridSearchCV,Array{Float64,2},Void}}) at ./array.jl:366
 in copy!(::Array{Any,1}, ::Base.Generator{ScikitLearn.Skcore.ParameterGrid,ScikitLearn.Skcore.##105#108{ScikitLearn.Skcore.GridSearchCV,Array{Float64,2},Void}}) at ./abstractarray.jl:476
 in _fit!(::ScikitLearn.Skcore.GridSearchCV, ::Array{Float64,2}, ::Void, ::ScikitLearn.Skcore.ParameterGrid) at /home/vagrant/.julia/v0.6/ScikitLearn/src/grid_search.jl:277
 in fit!(::ScikitLearn.Skcore.GridSearchCV, ::Array{Float64,2}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/grid_search.jl:526
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Density_Estimation_Julia.ipynb:In[6], in expression starting on line 1
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.821221e+04
      1       4.717918e+03      -1.349429e+04 |        2
      2       4.713803e+03      -4.115211e+00 |        0
      3       4.713803e+03       0.000000e+00 |        0
K-means converged with 3 iterations (objv = 4713.802531784694)
INFO: K-means with 400 data points using 3 iterations
66.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.060321
INFO: iteration 2, average log likelihood -2.060312
INFO: iteration 3, average log likelihood -2.060312
INFO: iteration 4, average log likelihood -2.060312
INFO: iteration 5, average log likelihood -2.060312
INFO: iteration 6, average log likelihood -2.060312
INFO: iteration 7, average log likelihood -2.060312
INFO: iteration 8, average log likelihood -2.060312
INFO: iteration 9, average log likelihood -2.060312
INFO: iteration 10, average log likelihood -2.060312
INFO: EM with 400 data points 10 iterations avll -2.060312
36.4 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       4.482854e+03
      1       3.436488e+03      -1.046366e+03 |        0
      2       3.436488e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 3436.4881168493393)
INFO: K-means with 400 data points using 2 iterations
66.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.039125
INFO: iteration 2, average log likelihood -2.039119
INFO: iteration 3, average log likelihood -2.039119
INFO: iteration 4, average log likelihood -2.039119
INFO: iteration 5, average log likelihood -2.039119
INFO: iteration 6, average log likelihood -2.039119
INFO: iteration 7, average log likelihood -2.039119
INFO: iteration 8, average log likelihood -2.039119
INFO: iteration 9, average log likelihood -2.039119
INFO: iteration 10, average log likelihood -2.039119
INFO: EM with 400 data points 10 iterations avll -2.039119
36.4 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       4.513648e+03
      1       2.162762e+03      -2.350886e+03 |        0
      2       2.162762e+03       0.000000e+00 |        0
K-means converged with 2 iterations (objv = 2162.76192501717)
INFO: K-means with 400 data points using 2 iterations
66.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.845672
INFO: iteration 2, average log likelihood -1.845664
INFO: iteration 3, average log likelihood -1.845664
INFO: iteration 4, average log likelihood -1.845664
INFO: iteration 5, average log likelihood -1.845664
INFO: iteration 6, average log likelihood -1.845664
INFO: iteration 7, average log likelihood -1.845664
INFO: iteration 8, average log likelihood -1.845664
INFO: iteration 9, average log likelihood -1.845664
INFO: iteration 10, average log likelihood -1.845664
INFO: EM with 400 data points 10 iterations avll -1.845664
36.4 data points per parameter
INFO: Initializing GMM, 3 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       5.399472e+03
      1       2.419982e+03      -2.979490e+03 |        2
      2       2.174379e+03      -2.456035e+02 |        2
      3       2.052976e+03      -1.214027e+02 |        2
      4       2.009106e+03      -4.386963e+01 |        2
      5       1.964242e+03      -4.486403e+01 |        2
      6       1.929268e+03      -3.497478e+01 |        2
      7       1.903249e+03      -2.601857e+01 |        2
      8       1.890915e+03      -1.233384e+01 |        2
      9       1.881258e+03      -9.657317e+00 |        2
     10       1.879398e+03      -1.859491e+00 |        2
     11       1.879267e+03      -1.317339e-01 |        0
     12       1.879267e+03       0.000000e+00 |        0
K-means converged with 12 iterations (objv = 1879.2667044382113)
INFO: K-means with 400 data points using 12 iterations
44.4 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 3 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.073097
INFO: iteration 2, average log likelihood -2.063712
INFO: iteration 3, average log likelihood -2.061034
INFO: iteration 4, average log likelihood -2.059899
INFO: iteration 5, average log likelihood -2.059317
INFO: iteration 6, average log likelihood -2.058981
INFO: iteration 7, average log likelihood -2.058771
INFO: iteration 8, average log likelihood -2.058631
INFO: iteration 9, average log likelihood -2.058532
INFO: iteration 10, average log likelihood -2.058460
INFO: EM with 400 data points 10 iterations avll -2.058460
23.5 data points per parameter
INFO: Initializing GMM, 3 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       4.499241e+03
      1       2.035751e+03      -2.463490e+03 |        2
      2       1.844393e+03      -1.913580e+02 |        2
      3       1.729037e+03      -1.153562e+02 |        2
      4       1.664768e+03      -6.426910e+01 |        2
      5       1.623391e+03      -4.137703e+01 |        2
      6       1.595023e+03      -2.836753e+01 |        2
      7       1.579721e+03      -1.530260e+01 |        2
      8       1.567036e+03      -1.268527e+01 |        2
      9       1.566232e+03      -8.038358e-01 |        2
     10       1.566041e+03      -1.905610e-01 |        0
     11       1.566041e+03       0.000000e+00 |        0
K-means converged with 11 iterations (objv = 1566.0411445832556)
INFO: K-means with 400 data points using 11 iterations
44.4 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 3 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.048481
INFO: iteration 2, average log likelihood -2.042271
INFO: iteration 3, average log likelihood -2.040435
INFO: iteration 4, average log likelihood -2.039638
INFO: iteration 5, average log likelihood -2.039221
INFO: iteration 6, average log likelihood -2.038977
INFO: iteration 7, average log likelihood -2.038823
INFO: iteration 8, average log likelihood -2.038720
INFO: iteration 9, average log likelihood -2.038648
INFO: iteration 10, average log likelihood -2.038596
INFO: EM with 400 data points 10 iterations avll -2.038596
23.5 data points per parameter
INFO: Initializing GMM, 3 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.779983e+03
      1       1.396880e+03      -3.831030e+02 |        2
      2       1.296204e+03      -1.006756e+02 |        2
      3       1.251941e+03      -4.426325e+01 |        2
      4       1.240744e+03      -1.119661e+01 |        0
      5       1.240744e+03       0.000000e+00 |        0
K-means converged with 5 iterations (objv = 1240.7441157403418)
INFO: K-means with 400 data points using 5 iterations
44.4 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 3 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.846966
INFO: iteration 2, average log likelihood -1.844312
INFO: iteration 3, average log likelihood -1.843706
INFO: iteration 4, average log likelihood -1.843485
INFO: iteration 5, average log likelihood -1.843385
INFO: iteration 6, average log likelihood -1.843333
INFO: iteration 7, average log likelihood -1.843302
INFO: iteration 8, average log likelihood -1.843284
INFO: iteration 9, average log likelihood -1.843271
INFO: iteration 10, average log likelihood -1.843262
INFO: EM with 400 data points 10 iterations avll -1.843262
23.5 data points per parameter
INFO: Initializing GMM, 4 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.027605e+03
      1       1.219705e+03      -8.079005e+02 |        3
      2       1.110925e+03      -1.087801e+02 |        3
      3       1.085668e+03      -2.525635e+01 |        2
      4       1.081649e+03      -4.019731e+00 |        2
      5       1.078715e+03      -2.933943e+00 |        2
      6       1.077851e+03      -8.634101e-01 |        0
      7       1.077851e+03       0.000000e+00 |        0
K-means converged with 7 iterations (objv = 1077.8512850137074)
INFO: K-means with 400 data points using 7 iterations
33.3 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 4 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.066971
INFO: iteration 2, average log likelihood -2.059522
INFO: iteration 3, average log likelihood -2.057279
INFO: iteration 4, average log likelihood -2.056240
INFO: iteration 5, average log likelihood -2.055647
INFO: iteration 6, average log likelihood -2.055261
INFO: iteration 7, average log likelihood -2.054985
INFO: iteration 8, average log likelihood -2.054774
INFO: iteration 9, average log likelihood -2.054604
INFO: iteration 10, average log likelihood -2.054461
INFO: EM with 400 data points 10 iterations avll -2.054461
17.4 data points per parameter
INFO: Initializing GMM, 4 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.254447e+03
      1       1.599607e+03      -6.548396e+02 |        4
      2       1.526053e+03      -7.355386e+01 |        4
      3       1.482710e+03      -4.334311e+01 |        4
      4       1.453337e+03      -2.937267e+01 |        4
      5       1.437610e+03      -1.572739e+01 |        4
      6       1.424843e+03      -1.276744e+01 |        4
      7       1.423950e+03      -8.926239e-01 |        4
      8       1.423742e+03      -2.076569e-01 |        0
      9       1.423742e+03       0.000000e+00 |        0
K-means converged with 9 iterations (objv = 1423.7423072752404)
INFO: K-means with 400 data points using 9 iterations
33.3 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 4 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.057174
INFO: iteration 2, average log likelihood -2.043711
INFO: iteration 3, average log likelihood -2.039123
INFO: iteration 4, average log likelihood -2.036080
INFO: iteration 5, average log likelihood -2.033800
INFO: iteration 6, average log likelihood -2.032461
INFO: iteration 7, average log likelihood -2.031634
INFO: iteration 8, average log likelihood -2.031089
INFO: iteration 9, average log likelihood -2.030719
INFO: iteration 10, average log likelihood -2.030466
INFO: EM with 400 data points 10 iterations avll -2.030466
17.4 data points per parameter
INFO: Initializing GMM, 4 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.221258e+03
      1       9.960676e+02      -2.251900e+02 |        3
      2       9.319000e+02      -6.416760e+01 |        3
      3       9.224220e+02      -9.478055e+00 |        2
      4       9.203370e+02      -2.085000e+00 |        3
      5       9.193201e+02      -1.016937e+00 |        0
      6       9.193201e+02       0.000000e+00 |        0
K-means converged with 6 iterations (objv = 919.3200542988874)
INFO: K-means with 400 data points using 6 iterations
33.3 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 4 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.841782
INFO: iteration 2, average log likelihood -1.840068
INFO: iteration 3, average log likelihood -1.839757
INFO: iteration 4, average log likelihood -1.839661
INFO: iteration 5, average log likelihood -1.839621
INFO: iteration 6, average log likelihood -1.839599
INFO: iteration 7, average log likelihood -1.839584
INFO: iteration 8, average log likelihood -1.839572
INFO: iteration 9, average log likelihood -1.839560
INFO: iteration 10, average log likelihood -1.839549
INFO: EM with 400 data points 10 iterations avll -1.839549
17.4 data points per parameter
INFO: Initializing GMM, 5 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.064129e+03
      1       1.473136e+03      -5.909922e+02 |        5
      2       1.381896e+03      -9.123998e+01 |        5
      3       1.336459e+03      -4.543716e+01 |        5
      4       1.290968e+03      -4.549115e+01 |        5
      5       1.249547e+03      -4.142154e+01 |        5
      6       1.193908e+03      -5.563888e+01 |        5
      7       1.147284e+03      -4.662385e+01 |        5
      8       1.121778e+03      -2.550640e+01 |        5
      9       1.100424e+03      -2.135330e+01 |        5
     10       1.082504e+03      -1.792050e+01 |        5
     11       1.064938e+03      -1.756617e+01 |        5
     12       1.039741e+03      -2.519679e+01 |        5
     13       1.020528e+03      -1.921285e+01 |        5
     14       1.013611e+03      -6.916698e+00 |        5
     15       1.009732e+03      -3.878957e+00 |        4
     16       1.008709e+03      -1.023144e+00 |        2
     17       1.008679e+03      -3.027137e-02 |        0
     18       1.008679e+03       0.000000e+00 |        0
K-means converged with 18 iterations (objv = 1008.6788360253636)
INFO: K-means with 400 data points using 18 iterations
26.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 5 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.062952
INFO: iteration 2, average log likelihood -2.054566
INFO: iteration 3, average log likelihood -2.052150
INFO: iteration 4, average log likelihood -2.051053
INFO: iteration 5, average log likelihood -2.050428
INFO: iteration 6, average log likelihood -2.050018
INFO: iteration 7, average log likelihood -2.049721
INFO: iteration 8, average log likelihood -2.049490
INFO: iteration 9, average log likelihood -2.049301
INFO: iteration 10, average log likelihood -2.049139
INFO: EM with 400 data points 10 iterations avll -2.049139
13.8 data points per parameter
INFO: Initializing GMM, 5 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.330008e+03
      1       1.070374e+03      -2.596344e+02 |        5
      2       1.033233e+03      -3.714067e+01 |        5
      3       1.011645e+03      -2.158854e+01 |        3
      4       1.001525e+03      -1.012008e+01 |        3
      5       9.865255e+02      -1.499931e+01 |        3
      6       9.777706e+02      -8.754873e+00 |        3
      7       9.693675e+02      -8.403075e+00 |        3
      8       9.622345e+02      -7.133011e+00 |        3
      9       9.572220e+02      -5.012565e+00 |        3
     10       9.525161e+02      -4.705911e+00 |        3
     11       9.428834e+02      -9.632658e+00 |        3
     12       9.279863e+02      -1.489714e+01 |        3
     13       9.196181e+02      -8.368133e+00 |        3
     14       9.162512e+02      -3.366898e+00 |        3
     15       9.124784e+02      -3.772787e+00 |        2
     16       9.123312e+02      -1.472307e-01 |        0
     17       9.123312e+02       0.000000e+00 |        0
K-means converged with 17 iterations (objv = 912.3312165848175)
INFO: K-means with 400 data points using 17 iterations
26.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 5 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.052925
INFO: iteration 2, average log likelihood -2.038996
INFO: iteration 3, average log likelihood -2.033803
INFO: iteration 4, average log likelihood -2.031361
INFO: iteration 5, average log likelihood -2.029937
INFO: iteration 6, average log likelihood -2.028999
INFO: iteration 7, average log likelihood -2.028338
INFO: iteration 8, average log likelihood -2.027855
INFO: iteration 9, average log likelihood -2.027490
INFO: iteration 10, average log likelihood -2.027209
INFO: EM with 400 data points 10 iterations avll -2.027209
13.8 data points per parameter
INFO: Initializing GMM, 5 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.361257e+03
      1       9.063186e+02      -4.549381e+02 |        5
      2       9.002513e+02      -6.067355e+00 |        3
      3       8.976158e+02      -2.635467e+00 |        3
      4       8.954073e+02      -2.208544e+00 |        3
      5       8.931700e+02      -2.237268e+00 |        3
      6       8.914833e+02      -1.686760e+00 |        2
      7       8.909720e+02      -5.112851e-01 |        2
      8       8.906234e+02      -3.485786e-01 |        2
      9       8.906046e+02      -1.874453e-02 |        2
     10       8.905777e+02      -2.694399e-02 |        0
     11       8.905777e+02       0.000000e+00 |        0
K-means converged with 11 iterations (objv = 890.5777033708408)
INFO: K-means with 400 data points using 11 iterations
26.7 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 5 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.866597
INFO: iteration 2, average log likelihood -1.847483
INFO: iteration 3, average log likelihood -1.842273
INFO: iteration 4, average log likelihood -1.840068
INFO: iteration 5, average log likelihood -1.838909
INFO: iteration 6, average log likelihood -1.838214
INFO: iteration 7, average log likelihood -1.837756
INFO: iteration 8, average log likelihood -1.837433
INFO: iteration 9, average log likelihood -1.837194
INFO: iteration 10, average log likelihood -1.837009
INFO: EM with 400 data points 10 iterations avll -1.837009
13.8 data points per parameter
INFO: Initializing GMM, 6 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.330284e+03
      1       6.986024e+02      -6.316814e+02 |        5
      2       6.401278e+02      -5.847456e+01 |        5
      3       6.295455e+02      -1.058237e+01 |        5
      4       6.240665e+02      -5.479028e+00 |        4
      5       6.216327e+02      -2.433801e+00 |        3
      6       6.202920e+02      -1.340627e+00 |        3
      7       6.201182e+02      -1.738221e-01 |        0
      8       6.201182e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 620.1182014824072)
INFO: K-means with 400 data points using 8 iterations
22.2 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 6 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.054284
INFO: iteration 2, average log likelihood -2.046531
INFO: iteration 3, average log likelihood -2.044151
INFO: iteration 4, average log likelihood -2.043007
INFO: iteration 5, average log likelihood -2.042314
INFO: iteration 6, average log likelihood -2.041830
INFO: iteration 7, average log likelihood -2.041459
INFO: iteration 8, average log likelihood -2.041155
INFO: iteration 9, average log likelihood -2.040897
INFO: iteration 10, average log likelihood -2.040672
INFO: EM with 400 data points 10 iterations avll -2.040672
11.4 data points per parameter
INFO: Initializing GMM, 6 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.600975e+03
      1       9.340277e+02      -6.669477e+02 |        6
      2       8.699684e+02      -6.405929e+01 |        6
      3       8.385330e+02      -3.143543e+01 |        6
      4       8.229759e+02      -1.555709e+01 |        5
      5       8.197077e+02      -3.268214e+00 |        4
      6       8.182413e+02      -1.466322e+00 |        3
      7       8.165274e+02      -1.713939e+00 |        2
      8       8.158777e+02      -6.497450e-01 |        3
      9       8.157692e+02      -1.085035e-01 |        0
     10       8.157692e+02       0.000000e+00 |        0
K-means converged with 10 iterations (objv = 815.7691602213533)
INFO: K-means with 400 data points using 10 iterations
22.2 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 6 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.055365
INFO: iteration 2, average log likelihood -2.038774
INFO: iteration 3, average log likelihood -2.033620
INFO: iteration 4, average log likelihood -2.031226
INFO: iteration 5, average log likelihood -2.029873
INFO: iteration 6, average log likelihood -2.029008
INFO: iteration 7, average log likelihood -2.028405
INFO: iteration 8, average log likelihood -2.027957
INFO: iteration 9, average log likelihood -2.027608
INFO: iteration 10, average log likelihood -2.027324
INFO: EM with 400 data points 10 iterations avll -2.027324
11.4 data points per parameter
INFO: Initializing GMM, 6 Gaussians diag covariance 2 dimensions using 400 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       1.451421e+03
      1       8.306607e+02      -6.207602e+02 |        6
      2       7.118493e+02      -1.188114e+02 |        6
      3       6.162297e+02      -9.561958e+01 |        6
      4       5.842937e+02      -3.193598e+01 |        5
      5       5.733342e+02      -1.095956e+01 |        5
      6       5.722676e+02      -1.066614e+00 |        5
      7       5.719144e+02      -3.531945e-01 |        0
      8       5.719144e+02       0.000000e+00 |        0
K-means converged with 8 iterations (objv = 571.9143585738548)
INFO: K-means with 400 data points using 8 iterations
22.2 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 6 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -1.856488
INFO: iteration 2, average log likelihood -1.840784
INFO: iteration 3, average log likelihood -1.836360
INFO: iteration 4, average log likelihood -1.834406
INFO: iteration 5, average log likelihood -1.833328
INFO: iteration 6, average log likelihood -1.832648
INFO: iteration 7, average log likelihood -1.832178
INFO: iteration 8, average log likelihood -1.831835
INFO: iteration 9, average log likelihood -1.831574
INFO: iteration 10, average log likelihood -1.831368
INFO: EM with 400 data points 10 iterations avll -1.831368
11.4 data points per parameter
INFO: Initializing GMM, 2 Gaussians diag covariance 2 dimensions using 600 data points
  Iters               objv        objv-change | affected 
-------------------------------------------------------------
      0       2.411528e+05
      1       4.081095e+04      -2.003419e+05 |        2
      2       5.181026e+03      -3.562992e+04 |        2
      3       5.165666e+03      -1.536031e+01 |        0
      4       5.165666e+03       0.000000e+00 |        0
K-means converged with 4 iterations (objv = 5165.6661634557)
INFO: K-means with 600 data points using 4 iterations
100.0 data points per parameter
INFO: Running 10 iterations EM on full cov GMM with 2 Gaussians in 2 dimensions
INFO: iteration 1, average log likelihood -2.026541
INFO: iteration 2, average log likelihood -2.026538
INFO: iteration 3, average log likelihood -2.026538
INFO: iteration 4, average log likelihood -2.026538
INFO: iteration 5, average log likelihood -2.026538
INFO: iteration 6, average log likelihood -2.026538
INFO: iteration 7, average log likelihood -2.026538
INFO: iteration 8, average log likelihood -2.026538
INFO: iteration 9, average log likelihood -2.026538
INFO: iteration 10, average log likelihood -2.026538
INFO: EM with 600 data points 10 iterations avll -2.026538
54.5 data points per parameter
Testing ../examples/Feature_Stacker.ipynb
WARNING: replacing module Testing
Fitting 3 folds for each of 18 candidates, totalling 54 fits
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1, score=0.96078  -  0.1s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1, score=0.90196  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1, score=0.96078  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=0.1, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=1.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2, score=0.90196  -  0.0s
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=1, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1, score=0.96078  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1, score=0.90196  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2, score=0.94118  -  0.0s
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=0.1, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=1.0, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=2, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1, score=0.98039  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1, score=0.94118  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=1, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1, score=0.92157  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=1, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2, score=0.98039  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2, score=0.94118  -  0.0s
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=0.1, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2, score=0.96078  -  0.0s
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=1.0, features__univ_select__k=2, score=0.97917  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2, score=0.92157  -  0.0s
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2
[CV] features__pca__n_components=3, svm__C=10.0, features__univ_select__k=2, score=1.00000  -  0.0s
ScikitLearn.Skcore.Pipeline(Tuple{Any,Any}[("features",ScikitLearn.Skcore.FeatureUnion(Tuple{Any,Any}[("pca",PyObject PCA(copy=True, n_components=2, whiten=False)),("univ_select",PyObject SelectKBest(k=2, score_func=<function f_classif at 0x7fdfeae8c938>))],1,nothing)),("svm",PyObject SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False))],Any[ScikitLearn.Skcore.FeatureUnion(Tuple{Any,Any}[("pca",PyObject PCA(copy=True, n_components=2, whiten=False)),("univ_select",PyObject SelectKBest(k=2, score_func=<function f_classif at 0x7fdfeae8c938>))],1,nothing),PyObject SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',
  max_iter=-1, probability=False, random_state=None, shrinking=True,
  tol=0.001, verbose=False)])Testing ../examples/Gaussian_Processes_Julia.ipynb
WARNING: replacing module Testing
WARNING: symbol is deprecated, use Symbol instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in symbol(::String, ::Vararg{String,N}) at ./deprecated.jl:30
 in @glue(::ANY) at /home/vagrant/.julia/v0.6/GaussianProcesses/src/GaussianProcesses.jl:22
 in include_from_node1(::String) at ./loading.jl:532
 in eval(::Module, ::Any) at ./boot.jl:236
 in require(::Symbol) at ./loading.jl:446
 in require(::Symbol) at /home/vagrant/.julia/v0.6/Requires/src/require.jl:12
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/GaussianProcesses/src/GaussianProcesses.jl, in expression starting on line 29

WARNING: deprecated syntax "[a=>b for (a,b) in c]".
Use "Dict(a=>b for (a,b) in c)" instead.
WARNING: exp{T <: Number}(x::AbstractArray{T}) is deprecated, use exp.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in exp(::Array{Float64,1}) at ./deprecated.jl:50
 in set_params!(::GaussianProcesses.SEIso, ::Array{Float64,1}) at /home/vagrant/.julia/v0.6/GaussianProcesses/src/kernels/se_iso.jl:18
 in #set_params!#23(::Array{Any,1}, ::Function, ::GaussianProcesses.SEIso) at /home/vagrant/.julia/v0.6/GaussianProcesses/src/glue/ScikitLearn.jl:95
 in (::ScikitLearnBase.#kw##set_params!)(::Array{Any,1}, ::ScikitLearnBase.#set_params!, ::GaussianProcesses.SEIso) at ./<missing>:0
 in #set_params!#22(::Array{Any,1}, ::Function, ::GaussianProcesses.GP) at /home/vagrant/.julia/v0.6/GaussianProcesses/src/glue/ScikitLearn.jl:79
 in (::ScikitLearnBase.#kw##set_params!)(::Array{Any,1}, ::ScikitLearnBase.#set_params!, ::GaussianProcesses.GP) at ./<missing>:0
 in #_fit_and_score#92(::Bool, ::Bool, ::String, ::Function, ::GaussianProcesses.GP, ::Array{Float64,2}, ::Array{Float64,1}, ::Function, ::Array{Int64,1}, ::Array{Int64,1}, ::Int64, ::Dict{Symbol,Any}, ::Dict{Symbol,Any}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/cross_validation.jl:559
 in (::ScikitLearn.Skcore.#kw##_fit_and_score)(::Array{Any,1}, ::ScikitLearn.Skcore.#_fit_and_score, ::GaussianProcesses.GP, ::Array{Float64,2}, ::Array{Float64,1}, ::Function, ::Array{Int64,1}, ::Array{Int64,1}, ::Int64, ::Dict{Symbol,Any}, ::Dict{Symbol,Any}) at ./<missing>:0
 in (::ScikitLearn.Skcore.##106#109{Dict{Any,Any},ScikitLearn.Skcore.GridSearchCV,Array{Float64,2},Array{Float64,1}})(::Tuple{Array{Int64,1},Array{Int64,1}}) at ./<missing>:0
 in collect(::Base.Generator{Array{Tuple{Array{Int64,1},Array{Int64,1}},1},ScikitLearn.Skcore.##106#109{Dict{Any,Any},ScikitLearn.Skcore.GridSearchCV,Array{Float64,2},Array{Float64,1}}}) at ./array.jl:366
 in copy!(::Array{Any,1}, ::Base.Generator{ScikitLearn.Skcore.ParameterGrid,ScikitLearn.Skcore.##105#108{ScikitLearn.Skcore.GridSearchCV,Array{Float64,2},Array{Float64,1}}}) at ./abstractarray.jl:476
 in _fit!(::ScikitLearn.Skcore.GridSearchCV, ::Array{Float64,2}, ::Array{Float64,1}, ::ScikitLearn.Skcore.ParameterGrid) at /home/vagrant/.julia/v0.6/ScikitLearn/src/grid_search.jl:277
 in fit!(::ScikitLearn.Skcore.GridSearchCV, ::Array{Float64,2}, ::Array{Float64,1}) at /home/vagrant/.julia/v0.6/ScikitLearn/src/grid_search.jl:526
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Gaussian_Processes_Julia.ipynb:In[3], in expression starting on line 1
WARNING: symbol is deprecated, use Symbol instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in symbol(::Symbol, ::Vararg{Symbol,N}) at ./deprecated.jl:30
 in (::GaussianProcesses.##19#21{Symbol})(::Pair{Symbol,Float64}) at ./<missing>:0
 in Dict{Symbol,Float64}(::Base.Generator{Dict{Symbol,Float64},GaussianProcesses.##19#21{Symbol}}) at ./dict.jl:108
 in dict_with_eltype(::Base.Generator{Dict{Symbol,Float64},GaussianProcesses.##19#21{Symbol}}, ::Type{T}) at ./dict.jl:165
 in Dict{K,V}(::Base.Generator{Dict{Symbol,Float64},GaussianProcesses.##19#21{Symbol}}) at ./dict.jl:144
 in add_prefix at /home/vagrant/.julia/v0.6/GaussianProcesses/src/glue/ScikitLearn.jl:49 [inlined]
 in get_params(::GaussianProcesses.GP) at /home/vagrant/.julia/v0.6/GaussianProcesses/src/glue/ScikitLearn.jl:51
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Gaussian_Processes_Julia.ipynb:In[3], in expression starting on line 229
(get_params(best_gp))[:logNoise] = 2.5999999999999996
(get_params(best_gp))[:k_l] = 4.8
WARNING: cos{T <: Number}(x::AbstractArray{T}) is deprecated, use cos.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in cos(::Array{Float64,1}) at ./deprecated.jl:50
 in true_fun(::Array{Float64,1}) at /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Gaussian_Processes_Julia.ipynb:In[7]:6
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Gaussian_Processes_Julia.ipynb:In[7], in expression starting on line 8
WARNING: cos{T <: Number}(x::AbstractArray{T}) is deprecated, use cos.(x) instead.
 in depwarn(::String, ::Symbol) at ./deprecated.jl:64
 in cos(::LinSpace{Float64}) at ./deprecated.jl:50
 in true_fun(::LinSpace{Float64}) at /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Gaussian_Processes_Julia.ipynb:In[7]:6
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Gaussian_Processes_Julia.ipynb:In[7], in expression starting on line 14
Testing ../examples/Outlier_Detection.ipynb
WARNING: replacing module Testing
Testing ../examples/Pipeline_PCA_Logistic.ipynb
WARNING: replacing module Testing
Testing ../examples/Plot_Kmeans_Digits.ipynb
WARNING: replacing module Testing
n_digits: 10, 	 n_samples 1797, 	 n_features 64
____________________________________________________________________________
init    time  inertia    homo   compl  v-meas     ARI AMI  silhouette
k-means++   0.32s    69493   0.611   0.658   0.634   0.479   0.607    0.139
   random   0.39s    70312   0.604   0.677   0.638   0.478   0.600    0.133
PCA-based   0.04s    71821   0.673   0.715   0.693   0.567   0.670    0.095
____________________________________________________________________________Testing ../examples/Plot_Kmeans_Digits_Julia.ipynb
WARNING: replacing module Testing
WARNING: Base.ASCIIString is deprecated, use String instead.
  likely near /home/vagrant/.julia/v0.6/LowRankModels/src/fit_dataframe.jl:135
ERROR: LoadError: LoadError: LoadError: UndefVarError: PCA not defined
 in include_string(::String, ::String) at ./loading.jl:478
 in my_include_string(::String, ::String, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:31
 in #nbinclude#1(::Bool, ::Function, ::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:97
 in nbinclude(::String) at /home/vagrant/.julia/v0.6/NBInclude/src/NBInclude.jl:62
 in eval(::Module, ::Any) at ./boot.jl:236
 in macro expansion; at /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl:6 [inlined]
 in anonymous at ./<missing>:?
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in include_from_node1(::String) at ./loading.jl:532
 in include(::String) at ./sysimg.jl:14
 in process_options(::Base.JLOptions) at ./client.jl:271
 in _start() at ./client.jl:335
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/../examples/Plot_Kmeans_Digits_Julia.ipynb:In[1], in expression starting on line 5
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/run_examples.jl, in expression starting on line 2
while loading /home/vagrant/.julia/v0.6/ScikitLearn/test/runtests.jl, in expression starting on line 25
=============================[ ERROR: ScikitLearn ]=============================

failed process: Process(`/home/vagrant/julia/bin/julia -Cx86-64 -J/home/vagrant/julia/lib/julia/sys.so --compile=yes --depwarn=yes --check-bounds=yes --code-coverage=none --color=no --compilecache=yes /home/vagrant/.julia/v0.6/ScikitLearn/test/runtests.jl`, ProcessExited(1)) [1]

================================================================================
INFO: Removing ArrayViews v0.6.4
INFO: Removing Blosc v0.1.7
INFO: Removing Calculus v0.1.15
INFO: Removing Clustering v0.7.0
INFO: Removing ColorTypes v0.2.12
INFO: Removing Colors v0.6.9
INFO: Removing DataArrays v0.3.10
INFO: Removing DataFrames v0.8.5
INFO: Removing DecisionTree v0.5.0
INFO: Removing DiffBase v0.0.1
INFO: Removing Distances v0.3.2
INFO: Removing Distributions v0.11.0
INFO: Removing DualNumbers v0.2.3
INFO: Removing FileIO v0.2.0
INFO: Removing FixedPointNumbers v0.2.1
INFO: Removing ForwardDiff v0.3.0
INFO: Removing GZip v0.2.20
INFO: Removing GaussianMixtures v0.1.0
INFO: Removing GaussianProcesses v0.2.1
INFO: Removing HDF5 v0.7.0
INFO: Removing Hiccup v0.0.3
INFO: Removing JLD v0.6.6
INFO: Removing Juno v0.2.5
INFO: Removing LaTeXStrings v0.2.0
INFO: Removing Lazy v0.11.4
INFO: Removing LegacyStrings v0.1.1
INFO: Removing LowRankModels v0.1.2
INFO: Removing Media v0.2.4
INFO: Removing NBInclude v1.0.1
INFO: Removing NMF v0.2.5
INFO: Removing NaNMath v0.2.2
INFO: Removing NearestNeighbors v0.1.0
INFO: Removing Optim v0.4.5
INFO: Removing PDMats v0.5.1
INFO: Removing Polynomials v0.1.1
INFO: Removing PositiveFactorizations v0.0.3
INFO: Removing Primes v0.1.1
INFO: Removing PyPlot v2.2.4
INFO: Removing RData v0.0.4
INFO: Removing RDatasets v0.2.0
INFO: Removing Reexport v0.0.3
INFO: Removing Rmath v0.1.4
INFO: Removing Roots v0.2.1
INFO: Removing SortingAlgorithms v0.1.0
INFO: Removing StaticArrays v0.1.0
INFO: Removing StatsFuns v0.3.1
ERROR: ScikitLearn had test errors
 in #test#61(::Bool, ::Function, ::Array{AbstractString,1}) at ./pkg/entry.jl:749
 in (::Base.Pkg.Entry.#kw##test)(::Array{Any,1}, ::Base.Pkg.Entry.#test, ::Array{AbstractString,1}) at ./<missing>:0
 in (::Base.Pkg.Dir.##2#3{Array{Any,1},Base.Pkg.Entry.#test,Tuple{Array{AbstractString,1}}})() at ./pkg/dir.jl:31
 in cd(::Base.Pkg.Dir.##2#3{Array{Any,1},Base.Pkg.Entry.#test,Tuple{Array{AbstractString,1}}}, ::String) at ./file.jl:69
 in #cd#1(::Array{Any,1}, ::Function, ::Function, ::Array{AbstractString,1}, ::Vararg{Array{AbstractString,1},N}) at ./pkg/dir.jl:31
 in (::Base.Pkg.Dir.#kw##cd)(::Array{Any,1}, ::Base.Pkg.Dir.#cd, ::Function, ::Array{AbstractString,1}, ::Vararg{Array{AbstractString,1},N}) at ./<missing>:0
 in #test#3(::Bool, ::Function, ::String, ::Vararg{String,N}) at ./pkg/pkg.jl:258
 in test(::String, ::Vararg{String,N}) at ./pkg/pkg.jl:258
 in eval(::Module, ::Any) at ./boot.jl:236
 in process_options(::Base.JLOptions) at ./client.jl:248
 in _start() at ./client.jl:335

>>> End of log
